Anmelden 
  Registrierung 
  Deutsch  English 
  Espa√±ol 
  Portugu√™s 
  Fran√ßais 

     Dom 
  Najlepsze kategorie | CAREER & MONEY 
  PERSONAL GROWTH 
  POLITICS & CURRENT AFFAIRS 
  SCIENCE & TECH 
  HEALTH & FITNESS 
  LIFESTYLE 
  ENTERTAINMENT 
  BIOGRAPHIES & HISTORY 
  FICTION 
  Najlepsze historie 
  Najlepsze historie 
  Dodaj historiƒô 
  Moje historie 

 Home 
  Integer Programming and Combinatorial Optimization. 24th International Conference, IPCO 2023 Madison, WI, USA, June 21‚Äì23, 2023 Proceedings 9783031327254, 9783031327261 

 Integer Programming and Combinatorial Optimization. 24th International Conference, IPCO 2023 Madison, WI, USA, June 21‚Äì23, 2023 Proceedings 9783031327254, 9783031327261   
   
  600    82    13MB    
  English   Pages 494   Year 2023    
  Report DMCA / Copyright    
  DOWNLOAD FILE   
   
 Polecaj historie   

 Integer Programming and Combinatorial Optimization: 24th International Conference, IPCO 2023, Madison, WI, USA, June 21‚Äì23, 2023, Proceedings 9783031327261, 9783031327254, 3031327268  
 This book constitutes the refereed proceedings of the 24th International Conference on Integer Programming and Combinato  
  168    46    44MB    Read more   

 Integer Programming and Combinatorial Optimization: 24th International Conference, IPCO 2023, Madison, WI, USA, June 21‚Äì23, 2023, Proceedings (Lecture Notes in Computer Science, 13904) [1st ed. 2023] 9783031327254, 303132725X  
 This book constitutes the refereed proceedings of the 24th International Conference on Integer Programming and Combinato  
  417    107    13MB    Read more   

 Integer Programming and Combinatorial Optimization: 21st International Conference, IPCO 2020, London, UK, June 8‚Äì10, 2020, Proceedings (Lecture Notes in Computer Science, 12125) [1st ed. 2020] 3030457702, 9783030457709  
 This book constitutes the refereed proceedings of the 21st International Conference on Integer Programming and Combinato  
  378    64    6MB    Read more   

 Combinatorial Optimization and Applications: 16th International Conference, COCOA 2023, Hawaii, HI, USA, December 15‚Äì17, 2023, Proceedings, Part I (Lecture Notes in Computer Science, 14461) 3031496108, 9783031496103  
 The two-volume set LNCS 14461 and LNCS 14462 constitutes the refereed proceedings of the 17th International Conference o  
  142    71    14MB    Read more   

 Combinatorial Optimization and Applications: 16th International Conference, COCOA 2023, Hawaii, HI, USA, December 15‚Äì17, 2023, Proceedings, Part II (Lecture Notes in Computer Science, 14462) 3031496132, 9783031496134  
 The two-volume set LNCS 14461 and LNCS 14462 constitutes the refereed proceedings of the 17th International Conference o  
  141    24    17MB    Read more   

 Integer Programming and Combinatorial Optimization: 20th International Conference, IPCO 2019, Ann Arbor, MI, USA, May 22-24, 2019, Proceedings [1st ed.] 978-3-030-17952-6;978-3-030-17953-3  
 This book constitutes the refereed proceedings of the 20th International Conference on Integer Programming and Combinato  
  495    67    11MB    Read more   

 Integer Programming and Combinatorial Optimization: 23rd International Conference, IPCO 2022, Eindhoven, The Netherlands, June 27‚Äì29, 2022, Proceedings (Lecture Notes in Computer Science, 13265) [1st ed. 2022] 3031069005, 9783031069000  
 This book constitutes the refereed proceedings of the 23rd International Conference on Integer Programming and Combinato  
  336    98    6MB    Read more   

 Artificial Intelligence in Education: 24th International Conference, AIED 2023, Tokyo, Japan, July 3‚Äì7, 2023, Proceedings 3031362713, 9783031362712  
 This book constitutes the refereed proceedings of the 24th International Conference on Artificial Intelligence in Educat  
  2,625    104    52MB    Read more   

 Algorithms and Complexity: 13th International Conference, CIAC 2023, Larnaca, Cyprus, June 13‚Äì16, 2023, Proceedings 3031304470, 9783031304477  
 This book constitutes the refereed proceedings of the 13th International Conference on Algorithms and Complexity, CIAC 2  
  486    96    10MB    Read more   

 Algorithms and Complexity: 13th International Conference, CIAC 2023, Larnaca, Cyprus, June 13‚Äì16, 2023, Proceedings 9783031304484, 9783031304477, 3031304489  
 This book constitutes the refereed proceedings of the 13th International Conference on Algorithms and Complexity, CIAC 2  
  207    96    38MB    Read more   

 Author / Uploaded 
  Alberto Del Pia 
  Volker Kaibel 

 Table of contents :  
  Preface  
  Organization  
  Contents  
  Information Complexity of Mixed-Integer Convex Optimization  
  1 First-order Information Complexity  
  1.1 Our Results  
  1.2 Formal Definitions and Statement of Results  
  1.3 Discussion and Future Avenues  
  2 Proof Sketches  
  2.1 Proof Sketch of Theorem 1  
  2.2 Proof of Theorem 3  
  2.3 Proof Sketch of Theorem 5  
  2.4 Proof Sketch of Theorems 2 and 4  
  References  
  Efficient Separation of RLT Cuts for Implicit and Explicit Bilinear Products  
  1 Introduction  
  2 RLT for Bilinear Products  
  3 Detection of Implicit Products  
  4 Separation Algorithm  
  4.1 Row Marking  
  4.2 Projection Filtering  
  5 Computational Results  
  5.1 Setup  
  5.2 Impact of RLT Cuts  
  5.3 Separation  
  5.4 Experiments with Gurobi  
  5.5 Summary  
  References  
  A Nearly Optimal Randomized Algorithm for Explorable Heap Selection  
  1 Introduction  
  2 The Explorable Heap Selection Problem  
  3 A New Algorithm  
  3.1 The Algorithm  
  3.2 Proof of Correctness  
  3.3 Running Time Analysis  
  3.4 Space Complexity Analysis  
  4 Lower Bound  
  References  
  Sparse Approximation over the Cube  
  1 Introduction and Literature Review  
  2 Preliminaries  
  3 The l1-Relaxation for Random Targets b  
  4 Proximity Between Optimal Solutions of ([P0]P0) and ([P1]P1)  
  5 A Deterministic Algorithm  
  6 Extension  
  References  
  Recycling Inequalities for Robust Combinatorial Optimization with Budget Uncertainty  
  1 Introduction  
  2 Recycling Valid Inequalities  
  3 Facet-Defining Recycled Inequalities  
  4 Computational Study  
  4.1 Robust Independent Set  
  4.2 Robust Bipartite Matching  
  5 Conclusion  
  References  
  Inapproximability of Shortest Paths on Perfect Matching Polytopes  
  1 Introduction  
  1.1 Our Result  
  1.2 Pivot Rules for Circuit-Augmentation Algorithms  
  1.3 Related Works  
  2 Proof of Theorem 1  
  2.1 Preliminaries  
  2.2 Reduction  
  2.3 Proof of Lemma 3  
  References  
  Monoidal Strengthening and Unique Lifting in MIQCPs  
  1 Introduction  
  2 Monoidal Strengthening in the Homogeneous Case  
  3 Monoidal Strengthening in the Non-homogeneous Case  
  3.1 A Technical Consideration for Sg  
  3.2 Monoid Construction  
  4 Solving the Monoidal Strengthening Problem  
  5 Unique Lifting  
  6 Computational Results  
  References  
  From Approximate to Exact Integer Programming  
  1 Introduction  
  1.1 Contributions of This Paper  
  1.2 Related Work  
  2 Preliminaries  
  3 The Cut-Or-Average Algorithm  
  3.1 Bounding the Number of Iterations  
  3.2 Correctness and Efficiency of Subroutines  
  3.3 Conclusion on the Cut-Or-Average Algorithm  
  4 An Asymmetric Approximate Carath√©odory Theorem  
  5 IPs with Polynomial Variable Range  
  References  
  Optimizing Low Dimensional Functions over the Integers  
  1 Introduction  
  1.1 Applications  
  1.2 Overview of Techniques  
  2 Non-negative Variables  
  3 Bounded Variables  
  4 Overview of Hunkenschr√∂der Et Al. ch9hunkenschroder2022optimizing and Related Improvements  
  5 Conclusion and Open Questions  
  References  
  Configuration Balancing for Stochastic Requests  
  1 Introduction  
  1.1 Our Results  
  1.2 Technical Overview  
  1.3 Related Work  
  2 Configuration Balancing with Stochastic Requests  
  2.1 Structural Theorem  
  2.2 Offline Setting  
  2.3 Online Setting  
  3 Load Balancing on Related Machines  
  References  
  An Update-and-Stabilize Framework for the Minimum-Norm-Point Problem  
  1 Introduction  
  2 Preliminaries  
  2.1 Optimal Solutions and Proximity  
  2.2 The Centroid Mapping  
  3 The Update-and-Stabilize Framework  
  4 Analysis  
  5 Computational Experiments  
  References  
  Stabilization of Capacitated Matching Games  
  1 Introduction  
  2 Preliminaries and Notation  
  3 M-vertex-stabilizer  
  4 Vertex-Stabilizer  
  5 Capacitated Cooperative Matching Games  
  References  
  Designing Optimization Problems with Diverse Solutions  
  1 Introduction  
  2 Statement of Main Results  
  2.1 The Cyclic Polytope  
  2.2 Results and Techniques  
  3 Preliminaries  
  4 Upper Bound (Proof of Theorem 1)  
  5 General Lower Bound (Proof of Theorem 2)  
  5.1 Construction Based on Moment Curve  
  5.2 Dual Certificate for Loadouts  
  5.3 Counting the Number of k-Loadouts  
  6 Conclusion  
  References  
  ReLU Neural Networks of Polynomial Size for Exact Maximum Flow Computation  
  1 Introduction  
  1.1 Our Main Results  
  1.2 Discussion of the Results  
  1.3 Further Related Work  
  2 Algorithms and Proof Overview  
  References  
  On the Correlation Gap of Matroids  
  1 Introduction  
  1.1 Our Techniques  
  2 Preliminaries  
  3 Locating the Correlation Gap  
  4 Lower Bounding the Correlation Gap  
  4.1 Lower Bounding G(x*)  
  4.2 Lower Bounding H(x*)  
  4.3 Putting Everything Together  
  References  
  A 4/3-Approximation Algorithm for Half-Integral Cycle Cut Instances of the TSP  
  1 Introduction  
  2 Preliminaries  
  3 Proof of Theorem 1  
  4 Conclusion and Open Questions  
  References  
  The Polyhedral Geometry of Truthful Auctions  
  1 Introduction  
  2 Preliminaries  
  3 Characterization of One-Player Mechanisms  
  4 Sensitivity of Mechanisms  
  5 Conclusion  
  References  
  Competitive Kill-and-Restart and Preemptive Strategies for Non-clairvoyant Scheduling  
  1 Introduction  
  2 Preliminaries  
  3 Lower Bound  
  4 The b-scaling Strategy  
  4.1 The Deterministic b-scaling Strategy  
  4.2 The Randomized b-scaling Strategy  
  5 Weighted Shortest Elapsed Time First  
  6 Upper Bounds for More General Settings  
  7 Conclusion  
  References  
  A Deterministic Better-than-3/2 Approximation Algorithm for Metric TSP  
  1 Introduction  
  1.1 High Level Proof Overview  
  2 Preliminaries  
  2.1 Notation  
  2.2 Randomized Algorithm of ch19KKO21a  
  2.3 Polyhedral Background  
  3 Computing Probabilities  
  3.1 Notation  
  3.2 Matrix Tree Theorem  
  3.3 Computing Parities in a Simple Case  
  4 A Deterministic Algorithm in the Degree Cut Case  
  5 General Case  
  References  
  Monoidal Strengthening of Simple V-Polyhedral Disjunctive Cuts  
  1 Introduction  
  2 Notation and Background  
  3 Correspondence Between PRLP and CGLP Solutions  
  3.1 Simple VPCs  
  3.2 Relaxations Without Primal Degeneracy  
  3.3 Relaxations with Primal Degeneracy  
  4 Computational Experiments  
  5 Choosing a Relaxation Amenable to Strengthening  
  6 Conclusion  
  References  
  Optimal General Factor Problem and Jump System Intersection  
  1 Introduction  
  1.1 General Factor Problem  
  1.2 Jump System Intersection  
  1.3 Our Contribution: Jump System with SBO Property  
  1.4 Organization  
  2 Preliminaries  
  3 Algorithm and Correctness  
  4 Outline of the Proof of Lemma 1  
  4.1 Minimal Counterexample  
  4.2 Part of Case Analysis: |U|=3  
  5 Extension to Valuated Problem  
  6 Weighted Optimal General Factor Problem  
  7 Concluding Remarks  
  References  
  Decomposition of Probability Marginals for Security Games in Abstract Networks  
  1 Introduction  
  1.1 Motivation  
  1.2 Abstract Networks  
  1.3 Previous Results  
  1.4 Our Results  
  1.5 Notation  
  2 Feasible Decompositions in Abstract Networks  
  3 Computing Feasible Decompositions  
  4 Computing Shortest Paths in Abstract Networks  
  5 Dahan et al.'s Network Security Game  
  6 The Conservation Law for Partially Ordered Sets  
  7 Other Set Systems  
  References  
  Set Selection Under Explorable Stochastic Uncertainty via Covering Techniques  
  1 Introduction  
  2 Algorithmic Framework  
  2.1 Offline Problems and Hardness of Approximation  
  2.2 Algorithmic Framework  
  3 MinSet with Deterministic Right-Hand Sides  
  4 MinSet Under Uncertainty  
  5 Disjoint MinSet  
  References  
  Towards a Characterization of Maximal Quadratic-Free Sets  
  1 Introduction  
  1.1 Contributions  
  2 Examples of Maximal Homogeneous Quadratic-Free Sets  
  3 A Proof of Theorem 5  
  4 A Proof of Theorem 1  
  5 A Proof of Theorem 2  
  6 Preliminary Results on Non-expansive Functions  
  7 A Proof of Theorem 3  
  8 A Proof of Theorem 4  
  References  
  Compressing Branch-and-Bound Trees  
  1 Introduction  
  2 The Tree Compression Problem (TCP)  
  3 Complexity Results and Lower Bounds  
  4 Compression Algorithms  
  4.1 An Exact Method  
  4.2 A Heuristic Method  
  5 Computational Experiments  
  5.1 Methodology  
  5.2 Full Strong Branching Results  
  5.3 Reliability Branching with Plunging  
  6 Future Work  
  References  
  Exploiting the Polyhedral Geometry of Stochastic Linear Bilevel Programming  
  1 Introduction  
  1.1 Problem Formulation and Contributions  
  2 Preliminaries  
  3 Vertex-Supported Beliefs and Bayesian Formulation  
  3.1 Sample Average Formulation  
  4 Geometrical Structure of Vertex-Supported Beliefs  
  5 Algorithms  
  5.1 Enumeration Algorithm  
  5.2 Monte-Carlo Approximation Scheme  
  6 Numerical Experiments  
  References  
  Towards an Optimal Contention Resolution Scheme for Matchings  
  1 Introduction  
  1.1 Our Results  
  1.2 Our Techniques  
  2 An Optimal CRS When "026B30D x"026B30D 0  
  2.1 The Karp-Sipser Algorithm  
  2.2 Random Trees  
  2.3 The Karp-Sipser Algorithm on Trees  
  2.4 Putting It Together  
  3 Improved CRSs for Bipartite Matchings  
  3.1 A 0.480-Balanced Scheme for Bipartite Matchings  
  3.2 A 0.509-Balanced Scheme for Bipartite Matchings  
  References  
  Advances on Strictly -Modular IPs  
  1 Introduction  
  1.1 Group-Constrained Problems and Proof Strategy for Theorem 1  
  1.2 Further Related Work  
  1.3 Structure of the Paper  
  2 GCTUF with Transposed Network Constraint Matrices  
  3 Overview of Our Techniques Leading to Theorem 3  
  3.1 Reducing to a Simpler Problem When the Target Elements Form a Union of Cosets  
  3.2 Decomposing the Problem  
  3.3 Handling Patterns  
  References  
  Cut-Sufficient Directed 2-Commodity Multiflow Topologies  
  1 Introduction  
  1.1 Other Related Work  
  2 Preliminaries  
  2.1 Cut-Deceptive Weights and Minors of Multiflow Topologies  
  3 Relevant Minors and Entry-Exit Connected Edge Sets  
  3.1 Relevant Minors  
  3.2 Contractions of Entry-Exit Connected Sets  
  4 Characterizations of Cut-Sufficiency  
  4.1 Opposingly Ordered Paths  
  4.2 Characterization for Roundtrip and Two-Path Demands  
  5 NP-Hardness of Recognizing Cut-Sufficiency  
  6 Towards a Complete 2-Commodity Characterization  
  References  
  Constant-Competitiveness for Random Assignment Matroid Secretary Without Knowing the Matroid  
  1 Introduction  
  2 Random-Assignment MSP and Densities  
  3 Outline of Our Approach  
  3.1 Rank-Density Curves  
  3.2 Proof Plan for Theorem1 via Rank-Density Curves  
  4 Learning Rank-Density Curves from a Sample  
  5 The Main Algorithm and Its Analysis  
  5.1 Proof (Sketch) of Theorem5  
  References  
  A Fast Combinatorial Algorithm for the Bilevel Knapsack Problem with Interdiction Constraints  
  1 Introduction  
  2 A Combinatorial Algorithm for BKP  
  2.1 The Bound Test  
  2.2 Computing Initial Bounds  
  3 Lower Bound  
  4 Computational Results  
  4.1 Implementation  
  4.2 Instances  
  4.3 Results  
  5 Conclusion  
  References  
  Multiplicative Auction Algorithm for Approximate Maximum Weight Bipartite Matching  
  1 Introduction  
  1.1 Dynamic Matching Algorithms  
  1.2 Linear Program for MWM  
  1.3 Multiplicative Weight Updates for Packing LPs  
  1.4 Auction Algorithms  
  1.5 Our Contribution  
  2 The Static Algorithm  
  2.1 Improving the Running Time  
  3 Dynamic Algorithm  
  A Combinatorial proof of Lemma 2  
  References  
  A Linear Time Algorithm for Linearizing Quadratic and Higher-Order Shortest Path Problems  
  1 Introduction  
  2 Notations and Preliminaries  
  3 A Characterization of Linearizable Instances of the GSPP on Acyclic Digraphs  
  4 A Linear Time Algorithm for the LinSPPd  
  4.1 The All Paths Equal Cost Problem of Order-d (APECPd)  
  4.2 The Linear Time LinSPPd algorithm  
  5 The Subspace of Linearizable Instances  
  References  
  Author Index   
 Citation preview   
  LNCS 13904  
   
  Alberto Del Pia ¬∑ Volker Kaibel (Eds.)  
   
  Integer Programming and Combinatorial Optimization 24th International Conference, IPCO 2023 Madison, WI, USA, June 21‚Äì23, 2023 Proceedings  
   
  Lecture Notes in Computer Science Founding Editors Gerhard Goos Juris Hartmanis  
   
  Editorial Board Members Elisa Bertino, Purdue University, West Lafayette, IN, USA Wen Gao, Peking University, Beijing, China Bernhard Steffen , TU Dortmund University, Dortmund, Germany Moti Yung , Columbia University, New York, NY, USA  
   
  13904  
   
  The series Lecture Notes in Computer Science (LNCS), including its subseries Lecture Notes in Artificial Intelligence (LNAI) and Lecture Notes in Bioinformatics (LNBI), has established itself as a medium for the publication of new developments in computer science and information technology research, teaching, and education. LNCS enjoys close cooperation with the computer science R & D community, the series counts many renowned academics among its volume editors and paper authors, and collaborates with prestigious societies. Its mission is to serve this international community by providing an invaluable service, mainly focused on the publication of conference and workshop proceedings and postproceedings. LNCS commenced publication in 1973.  
   
  Alberto Del Pia ¬∑ Volker Kaibel Editors  
   
  Integer Programming and Combinatorial Optimization 24th International Conference, IPCO 2023 Madison, WI, USA, June 21‚Äì23, 2023 Proceedings  
   
  Editors Alberto Del Pia University of Wisconsin-Madison Madison, WI, USA  
   
  Volker Kaibel Otto-von-Guericke-Universit√§t Magdeburg, Sachsen-Anhalt, Germany  
   
  ISSN 0302-9743 ISSN 1611-3349 (electronic) Lecture Notes in Computer Science ISBN 978-3-031-32725-4 ISBN 978-3-031-32726-1 (eBook) https://doi.org/10.1007/978-3-031-32726-1 ¬© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. This Springer imprint is published by the registered company Springer Nature Switzerland AG The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland  
   
  Preface  
   
  This volume contains extended abstracts of the papers presented at IPCO 2023, the 24th Conference on Integer Programming and Combinatorial Optimization, held on June 21‚Äì23, 2023 in Madison, Wisconsin, USA. IPCO is under the auspices of the Mathematical Optimization Society. Since its first edition, held at the University of Waterloo, Canada in May 1990, it has become a most important forum for presenting the latest results on the theory and practice of the various aspects of discrete optimization. For this year‚Äôs 24th edition the conference had a Program Committee consisting of 16 members. In response to the Call for Papers, we received 119 submissions and accepted 33 papers, with an acceptance ratio of 28%. In a single-blind review process, each submission was reviewed by at least three Program Committee members, and 246 additional reviews were provided by external experts. Because of the limited number of time slots for presentations, many excellent submissions could not be accepted. The page limit for contributions to these proceedings was set to 15. We expect the full versions of the extended abstracts appearing in this Lecture Notes in Computer Science volume to be submitted for publication in refereed journals. A special issue of Mathematical Programming, Series B containing such versions is in process. As has become a good tradition, IPCO 2023 had a Best Paper Award, which was given to Daniel Dadush, Friedrich Eisenbrand, and Thomas Rothvoss for their paper ‚ÄúFrom approximate to exact integer programming.‚Äù This year, IPCO was preceded by a Summer School held during June 19‚Äì20, 2023, with lectures by Amitabh Basu (Johns Hopkins University, USA), Fatma Kilin√ß-Karzan (Carnegie Mellon University, USA), and Domenico Salvagnin (University of Padova, Italy). We thank them warmly for their contributions. We would also like to thank ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì  
   
  the authors who submitted their research to IPCO; the members of the Program Committee; the expert additional reviewers; the members of the Local Organizing Committee; the Mathematical Optimization Society, in particular the members of its IPCO Steering Committee, Karen Aardal, Oktay G√ºnl√ºk, Jochen K√∂nemann, and Giacomo Zambelli; ‚Äì EasyChair for making the paper management simple and effective; and ‚Äì Springer for their efficient cooperation in producing this volume and for financial support for the Best Paper Award. We would further like to thank the following sponsors for their financial support: the Wisconsin Institute for Discovery and the Department of Industrial & Systems Engineering at the University of Wisconsin-Madison, the Air Force Office of Scientific Research,  
   
  vi  
   
  Preface  
   
  the Office of Naval Research, FICO, Google, Gurobi Optimization, and The Optimization Firm. March 2023  
   
  Alberto Del Pia Volker Kaibel  
   
  Organization  
   
  Program Committee Merve Bodur Jose Correa Alberto Del Pia Yuri Faenza Volker Kaibel (Chair) Simge Kucukyavuz Andrea Lodi Diego Moran Giacomo Nannicini Britta Peis Mohit Singh Martin Skutella Juan Pablo Vielma Jens Vygen Stefan Weltge Giacomo Zambelli  
   
  University of Toronto, Canada Universidad de Chile, Chile University of Wisconsin-Madison, USA Columbia University, USA OVGU Magdeburg, Germany Northwestern University, USA Cornell Tech, USA Universidad Adolfo Iba√±ez, Chile University of Southern California, USA RWTH Aachen, Germany Georgia Institute of Technology, USA TU Berlin, Germany Massachusetts Institute of Technology, USA University of Bonn, Germany TU M√ºnchen, Germany London School of Economics and Political Science, UK  
   
  viii  
   
  Organization  
   
  Sponsors  
   
  Organization  
   
  ix  
   
  Contents  
   
  Information Complexity of Mixed-Integer Convex Optimization . . . . . . . . . . . . . . Amitabh Basu, Hongyi Jiang, Phillip Kerger, and Marco Molinaro  
   
  1  
   
  Efficient Separation of RLT Cuts for Implicit and Explicit Bilinear Products . . . Ksenia Bestuzheva, Ambros Gleixner, and Tobias Achterberg  
   
  14  
   
  A Nearly Optimal Randomized Algorithm for Explorable Heap Selection . . . . . . Sander Borst, Daniel Dadush, Sophie Huiberts, and Danish Kashaev  
   
  29  
   
  Sparse Approximation over the Cube . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Sabrina Bruckmeier, Christoph Hunkenschr√∂der, and Robert Weismantel  
   
  44  
   
  Recycling Inequalities for Robust Combinatorial Optimization with Budget Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Christina B√ºsing, Timo Gersing, and Arie M. C. A. Koster  
   
  58  
   
  Inapproximability of Shortest Paths on Perfect Matching Polytopes . . . . . . . . . . . Jean Cardinal and Raphael Steiner  
   
  72  
   
  Monoidal Strengthening and Unique Lifting in MIQCPs . . . . . . . . . . . . . . . . . . . . Antonia Chmiela, Gonzalo Mu√±oz, and Felipe Serrano  
   
  87  
   
  From Approximate to Exact Integer Programming . . . . . . . . . . . . . . . . . . . . . . . . . . 100 Daniel Dadush, Friedrich Eisenbrand, and Thomas Rothvoss Optimizing Low Dimensional Functions over the Integers . . . . . . . . . . . . . . . . . . . 115 Daniel Dadush, Arthur L√©onard, Lars Rohwedder, and Jos√© Verschae Configuration Balancing for Stochastic Requests . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 Franziska Eberle, Anupam Gupta, Nicole Megow, Benjamin Moseley, and Rudy Zhou An Update-and-Stabilize Framework for the Minimum-Norm-Point Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 Satoru Fujishige, Tomonari Kitahara, and L√°szl√≥ A. V√©gh Stabilization of Capacitated Matching Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157 Matthew Gerstbrein, Laura Sanit√†, and Lucy Verberk  
   
  xii  
   
  Contents  
   
  Designing Optimization Problems with Diverse Solutions . . . . . . . . . . . . . . . . . . . 172 Oussama Hanguir, Will Ma, and Christopher Thomas Ryan ReLU Neural Networks of Polynomial Size for Exact Maximum Flow Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187 Christoph Hertrich and Leon Sering On the Correlation Gap of Matroids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 Edin Husi¬¥c, Zhuan Khye Koh, Georg Loho, and L√°szl√≥ A. V√©gh A 4/3-Approximation Algorithm for Half-Integral Cycle Cut Instances of the TSP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217 Billy Jin, Nathan Klein, and David P. Williamson The Polyhedral Geometry of Truthful Auctions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231 Michael Joswig, Max Klimm, and Sylvain Spitz Competitive Kill-and-Restart and Preemptive Strategies for Non-clairvoyant Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246 Sven J√§ger, Guillaume Sagnol, Daniel Schmidt genannt Waldschmidt, and Philipp Warode A Deterministic Better-than-3/2 Approximation Algorithm for Metric TSP . . . . 261 Anna R. Karlin, Nathan Klein, and Shayan Oveis Gharan Monoidal Strengthening of Simple V-Polyhedral Disjunctive Cuts . . . . . . . . . . . . 275 Aleksandr M. Kazachkov and Egon Balas Optimal General Factor Problem and Jump System Intersection . . . . . . . . . . . . . . 291 Yusuke Kobayashi Decomposition of Probability Marginals for Security Games in Abstract Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306 Jannik Matuschke Set Selection Under Explorable Stochastic Uncertainty via Covering Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319 Nicole Megow and Jens Schl√∂ter Towards a Characterization of Maximal Quadratic-Free Sets . . . . . . . . . . . . . . . . . 334 Gonzalo Mu√±oz, Joseph Paat, and Felipe Serrano Compressing Branch-and-Bound Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348 Gonzalo Mu√±oz, Joseph Paat, and √Ålinson S. Xavier  
   
  Contents  
   
  xiii  
   
  Exploiting the Polyhedral Geometry of Stochastic Linear Bilevel Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363 Gonzalo Mu√±oz, David Salas, and Anton Svensson Towards an Optimal Contention Resolution Scheme for Matchings . . . . . . . . . . . 378 Pranav Nuti and Jan Vondr√°k Advances on Strictly Œî-Modular IPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393 Martin N√§gele, Christian N√∂bel, Richard Santiago, and Rico Zenklusen Cut-Sufficient Directed 2-Commodity Multiflow Topologies . . . . . . . . . . . . . . . . . 408 Joseph Poremba and F. Bruce Shepherd Constant-Competitiveness for Random Assignment Matroid Secretary Without Knowing the Matroid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423 Richard Santiago, Ivan Sergeev, and Rico Zenklusen A Fast Combinatorial Algorithm for the Bilevel Knapsack Problem with Interdiction Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438 Noah Weninger and Ricardo Fukasawa Multiplicative Auction Algorithm for Approximate Maximum Weight Bipartite Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453 Da Wei Zheng and Monika Henzinger A Linear Time Algorithm for Linearizing Quadratic and Higher-Order Shortest Path Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466 Eranda √áela, Bettina Klinz, Stefan Lendl, Gerhard J. Woeginger, and Lasse Wulf Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481  
   
  Information Complexity of Mixed-Integer Convex Optimization Amitabh Basu1 , Hongyi Jiang2 , Phillip Kerger1 , and Marco Molinaro3,4(B) 1  
   
  2  
   
  Department of Applied Mathematics and Statistics, Johns Hopkins University, Baltimore, USA {abasu9,pkerger}@jhu.edu School of Civil and Environmental Engineering, Cornell University, Ithaca, USA [email protected]  3 Microsoft Research, Redmond, USA [email protected]  4 Computer Science Department, PUC-Rio, Rio de Janeiro, Brazil  
   
  Abstract. We investigate the information complexity of mixed-integer convex optimization under diÔ¨Äerent kinds of oracles. We establish new lower bounds for the standard Ô¨Årst-order oracle, improving upon the previous best known lower bound. This leaves only a lower order linear term (in the dimension) as the gap between the lower and upper bounds. Further, we prove the Ô¨Årst set of results in the literature (to the best of our knowledge) on information complexity with respect to oracles based on Ô¨Årst-order information but restricted to binary queries, and discuss various special cases of interest thereof. Keywords: Mixed-integer optimization  
   
  1  
   
  ¬∑ Information complexity  
   
  First-order Information Complexity  
   
  We consider the problem class of mixed-integer convex optimization: inf{f (x, y) : (x, y) ‚àà C, (x, y) ‚àà Zn √ó Rd },  
   
  (1)  
   
  where f : Rn √ó Rd ‚Üí R ‚à™ {+‚àû} is a convex (possibly nondiÔ¨Äerentiable) function and C ‚äÜ Rn √ó Rd is a closed, convex set. Given Œµ > 0, we wish to report a point in S((f, C), Œµ) := {(x, y) ‚àà C ‚à© dom(f ) ‚à© (Zn √ó Rd ) : f (x, y) ‚â§ f (x , y ) + Œµ, ‚àÄ(x , y ) ‚àà C ‚à© dom(f ) ‚à© (Zn √ó Rd )}. The Ô¨Årst and third authors gratefully acknowledge support from Air Force OÔ¨Éce of ScientiÔ¨Åc Research (AFOSR) grant FA95502010341 and National Science Foundation (NSF) grant CCF2006587. The fourth author was supported in part by the Coordena¬∏ca Àúo de Aperfei¬∏coamento de Pessoal de N¬¥ƒ±vel Superior (CAPES, Brasil) - Finance Code 001, by Bolsa de Produtividade em Pesquisa #312751/2021-4 from CNPq, and FAPERJ grant ‚ÄúJovem Cientista do Nosso Estado‚Äù. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 1‚Äì13, 2023. https://doi.org/10.1007/978-3-031-32726-1_1  
   
  2  
   
  A. Basu et al.  
   
  A point in S((f, C), Œµ) will be called an Œµ-approximate solution and points in C ‚à© dom(f ) ‚à© (Zn √ó Rd ) will be called feasible solutions. We say that x1 , . . . , xn are the integer-valued decision variables or simply the integer variables of the problem, and y1 , . . . , yd are called the continuous variables. The notion of information complexity (a.k.a oracle complexity or analytical complexity) goes back to foundational work by Nemirovski and Yudin [8] on convex optimization (without integer variables) and is based on the following. An algorithm for reporting an Œµ-approximate solution to an instance (f, C) must be ‚Äúgiven‚Äù the instance somehow. Allowing only instances with explicit, algebraic descriptions (e.g., the case of linear programming) can be restrictive. To work with more general, nonlinear instances, the algorithm is allowed to make queries to an oracle to collect information about the instance. The standard oracle that has been studied over the past several decades is the so-called Ô¨Årst-order oracle, which consists of two parts: i) a separation oracle that receives a point z ‚àà Rn+d and reports ‚ÄúYES‚Äù if z ‚àà C and otherwise reports a separating hyperplane for z and C, ii) a subgradient oracle that receives a point z ‚àà Rn+d and reports f (z) and a subgradient for f at z. The goal is to design a query strategy that can report an Œµ-approximate solution after making the smallest number of queries. Tight lower and upper bounds (diÔ¨Äering by only a small constant factor) on the number of queries were obtained by Nemirovski and Yudin in their seminal work  case with no integer variables); roughly speaking, the bound is  [8] (the Œò d log 1Œµ . These insights were extended to the mixed-integer setting in [2,3, 9], with the best known lower and upper bounds stated in [2]. Observe that the response to any separation/subgradient query is a vector in Rn+d . Thus, each query reveals at least n+d bits of information about the instance. A more careful accounting that measures the ‚Äúamount of information‚Äù accrued would track the total number of bits obtained as opposed to just the total number of oracle queries made. A natural question, posed in [2], is whether the bounds from the classical analysis would change if one uses this new measure of the total number of bits, as opposed to the number  of queries. The intuition, roughly, is that one should need a factor (n + d) log 1Œµ larger than   the number of Ô¨Årst-order queries, because one should need to probe at least log 1Œµ bits in n+d coordinates to recover the full subgradient/separating hyperplane (up to desired approximations). We attempt to make some progress on this question in this paper. The above discussion suggests that one should consider oracles that return a desired bit of a desired coordinate of the separating hyperplane vector or subgradient. However, one can imagine making other binary queries on the instance; for example, one can pick a direction and ask for the sign of the inner product of the subgradient and this direction. In fact, one can consider more general binary queries that have nothing to do with subgradients/separating hyperplanes. If one allows all possible binary queries, i.e., one can use any function from the space of instances to {0, 1} as a query, then one can simply ask for the appropriate bits of the true minimizer and in O((n + d) log(1/Œµ)) queries, one can get an Œµ-approximate solution. A matching lower bound follows from a fairly straightforward counting argument. Thus, allowing for all possible binary queries gives the same information complexity bound as the original Nemirovski-Yudin bound  
   
  Information Complexity of Mixed-Integer Convex Optimization  
   
  3  
   
  with subgradient queries in the n = 0 (no integer variables) case, but is an exponential improvement when n ‚â• 1 (see [2] and the discussion below). What this shows is that the bounds on information complexity can be quite diÔ¨Äerent under diÔ¨Äerent oracles. With all possible binary queries, while each query reveals only a single bit of information, the queries themselves are a much richer class and this compensates to give the same bound in the continuous case and exponentially better bounds in the presence of integer variables. Thus, to get a better understanding of this trade-oÔ¨Ä, we restrict to queries that still extract information from the subgradient or separating hyperplane at a point, and are thus ‚Äúlocal‚Äù in a sense. (In,d,R,œÅ,M is the set of instances we focus on throughout the paper, see DeÔ¨Ånition 2 for a formal deÔ¨Ånition.) Definition 1. An oracle using Ô¨Årst-order information consists of two parts: 1. For every z ‚àà [‚àíR, R]n+d , there exist two maps gzsep : In,d,R,œÅ,M ‚Üí Rn+d and gzsub : In,d,R,œÅ,M ‚Üí R √ó Rn+d such that for all (f, C) ‚àà In,d,R,œÅ,M the following properties hold. (a) C ‚äÜ {z ‚àà Rn+d : gzsep (C), z < gzsep (C), z } if z ‚àà C and gzsep (C) = 0 if z ‚àà C. In other words, gzsub (f ) returns a (normal vector to a) separating hyperplane if z ‚àà C. We will assume that a nonzero response gzsep (C) has norm 1, since scalings do not change the separation property. (b) gzsub (f ) ‚àà {f (z)} √ó ‚àÇf (z), where ‚àÇf (z) denotes the subdiÔ¨Äerential (the set of all subgradients) of f at z. In other words, gzsub (f ) returns the function value and a subgradient for f at z. If f (z) = +‚àû, gzsub (f ) returns a separating hyperplane for z and the domain of f .1 Such maps will be called Ô¨Årst-order maps. A collection of Ô¨Årst-order maps, one for every z, is called a (complete) Ô¨Årst-order chart and will be denoted by G. 2. There are two sets of functions Hsep and Hsub with domains Rn+d and R √ó Rn+d respectively. We will use the notation H = Hsep ‚à™ Hsub . H will be called the collection of permissible queries of the oracle. The algorithm, at any iteration, can choose a point z and a function h ‚àà H  and it receives the response h(gzsep (f)) or h(gzsub (C)), depending on whether sep sub   or h ‚àà H , where (f , C) is the unknown instance. h‚ààH In particular, when Hsep and Hsub consist only of the identity function, we recover a standard Ô¨Årst-order oracle. We will also study the cases where Hsep and Hsub consist of functions that map a vector to a particular bit of a particular coordinate, or the sign of the inner product with a particular direction, or the set of all possible binary functions. In the last case, the oracle will be called the general binary oracle based on G.  
   
  1  
   
  When the function value is +‚àû, we will count this as a separation query. In other words, below we will assume without further comment that every functional query returns a Ô¨Ånite real value for the function and a subgradient at the queried point.  
   
  4  
   
  1.1  
   
  A. Basu et al.  
   
  Our Results  
   
  It is not hard to see that if we consider all possible instances of (1), then any adaptive query strategy has inÔ¨Ånite information complexity because a Ô¨Ånite number of queries cannot distinguish between all possible instances. Thus, bounds on the information complexity must be based on appropriate parameterizations of the problem. We will focus on the following standard parameterization. Definition 2. In,d,R,œÅ,M is the set of all instances of (1) such that (i) C ‚à© dom(f ) is contained in the box {z ‚àà Rn √ó Rd : z‚àû ‚â§ R}. ÀÜ ‚àà Rd (ii) If (x , y ) is an optimal solution of the instance, then there exists y  ÀÜ ‚àû ‚â§ œÅ} ‚äÜ C. In other words, there is a ‚Äústrictly satisfying {(x , y) : y ‚àí y ÀÜ ) in the same Ô¨Åber as the optimum (x , y ). feasible‚Äù point (x , y (iii) f is Lipschitz continuous with respect to the  ¬∑ ‚àû -norm with Lipschitz constant M on {x} √ó [‚àíR, R]d for all x ‚àà [‚àíR, R]n ‚à© Zn , where we use the convention that if f is identically +‚àû on {x}√ó[‚àíR, R]d , then any M works on this Ô¨Åber. In other words, for any (x, y), (x, y ) ‚àà (Zn √óRd )‚à©[‚àíR, R]n+d with y ‚àíy ‚àû ‚â§ R, |f (x, y)‚àíf (x, y )| ‚â§ M y ‚àíy ‚àû with the convention that ‚àû ‚àí ‚àû = 0. We obtain the following results in this paper. Results for n ‚â• 1 (allowing integer variables). 1. In the classical setting where Hsep and Hsub consist only of the identity function (i.e. each query receives the entire subgradient/separating hyperplane), we improve the best known lower bound (from [2]) on the number of queries needed for the general mixed-integer case. In particular, we show that one   MR n queries, improving upon the previous needs at least Œ© 2 d log min{œÅ,1}Œµ    R bound of Œ© 2n d log œÅ . We mention here that the Ô¨Årst lower bound on information complexity with integer constrained variables was established in [3], for a speciÔ¨Åc class of algorithms/query strategies called cutting-plane schemes. The lower bounds stated here (and in [2]) do not make any assumptions on the algorithms/query strategies.    MR n for the classical setting is com2. This lower bound of Œ© 2 d log min{œÅ,1}Œµ    MR in the litplemented by an upper bound of O 2n d(n + d) log min{œÅ,1}Œµ erature. This was Ô¨Årst obtained in [9]; see [2] for a self-contained exposition. As mentioned before, we expect the information complexity in the setting of binary  oracles using Ô¨Årst-order information to be at most a factor MR larger than the classical setting. We rigorously prove (n + d) log min{œÅ,1}Œµ this, under the additional assumption that the Ô¨Åber containing the optimal solution contains a point that is œÅ-deep inside the feasible region C, i.e., a ball of radius œÅ centered at this point is contained in the set C. Note that this is a stronger assumption compared to item (ii) in the deÔ¨Ånition of In,d,R,œÅ,M .  
   
  Information Complexity of Mixed-Integer Convex Optimization  
   
  5  
   
  Results for n = 0 (continuous case with no integer variables). 1. When the separation and subgradient oracles can only be accessed through (all) binary queries (i.e., Hsep and Hsub consist of all possible binary functions on Rd ), we show that strictly more queries are needed compared to the classic setting of  full oracle access.  More precisely, compared to the MR classic bound of Œò d log min{œÅ,1}Œµ , we show that one needs at least     MR Àú hides polylogarithÀú max d 87 , d log binary queries (where Œ© Œ© min{œÅ,1}Œµ mic factors in d). This is obtained using recent lower bounds on ‚Äúmemoryconstrained‚Äù algorithms for convex optimization from [6].  2   MR 2. We establish an upper bound of O d2 log min{œÅ,1}Œµ for binary queries using Ô¨Årst-order oracles. This is an extension of a result from [11] that considered the unconstrained optimization setting.    MR for any Ô¨Ånite 3. We establish an upper bound of O log |I| + d log min{œÅ,1}Œµ subclass of instances I ‚äÜ In,d,R,œÅ,M . Note that this can beat the lower bound from Item 1 above, e.g., if |I| = 2O(d) . 1.2  
   
  Formal Definitions and Statement of Results  
   
  For any set X, we will use X ‚àó to denote the set of all Ô¨Ånite sequences of elements from X (e.g., {0, 1}‚àó denotes the set of all Ô¨Ånite binary strings). Definition 3. Given an oracle using Ô¨Årst-order information (G, H), let Q be the set of possible queries that can be made under this oracle (i.e., pairs (z, h) where z ‚àà Rn+d is a query point and h ‚àà H). Let H denote the response set of the functions in H (e.g., vectors for standard Ô¨Årst-order oracles, or {0, 1} for binary queries). A query strategy is a function D : (Q √ó H)‚àó ‚Üí Q. The transcript Œ†(D, I) of a strategy D on an instance I = (f, C) is the sequence of query and response pairs (qi , qi (I)), i = 1, 2, . . . obtained when one applies D on I, i.e., q1 = D(‚àÖ) and qi = D((q1 , q1 (I)), . . . , (qi‚àí1 , qi‚àí1 (I))) for i ‚â• 2. The Œµ-information complexity icomp Œµ (D, I, G, H) of an instance I for a query strategy D, with access to an oracle using Ô¨Årst-order information (G, H), is deÔ¨Åned as the minimum natural number k such that the set of all instances which return the same responses as the instance I to the Ô¨Årst k queries of D have a common Œµ-approximate solution. The Œµ-information complexity of the problem class In,d,R,œÅ,M , with access to an oracle using Ô¨Årst-order information (G, H), is deÔ¨Åned as icomp Œµ (n, d, R, œÅ, M, G, H) := inf D supI‚ààIn,d,R,œÅ,M icomp Œµ (D, I, G, H) where the inÔ¨Åmum is taken over all query strategies. We can now formally state our main results. Let Hbit be the set of binary queries that return a desired bit (of a desired coordinate) of a subgradient/separating hyperplane/function value. Let Hdir be the set of binary queries that returns  
   
  6  
   
  A. Basu et al.  
   
  the sign of the inner product of the subgradient/separating hyperplane with a desired direction, or a desired bit of the function value. Results for n ‚â• 1 (allowing integer variables). Theorem 1. There exists a complete Ô¨Årst-order chart G such that for the standard Ô¨Årst-order oracle based on G (i.e., H consists of the identity functions), we have     MR . icomp Œµ (n, d, R, œÅ, M, G, H) = Œ© 2n 1 + d log min{œÅ,1}Œµ Theorem 2. Assume d ‚â• 1. For U > 0, consider the subclass of instances of In,d,R,œÅ,M whose objective function values lie in [‚àíU, U ], and the Ô¨Åber over the optimal solution contains a z such that the (n + d)-dim œÅ-radius ball centered at z is contained in C. There exists a query strategy for this subclass that uses (G, H), where G is any complete Ô¨Årst-order chart and H is either Hbit or Hdir , that reports an Œµ-approximate solution by making at most  

  dM R (n + d)M R U O 2n d (n + d) log ¬∑ (n + d) log + log min{œÅ, 1}Œµ Œµ Œµ queries. Prescribing an a priori range for objective function values is not a serious restriction for two reasons: i) The diÔ¨Äerence between the maximum and the minimum values of an objective function in In,d,R,œÅ,M is at most 2M R, and ii) All optimization problems whose objective functions diÔ¨Äer by a constant are equivalent. We also comment that while we assume d ‚â• 1 in Theorem 2, similar bounds can be established for the d = 0 case. We omit this here because a uniÔ¨Åed expression for the d = 0 and d ‚â• 1 cases becomes unwieldy and diÔ¨Écult to parse. We remark that we can obtain the same result when the objective function can only be accessed through comparisons of the form ‚ÄúIs f (z) ‚â§ f (z )?‚Äù, i.e., no access to the subgradients ‚àÇf . Such algorithms are particularly useful in learning from users‚Äô behaviors, since while a user typically cannot accurately report its (dis)utility value f (z) for an option z, it can more reliably compare the values f (z) and f (z ) of two options; see [5,10] and references therein for discussions and algorithms in the continuous case. To the best of our knowledge, no such algorithm for the mixed-integer case has appeared explicitly in the literature. We also remark that the additional assumption that a (n + d)-dim œÅ-radius is contained in C can be weakened by using a Lenstra-style algorithm, but this yields a much worse dependence in d and n. Results for n = 0 (continuous case with no integer variables). Theorem 3. There exists a complete Ô¨Årst-order chart G such that for the general binary oracle based on G (i.e., H consists of all possible binary functions on Rd ), we have     MR Àú max d 87 , d log icomp Œµ (d, R, œÅ, M, G, H) = Œ© , min{œÅ,1}Œµ Àú hides polylogarithmic factors in d. where Œ©  
   
  Information Complexity of Mixed-Integer Convex Optimization  
   
  7  
   
  Theorem 4. For U > 0, consider the subclass of instances of Id,R,œÅ,M whose objective function values lie in [‚àíU, U ]. There exists a query strategy for this subclass that uses (G, H), where G is any complete Ô¨Årst-order chart and H is either Hbit or Hdir , that reports an Œµ-approximate solution by making at most  

  dM R dM R U O d log ¬∑ d log + log min{œÅ, 1}Œµ Œµ Œµ queries. Theorem 5. Given any subclass of Ô¨Ånitely many instances I ‚äÇ In,d,R,œÅ,M and any complete Ô¨Årst-order chart G, there exists a query strategy for this subclass using the general binary on G that reports an Œµ-approximate solution  oracle based   MR by making at most O log |I| + d log min{œÅ,1}Œµ queries.  
   
  1.3  
   
  Discussion and Future Avenues  
   
  The concept of information complexity in continuous convex optimization and its study go back several decades, and it is considered a fundamental question in convex optimization. In comparison, much less work on information complexity has been carried out in the presence of integer constrained variables. Nevertheless, we believe there are important and challenging questions that come up in that domain that are worth studying. Further, even within the context of continuous convex optimization, the notion of information complexity has almost exclusively focused on the number of Ô¨Årst-order queries. As we hope to illustrate with the results of this paper, considering other kinds of oracles lead to very interesting questions at the intersection of mathematical optimization and information theory. In particular, the study of binary oracles promises to give a more reÔ¨Åned understanding of the fundamental question ‚ÄúHow much information about an optimization instance do we need to be able to solve it with provable guarantees?‚Äù. For instance, establishing any superlinear (in the dimension) lower bound for the continuous problem with binary oracles, like the one in Theorem 3, seems to be nontrivial. In fact, the results from [6], on which Theorem 3 is based, were considered a breakthrough in establishing superlinear lower bounds on space complexity of convex optimization. Even so, the right bound is conjectured to be quadratic in the dimension (see Theorem 4) and our Theorem 3 is far from that at this point. We thus view the results of this paper as expanding our understanding of information complexity of optimization in two diÔ¨Äerent dimensions: what role does the presence of integer variables play and what role does the nature of the oracle play (with or without integer variables)? For integer variables, our Ô¨Årst result brings the lower bound closer to the best  
   
  8  
   
  A. Basu et al.  
   
  known upper bound on information complexity based on the classical subgradient oracle. The remaining gap is now simply a factor linear in the dimension. A conjecture in convex geometry Ô¨Årst articulated in [9, Conjecture 4.1.20] and elaborated upon in [2,3] would resolve this and would show that the right upper bound is essentially equal to the lower bound we prove in this paper. Therefore, we have reasons to believe that the right bound is the one we obtain in this paper. Beyond this, we believe the following additional conjectures to be good catalysts for future research, especially in regard to understanding the interplay of integer variables and other oracles. Conjecture 1. Given an oracle (G, H) based on Ô¨Årst-order information, suppose there is a family of instances that establishes a lower bound (d, R, œÅ, M, G, H) for the n = 0 (continuous) case. Then there exists a family of instances that establishes a lower bound of 2n ¬∑ (d, R, œÅ, M, G, H) for the n ‚â• 1 case, i.e., the general mixed-integer case. Conjecture 2. If there exists a query strategy with worst case information complexity u(n, d, R, œÅ, M, G) under the standard Ô¨Årst-order oracle based on a complete Ô¨Årst-order chart G, then there existsa query strategy with  worst case infor under the general mation complexity u(n, d, R, œÅ, M, G) ¬∑ O (n + d) log MœÅŒµR binary oracle based G. Both of the above results, if true, would be useful ‚Äútransfer‚Äù theorems: the Ô¨Årst one for lower bounds, the second one for upper bounds. Conjecture 1 takes a lower bound result for the continuous problem and lifts it to the general mixedinteger case with a factor of 2n . This would be a general tool that can then give Theorem 1 as a special case and also give a mixed-integer version of Theorem 3 as a corollary. Further, if future research on the information complexity of continuous convex optimization results in better/diÔ¨Äerent lower bounds, these would immediately imply new lower bounds for the mixed-integer case as well. For instance, we believe the following conjecture to be true for the continuous convex optimization problem. Conjecture 3. There exists a complete Ô¨Årst-order chart G such that the general 2    . binary oracle based on G has information complexity Œ© d2 log MœÅŒµR Another version of Conjecture 3 is also stated in the language of ‚Äúmemoryconstrained‚Äù algorithms (see Sect. 2.2 below) in [6,11]. Conjecture 2 can be used to take upper bound results proved in the standard Ô¨Årst-order oracle setting and get upper bound results in the general binary oracle setting. For instance, if the upper bound for the general mixed-integer problem is improved by resolving the convex geometry conjecture mentioned above (and we believe the lower bound is correct and the upper bound is indeed loose), then this would also give better upper bounds for the general binary oracle setting.  
   
  Information Complexity of Mixed-Integer Convex Optimization  
   
  2 2.1  
   
  9  
   
  Proof Sketches Proof Sketch of Theorem 1  
   
  The general strategy to prove Theorem 1 is the following: Given any query strategy D, we will construct two instances (f1 , C1 ), (f2 , C2 ) ‚àà In,d,R,œÅ,M such that the transcripts Œ†(D, (f1 , C1 )) and Œ†(D, (f2 , C2 )) are equal for the Ô¨Årst k C2 ), Œµ) = ‚àÖ. terms if k is less than the lower bound, but S((f1 , C 1 ), Œµ) ‚à© S((f  2 , was estabThe lower bound icomp Œµ (n, d, R, œÅ, M, G, H) ‚â• 2n ¬∑ d log 2R 3œÅ lished 4.2]. Thus, it suÔ¨Éces to show icomp Œµ (n, d, R, œÅ, M, G, H) ‚â• in [2,  Theorem  d2n log8 M2ŒµR . The idea is to use a family F of convex functions over [‚àíR, R]d  M R deÔ¨Åned  subgradient queries described in [7,8] such that one needs at least d log8 2Œµ to report an Œµ-approximate solution. In fact, for any query strategy in Rd , one construct subgradient responses such that if less than  can adversarially d log8 M2ŒµR are made, one can report two functions from F that would have provided the same responses as given by the adversary and yet have disjoint sets of Œµ-approximate solutions. We now mimic this by putting the family F over the Ô¨Åbers {x} √ó [‚àíR, R]d for x ‚àà {0, 1}n . Our constraint set C is going to be simply [0, 1]n √ó[‚àíR, R]d ; thus, the separation oracle queries will provide no information. We will create a nested sequence of polyhedra Y0 ‚äá Y1 ‚äá . . . ‚äá Yk contained in [0, 1]n √ó [‚àíR, R]d √ó R, where Yi corresponds to query i. This sequence will depend on the queries made and will determine our responses. The set Yk will be used to construct the epigraphs of two functions f1 and f2 , whose Œµ-approximate minimizers in Zn √ó Rd will be disjoint. We now enumerate diÔ¨Äerent cases:  
   
  1. If the query point is outside [0, 1]n √ó [‚àíR, R]d , then the function value is reported to be +‚àû and a separating hyperplane is reported. Yi is not updated. 2. If the query point z = (x, y) is inside [0, 1]n √ó [‚àíR, R]d , but x ‚àà {0, 1}n , then we simply report the function value from the current Yi set interpreted as an epigraph and any subgradient at this point on Yi . Yi is not updated. 3. If the query point z = (x, y) ‚àà {0, 1}n √ó [‚àíR, R]d , we look at what the response from the adversary would have been for the family F at the query point y, and we rotate the corresponding subgradient hyperplane so that it is valid for all other Ô¨Åbers {x } √ó [‚àíR, R]d for x ‚àà {0, 1}n \ {x}, as well as all points queried inside the hypercube, but not the Ô¨Åbers. This can be done because the Ô¨Åbers are compact, and only a Ô¨Ånite number of queries have been made. We then update Yi by intersecting with this rotated halfspace, and report this rotated halfspace as the response for this query.   queries have been made, there must exist a Ô¨Åber ‚Äì If k < 2n d log8 M2ŒµR    corresponding to say x ‚àà {0, 1}n ‚Äì on which less than d log8 M2ŒµR queries were made. We now take the two functions fÀú1 , fÀú2 from F that would have given the same responses on that Ô¨Åber, with disjoint Œµ-approximate solutions. On the other Ô¨Åbers corresponding to x = x , we consider any function fÀúx from F that would have returned the same responses on that Ô¨Åber (this can be ensured to  
   
  10  
   
  A. Basu et al.  
   
  exist given the structure of the family F), and deÔ¨Åne YÀú x := {x} √ó epi(fÀúx ). For query points z not on the Ô¨Åbers, we consider the sets YÀú z := Yk ‚à© ({z} √ó R). For i = 1, 2, we deÔ¨Åne Ei as the convex hull of {x } √ó epi(fÀúi ) and all the YÀú x , x = x and YÀú z for query points z not on the Ô¨Åbers. These convex hulls are the epigraphs of two functions f1 , f2 whose Œµ-approximate minimizers can be shown to be exactly the points of the form (x , y), where y is an Œµ-approximate minimizer of fÀú1 , fÀú2 , respectively. These are disjoint sets and we are done. 2.2  
   
  Proof of Theorem 3  
   
  We need to introduce the idea of information memory of any query strategy/algorithm. Definition 4. A Ô¨Årst-order query strategy with information memory comprises three functions: 1. œÜquery : {0, 1}‚àó ‚Üí [‚àíR, R]n √ó [‚àíR, R]d . n d ‚àó ‚àó 2. œÜsep update : (R √ó R ) √ó {0, 1} ‚Üí {0, 1} . sub n d ‚àó 3. œÜupdate : (R √ó (R √ó R )) √ó {0, 1} ‚Üí {0, 1}‚àó . Given access to a (complete) Ô¨Årst-order chart G, the query strategy maintains an information memory, which is a Ô¨Ånite length binary string in {0, 1}‚àó , initialized as the empty string. At every iteration k = 1, 2, . . ., the query œÜquery (rk‚àí1 ) andupdates its memory using either strategy computes zk :=    sep sep sep  sub   is the gz (C), rk‚àí1 or rk = œÜ gz (f ), rk‚àí1 , where (f, C) rk = œÜ update  
   
  k  
   
  update  
   
  k  
   
  unknown true instance. After Ô¨Ånitely many iterations, the query strategy does a Ô¨Ånal computation based on its information memory and reports an Œµ-approximate solution, i.e., there is a Ô¨Ånal function œÜfin : {0, 1}‚àó ‚Üí Zn √ó Rd . The information memory complexity of an algorithm for an instance is the maximum length of its information memory rk over all iterations k during the processing of this instance. Proposition 6. Let G be a (complete) Ô¨Årst-order chart. For any Ô¨Årst-order query strategy A with information memory that uses G, there exists a query strategy A using the general binary oracle based on G, such that for any instance (f, C), if A stops after T iterations with information memory complexity Q, A stops after making at most Q ¬∑ T oracle queries. Conversely, for any query strategy A using the general binary oracle based on G, there exists a Ô¨Årst-order query strategy A with information memory such that for any instance (f, C), if A stops after T iterations, A stops after making at most T iterations with information memory complexity at most T . Proof. Let A be a Ô¨Årst-order query strategy with information memory. We can simulate A by the query strategy whose queries are precisely the bits of the information memory state rk at each iteration k of A. More formally, the query  
   
  Information Complexity of Mixed-Integer Convex Optimization  
   
  11  
   
  sub is z = œÜquery (rk‚àí1 ) and h(¬∑) = (œÜsep update (¬∑, rk‚àí1 ))i or h(¬∑) = (œÜupdate (¬∑, rk‚àí1 ))i , depending on which type of query was made. Conversely, given a query strategy A based on the general binary oracle, we can simulate it with a Ô¨Årst-order query strategy with information memory where in each iteration, we simply append the new bit queried by A to the current state of the memory.    
   
  The following is (a rephrasing of) a result from [6]. Theorem 7. [6, Theorem 1] For every Œ¥ ‚àà [0, 1/4], there is a class of instances I ‚äÜ In,d,R,œÅ,M and a (complete) Ô¨Årst-order chart G such that any Ô¨Årst-order query strategy with information memory must have either d1.25‚àíŒ¥ information Àú 1+ 43 Œ¥ ) iterations (in memory complexity (in the worst case) or make at least Œ©(d the worst case). 3 in Theorem 7, we obtain that any Ô¨Årst-order Proof of Theorem 3. Setting Œ¥ = 28 8/7 Àú 8/7 ) query strategy uses either d information memory or makes at least Œ©(d iterations. Using the second part of Proposition 6, we obtain the desired lower Àú 8/7 ) on the number of queries made by any query strategy using bound of Œ©(d the general binary oracle based on G.    
   
  2.3  
   
  Proof Sketch of Theorem 5  
   
  We will sketch the proof for solving the feasibility problem, optimization being handled in a similar way by incorporating subgradients. Thus, we have a Ô¨Ånite set of instance I ‚äÜ Id,R,œÅ with only continuous variables, a true (unknown) instance C ‚àà I, and our goal is to report a point in C using few binary queries to a separation oracle. For that, we design a procedure that maintains a family U ‚äÜ I of the instances that are still possible (which always includes the true instance C), along with a polyhedron P containing C. We start with U = I and P = [‚àíR, R]d . We will show that we can always either reduce |U| or vol(P ) by a constant fraction with each query. For that, while |U| > 1 do the following: ‚Äì Set p equal to be the centroid of P . If the separation oracle at p reports that p ‚àà C, then we return p. Otherwise: ‚Äì Case 1: For every possible answer v ‚àà Rd to the separation oracle, at most half of the sets C  in U give that answer for the point p, namely gpsep (C  ) = v. Then, there is a set of answers V ‚äÜ Rd such that the number of sets C  ‚àà U with gpsep (C  ) ‚àà V is between 14 |U| and 34 |U|. Then querying whether the true instance has gpsep (C) ‚àà V (using the binary query h where h(v) = 1 iÔ¨Ä v ‚àà V ) we can eliminate at least a quarter of the instances of U as not possible. So update U by deleting those instances from it. ¬Ø ‚àà Rd such that more than half of the instances C  ‚Äì Case 2: There exists v sep  ¬Ø . Query whether the true instance has gpsep (C) = v ¬Ø in U have gp (C ) = v ¬Ø and 0 everywhere else). If (using the binary query h that takes value 1 at v ¬Ø , then remove from U all instances C  such that gpsep (C  ) = v ¬Ø, gpsep (C) = v  
   
  12  
   
  A. Basu et al.  
   
  reducing the size of U by at least half. Otherwise, we then know the exact ¬Ø , and so separating hyperplane for the true instance C, namely gpsep (C) = v d v, x ‚â§ ¬Ø v, p }. employ it to update the relaxation as P ‚Üê P ‚à© {x ‚àà R : ¬Ø In each step, either the size of U decreases by at least 1/4, or the volume of P decreases by a factor of at least 1e (by Gr¬® unbaum‚Äôs Theorem [4]). The former can only happen O(log |I|) times until U becomes a singleton (in which case we know    times, the true instance), whereas the latter can happen at most O d log R œÅ since C is always a subset of P and C contains an  -ball of radius œÅ; thus, ‚àû    R d queries. vol(P ) ‚â• (2œÅ) . So the procedure succeeds in O log |I| + d log œÅ 2.4  
   
  Proof Sketch of Theorems 2 and 4  
   
  Here we consider all instances In,d,R,œÅ,M and again the goal is to solve convex mixed-integer (Theorem 2) and continuous (Theorem 4) instances using few binary queries (more speciÔ¨Åcally, bit queries or inner product sign queries) to the separation and subgradient oracles. Our strategy is to: 1) solve the the problems using approximate subgradients/separating hyperplanes; 2) use binary queries to construct such approximations. For the Ô¨Årst item, we use the algorithm of [3] based on the centerpoint: this is a point in the convex set where every halfspace supported on it cuts oÔ¨Ä a signiÔ¨Åcant (mixed-integer) volume of the set. Similar to the previous section, the algorithm keeps an outer relaxation P of the feasible region C, and repeatedly applies separation or subgradient-based cuts through the centerpoint of P ; the assumption that the feasible region contains a ball (in the optimal Ô¨Åber) establishes a volume lower bound that essentially limits the number of iterations of the algorithm. While the original algorithm of [3] uses exact separation/subgradient oracles, we show, not surprisingly, that approximate ones suÔ¨Éce. The next item is to construct approximate separation/subgradient oracles by using few binary queries to the exact ones. In case of bit queries Hbit this is can be easily done by querying enough bits of the latter. The case of inner product sign queries Hdir , namely that given a vector g we can pick a direction a and ask ‚ÄúIs a, g ‚â• 0?‚Äù, is more interesting. It boils down to approximating the vector g (subgradient/separating hyperplane) using few such queries.2 Lemma 1. For any vector g ‚àà Rd , using O(d log Œµd ) inner product sign queries  
   
  g ÀÜ ‚àí g ÀÜ ‚àà Rd such that g ‚â§ Œµ . we can obtain a unit-length vector g Proof. sketch. We prove by induction on d that with d log 8Œ¥ queries we can obtain  
   
  g Œµ ÀÜ ‚àí g ÀÜ with g ‚â§ 2dŒ¥; the lemma follows by setting Œ¥ = 2d a unit-length vector g . 8 In the 2-dimensional case can use log Œ¥ queries to perform binary search and ÀÜ obtain a cone of angle Œ¥œÄ 4 that contains g; any unit-length vector g in this cone g  ‚â§ Œ¥ ‚â§ 4Œ¥ as desired. has ÀÜ g ‚àí g 2  
   
  This is related to (actively) learning the linear classiÔ¨Åer whose normal is given by g [1]. These methods can perhaps be adapted to our setting, but we present a diÔ¨Äerent and self-contained statement and proof.  
   
  Information Complexity of Mixed-Integer Convex Optimization  
   
  13  
   
  For the general case d > 2, we consider any 2-dim subspace A of Rd and Àú ‚àà A to the apply the previous case to obtain in log dŒ¥ queries an approximation g Àú ‚àí Œ†A g ‚â§ Œ¥Œ†A g ‚â§ projection Œ†A g of g onto A with guarantee Œ†A g ¬∑ g Œ¥g. Then we consider the (d ‚àí 1)-dim subspace B := span{Àú g, A‚ä• } and by 8 ÀÜ ‚àà B that approximates induction, with 2(d‚àí1) log Œ¥ queries we obtain a vector g ÀÜ ‚àí Œ†B g ‚â§ 2(d ‚àí the projection Œ†B g of g onto B with guarantee Œ†B g ¬∑ g 1)Œ¥Œ†B g ‚â§ 2(d ‚àí 1)Œ¥g (a total of less than 2d log 8Œ¥ queries was then used). ÀÜ is the desired approximation of g, namely (letting One can then show that g ŒªA := Œ†A g and ŒªB := Œ†B g)  

  g ‚àí g ¬∑ g ÀÜ ‚â§ g ‚àí Œ†B g + Œ†B g ‚àí ŒªB ¬∑ g ÀÜ  + ŒªB ¬∑ g ÀÜ ‚àí g ¬∑ g ÀÜ ‚â§ 2dŒ¥g, the upper bound on the Ô¨Årst and third terms in the middle inequality following Àú ) and the upper bound from the fact dist(g, B) ‚â§ Œ¥g (by the guarantee of g ÀÜ. on the second term following from the approximation guarantee of g    
   
  References 1. Balcan, M.-F., Long, P.: Active and passive learning of linear separators under log-concave distributions. In: Shalev-Shwartz, S., Steinwart, I., (eds.) Proceedings of the 26th Annual Conference on Learning Theory, volume 30 of Proceedings of Machine Learning Research, pp. 288‚Äì316. Princeton, NJ, USA, 12‚Äì14 June 2013. PMLR (2013) 2. Basu, A.: Complexity of optimizing over the integers. to appear in Mathematical Programming, Series A (2022) 3. Basu, A., Oertel, T.: Centerpoints: a link between optimization and convex geometry. SIAM J. Optim. 27(2), 866‚Äì889 (2017) 4. Gr¬® unbaum, B.: Partitions of mass-distributions and of convex bodies by hyperplanes. PaciÔ¨Åc J. Math. 10, 1257‚Äì1261 (1960) 5. Jamieson, K.G., Nowak, R.D., Recht, B.: Query complexity of derivative-free optimization. In: Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2, NIPS2012, pp. 2672‚Äì2680, Red Hook, NY, USA. Curran Associates Inc. (2012) 6. Marsden, A., Sharan, V., Sidford, A., Valiant, G.: EÔ¨Écient convex optimization requires superlinear memory. arXiv preprint arXiv:2203.15260 (2022) 7. Nemirovski, A.: EÔ¨Écient methods in convex programming. Lecture Notes (1994) 8. Nemirovski, A.S., Yudin, D,B.: Problem complexity and method eÔ¨Éciency in optimization. John Wiley (1983) 9. Oertel, T.: Integer convex minimization in low dimensions, Ph. D. thesis, Diss., Eidgen¬® ossische Technische Hochschule ETH Z¬® urich, Nr. 22288 (2014) 10. Protasov, V.Y.: Algorithms for approximate calculation of the minimum of a convex function from its values. Math. Notes 59(1), 69‚Äì74 (1996) 11. Woodworth, B., Srebro, N.: Open problem: the oracle complexity of convex optimization with limited memory. In: Conference on Learning Theory, pp. 3202‚Äì3210. PMLR (2019)  
   
  EÔ¨Écient Separation of RLT Cuts for Implicit and Explicit Bilinear Products Ksenia Bestuzheva1(B) , Ambros Gleixner1,2 , and Tobias Achterberg3 1  
   
  3  
   
  Zuse Institute Berlin, Berlin, Germany {bestuzheva,gleixner}@zib.de 2 HTW Berlin, Berlin, Germany Gurobi GmbH, Frankfurt am Main, Germany [email protected]   
   
  Abstract. The reformulation-linearization technique (RLT) is a prominent approach to constructing tight linear relaxations of non-convex continuous and mixed-integer optimization problems. The goal of this paper is to extend the applicability and improve the performance of RLT for bilinear product relations. First, a method for detecting bilinear product relations implicitly contained in mixed-integer linear programs is developed based on analyzing linear constraints with binary variables, thus enabling the application of bilinear RLT to a new class of problems. Our second contribution addresses the high computational cost of RLT cut separation, which presents one of the major diÔ¨Éculties in applying RLT eÔ¨Éciently in practice. We propose a new RLT cutting plane separation algorithm which identiÔ¨Åes combinations of linear constraints and bound factors that are expected to yield an inequality that is violated by the current relaxation solution. A detailed computational study based on implementations in two solvers evaluates the performance impact of the proposed methods. Keywords: Reformulation-linearization technique ¬∑ Bilinear products ¬∑ Cutting planes ¬∑ Mixed-integer programming  
   
  1  
   
  Introduction  
   
  The reformulation-linearization technique (RLT) was Ô¨Årst proposed by Adams and Sherali [1‚Äì3] for bilinear problems with binary variables, and has been applied to mixed-integer [18‚Äì20], general bilinear [21] and polynomial [24] problems. RLT constructs valid polynomial constraints, then linearizes these constraints by using nonlinear relations given in the problem and applying relaxations when such relations are not available. If relations used in the linearization step are violated by a relaxation solution, this procedure may yield violated cuts. By increasing the degree of derived polynomial constraints, hierarchies of c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 14‚Äì28, 2023. https://doi.org/10.1007/978-3-031-32726-1_2  
   
  EÔ¨Écient Separation of RLT Cuts for Implicit and Explicit Bilinear Products  
   
  15  
   
  relaxations can be constructed, which were shown to converge to the convex hull representation of MILPs and mixed-integer polynomial problems where continuous variables appear linearly [18‚Äì20]. RLT has been shown to provide strong relaxations [21,23], but this comes at the cost of excessive numbers of cuts. To address this, Sherali and Tuncbilek [25] proposed a technique to add a subset of RLT cuts, depending on signs of coefÔ¨Åcients of monomial terms in the original constraints and the RLT constraints. Furthermore, the reduced RLT technique [12‚Äì14,22] yields equivalent representations with fewer nonlinear terms for polynomial problems containing linear equality constraints. We focus on RLT for bilinear products, which is of particular interest due to the numerous applications whose models involve nonconvex quadratic nonlinearities [3,5,7‚Äì9,17]. Even in the bilinear case, large numbers of factors to be multiplied and of RLT cuts that are generated as a result remain an issue that can lead to considerable slowdowns, both due to the cost of cut separation and the large sizes of resulting LP relaxations. The Ô¨Årst contribution of this paper is a new approach to applying RLT to MILPs. Unlike the approaches that only introduce multilinear relations via multiplication [18,19], this approach detects and enforces bilinear relations that are already implicitly present in the model. A bilinear product relation where one multiplier is a binary variable and the other multiplier is a variable with Ô¨Ånite bounds can be equivalently written as two linear constraints. We identify such pairs of linear constraints that implicitly encode a bilinear product relation, then utilize this relation in the generation of RLT cuts. The second contribution of this paper addresses the major bottleneck for applying RLT successfully in practice, which stems from prohibitive costs of separating RLT cuts, by proposing an eÔ¨Écient separation algorithm. This algorithm considers the signs of bilinear relation violations in a current LP relaxation solution and the signs of coeÔ¨Écients in linear constraints in order to ignore combinations of factors that will not produce a violated inequality. Furthermore, we propose a technique which projects the linear constraints onto a reduced space and constructs RLT cuts based on the resulting much smaller system. The rest of the paper is organized as follows. In Sect. 2, RLT for bilinear products is explained. In Sect. 3, we describe the technique for deriving bilinear product relations from MILP constraints. Section 4 presents the new cut separation algorithm, and computational results are presented in Sect. 5.  
   
  2  
   
  RLT for Bilinear Products  
   
  We consider mixed-integer (nonlinear) programs (MI(N)LPs) of the extended form where auxiliary variables w are introduced for all bilinear products:  
   
  16  
   
  K. Bestuzheva et al.  
   
  min cT x s.t. Ax ƒè b, g(x, w) ƒè 0,  
   
  (1a) (1b) (1c)  
   
  xi xj ƒ≥ wij for all (i, j) P I w , x ƒè x ƒè x, w ƒè w ƒè w,  
   
  (1d) (1e)  
   
  xj P R for all j P I c , xj P {0, 1} for all j P I b ,  
   
  (1f)  
   
  with I ‚Äú I c YI b being a disjoint partition of variables x and x having dimension n |I w | (R ‚Äú RY{¬¥‚àû, `‚àû}), |I| ‚Äú n. In the above formulation, x, x P R , w, w P R (l) (l) c P Rn and b P Rm are constant vectors and A P Rm √ón is a coeÔ¨Écient matrix, and the function g deÔ¨Ånes the nonlinear constraints. Constraint (1d) deÔ¨Ånes the bilinear product relations in the problem and allows for inequalities and equations. Let I p denote the set of indices of all variables that participate in bilinear product relations (1d). Solvers typically employ McCormick inequalities [16] to construct an LP relaxation of constraints (1d). These inequalities describe the convex hull of the set given by the relation xi xj ƒ≥ wij : xi xj ` xi xj ¬¥ xi xj ƒè wij ,  
   
  xi xj ` xi xj ¬¥ xi xj ƒè wij ,  
   
  (2a)  
   
  xi xj ` xi xj ¬¥ xi xj ƒõ wij ,  
   
  xi xj ` xi xj ¬¥ xi xj ƒõ wij ,  
   
  (2b)  
   
  where (2a) is a relaxation of xi xj ƒè wij and (2b) is a relaxation of xi xj ƒõ wij . In the presence of linear constraints (1b), this relaxation can be strengthened n by adding RLT cuts. Consider a linear constraint: k‚Äú1 a1k xk ƒè b1 . Multiplying this constraint by nonnegative bound factors (xj ¬¥ xj ) and (xj ¬¥ xj ), where xj and xj are Ô¨Ånite, yields valid nonlinear inequalities. We will derive the RLT cut using the lower bound factor. The derivation is analogous for the upper bound factor. The multiplication, referred to as the reformulation step, yields: n   
   
  a1k xk (xj ¬¥ xj ) ƒè b1 (xj ¬¥ xj ).  
   
  k‚Äú1  
   
  This nonlinear inequality is then linearized in order to obtain a valid linear inequality. The following linearizations are applied to each nonlinear term xk xj : ‚Äì xk xj is replaced by wkj if the relation xk xj ƒè wkj exists in the problem and a1k ƒè 0, or if the relation xk xj ƒõ wkj exists and a1k ƒõ 0, or if the relation xk xj ‚Äú wkj exists in the problem, ‚Äì if k ‚Äú j P I b , then xk xj ‚Äú x2j ‚Äú xj , ‚Äì if k ‚Äú j R I b , then xk xj ‚Äú x2j is outer approximated by a secant from above or by a tangent from below, depending on the sign of the coeÔ¨Écient, ‚Äì if k ‚Ä∞ j, k, j P I b and one of the four clique constraints is implied by the linear constraints (1b), then: xk ` xj ƒè 1 ‚áí xk xj ‚Äú 0; xk ¬¥ xj ƒè 0 ‚áí xk xj ‚Äú xk ; ¬¥xk ` xj ƒè 0 ‚áí xk xj ‚Äú xj ; ¬¥xk ¬¥ xj ƒè ¬¥1 ‚áí xk xj ‚Äú xj ` xj ¬¥ 1, ‚Äì otherwise, xk xj is replaced by its McCormick relaxation.  
   
  EÔ¨Écient Separation of RLT Cuts for Implicit and Explicit Bilinear Products  
   
  17  
   
  The key step is the replacing of products xk xj with the variables wkj . When a bilinear product relation xk xj ƒ≥ wkj does not hold for the current relaxation solution, this substitution may lead to an increase in the violation of the inequality, thus possibly producing a cut that is violated by the relaxation solution. n In the case that we have a linear equation constraint k‚Äú1 a1k xk ‚Äú b1 and all nonlinear terms can be replaced using equality relations, then RLT produces an n the equation constraint is treated as two inequalities nequation cut. Otherwise, a x ƒè b and 1 k‚Äú1 1k k k‚Äú1 a1k xk ƒõ b1 to produce inequality cuts.  
   
  3  
   
  Detection of Implicit Products  
   
  Consider a product relation wij ‚Äú xi xj , where xi is binary. It can be equivalently rewritten as two implications: xi ‚Äú 0 ‚áí wij ‚Äú 0 and xi ‚Äú 1 ‚áí wij ‚Äú xj . With the use of the big-M technique, these implications can be represented as linear constraints, provided that the bounds of xj are Ô¨Ånite: wij ¬¥ xj xi ƒè 0, wij ¬¥ xj ¬¥ xj xi ƒè ¬¥xj  
   
  (3a)  
   
  ¬¥wij ` xj xi ƒè 0, ¬¥ wij ` xj ` xj xi ƒè xj .  
   
  (3b)  
   
  Linear constraints with binary variables can be analyzed in order to detect constraint pairs of the forms (3). The method can be generalized to allow for bilinear relations of the following form, with A, B, C, D P R: Axi ` Bwij ` Cxj ` D ƒ≥ xi xj  
   
  (4)  
   
  Theorem 1. Consider two linear constraints depending on the same three variables xi , xj and wij , where xi is binary: a1 xi ` b1 wij ` c1 xj ƒè d1 , a2 xi ` b2 wij ` c2 xj ƒè d2 .  
   
  (5a) (5b)  
   
  If b1 b2 ƒÖ 0 and Œ≥ ‚Äú c2 b1 ¬¥ b2 c1 ‚Ä∞ 0, then these constraints imply the following product relation: (1/Œ≥)((b2 (a1 ¬¥ d1 ) ` b1 d2 )xi ` b1 b2 wij ` b1 c2 xj ¬¥ b1 d2 ) ƒè xi xj if b1 /Œ≥ ƒõ 0, (1/Œ≥)((b2 (a1 ¬¥ d1 ) ` b1 d2 )xi ` b2 b2 wij ` b1 c2 xj ¬¥ b1 d2 ) ƒõ xi xj if b1 /Œ≥ ƒè 0. Proof. We begin by writing the bilinear relation (4), treating its coeÔ¨Écients and inequality sign as unknown, and reformulating it as two implications: xi ‚Äú 1 xi ‚Äú 0  
   
  ‚áí Bwij ` (C ¬¥ 1)xj ƒ≥ ¬¥D ¬¥ A, ‚áí Bwij ` Cxj ƒ≥ ¬¥D,  
   
  (6a) (6b)  
   
  18  
   
  K. Bestuzheva et al.  
   
  where the inequality sign must be identical in both implied inequalities. Similarly, we rewrite constraints (5) with scaling parameters Œ± and Œ≤: xi ‚Äú 1  
   
  ‚áí Œ±b1 wij ` Œ±c1 xj ƒ≥ Œ±(d1 ¬¥ a1 ),  
   
  (7a)  
   
  xi ‚Äú 0  
   
  ‚áí Œ≤b2 wij ` Œ≤c2 xj ƒ≥ Œ≤d2 ,  
   
  (7b)  
   
  where the inequality signs depend on the signs of Œ± and Œ≤. The goal is to Ô¨Ånd the coeÔ¨Écients A, B, C and D and the inequality sign. We require that coeÔ¨Écients and inequality signs in implications (6) and (7) match. Solving the resulting system yields: b1 b2 ƒÖ 0, A ‚Äú (1/Œ≥)(b2 (a1 ¬¥ d1 ) ` b1 d2 ) B ‚Äú b1 b2 /Œ≥, C ‚Äú b1 c2 /Œ≥, D ‚Äú ¬¥b1 d2 /Œ≥, Œ≥ ‚Ä∞ 0, where Œ≥ ‚Äú c2 b1 ¬¥ b2 c1 and the inequality sign is ‚Äòƒè‚Äò if b1/Œ≥ ƒõ 0, and ‚Äòƒõ‚Äò if b1/Œ≥ ƒè 0. Thus, the bilinear relation stated in this theorem is obtained.   Although the conditions of the theorem are suÔ¨Écient for the bilinear product relation to be implied by the linear constraints, in practice more conditions are checked before deriving such a relation. In particular: ‚Äì At least one of the coeÔ¨Écients a1 and a2 must be nonzero. Otherwise, the product relation is always implied by the linear constraints, including when 0 ƒÉ xi ƒÉ 1. ‚Äì The signs of the coeÔ¨Écients of the binary variable xi must be diÔ¨Äerent, that is, one linear relation is more restrictive when xi ‚Äú 1 and the other when xi ‚Äú 0. While this is not necessary for the non-redundancy of the derived product relation, by requiring this we focus on stronger implications (for instance, for a linear relation a1 xi ` b1 wij ` c1 xj ƒè d1 with a1 ƒÖ 0, we use the more restrictive implication xi ‚Äú 1 ‚áí b1 wij ` c1 xj ƒè d1 ¬¥ a1 rather than the less restrictive implication xi ‚Äú 0 ‚áí b1 wij ` c1 xj ƒè d1 ). In separation, the product relation (4) is treated similarly to product relations wij ƒ≥ xi xj , with the linear left-hand side Axi ` Bwij ` Cxj ` D being used instead of the individual auxiliary variable wij . The detection algorithm searches for suitable pairs of linear relations and derives product relations from them. Let xi , as before, be a binary variable. The following relation types are considered as candidates for the Ô¨Årst relation in such a pair: implied relations of the form xi ‚Äú Œæ ‚áí Àúb1 wij ` cÀú1 xj ƒè dÀú1 , where Œæ ‚Äú 0 or Œæ ‚Äú 1; and implied bounds of the form xi ‚Äú Œæ ‚áí wij ƒè dÀú1 . The second relation in a pair can be: an implied relation of the form xi ‚Äú Œæ ‚áí Àúb2 wij ` cÀú2 xj ƒè dÀú2 , where Œæ is the complement of Œæ; if wij is non-binary, an implied bound of the form xi ‚Äú Œæ ‚áí wij ƒè dÀú2 ; if wij is binary, a clique containing the complement of xi if Œæ ‚Äú 1 or xi if Œæ ‚Äú 0, and wij or its complement; a constraint wij ; or a global bound on wij . Cliques are constraints of  on xj and the form: kPJ xk ` kPJ (1 ¬¥ xk ) ƒè 1, where J ƒé I b , J ƒé I b and J X J ‚Äú H.  
   
  EÔ¨Écient Separation of RLT Cuts for Implicit and Explicit Bilinear Products  
   
  4  
   
  19  
   
  Separation Algorithm  
   
  We present a new algorithm for separating RLT cuts within an LP-based branchand-bound solver. The branch-and-bound algorithm builds LP relaxations of problem (1) by constructing linear underestimators of functions g in the constraint g(x, w) ƒè 0 and McCormick inequalities for constraints (1d). Let (x‚àó , w‚àó ) be the solution of an LP relaxation at a node of the branchand-bound tree, and suppose that (x‚àó , w‚àó ) violates the relation xi xj ƒ≥ wij for some i, j P I w . Separation algorithms generate cuts that separate (x‚àó , w‚àó ) from the feasible region, and add those cuts to the solver‚Äôs cut storage. The standard separation algorithm, which will serve as a baseline for comparisons, iterates over all linear constraints. For each constraint, it iterates over all variables xj that participate in bilinear relations and generates RLT cuts using bound factors of xj . Violated cuts are added to the MINLP solver‚Äôs cut storage. 4.1  
   
  Row Marking ()  
   
  (u)  
   
  Let the bound factors be denoted as fj (x) ‚Äú xj ¬¥ xj and fj (x) ‚Äú xj ¬¥ xj . Consider a linear constraint multiplied by a bound factor: (.)  
   
  (.)  
   
  fj (x)ar x ƒè fj (x)br .  
   
  (8)  
   
  The ith nonlinear term is ari xi xj , where ari ‚Äú ari when multiplying by (xj ¬¥ xj ) and ari ‚Äú ¬¥ari when multiplying by (xj ¬¥ xj ). Following the procedure described in Sect. 2, RLT may replace the product xi xj with wij . The product can also be replaced with a linear expression, but this does not change the reasoning, and we will only use wij in this section. ‚àó ‚Ä∞ x‚àói x‚àój , then such a replacement will change the violation of (8). The If wij terms whose replacement will increase the violation are of interest, that is, the terms where: ‚àó . ari x‚àói x‚àój ƒè ari wij This determines the choice of bound factors to multiply with: ‚àó x‚àói x‚àój ƒÉ wij ‚áí  
   
  multiply by (xj ¬¥ xj ) if ari ƒÖ 0, multiply by (xj ¬¥ xj ) if ari ƒÉ 0,  
   
  ‚àó x‚àói x‚àój ƒÖ wij ‚áí  
   
  multiply by (xj ¬¥ xj ) if ari ƒÖ 0, multiply by (xj ¬¥ xj ) if ari ƒÉ 0.  
   
  The separation algorithm is initialized by creating data structures to enable eÔ¨Écient access to 1) all variables appearing in bilinear products together with a given variable and 2) the bilinear product relation involving two given variables. For each variable xi , linear rows are marked in order to inform the separation algorithms which bound factors of xi they should be multiplied with, if any. The algorithm can work with inequality rows in both ‚Äòƒè‚Äô and ‚Äòƒõ‚Äô forms as well as equation rows. For each bilinear product xi xj , the row marking algorithm iterates over all linear rows that contain xj with a nonzero coeÔ¨Écient. These rows are stored in a sparse array and have one of the following marks:  
   
  20  
   
  K. Bestuzheva et al.  
   
  ‚àó ‚Äì MARK LT: the row contains a term arj xj such that arj x‚àói x‚àój ƒÉ arj wij ; ‚àó ; ‚Äì MARK GT: the row contains a term arj xj such that arj x‚àói x‚àój ƒÖ arj wij ‚Äì MARK BOTH: the row contains terms Ô¨Åtting both cases above.  
   
  Row marks are represented by integer values 1, 2 and 3, respectively, and are stored in two sparse arrays, row idcs and row marks, the Ô¨Årst storing sorted row indices and the second storing the corresponding marks. In the algorithm below, we use the notation mark(r) to denote accessing the mark of row r by performing a search in row idcs and retrieving the corresponding entry in row marks. We also deÔ¨Åne a sparse matrix W with entries wij .  
   
  1 2 3 4 5 6 7 8 9 10  
   
  Input: x‚àó , w‚àó , W marks :‚Äú H for i P I p , j P nnz(wi ) do for r such that j P nnz(ar ) do if r R marks then marks ‚Üê r mark(r) :‚Äú 0 ‚àó if arj x‚àói x‚àój ƒÉ arj wij then mark(r) |‚Äú MARK LT else mark(r) |‚Äú MARK GT  
   
  The algorithm iterates over the sparse array of marked rows and generates RLT cuts for the following combinations of linear rows and bound factors: ‚Äì If mark ‚Äú MARK LT, then ‚Äúƒè‚Äù constraints are multiplied with the lower bound factor and ‚Äúƒõ‚Äù constraints are multiplied with the upper bound factor; ‚Äì If mark ‚Äú MARK GT, then ‚Äúƒè‚Äù constraints are multiplied with the upper bound factor and ‚Äúƒõ‚Äù constraints are multiplied with the lower bound factor; ‚Äì If mark ‚Äú MARK BOTH, then both ‚Äúƒè‚Äù and ‚Äúƒõ‚Äù constraints are multiplied with both the lower and the upper bound factors; ‚Äì Marked equality constraints are always multiplied with xi itself. 4.2  
   
  Projection Filtering  
   
  If at least one of the variables xi and xj has a value equal to one of its bounds, then the McCormick relaxation (2) is tight for the relation wij ‚Äú xi xj . Therefore, if xi or xj is at a bound and the McCormick inequalities are satisÔ¨Åed, then the product relation is also satisÔ¨Åed. We describe the equality case here, and the reasoning is analogous for the inequality case of xi xj ƒ≥ wij .  
   
  EÔ¨Écient Separation of RLT Cuts for Implicit and Explicit Bilinear Products  
   
  21  
   
  Consider the linear system Ax ƒè b projected onto the set of variables whose values are not equal to either of their bounds.   ark xk ƒè br ¬¥ ark x‚àók , ‚àÄr P 1, . . . , m(l) , kPJ 1  
   
  kPJ 2  
   
  where J 1 ƒé I is the set of all problem variables whose values in the solution x‚àó of the current LP relaxation are not equal to one of their bounds, and J 2 ‚Äú I \ J 1 . Violation is then Ô¨Årst checked for RLT cuts generated based on the projected linear system. Only if such a cut, which we will refer to as a projected RLT cut, is violated, then the RLT cut for the same bound factor and the corresponding constraint in the original linear system will be constructed. Since x‚àó is a basic LP solution, in practice either x‚àók ‚Äú xk or x‚àók ‚Äú xk holds for many of the variables, and the projected system often has a considerably smaller size than the original system. (.) In the projected system multiplied with a bound factor fj (x): (.)  
   
  fj (x) ¬∑  
   
    
   
  (.)  
   
  ark xk ƒè fj (x)(br ¬¥  
   
  kPJ 1  
   
    
   
  ark x‚àók ), ‚àÄr P 1, . . . , m(l) ,  
   
  kPJ 2  
   
  the only nonlinear terms are xj xk with k P J 1 , and therefore, no substitution xi xk ‚Üí wik is performed for k P J 2 . If the McCormick inequalities for xi , xk and ‚àó for k P J 2 , and checking the violation of a projected wik hold, then x‚àói x‚àók ‚Äú wik RLT cut is equivalent to checking the violation of a full RLT cut. Depending on the solver, McCormick inequalities may not be satisÔ¨Åed at ‚àó for some k P J 2 , but these vio(x‚àó , w‚àó ). Thus, it is possible that x‚àói x‚àók ‚Ä∞ wik lations will not contribute to the violation of the projected RLT cut. In this case, projection Ô¨Åltering has an additional eÔ¨Äect: for violated bilinear products involving variables whose values in x‚àó are at bound, the violation of the product will be disregarded when checking the violation of RLT cuts. Thus, adding McCormick cuts will be prioritized over adding RLT cuts.  
   
  5  
   
  Computational Results  
   
  5.1  
   
  Setup  
   
  We tested the proposed methods on the MINLPLib1 [6] test set and a test set comprised of instances from MIPLIB3, MIPLIB 2003, 2010 and 2017 [10], and Cor@l [15]. These test sets consist of 1846 MINLP instances and 666 MILP instances, respectively. After structure detection experiments, only those instances were chosen for performance evaluations that either contain bilinear products in the problem formulation, or where our algorithm derived bilinear products. This resulted in test sets of 1357 MINLP instances and 195 MILP instances. 1  
   
  https://www.minlplib.org.  
   
  22  
   
  K. Bestuzheva et al.  
   
  The algorithms were implemented in the MINLP solver SCIP [4]. We used a development branch of SCIP (githash dd6c54a9d7) compiled with SoPlex 5.0.2.4, CppAD 20180000.0, PaPILO 1.0.0.1, bliss 0.73p and Ipopt 3.12.11. The experiments were carried out on a cluster of Dell Poweredge M620 blades with 2.50GHz Intel Xeon CPU E5-2670 v2 CPUs, with 2 CPUs and 64GB memory per node. The time limit was set to one hour, the optimality gap tolerance to 10¬¥4 for MINLP instances and to 10¬¥6 for MILP instances, and the following settings were used for all runs, where applicable: ‚Äì The maximum number of unknown bilinear terms that a product of a row and a bound factor can have in order to be used was 20. Unknown bilinear terms are those terms xi xj for which no wij variable exists in the problem, or its extended formulation which SCIP constructs for the purposes of creating an LP relaxation of an MINLP. ‚Äì RLT cut separation was called every 10 nodes of the branch-and-bound tree. ‚Äì In every non-root node where separation was called, 1 round of separation was performed. In the root node, 10 separation rounds were performed. ‚Äì Unless speciÔ¨Åed otherwise, implicit product detection and projection Ô¨Åltering were enabled and the new separation algorithm was used. 5.2  
   
  Impact of RLT Cuts  
   
  In this subsection we evaluate the performance impact of RLT cuts. The following settings were used: OÔ¨Ä - RLT cuts are disabled; ERLT - RLT cuts are added for products that exist explicitly in the problem; IERLT - RLT cuts are added for both implicit and explicit products. The setting ERLT was used for the MINLP test set only, since MILP instances contain no explicitly deÔ¨Åned bilinear products. We report overall numbers of instances, numbers of solved instances, shifted geometric means of the runtime (shift 1 s), and the number of nodes in the branch-and-bound tree (shift 100 nodes), and relative diÔ¨Äerences between settings. Additionally, we report results on subsets of instances. AÔ¨Äected instances are instances where a change of setting leads to a diÔ¨Äerence in the solving process, indicated by a diÔ¨Äerence in the number of LP iterations. [x,timelim] denotes the subset of instances which took the solver at least x seconds to solve with at least one setting, and were solved to optimality with at least one setting. All-optimal is the subset of instances which were solved to optimality with both settings. Table 1 shows the impact of RLT cuts on MILP performance. We observe a slight increase in time when RLT cuts are enabled, and a slight decrease in number of nodes. The diÔ¨Äerence is more pronounced on ‚ÄòdiÔ¨Écult‚Äô instances: a 9% decrease in number of nodes on subset [100,timelim] and 28% on subset [1000,timelim], and a decrease of 21% in the mean time on subset [1000,timelim]. Table 2 reports the impact of RLT cuts derived from explicitly deÔ¨Åned bilinear products. A substantial decrease in running times and tree sizes is observed across all subsets, with a 15% decrease in the mean time and a 19% decrease in  
   
  EÔ¨Écient Separation of RLT Cuts for Implicit and Explicit Bilinear Products  
   
  23  
   
  Table 1. Impact of RLT cuts: MILP instances  
   
  Subset  
   
  OÔ¨Ä instances solved time  
   
  IERLT nodes solved time  
   
  IERLT /OÔ¨Ä nodes time nodes  
   
  All AÔ¨Äected [0,tilim] [1,tilim] [10,tilim] [100,tilim] [1000,tilim] All-optimal  
   
  971 581 915 832 590 329 96 899  
   
  1339 1936 1127 1451 3604 9121 43060 1033  
   
  1310 1877 1104 1420 3495 8333 31104 1053  
   
  905 571 905 822 580 319 88 899  
   
  45.2 48.8 34.4 47.2 126.8 439.1 1436.7 31.9  
   
  909 575 909 826 584 323 92 899  
   
  46.7 51.2 35.6 49.0 133.9 430.7 1140.9 34.1  
   
  1.03 1.05 1.04 1.04 1.06 0.98 0.79 1.07  
   
  0.98 0.97 0.98 0.98 0.97 0.91 0.72 1.02  
   
  the number of nodes on all instances, and a 87% decrease in the mean time and a 88% decrease in the number of nodes on the subset [1000,timelim]. 223 more instances are solved with ERLT than with OÔ¨Ä. Table 3 evaluates the impact of RLT cuts derived from implicit bilinear products. Similarly to MILP instances, the mean time slightly increases and the mean number of nodes slightly decreases when additional RLT cuts are enabled, but on MINLP instances, the increase in the mean time persists across diÔ¨Äerent instance subsets and is most pronounced (9%) on the subset [100,timelim], and the number of nodes increases by 6 ¬¥ 7% on subsets [100,timelim] and [1000,timelim]. Table 2. Impact of RLT cuts derived from explicit products: MINLP instances  
   
  Subset  
   
  OÔ¨Ä instances solved time  
   
  nodes  
   
  ERLT solved time  
   
  ERLT /OÔ¨Ä nodes time nodes  
   
  All AÔ¨Äected [0,timelim] [1,timelim] [10,timelim] [100,tilim] [1000,tilim] All-optimal  
   
  6622 2018 4568 3124 1871 861 284 4423  
   
  3375 1534 778 2081 6729 35991 196466 627  
   
  4557 2007 4557 3113 1860 850 273 4423  
   
  2719 3375 569 1383 3745 12873 23541 518  
   
  4434 1884 4434 2990 1737 727 150 4423  
   
  67.5 18.5 10.5 28.3 108.3 519.7 2354.8 8.6  
   
  57.5 10.6 8.2 20.0 63.6 196.1 297.6 7.5  
   
  0.85 0.57 0.78 0.71 0.59 0.38 0.13 0.87  
   
  0.81 0.51 0.73 0.67 0.56 0.36 0.12 0.83  
   
  24  
   
  K. Bestuzheva et al. Table 3. Impact of RLT cuts derived from implicit products: MINLP instances  
   
  Subset  
   
  ERLT instances solved time  
   
  IERLT nodes solved time  
   
  nodes  
   
  ERLT /IERLT time nodes  
   
  All AÔ¨Äected [0,timelim] [1,timelim] [10,timelim] [100,tilim] [1000,tilim] All-optimal  
   
  6622 1738 4601 3141 1828 706 192 4532  
   
  2686 1567 587 1436 4157 22875 99996 540  
   
  2638 1494 576 1398 4012 24339 107006 529  
   
  1.01 1.02 1.01 1.01 1.02 1.09 1.03 1.02  
   
  4565 1702 4565 3105 1792 670 156 4532  
   
  57.0 24.2 8.5 21.1 74.1 359.9 1493.3 7.7  
   
  4568 1705 4568 3108 1795 673 159 4532  
   
  57.4 24.8 8.6 21.4 75.4 390.4 1544.7 7.8  
   
  0.98 0.95 0.98 0.97 0.97 1.06 1.07 0.98  
   
  Table 4 reports numbers of instances for which a change in the root node dual 1 , where bound was observed, where the relative diÔ¨Äerence is quantiÔ¨Åed as Œ≥2Œ≥¬¥Œ≥ 1 Œ≥1 and Œ≥2 are root node dual bounds obtained with the Ô¨Årst and second settings, respectively. The range of the change is speciÔ¨Åed in the column ‚ÄòDiÔ¨Äerence‚Äô, and each column shows numbers of instances for which one or the other setting provided a better dual bound, within given range. The results of comparisons OÔ¨Ä /IERLT for MILP instances and OÔ¨Ä /ERLT for MINLP instances are consistent with the eÔ¨Äect of RLT cuts on performance observed in Tables 1 and 2. Interestingly, IERLT performs better than ERLT in terms of root node dual bound quality. Thus, RLT cuts derived from implicit products in MINLP instances tend to improve root node relaxations. 5.3  
   
  Separation  
   
  In Table 5, the setting Marking-oÔ¨Ä employs the standard separation algorithm, and Marking-on enables the row marking and projection Ô¨Åltering algorithms described in Sect. 4. Row marking reduces the running time by 63% on MILP instances, by 70% on aÔ¨Äected MILP instances, by 12% on MINLP instances and by 22% on aÔ¨Äected MINLP instances. The number of nodes increases when row marking is enabled because, due to the decreased separation time, the solver can Table 4. Root node dual bound diÔ¨Äerences MILP MINLP DiÔ¨Äerence OÔ¨Ä / IERLT OÔ¨Ä / ERLT ERLT / IERLT 0.01-0.2 0.2-0.5 0.5-1.0 ƒÖ1.0  
   
  54 / 62 2/4 0/3 0/2  
   
  224 / 505 23 / 114 40 / 150 4 / 182  
   
  379 / 441 44 / 48 19 / 30 4 / 23  
   
  EÔ¨Écient Separation of RLT Cuts for Implicit and Explicit Bilinear Products  
   
  25  
   
  explore more nodes before reaching the time limit: this is conÔ¨Årmed by the fact that on the subset All-optimal, the number of nodes remains nearly unchanged. Table 5. Separation algorithm comparison  
   
  Test set subset MILP  
   
  All AÔ¨Äected All-optimal MINLP All AÔ¨Äected All-optimal  
   
  Marking-oÔ¨Ä instances solved time  
   
  Marking-on M-on/M-oÔ¨Ä nodes solved time nodes time nodes  
   
  949 728 774 6546 3031 4448  
   
  952 1118 823 2317 1062 494  
   
  780 612 774 4491 2949 4448  
   
  124.0 156.6 58.4 64.5 18.5 9.1  
   
  890 722 774 4530 2988 4448  
   
  45.2 46.4 21.2 56.4 14.3 7.4  
   
  1297 1467 829 2589 1116 502  
   
  0.37 0.30 0.36 0.88 0.78 0.81  
   
  1.37 1.31 1.01 1.12 1.05 1.02  
   
  Table 6 analyzes the percentage of time that RLT cut separation takes out of overall running time, showing the arithmetic mean and maximum over all instances, numbers of instances for which the percentage was within a given interval, and numbers of failures. The average percentage is reduced from 54.2% to 2.8% for MILP instances and from 15.1% to 2.4% for MINLP instances, and the maximum percentage is reduced from 99.6% to 71.6% for MILP instances, but remains at 100% for MINLP instances. The numbers of failures are reduced with Marking-on, mainly due to avoiding failures that occur when the solver runs out of memory. Table 6. Separation times Test set Setting  
   
  avg % max % N(ƒÉ 5%) N(5-20%) N(20-50%) N(50-100%) fail  
   
  Marking-oÔ¨Ä Marking-on MINLP Marking-oÔ¨Ä Marking-on  
   
  54.2 2.8 15.1 2.4  
   
  MILP  
   
  99.6 71.6 100.0 100.0  
   
  121 853 3647 6140  
   
  117 87 1265 376  
   
  169 31 1111 204  
   
  552 4 685 49  
   
  16 0 77 16  
   
  Projection Ô¨Åltering has a minor impact on performance. When comparing the runs where projection Ô¨Åltering is disabled and enabled, the relative diÔ¨Äerence in time and nodes does not exceed 1% on both MILP and MINLP instances, except for aÔ¨Äected MILP instances where projection Ô¨Åltering decreases the number of nodes by 4%. This is possibly occurring due to the eÔ¨Äect of prioritizing McCormick inequalities to RLT cuts when enforcing derived product relations. The number of solved instances remains almost unchanged, with one less instance being solved on both MILP and MINLP test sets when projection Ô¨Åltering is enabled.  
   
  26  
   
  5.4  
   
  K. Bestuzheva et al.  
   
  Experiments with Gurobi  
   
  In this subsection we present results obtained by running the mixed-integer quadratically-constrained programming solver Gurobi 10.0 beta [11]. The algorithms for implicit product detection and RLT cut separation are the same as in SCIP, although implementation details may diÔ¨Äer between the solvers. The internal Gurobi test set was used, comprised of models sent by Gurobi customers and models from public benchmarks, chosen in a way that avoids overrepresenting any particular problem class. Whenever RLT cuts were enabled, so was implicit product detection, row marking and projection Ô¨Åltering. The time limit was set to 10000 s. Table 7 shows, for both MILP and MINLP test sets, the numbers of instances in the test sets and their subsets, and the ratios of shifted geometric means of running time and number of nodes of the runs with RLT cuts enabled, to the same means obtained with RLT cuts disabled. The last row shows the numbers of instances solved with one setting and unsolved with the other, that is, for example, ‚ÄúRLT oÔ¨Ä: +41‚Äù means that 41 instances were solved with the setting ‚ÄúoÔ¨Ä‚Äù that were not solved with the setting ‚Äúon‚Äù. While the results cannot be directly compared to those obtained with SCIP due to the diÔ¨Äerences in the experimental setup, we observe the same tendencies. In particular, RLT cuts yield small improvements on MILP instances which become more pronounced on subsets [100,timelim] and [1000,timelim], and larger improvements are observed on MINLP instances both in terms of geometric means and numbers of solved instances. Relative diÔ¨Äerences are comparable to those observed with SCIP, but the impact of RLT cuts is larger in Gurobi, and no slowdown is observed with Gurobi on any subset of MILP instances. Table 7. Results obtained with Gurobi 10.0 beta  
   
  5.5  
   
  Subset  
   
  MILP instances timeR nodeR  
   
  MINLP instances timeR nodeR  
   
  All [0,timelim] [1,timelim] [10,timelim] [100,timelim] [1000,timelim]  
   
  5011 4830 3332 2410 1391 512  
   
  806 505 280 188 114 79  
   
  Solved  
   
  RLT oÔ¨Ä: +41; RLT on: +37 RLT oÔ¨Ä: +2; RLT on: +35  
   
  0.99 0.99 0.98 0.97 0.95 0.89  
   
  0.97 0.96 0.96 0.93 0.91 0.83  
   
  0.73 0.57 0.40 0.29 0.17 0.12  
   
  0.57 0.44 0.29 0.20 0.11 0.08  
   
  Summary  
   
  RLT cuts yield a considerable performance improvement for MINLP problems and a small performance improvement for MILP problems which becomes more  
   
  EÔ¨Écient Separation of RLT Cuts for Implicit and Explicit Bilinear Products  
   
  27  
   
  pronounced for challenging instances. The new separation algorithm drastically reduces the computational burden of RLT cut separation and is essential to an eÔ¨Écient implementation of RLT cuts, enabling the speedups we observed when activating RLT. Acknowledgements. The work for this article has been conducted within the Research Campus Modal funded by the German Federal Ministry of Education and Research (BMBF grant numbers 05M14ZAM, 05M20ZBM).  
   
  References 1. Adams, W.P., Sherali, H.D.: A tight linearization and an algorithm for zero-one quadratic programming problems. Manage. Sci. 32(10), 1274‚Äì1290 (1986) 2. Adams, W.P., Sherali, H.D.: Linearization strategies for a class of zero-one mixed integer programming problems. Oper. Res. 38(2), 217‚Äì226 (1990) 3. Adams, W.P., Sherali, H.D.: Mixed-integer bilinear programming problems. Math. Program. 59(1), 279‚Äì305 (1993) 4. Bestuzheva, K., et al.: Enabling research through the SCIP optimization suite 8.0. ACM Trans. Math. Softw. (2023). https://doi.org/10.1145/3585516 5. Buchheim, C., Wiegele, A., Zheng, L.: Exact algorithms for the quadratic linear ordering problem. INFORMS J. Comput. 22(1), 168‚Äì177 (2010) 6. Bussieck, M.R., Drud, A.S., Meeraus, A.: MINLPLib - a collection of test models for mixed-integer nonlinear programming. INFORMS J. Comput. 15(1), 114‚Äì119 (2003). https://doi.org/10.1287/ijoc.15.1.114.15159 7. Castillo, I., Westerlund, J., Emet, S., Westerlund, T.: Optimization of block layout design problems with unequal areas: a comparison of MILP and MINLP optimization methods. Comput. Chem. Eng. 30(1), 54‚Äì69 (2005) 8. Frank, S., Steponavice, I., Rebennack, S.: Optimal power Ô¨Çow: a bibliographic survey I. Energy syst. 3(3), 221‚Äì258 (2012) 9. Frank, S., Steponavice, I., Rebennack, S.: Optimal power Ô¨Çow: a bibliographic survey II. Energy Syst. 3(3), 259‚Äì289 (2012) 10. Gleixner, A., et al.: MIPLIB 2017: data-driven compilation of the 6th mixed-integer programming library. Math. Program. Comput. 13(3), 443‚Äì490 (2021). https://doi. org/10.1007/s12532-020-00194-3 11. Gurobi Optimization, LLC: Gurobi Optimizer Reference Manual (2022). https:// www.gurobi.com 12. Liberti, L.: Reduction constraints for the global optimization of NLPs. Int. Trans. Oper. Res. 11(1), 33‚Äì41 (2004) 13. Liberti, L.: Reformulation and convex relaxation techniques for global optimization. Ph.D. thesis. Springer (2004) 14. Liberti, L.: Linearity embedded in nonconvex programs. J. Global Optim. 33(2), 157‚Äì196 (2005) 15. Linderoth, J.T., Ralphs, T.K.: Noncommercial software for mixed-integer linear programming. Integer Programm. Theory Practice 3, 253‚Äì303 (2005) 16. McCormick, G.P.: Computability of global solutions to factorable nonconvex programs: Part I - convex underestimating problems. Math. Program. 10(1), 147‚Äì175 (1976) 17. Misener, R., Floudas, C.A.: Advances for the pooling problem: modeling, global optimization, and computational studies. Appl. Comput. Math. 8(1), 3‚Äì22 (2009)  
   
  28  
   
  K. Bestuzheva et al.  
   
  18. Sherali, H.D., Adams, W.P.: A hierarchy of relaxations between the continuous and convex hull representations for zero-one programming problems. SIAM J. Discret. Math. 3(3), 411‚Äì430 (1990) 19. Sherali, H.D., Adams, W.P.: A hierarchy of relaxations and convex hull characterizations for mixed-integer zero-one programming problems. Discret. Appl. Math. 52(1), 83‚Äì106 (1994) 20. Sherali, H.D., Adams, W.P.: A reformulation-linearization technique (RLT) for semi-inÔ¨Ånite and convex programs under mixed 0‚Äì1 and general discrete restrictions. Discret. Appl. Math. 157(6), 1319‚Äì1333 (2009) 21. Sherali, H.D., Alameddine, A.: A new reformulation-linearization technique for bilinear programming problems. J. Global Optim. 2(4), 379‚Äì410 (1992) 22. Sherali, H.D., Dalkiran, E., Liberti, L.: Reduced RLT representations for nonconvex polynomial programming problems. J. Global Optim. 52(3), 447‚Äì469 (2012) 23. Sherali, H.D., Smith, J.C., Adams, W.P.: Reduced Ô¨Årst-level representations via the reformulation-linearization technique: results, counterexamples, and computations. Discret. Appl. Math. 101(1‚Äì3), 247‚Äì267 (2000) 24. Sherali, H.D., Tuncbilek, C.H.: A global optimization algorithm for polynomial programming problems using a reformulation-linearization technique. J. Global Optim. 2(1), 101‚Äì112 (1992) 25. Sherali, H.D., Tuncbilek, C.H.: New reformulation linearization/convexiÔ¨Åcation relaxations for univariate and multivariate polynomial programming problems. Oper. Res. Lett. 21(1), 1‚Äì9 (1997)  
   
  A Nearly Optimal Randomized Algorithm for Explorable Heap Selection Sander Borst1(B) , Daniel Dadush1(B) , Sophie Huiberts2(B) , and Danish Kashaev1(B) 1  
   
  Centrum Wiskunde & Informatica (CWI), Amsterdam, The Netherlands {sander.borst,dadush,danish.kashaev}@cwi.nl 2 Columbia University, New York, USA [email protected]   
   
  Abstract. Explorable heap selection is the problem of selecting the nth smallest value in a binary heap. The key values can only be accessed by traversing through the underlying inÔ¨Ånite binary tree, and the complexity of the algorithm is measured by the total distance traveled in the tree (each edge has unit cost). This problem was originally proposed as a model to study search strategies for the branch-and-bound algorithm with storage restrictions by Karp, Saks and Widgerson (FOCS ‚Äô86), who ‚àö log n)) time algorithms gave deterministic and randomized n ¬∑ exp(O( ‚àö using O(log(n)2.5 ) and O( log n) space respectively. We present a new randomized algorithm with running time O(n log(n)3 ) against an oblivious adversary using O(log n) space, substantially improving the previous best randomized running time at the expense of slightly increased space usage. We also show an Œ©(log(n)n/ log(log(n))) lower bound for any algorithm that solves the problem in the same amount of space, indicating that our algorithm is nearly optimal.  
   
  1  
   
  Introduction  
   
  Many important problems in theoretical computer science are fundamentally search problems. The objective of these problems is to Ô¨Ånd a certain solution from the search space. In this paper we analyze a search problem that we call explorable heap selection. The problem is related to the famous branch-andbound algorithm and was originally proposed by Karp, Widgerson and Saks [13] to model node selection for branch-and-bound with low space-complexity. Furthermore, as we will explain later, the problem remains practically relevant to branch-and-bound even in the full space setting.  
   
  Due to space limitations, we have omitted several proofs. These can be found in [7]. This project has received funding from the European Research Council (ERC) under the European Union‚Äôs Horizon 2020 research and innovation programme (grant agreement QIP‚Äì805241). c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 29‚Äì43, 2023. https://doi.org/10.1007/978-3-031-32726-1_3  
   
  30  
   
  S. Borst et al.  
   
  The explorable heap selection problem1 is an online graph exploration problem for an agent on a rooted (possibly inÔ¨Ånite) binary tree. The nodes of the tree are labeled by distinct real numbers (the key values) that increase along every path starting from the root. The tree can thus be thought of as a min-heap. Starting at the root, the agent‚Äôs objective is to select the nth smallest value in the tree while minimizing the distance traveled, where each edge of the tree has unit travel cost. The key value of a node is only revealed when the agent visits it, and the problem thus has an online nature. When the agent learns the key value of a node, it still does not know the rank of this value. A simple selection strategy is to use the best-Ô¨Årst rule, which repeatedly explores the unexplored node whose parent has the smallest key value. While this rule is optimal in terms of the number of nodes that it explores, namely Œò(n), the distance traveled by the agent can be far from optimal. In the worstcase, an agent using this rule will need to travel a distance of Œò(n2 ) to Ô¨Ånd the nth smallest value. A simple bad example for this rule is to consider a rooted tree consisting of two paths (which one can extend to a binary tree), where the two paths are consecutively labeled by all positive even and odd integers respectively. Improving on the best-Ô¨Årst strategy, Karp, Saksand Wigderson [13]  gave a randomized algorithm with expected cost n¬∑exp(O( log(n))) using O( log(n)) working space. They also showed how to make the algorithm deterministic using O(log(n)2.5 ) space. In this work, our main contribution is an improved randomized algorithm with expected cost O(n log(n)3 ) using O(log(n)) space. Given the Œ©(n) lower bound, our travel cost is optimal up to logarithmic factors. Furthermore we show that any algorithm for explorable heap selection that only uses s units of memory, must take at least n ¬∑ logs (n) time in expectation. An interesting open problem is the question whether a superlinear lower bound also holds without any restriction on the memory usage. To clarify the memory model, it is assumed that any key value and O(log n) bit integer can be stored using O(1) space. We also assume that maintaining the current position in the tree does not take up memory. Furthermore, we assume that key value comparisons and moving across an edge of the tree require O(1) time. Under these assumptions, the running times of the above algorithms are in fact proportional to their travel cost. Throughout the paper, we will thus use travel cost and running time interchangeably. Motivation. The motivation to look at this problem comes from the branchand-bound algorithm. This is a well-known algorithm that can be used for solving many types of problems. In particular, it is often used to solve integer linear programs (ILP), which are of the form arg min{c x : x ‚àà Zn , Ax ‚â§ b}. In that setting, branch-and-bound works by Ô¨Årst solving the linear programming (LP) relaxation, which does not have integrality constraints. The value of the solution to the relaxation forms a lower bound on the objective value of the original problem. Moreover, if this solution only has integral components, it is also optimal for the original problem. Otherwise, the algorithm chooses a ÀÜi is not integral. It then creates two component xi for which the solution value x xi  or xi ‚â• ÀÜ xi . This new subproblems, by either adding the constraint xi ‚â§ ÀÜ 1  
   
  [13] did not name the problem, so we have given a descriptive name here.  
   
  A Nearly Optimal Randomized Algorithm  
   
  31  
   
  operation is called branching. The tree of subproblems, in which the children of a problem are created by the branching operation, is called the branch-andbound tree. Because a subproblem contains more constraints than its parent, its objective value is greater or equal to the one of its parent. At the core, the algorithm consists of two important components: the branching rule and the node selection rule. The branching rule determines how to split up a problem into subproblems, by choosing a variable to branch on. Substantial research has been done on branching rules, see, e.g., [2,4,14,15]. The node selection rule decides which subproblem to solve next. Not much theoretical research has been done on the choice of the node selection rule. Traditionally, the best-Ô¨Årst strategy is thought to be optimal from a theoretical perspective because this rule minimizes the number of nodes that need to be visited. However, to eÔ¨Éciently implement this rule the solver needs space proportional to the number of explored nodes, because all of them need to be kept in memory. In contrast to this, a simple strategy like depth-Ô¨Årst search only needs to store the current solution. Unfortunately, performing a depth-Ô¨Årst search can lead to an arbitrarily bad running time. This was the original motivation for introducing the explorable heap selection problem [13]. By guessing the number N of branch-and-bound nodes whose LP values are at most that of the optimal IP solution (which can be done via successive doubling), a search strategy for this problem can be directly interpreted as a node selection rule. The algorithm that they introduced be used to implement branch-and-bound eÔ¨É  can therefore log(N ) space. ciently in only O In practice, computers are usually able to store all explored nodes of the branch-and-bound tree in memory. However, many MIP-solvers still make use of a hybrid method that consists of both depth-Ô¨Årst and best-Ô¨Årst searches. This is not only done because depth-Ô¨Årst search uses less memory, but also because it is often faster. Experimental studies have conÔ¨Årmed that the depth-Ô¨Årst strategy is in many cases faster than best-Ô¨Årst one [8]. This seems contradictory, because the running time of best-Ô¨Årst search is often thought to be theoretically optimal. In part, this contradiction can be explained by the fact that actual IP-solvers often employ complementary techniques and heuristics on top of branch-andbound, which might beneÔ¨Åt from depth-Ô¨Årst searches. Additionally, a best-Ô¨Årst search can hop between diÔ¨Äerent parts of the tree, while a depth Ô¨Årst search subsequently explores nodes that are very close to each other. In the latter case, the LP-solver can start from a very similar state, which is known as warm starting. This is faster for a variety of technical reasons [1]. For example, this can be the case when the LP-solver makes use of the LU-factorization of the optimal basis matrix [16]. Through the use of dynamic algorithms, computing this can be done faster if a factorization for a similar LP-basis is known [19]. Because of its large size, MIP-solvers will often not store the LU-factorization for all nodes in the tree. This makes it beneÔ¨Åcial to move between similar nodes in the branch-and-bound tree. Furthermore, moving from one part of the tree to another means that the solver needs to undo and redo many bound changes, which also takes up time. Hence, the amount of distance traveled between nodes  
   
  32  
   
  S. Borst et al.  
   
  in the tree is a metric that inÔ¨Çuences the running time. This can also be observed when running the academic MIP-solver SCIP [12]. The explorable heap selection problem captures these beneÔ¨Åts of locality by measuring the running time in terms of the amount of travel through the tree. Therefore, we argue that this problem is still relevant for the choice of a node selection rule, even if all nodes can be stored in memory. Related Work. The explorable heap selection problem was Ô¨Årst introduced in [13]. Their result was later applied to prove an upper bound on the parallel running time of branch-and-bound [18]. When random access to the heap is provided at constant cost, selecting the nth value in the heap can be done by a deterministic algorithm in O(n) time by using an additional O(n) memory for auxilliary data structures [11]. The explorable heap selection problem can be thought of as a search game [3] and bears some similarity to the cow path problem. In the cow path problem, an agent explores an unweighted unlabeled graph in search of a target node. The location of the target node is unknown, but when the agent visits a node they are told whether or not that node is the target. The performance of an algorithm is judged by the ratio of the number of visited nodes to the distance of the target from the agent‚Äôs starting point. In both the cow path problem and the explorable heap selection problem, the cost of backtracking and retracing paths is an important consideration. The cow path problem on inÔ¨Ånite b-ary trees was studied in [9] under the assumption that when present at a node the agent can obtain an estimate on that node‚Äôs distance to the target. Other explorable graph problems exist without a target, where typically the graph itself is unknown at the outset. There is an extensive literature on exploration both in graphs and in the plane. Models have been studied, in which one tried to minimize either the distance traveled or the amount of used memory. For more information we refer to [6,20] and the references therein. Outline. In Sect. 2 we formally introduce the explorable heap selection problem and any notation we will use. In Sect. 3 we introduce a new algorithm for solving this problem and provide a running time analysis. In Sect. 4 we give a lower bound on the complexity of solving explorable heap selection using a limited amount of memory.  
   
  2  
   
  The Explorable Heap Selection Problem  
   
  In this section we introduce the formal model for the explorable heap selection problem. The input to the algorithm is an inÔ¨Ånite binary tree T = (V, E), where each node v ‚àà V has an associated real value, denoted by val(v) ‚àà R. We assume that all the values are distinct and that for each node in the tree, the values of its children are larger than its own value. The binary tree T is thus a heap. We want to Ô¨Ånd the nth smallest value in this tree. This may be seen as an online graph exploration problem where an agent can move in the tree and learns the value of a node each time he explores it. At each time step, the agent resides  
   
  A Nearly Optimal Randomized Algorithm  
   
  33  
   
  at a vertex v ‚àà V and may decide to move to either the left child, the right child or the parent of v (if it exists, i.e. if v is not the root of the tree). Each traversal of an edge costs one unit of time, and the complexity of an algorithm for this problem is thus measured by the total traveled distance in the binary tree. The algorithm is also allowed to store values in memory. For a node v ‚àà V , also per abuse of notation written v ‚àà T , we denote by T (v) the subtree of T rooted at v. For a tree T and a value L ‚àà R, we deÔ¨Åne the subtree TL := {v ‚àà T | val(v) ‚â§ L}. We denote the nth smallest value in T by SELECTT (n). This is the quantity that we are interested in Ô¨Ånding algorithmically. We say that a value V ‚àà R is good for a tree T if V ‚â§ SELECTT (n) and bad otherwise. Similarly, we call a node v ‚àà T good if val(v) ‚â§ SELECTT (n) and bad otherwise. We use [k] to refer to the set {1, . . . , k}. When we write log(n), we assume the base of the logarithm to be 2. We will often instruct the agent to move to an already discovered good vertex v ‚àà V . The way this is done algorithmically is by saving val(v) in memory and starting a depth Ô¨Årst search at the root, turning back every time a value strictly bigger than val(v) is encountered until Ô¨Ånally Ô¨Ånding val(v). This takes at most O(n) time, since we assume v to be a good node. If we instruct the agent to go back to the root from a certain vertex v ‚àà V , this is simply done by traveling back in the tree, choosing to go to the parent of the current node at each step. In later sections, we will often say that a subroutine takes a subtree T (v) as input. This implicitly means that we in fact pass it val(v) as input, make the agent travel to v ‚àà T using the previously described procedure, call the subroutine from that position in the tree, and travel back to the original position at the end of the execution. Because the subroutine knows the value val(v) of the root of T (v) , it can ensure it never leaves the subtree T (v) , thus making it possible to recurse on a subtree as if it were a rooted tree by itself. We will sometimes want to pick a value uniformly at random from a set of values {V1 , . . . , Vk } of unknown size that arrives in a streaming fashion, for instance when we traverse a part of the tree T by doing a depth Ô¨Årst search. That is, we see the value Vi at the ith time step, but do not longer have access to it in memory once we move on to Vi+1 . This can be done by generating random values {X1 , . . . , Xk } where, at the ith time step, Xi = Vi with probability 1/i, and Xi = Xi‚àí1 otherwise. It is easy to check that Xk is a uniformly distributed sample from {V1 , . . . , Vk }.  
   
  3  
   
  A New Algorithm  
   
  The authors of [13] presented a deterministic algorithm that solves  the explorable  heap selection problem in n ¬∑ exp(O( log(n))) time and O(n log(n)) space. By replacing the binary search that is used in the algorithm by a randomized variant, they can decrease the space requirements. This  way, they get a randomized log(n))) and space complexity algorithm with expected running time n¬∑exp(O(  O( log(n)). Alternatively, the binary search can be implemented in a deterministic way by [17] to get the same running time with O(log(n)2.5 ) space.  
   
  34  
   
  S. Borst et al.  
   
  We present a randomized algorithm with a running time O(n log(n)3 ) and space complexity O(log(n)). Unlike the algorithms mentioned before, our algorithm fundamentally relies on randomness to bound its running time. This bound only holds when the algorithm is run on a tree with labels that are Ô¨Åxed before the execution of the algorithm. That is, the tree must be generated by an adversary that is oblivious to the choices made by the algorithm. This is a stronger assumption than is needed for the algorithm that is given in [13], which also works against adaptive adversaries. An adaptive adversary is able to defer the decision of the node label to the time that the node is explored. Note that this distinction does not really matter for the application of the algorithm as a node selection rule in branch-and-bound, since there the node labels are Ô¨Åxed because they are derived from the integer program. Theorem 1. There exists a randomized algorithm that solves the explorable heap selection problem, with expected running time O(n log(n)3 ) and O(log(n)) space. The explorable heap selection problem can be seen as the problem of Ô¨Ånding all n good nodes. Both our method and that of [13] function by Ô¨Årst identifying a subtree consisting of only good nodes. The children of the leaves of this subtree are called ‚Äúroots‚Äù and the subtree is extended by Ô¨Ånding a number of new good nodes under these roots in multiple rounds. ‚àö In [13] this is done by running O(c 2 log(n) ) diÔ¨Äerent ‚àö rounds, for some constant c > 1. In each round, the algorithm Ô¨Ånds n/c 2 log(n) new good nodes. These nodes are found by recursively exploring each active root and using binary search on the observed values to discover which of these values are good. Which active roots are recursively explored furtherdepends on which values are good. The recursion in the algorithm is at most O( log(n)) levels deep, which is where the space complexity bound comes from. In our algorithm, we take a diÔ¨Äerent approach. We will call our algorithm consecutively with n = 1, 2, 4, 8, . . . . Hence, for a call to the algorithm, we can assume that we have already found at least n/2 good nodes. These nodes form a subtree of the original tree T . In each round, our algorithm chooses a random root under this subtree and Ô¨Ånds every good node under it. It does so by doing recursive subcalls to the main algorithm on this root with values n = 1, 2, 4, 8, . . .. As soon as the recursively obtained node is a bad node, the algorithm stops searching the subtree of this root, since it is guaranteed that all the good nodes there have been found. The largest good value that is found can then be used to Ô¨Ånd additional good nodes under the other roots without recursive calls, through a simple depth-Ô¨Årst search. Assuming that the node values were Ô¨Åxed in advance, we expect this largest good value to be greater than half of the other roots‚Äô largest good values. Similarly, we expect its smallest bad value to be smaller than half of the other roots‚Äô smallest bad values. By this principle, a sizeable fraction of the roots can, in expectation, be ruled out from getting a recursive call. Each round a new random root is selected until all good nodes have been found. This algorithm allows us to eÔ¨Äectively perform binary search  
   
  A Nearly Optimal Randomized Algorithm  
   
  35  
   
  on the list of roots, ordered by the largest good value contained in each of their subtrees in O(log n) rounds, and the same list ordered by the smallest bad values (Lemma 2). Bounding the expected number of good nodes found using recursive subcalls requires a subtle induction on two parameters (Lemma 1): both n and the number of good nodes that have been identiÔ¨Åed so far. 3.1  
   
  The Algorithm  
   
  The Extend procedure is the core of our algorithm. It Ô¨Ånds the nth smallest value in the tree, under the condition that the kth smallest value L0 is provided to the algorithm for some k ‚â• n/2. Using this procedure, SELECT(n) can be solved by consecutively calling Extend(T , ni , ki , Li ) with parameters (ni , ki ) = (2i , 2i‚àí1 ) for i ‚àà {1, . . . , log(n)}. Algorithm 1. The Extend procedure 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25:  
   
  Input: T : tree which is to be explored. n ‚àà N: total number of good values to be found in the tree T , satisfying n ‚â• 2. k ‚àà N: number of good values already found in the tree T , satisfying k ‚â• n/2. L0 ‚àà R: value satisfying DFS(T, L0 , n) = k. Output: the nth smallest value in T . procedure Extend(T , n, k, L0 ) L ‚Üê L0 , U ‚Üê ‚àû while k < n do r ‚Üê random element from Roots(T , L0 , L, U) L ‚Üê max(L, val(r)) k ‚Üê DFS(T , L , n) // count the number of values ‚â§ L in T c ‚Üê DFS(T (r) , L , n) // count the number of values ‚â§ L in T (r) c ‚Üê min(n‚àík +c, 2c) // increase the number of values to be found in T (r) while k < n do // loop until it is certiÔ¨Åed that SELECTT (n) ‚â§ L L ‚Üê Extend(T (r) , c , c, L ) k ‚Üê DFS(T , L , n) c ‚Üê c c ‚Üê min(n ‚àí k + c, 2c) end while Àú UÀú ‚Üê GoodValues(T, T (r) , L , n) // Ô¨Ånd the good values in T (r) L, Àú Àú L ‚Üê max(L, L), U ‚Üê min(U, U) k ‚Üê DFS(T , L, n) // compute the number of good values found in T end while return L end procedure  
   
  Let us describe a few invariants from the Extend procedure. ‚Äì L and U are respectively lower and upper bounds on SELECTT (n) during the whole execution of the procedure. ‚Äì The integer k counts the number of values ‚â§ L in the full tree T . ‚Äì After an iteration of the inner while loop, L is set to the cth smallest value in T (r) . The variable c then corresponds to the next value we would like to Ô¨Ånd  
   
  36  
   
  S. Borst et al.  
   
  in T (r) if we were to continue the search. Note that c ‚â§ 2c, enforcing that the recursive call to Extend satisÔ¨Åes its precondition, and that c ‚â§ n ‚àí (k  ‚àí c) implies that (k  ‚àí c) + c ‚â§ n, which implies that the recursive subcall will not spend time searching for a value that is known in advance to be bad. ‚Äì k  always counts the number of values ‚â§ L in the full tree T . It is important to observe that this is a global parameter, and does not only count values below the current root. Moreover, k  ‚â• n implies that we can stop searching below the current root, since it is guaranteed that all good values in T (r) have been found, i.e., L is larger than all the good values in T (r) . We now describe the subroutines used in the Extend procedure. The Procedure DFS. The procedure DFS is a variant of depth Ô¨Årst search. The input to the procedure is T , a cutoÔ¨Ä value L ‚àà R and an integer n ‚àà N. The procedure returns the number of vertices in T whose value is at most L. It achieves that by exploring the tree T in a depth Ô¨Årst search manner, starting at the root and turning back as soon as a node w ‚àà T such that val(w) > L is encountered. Moreover, if the number of nodes whose value is at most L exceeds n during the search, the algorithm stops and returns n + 1. The algorithm output is the following integer. whose value is at most L:    DFS(T, L, n) := min TL , n + 1 . Observe that the DFS procedure allows us to check whether a node w ‚àà T is a good node, i.e. whether val(w) ‚â§ SELECTT (n). Indeed, w is good if and only if DFS(T, val(w), n) ‚â§ n. This procedure visits only nodes in TL or its direct descendants and its running time is thus O(n). The space complexity is O(1), since the only values needed to be stored in memory are L, val(v), where v is the root of the tree T , and a counter for the number of good values found so far. The Procedure Roots. The procedure Roots takes as input a tree T as well as a lower bound L0 ‚àà R on the value of SELECTT (n). We assume that the main algorithm has already found all the nodes w ‚àà T satisfying val(w) ‚â§ L0 . This means that the remaining values the main algorithm needs to Ô¨Ånd in T are all lying in the subtrees of the following nodes, that we call the L0 -roots of T (Fig. 1):    R(T, L0 ) := r ‚àà T \ TL0  r is a child of a node in TL0 In words, these are all the vertices in T one level deeper in the tree than TL0 . In addition to that, the procedure takes two other parameters L, U ‚àà R as input, which correspond to (another) lower and upper bound on the value of SELECTT (n). These lower and upper bounds will be updated during the execution of the main algorithm. A key observation is that these bounds can allow us to remove certain roots in R(T, L0 ) from consideration, in the sense that all the good values in that root‚Äôs subtree will be certiÔ¨Åed to have already been found (Fig. 2):  Roots(T, L0 , L, U) := r ‚àà R(T, L0 ) | ‚àÉw ‚àà T (r) with val(w) ‚àà (L, U)  
   
  A Nearly Optimal Randomized Algorithm  
   
  37  
   
  1 2  
   
  4  
   
  3 5  
   
  5.5  
   
  3.5 11  
   
  13  
   
  4.5  
   
  17  
   
  Fig. 1. An illustration of R(T, L0 ) with L0 = 4. The number above each vertex is its value, the blue nodes are R(T, L0 ), whereas the subtree above is TL0 . (Color Ô¨Ågure online)  
   
  5.5 5  
   
  9.5  
   
  11  
   
  7.5  
   
  13  
   
  17  
   
  6.5  
   
  8.5  
   
  4.5 8  
   
  9  
   
  12  
   
  6  
   
  15  
   
  10  
   
  7  
   
  16  
   
  19  
   
  18  
   
  Fig. 2. An illustration of the Roots procedure with L0 = 4, L = 7 and U = 10. Only two active roots remain, and are both colored in blue. The other roots are considered killed since all the good values have been found in their subtrees. (Color Ô¨Ågure online)  
   
  This subroutine can be implemented by running a depth Ô¨Årst search starting at the root of T and exploring TL with its direct descendants. Since L is known to be good, the running time is bounded by O(|TL |) = O(n). In the main algorithm, we will only need this procedure in order to select a root from Roots(T, L0 , L, U) uniformly at random, without having to store the whole list in memory. This can then be achieved in O(1) space, since one then only needs to store val(v), L0 , L and U in memory, where v is the root of the tree T . The Procedure GoodValues. The procedure GoodValues takes as input a  tree T , a subtree T (r) , a value an integer n  ‚àà N. The procedure  L ‚àà R‚â•0 and then analyzes the set S := val(w)  w ‚àà T (r) , val(w) ‚â§ L and outputs both the largest good value and the smallest bad value in that set, that we respectively call L and U. If no bad values exist in S, the algorithm sets U = ‚àû. The procedure can be implemented using a randomized binary search on the values in S, where the procedure DFS is used to check whether a value is good. This makes the procedure have a running time of O(n log n). The procedure only needs O(1) space, since the only values necessary to be kept in memory are val(v) (where v is the root of the tree T ), val(r), L, U and L , as well as the fact that every call to DFS also requires O(1) space.  
   
  38  
   
  3.2  
   
  S. Borst et al.  
   
  Proof of Correctness  
   
  Theorem 2. At the end of the execution of Algorithm 1, L is set to the nth smallest value in T . Moreover, the algorithm is guaranteed to terminate. Proof sketch. The variable L is always set to the Ô¨Årst output of the procedure GoodValues, which is always the value of a good node, implying L ‚â§ SELECTT (n). The other inequality follows since the outer while loop ends when at least n good nodes have been found in T . 3.3  
   
  Running Time Analysis  
   
  The main challenge in analyzing the running time of the algorithm is dealing with the cost of the recursive subcalls in the Extend procedure. For this we rely on two important ideas. Firstly, note that n is the index of the node value that we want to Ô¨Ånd, while k is the index of the node value that is passed to the procedure. So, the procedure needs to only Ô¨Ånd n ‚àí k new good nodes. Our runtime bound for the recursive subcalls that are performed does not just depend on n, but also on n ‚àí k. We will show that the amount of travel done in the non-recursive part of a call of Extend with parameters n and k is bounded by O(n log(n)2 ). We will charge this travel to the parent call that makes these recursive calls. Hence, a parent call that does z recursive calls with parameters (n1 , k1 ), . . . , (nz , kz ) will z be charged a cost of i=1 ni log(ni )2 . In our analysis, we will show that this sum can be upper bounded by (n ‚àí k) log(n)2 . So, for every recursive call with parameters n and k, a cost of at most (n ‚àí k) log(n)2 is incurred by the caller. Now we just need to bound the sum over (n ‚àí k) log(n)2 for all calls with parameters n and k that are done. We do this by Ô¨Årst considering a single algorithm call with parameters n and k that makes z recursive subcalls with parameters (n1 , k1 ), . . . , (nz , kz ). For such a subcall, we would like to bound the  
   
  z sum i=1 (ni ‚àíki ) log(ni )2 by (n‚àík) log(n)2 . However, this bound does not hold deterministically. Instead, we show that this bound does hold in expectation. Now we know that every layer of recursion incurs an expected cost of at most (n‚àík) log(n)2 . Because the parameter n will decrease by at least a constant factor in each layer of recursion, there can be at most O(log(n)) layers. An upper bound of O((n ‚àí k) log(n)3 ) on the expected running time of the Extend then follows for the recursive part. Combining this with the upper bound of O(n log(n)2 ) on the non-recursive part, we get a total running time of O(n log(n)2 ) + O((n ‚àí k) log(n)3 ) for the Extend procedure, which then implies a running time of O(n log(n)3 ) for the SELECT procedure.  
   
  z Let us now prove these claims. We Ô¨Årst show that the expectation of i=1 (ni ‚àí ki ) is bounded. Lemma 1. Let z be the number of recursive calls that are done in the main loop of Extend(T , n , k  , L) with parameter k ‚â• 1. For i ‚àà [z], let ni and ki be the  
   
  A Nearly Optimal Randomized Algorithm  
   
  39  
   
  values of n and k that are given as parameters to the ith such subcall. Then: E  
   
  z  
   
  ni ‚àí ki ‚â§ n ‚àí k  .  
   
  i=1  
   
  Proof. Assume we have m roots, whose order is Ô¨Åxed. For i ‚àà [z], let ri ‚àà [m] be such that  
   
  the ith recursive subcall is done on the root with index ri . For t ‚àà [m], z let st = i=1 1ri =t (ni ‚àí ki ). From the algorithm we see that when ri = t, all successive recursive calls will also be on root t, until all good nodes under this root have been found. The updated values of L and U ensure this root is never selected again after this, hence all iterations i with ri = t are consecutive. Now let at , bt be variables that respectively denote the Ô¨Årst and last indices i with ri = t. When there is no iteration i with ri = t, then at = bt = ‚àû. For two calls i and i+1 with ri = t = ri+1 , observe that after call i already ni good nodes under root t have been found. On line 15, c corresponds to ni and c corresponds to ki , hence ki+1 = ni . Therefore, the deÔ¨Ånition of st is a telescoping series and can be rewritten as st = nbt ‚àí kat , when we deÔ¨Åne k‚àû = n‚àû = 0. Let p = n ‚àí k  and let W = {w1 , . . . , wp } denote the p smallest values under T that are larger than L0 , in increasing order. Now each of these values in W will be part of a subtree generated by one of the roots. For the j ‚àà [p], let dj ‚àà [m] be such that value wj is part of the subtree of root dj . Let St = {j ‚àà [p] : dj = t}. We will  
   
  now show that  
   
  for each root  
   
  rt , we have E[st ] ‚â§ |St |. This will imply z m m that E [ i=1 ni ‚àí ki ] = t=1 E[st ] ‚â§ t=1 |St | = n ‚àí k  . First, consider a root t with t = dp . On line 9, each iteration a random root is chosen. In every iteration root dp will be among the active roots. So the probability that this root is chosen before root t is at least a half. In that case, after the iteration of root dp , L will be set to wp . Then DFS(T , L, n) returns n, and the algorithm terminates. Since no subcalls are done on root t, si = 0. If the algorithm does do subcalls i with ri = t, then consider iteration bt , the last iteration i that has ri = t. Before this iteration, already kbt good nodes under the root have been found by the algorithm. It can be seen in the algorithm on lines 13 and 18 that nbt ‚â§ 2kbt . Hence st = nbt ‚àí kat ‚â§ nbt ‚â§ 2kbt ‚â§ 2|St |. We therefore have E[st ] ‚â§ 12 ¬∑ 0 + 12 ¬∑ 2|St | = |St |. Now consider the root dp . If Sdp = [p], then sp = nbdp ‚àíkadp ‚â§ n ‚àík  = |Sdp |, because nbdp ‚â§ n and kadp ‚â• k  . If Sdp  [p], then there exists a j with dj = dp . Thus, we can deÔ¨Åne j ‚àó = max{j ‚àà [p] : dj = dp }. With probability a half, root dj ‚àó is considered before root dp . If this happens, L will be equal to wj ‚àó when root dp is selected by the algorithm. In particular, this means that kadp will be equal to j ‚àó . Recall the stated invariant that c ‚â§ n ‚àí k  = p, and hence nbdp = c ‚â§ p. Now we can see that sdp = nbdp ‚àí kadp ‚â§ p ‚àí j ‚àó . If root dp is chosen before root dj ‚àó , then consider the last recursive call bdp to Extend that we do on root dp . DeÔ¨Åne A = [k  ‚àí k  ] ‚à© Sdp , i.e. the set of all good values under root dp that have been found so far. We distinguish two cases. If k  ‚àí k  ‚â• j ‚àó , i.e., when all good values under dj ‚àó have been found, then by deÔ¨Ånition of j  , [p] \ [k  ‚àí k  ] ‚äÜ [p] \ [j  ] ‚äÜ Sdp . Because A and [p] \ [k  ‚àí k  ]  
   
  40  
   
  S. Borst et al.  
   
  are disjoint, we have |A| + (n ‚àí k  ) = |A| + |[p] \ [k  ‚àí k  ]| ‚â§ |Sdp |. Hence, we have c ‚â§ n ‚àí k  + c = n ‚àí k  + |A| ‚â§ |Sdp |. Therefore, sdp ‚â§ nbdp = c ‚â§ |Sdp |. If k  ‚àí k  < j ‚àó at the time of subcall bdp , then the last good value under dj ‚àó has yet to be found, implying that A ‚äÜ [j ‚àó ]. From the deÔ¨Ånition of j  we get [p] \ [j  ] ‚äÜ Sdp . Hence, |A| ‚â§ |Sdp | ‚àí |[p] \ [j  ]| = |Sdp | ‚àí (p ‚àí j  ). Thus c ‚â§ 2c = 2|A| ‚â§ 2(|Sdp | ‚àí (p ‚àí j ‚àó )). So, in this case we have sdp ‚â§ nbdp = c ‚â§ 2(|Sdp | ‚àí (p ‚àí j ‚àó )). Collecting the three cases above, we Ô¨Ånd that   1 1 ¬∑ (p ‚àí j ‚àó ) + ¬∑ max |Sdp |, 2(|Sdp | ‚àí (p ‚àí j ‚àó )) 2  2  1 1 1 ‚àó ‚àó |Sd | + (p ‚àí j ), |Sdp | ‚àí (p ‚àí j ) . ‚â§ max 2 p 2 2  
   
  E[sdp ] ‚â§  
   
  Lastly, by deÔ¨Ånition of j ‚àó we have [p] \ [j ‚àó ] ‚äÜ Sdp , from which it follows that p ‚àí j ‚àó ‚â§ |Sdp |. We Ô¨Ånish the proof by observing that this implies   1 1 1 ‚àó ‚àó |Sd | + (p ‚àí j ), |Sdp | ‚àí (p ‚àí j ) ‚â§ |Sdp |, max 2 p 2 2 which Ô¨Ånishes the proof. Lemma 2. The expected number of times that the outermost while-loop (at line 8) is executed by the procedure Extend is at most O(log(n)). Proof sketch. Let A (L) := {rj : j > L} and Au (U) := {rj : uj < U}. Observe that Roots(T, L0 , L, U) = A (L) ‚à™ Au (U) for any L ‚â§ U. One can show that in each iteration the size of either A (L) or Au (U) halves in expectation. Hence, in expectation at most O(log R) iterations are needed, where log R is the initial number of roots. Since R ‚â§ n, the lemma follows. By an elementary analysis of the algorithm and applying Lemma 2 we can prove the following lemma. Lemma 3. The expected running time of the non-recursive part of every call to Extend is O(n log(n)2 ). Finally we are able to prove the running time bound. Lemma 4. Let R(T, n, k) denote the running time of a call to Extend(T , n, k, L0 ). Then there exists C > 0 such that E[R(T, n, k)] ‚â§ 5C(n ‚àí k) log(n)3 + Cn log(n)2 . Proof. We will prove this with induction on r := log(n). For r = 1, we have n ‚â§ 2. In this case R is constant, proving our induction base. Now consider a call Extend(T , n, k, L0 ) and assume the induction claim is true when log(n) ‚â§ r ‚àí 1. By Lemma 3, we can choose C such that this running time is bounded by C ¬∑ n log(n).  
   
  A Nearly Optimal Randomized Algorithm  
   
  41  
   
  Now we move on to the recursive part of the algorithm. All calls to Extend(T , n, k, L0 ) with k = 0 will have n = 1, so each of these calls takes only O(1) time. Hence we can safely ignore these calls. Let z be the number of recursive calls to Extend(T , n, k, L0 ) that are done from the base call with k ‚â• 1. Let Ti , ki , ni for i ‚àà [z] be the arguments of these function calls. Note that n/2 ‚â• ni ‚â• 2 for all i. By the induction hypothesis the expectation of the recursive part of the running time is:   z   z R(Ti , ni , ki ) ‚â§ E 5C log(ni )(ni ‚àí ki ) log(ni )2 + Cni log(ni )2 E i=1  
   
  i=1  
   
  ‚â§ 5C log(n/2) E  
   
    
   
  r  
   
   ni ‚àí ki log(n)2 + C log(n)2  
   
  i=1  
   
  r  
   
  ni  
   
  i=1  
   
  ‚â§ 5C(log(n) ‚àí 1)(n ‚àí k) log(n)2 + 5C log(n)2 (n ‚àí k) ‚â§ 5C(n ‚àí k) log(n)3 .  
   
  r Here we used Lemma 1 as well as the fact that i=1 ni ‚â§ 4(n ‚àí k). To see this, z consider an arbitrary root q with s good values under it. Now i=1 1Ti =T (q) ni ‚â§  
   
  log(s+1) i 2 ‚â§ 2log(s+1) +1 ‚â§ 4s. In total there are n ‚àí k good values under i=2  
   
  z the roots, and hence i=1 ni ‚â§ 4(n ‚àí k). Adding the expected running time of the recursive and the non-recursive part, we see that E[R(T, n, k)] ‚â§ 5C(n ‚àí k) log(n)3 + Cn log(n)2 . 3.4  
   
  Space Complexity Analysis  
   
  We prove in this section the space complexity of our algorithm. Theorem 3. The procedure Extend runs in O(log(n)) space. Proof. Observe that the subroutines DFS, Roots and GoodValues all require O(1) memory, as argued in their respective analyses. Hence the space complexity of the non-recursive part of the Extend is O(1). Any recursive subcall Extend(Ti , ni , ki , Li ) resulting from a call to Extend(T , n, k, L), will have ni ‚â§ n/2. Hence, the depth of recursion is at most O(log(n)), which implies that the same is true for the space complexity.  
   
  4  
   
  Lower Bound  
   
  In general, no lower bound is known for the running time of the selection problem. However, we will show that any algorithm with space complexity at most s, has a running time of at least Œ©(n logs (n)). The tree that is used for the lower bound construction is very simple: a root with two trails of length O(n) attached to it. We will make use of a variant of the communication complexity model. In this model there are two agents A and B, that both have access to their own  
   
  42  
   
  S. Borst et al.  
   
  sets of values in SA and SB respectively. These sets are the input. We have |SA | = n + 1 and |SB | = n. Assume that all values SA and SB are diÔ¨Äerent. Now consider the problem where player A wants to compute the median of SA ‚à™ SB . Because the players only have access to their own values, they need to communicate. They use a protocol, that can consist of multiple rounds. In every odd round, player A can do computations and send units of information to player B. In every even round, player B does computations and sends information to player A. We assume that sending one value from SA or SB takes up one unit of information. Furthermore, we assume that, except for comparisons, no operations can be performed on the values. We can reduce median computation to the explorable heap selection problem. Lemma 5. If there is a algorithm that solves SELECT(3n) in f (n)n time and g space, then there is a protocol for median computation that uses f (n)/2 rounds in each of which at most g units of information are sent. By showing a lower bound on the number of necessary rounds for median computation we can now prove the lower bound. Theorem 4. The time complexity of any randomized algorithm for SELECT(n) with at most g units of storage is Œ©(n logg+1 (n)).  
   
  References 1. Achterberg, T.: Constraint Integer Programming. Ph.D. thesis, TU Berlin (2009) 2. Achterberg, T., Koch, T., Martin, A.: Branching rules revisited. Oper. Res. Lett. 33(1), 42‚Äì54 (2005). https://doi.org/10.1016/j.orl.2004.04.002 3. Alpern, S., Gal, S.: The Theory of Search Games and Rendezvous, vol. 55. Springer, New York (2006). https://doi.org/10.1007/b100809 4. Balcan, M.F., Dick, T., Sandholm, T., Vitercik, E.: Learning to branch. In: ICML (2018) 5. Banerjee, S., Cohen-Addad, V., Gupta, A., Li, Z.: Graph searching with predictions, December 2022 6. Berman, P.: On-line searching and navigation. In: Fiat, A., Woeginger, G.J. (eds.) Online Algorithms. LNCS, vol. 1442, pp. 232‚Äì241. Springer, Heidelberg (1998). https://doi.org/10.1007/BFb0029571 7. Borst, S., Dadush, D., Huiberts, S., Kashaev, D.: A nearly optimal randomized algorithm for explorable heap selection, October 2022. https://doi.org/10.48550/ arXiv.2210.05982 8. Clausen, J., Perregaard, M.: On the best search strategy in parallel branch-andbound: best-Ô¨Årst search versus lazy depth-Ô¨Årst search. Ann. Oper. Res. 90, 1‚Äì17 (1999) 9. Dasgupta, P., Chakrabarti, P.P., DeSarkar, S.C.: A near optimal algorithm for the extended cow-path problem in the presence of relative errors. In: Thiagarajan, P.S. (ed.) FSTTCS 1995. LNCS, vol. 1026, pp. 22‚Äì36. Springer, Heidelberg (1995). https://doi.org/10.1007/3-540-60692-0 38 10. Diks, K., Fraigniaud, P., Kranakis, E., Pelc, A.: Tree exploration with little memory. J. Algorithms 51(1), 38‚Äì63 (2004). https://doi.org/10.1016/j.jalgor.2003.10. 002  
   
  A Nearly Optimal Randomized Algorithm  
   
  43  
   
  11. Frederickson, G.: An optimal algorithm for selection in a min-heap. Inf. Comput. 104(2), 197‚Äì214 (1993). https://doi.org/10.1006/inco.1993.1030 12. Gleixner, A.M.: Personal communication, November 2022 13. Karp, R.M., Saks, M.E., Wigderson, A.: On a search problem related to branchand-bound procedures. In: FOCS, pp. 19‚Äì28 (1986) 14. Linderoth, J.T., Savelsbergh, M.W.P.: A computational study of search strategies for mixed integer programming. INFORMS J. Comput. 11(2), 173‚Äì187 (1999). https://doi.org/10.1287/ijoc.11.2.173 15. Lodi, A., Zarpellon, G.: On learning and branching: a survey. TOP 25(2), 207‚Äì236 (2017). https://doi.org/10.1007/s11750-017-0451-6 16. Morrison, D.R., Jacobson, S.H., Sauppe, J.J., Sewell, E.C.: Branch-and-bound algorithms: a survey of recent advances in searching, branching, and pruning. Discret. Optim. 19, 79‚Äì102 (2016). https://doi.org/10.1016/j.disopt.2016.01.005 17. Munro, J., Paterson, M.: Selection and sorting with limited storage. Theoret. Comput. Sci. 12(3), 315‚Äì323 (1980). https://doi.org/10.1016/0304-3975(80)90061-4 18. Pietracaprina, A., Pucci, G., Silvestri, F., Vandin, F.: Space-eÔ¨Écient parallel algorithms for combinatorial search problems. J. Parallel Distrib. Comput. 76, 58‚Äì65 (2015) 19. Suhl, L.M., Suhl, U.H.: A fast LU update for linear programming. Ann. Oper. Res. 43(1), 33‚Äì47 (1993). https://doi.org/10.1007/BF02025534 20. Kamphans, T.: Models and algorithms for online exploration and search. Ph.D. thesis, Rheinische Friedrich-Wilhelms-Universit¬® at Bonn (2006). https://hdl.handle. net/20.500.11811/2622  
   
  Sparse Approximation over the Cube Sabrina Bruckmeier1(B) , Christoph Hunkenschr¬® oder2 , and Robert Weismantel1 1 ETH Z¬® urich, Z¬® urich, Switzerland {sabrina.bruckmeier,robert.weismantel}@ifor.math.ethz.ch 2 TU Berlin, Berlin, Germany [email protected]   
   
  Abstract. This paper presents an analysis of the NP-hard minimization problem min{b ‚àí Ax2 : x ‚àà [0, 1]n , |supp(x)| ‚â§ œÉ}, where supp(x) := of investigation {i ‚àà [n] : xi = 0} and œÉ is a positive integer. The object  is a natural relaxation where we replace |supp(x)| ‚â§ œÉ by i xi ‚â§ œÉ. Our analysis includes a probabilistic view on when the relaxation is exact. We also consider the problem from a deterministic point of view and provide a bound on the distance between the images of optimal solutions of the original problem and its relaxation under A. This leads to an algorithm for generic matrices A ‚àà Zm√ón and achieves a polynomial running time provided that m and A‚àû are Ô¨Åxed. Keywords: Sparse Approximation Recovery  
   
  1  
   
  ¬∑ Subset Selection ¬∑ Signal  
   
  Introduction and Literature Review  
   
  Due to the recent development of machine learning, data science and signal processing, more and more data is generated, but only a part of it might be necessary in order to already make predictions in a suÔ¨Éciently good manner. Therefore, the question arises to best approximate a signal b by linear combinations of no   more than œÉ vectors Ai from a suitable dictionary A = A1 , . . . , An ‚àà Rm√ón : min Ax ‚àí b2 subject to x0 ‚â§ œÉ,  
   
  (1)  
   
  where x0 := |{i ‚àà [n] : xi = 0}|. Additionally, many areas of application ‚Äì as for example portfolio selection theory, sparse linear discriminant analysis, general linear complementarity problems or pattern recognition ‚Äì require the solution x to satisfy certain polyhedral constraints. For instance motivated by computer tomography, lower and upper bounds on the variables are considered in [31]. While there exists a large variety of ideas how to tackle this problem, the majority of them relies on the matrix A satisfying conditions such as being sampled in a speciÔ¨Åc way or being close to behaving like an orthogonal system, that might be hard to verify. Additionally, these algorithms commonly yield results only with a certain probability or within an approximation factor that again highly depends on the properties of A. A discussion of these ideas and c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 44‚Äì57, 2023. https://doi.org/10.1007/978-3-031-32726-1_4  
   
  Sparse Approximation Over the Cube  
   
  45  
   
  diÔ¨Äerent names and variants of this problem is postponed to the end of the introduction. In this work, we develop an exact algorithm that, without these limitations on A, solves the Sparse Approximation problem in [0, 1]-variables, min Ax ‚àí b2 subject to x ‚àà [0, 1]n and x0 ‚â§ œÉ. x  
   
  (P0 )  
   
  Theorem 1. Given A ‚àà Zm√ón , b ‚àà Zm and œÉ ‚àà Z‚â•1 , we can Ô¨Ånd an opti2 mal solution x to Problem (P0 ) in (mA‚àû )O(m ) ¬∑ poly(n, ln(b1 )) arithmetic operations. Relaxing the pseudonorm  ¬∑ 0 by  ¬∑ 1 is a commonly used technique in the literature. In contrast to previous results we are able to bound the distance between the images of these solutions under A without any further assumptions on the input data and therefore derive a proximity result that ‚Äì to the best of our knowledge ‚Äì has not been known before. Theorem 2. Let x ÀÜ be an optimal solution to the following relaxation of (P0 ): min Ax ‚àí b2 subject to x ‚àà [0, 1]n and x1 ‚â§ œÉ. x  
   
  Every optimal solution x of (P0 ) satisÔ¨Åes x2 ‚â§ 2ÀÜ x ‚àí ÀÜ x1 max Ai 2 ‚â§ 2 m3/2 A‚àû , Ax ‚àí AÀÜ i=1,...,n  
   
  where ÀÜ x denotes the vector x ÀÜ rounded down component-wise. We also illuminate our approach from a probabilistic point of view. SpeciÔ¨Åcally, the hard instances are those where b is relatively close to the boundary of the polytope Q := {Ax : x ‚àà [0, 1]n , x1 ‚â§ œÉ}. Conversely, if b is deep inside Q or far outside of Q, then with high probability, an optimal solution to the relaxation solves the initial problem (P0 ). The paper is organized as follows. We conclude the introduction by providing an overview on related literature. Section 2 discusses preliminaries. The probabilistic analysis of a target vector b is carried out in Sect. 3. We then discuss a worst-case proximity bound between optimal solutions of (P0 ) and a natural relaxation in Sect. 4. This will allow us to formalize a deterministic algorithm in Sect. 5. In the literature, Problem (1) can be found under various modiÔ¨Åcations and names, see e.g. [4,5,8,27]. A common variant in the context of random measurements is often called Sparse Recovery, cf. [20], or Subset Selection for (linear) regression, cf. [10], while the name (Best) Subset Selection is generally used without further interpretation cf. [8,11,34], in contrast to Signal Recovery or Signal Reconstruction as in [3]. If the vector b can be represented exactly, the problem is called Exact Sparse Approximation or Atomic Decomposition, cf. [7,21,32,35]. Since the diÔ¨Äerences are marginal and the names in the literature not welldeÔ¨Åned, we restrain ourselves to the name Sparse Approximation for simplicity.  
   
  46  
   
  S. Bruckmeier et al.  
   
  In general, there are two common strategies used to tackle Sparse Approximation: Greedy algorithms and algorithms based on relaxations. A detailed discussion of those is beyond the scope of this paper. Let us rather put these approaches into context below. The algorithms either recover the optimal support only under certain conditions (compare [1,8,9,32]), recover it with high probability (see for example [13,34]) or approximate the solution (for instance [10,14,21]). Unfortunately, because of their high computational cost most common greedy algorithms are not suÔ¨Écient for large systems, though experiments suggest that there still exist applicable greedy approaches, such as the Dropping Forward-Backward Scheme, introduced by Nguyen [26]. While the idea of relaxing the pseudonorm ¬∑0 by the norm ¬∑1 , as done for example in Basis Pursuit by Chen, Donoho and Saunders [7], might seem intuitive, for a long time the success of this method was not quite understood. This changed as Candes, Romberg and Tao [4,6] discovered and improved the Uniform Uncertainty Principle. For the usually problematic case of having not enough data points, the Dantzig Selector presented by Candes and Tao [5] yields a sophisticated estimator with high probability. Similarly, LASSO based methods, see for instance [27], either recover the support with high probability exactly under certain conditions, or fail with high probability if the conditions are not met, cf. [33]. Finally, Garmanik and Zadik [17] revealed interesting structural results, that explain the above mentioned all-or-nothing behavior. There also exists a series of papers in a similar line of thought that relaxes  ¬∑ 0 by smooth, non-decreasing, concave functions, see [12,15,16,19,22,24,29,30]. It can be shown that these relaxations converge ¬∏ ivril [35] proved that, towards the optimal solution of (P0 ). Qian et al. [28] and C unless P = N P , for a general matrix A Pareto Optimization and the two greedy algorithms, Forward Selection and Orthogonal Matching Pursuit, are almost the best we can hope for. This motivated a search for more eÔ¨Éciently solvable classes of A, cf. [3,11,18,20]. Finally, it should be mentioned that there exists a variety of Branch-and-Bound algorithms whose success though is in general only tested experimentally, see [2,23].  
   
  2  
   
  Preliminaries  
   
  Let A ‚àà Zm√ón and b ‚àà Zm . Moreover, let supp(x) denote the support of x, i.e. supp(x) := {i ‚àà [n] : xi = 0} and set x0 := |supp(x)|. For the rest of the paper, x denotes an optimal solution for (P0 ) for a given integer œÉ ‚àà Z‚â•1 . A natural convex relaxation of (P0 ) is given by min Ax ‚àí b2 subject to x ‚àà [0, 1]n and x1 ‚â§ œÉ. x  
   
  (P1 )  
   
  An optimal solution to (P1 ) will be denoted by x ÀÜ throughout the paper. When m = 1, there exists an optimal solution x ÀÜ for (P1 ) that has at most one fractional variable (see Lemma 4). This solution is also feasible for (P0 ), and hence optimal. The idea of our approach is to establish a proximity result for AÀÜ x and Ax respectively, that we can exploit algorithmically. This proximity bound depends on m which comes as no surprise, given that the problem is NP-hard even for  
   
  Sparse Approximation Over the Cube  
   
  47  
   
  Ô¨Åxed values of m. The latter statement can be veriÔ¨Åed by reducing the NP-hard partition problem to an instance of (P0 ). Theorem 3. The problem (P0 ) is NP-hard, even if m = 2. A simple but important ingredient of our proximity theorem is the following fact that can be derived from elementary linear programming theory. Lemma 4 (Few fractional entries). 1. Let x be a feasible point for (P0 ). There exists a solution x such that Ax = Ax with at most m fractional entries. 2. Let x be a feasible point for (P1 ). There exists a solution x such that Ax = Ax with at most m fractional entries. Proof. 1. Let x be a solution of (P0 ) and denote S = supp(x). Let AS denote the submatrix of A comprising the columns with indices in S. The set PS (x) := {y ‚àà R|S| : Ax = AS y, 0 ‚â§ y ‚â§ 1} is a polytope. It is non-empty since x ‚àà P , hence it has at least one vertex v. By standard LP theory, at least |S| ‚àí m inequalities of the form 0 ‚â§ y ‚â§ 1 are tight at v. It follows that v has at most m fractional entries. The vertex v can easily be extended to a solution x of (P0 ) by adding zero-entries. 2. Given the solution x to (P1 ), consider the optimization problem n  min{ yi : y ‚àà P{1,...,n} (x)}. i=1  
   
  Let v be an optimal vertex solution. From Part 1.  v has at most nm fractional n entries. Since x is feasible for the above problem, i=1 vi ‚â§ i=1 xi ‚â§ œÉ.   
   
  3  
   
  The 1 -Relaxation for Random Targets b  
   
  In order to shed some light on Problem (P0 ) and its natural convex relaxation (P1 ) we Ô¨Årst provide a probabilistic analysis to what extend optimal solutions of (P1 ) already solve (P0 ). Let Q := {Ax ‚àà Rm : x ‚àà [0, 1]n , x1 ‚â§ œÉ} be the set of all points we can represent with the 1 -relaxation. This section deals with the question which vectors b are ‚Äúeasy‚Äù target vectors. It turns out that if b is ‚Äúdeep‚Äù inside Q or far outside of Q, then the corresponding instances of (P0 ) are easy with very high probability. In fact, there almost always exist optimal solutions of (P1 ) that are already feasible for (P0 ) and hence optimal. Conversely, if b is close to the boundary of Q, then the probability that an optimal solution of (P1 ) solves (P0 ) is almost 0. Theorem 5. Let A ‚àà Rm√ón and œÉ ‚â• m be an integer. If b ‚àà there exists x ‚àà [0, 1]n with x 0 ‚â§ œÉ and Ax = b.  
   
  œÉ‚àím+1 Q, œÉ  
   
  then  
   
  48  
   
  S. Bruckmeier et al.  
   
  Proof. If b ‚àà œÉ‚àím+1 Q, there exists a vector x ÀÜ ‚àà [0, œÉ‚àím+1 ]n such that b = AÀÜ x œÉ œÉ and ÀÜ x1 ‚â§ œÉ‚àím+1. Let v be a vertex of {x ‚àà [0, 1]n : Ax = b, x1 ‚â§ œÉ‚àím+1}, which contains x ÀÜ. According to the constraint x1 ‚â§ œÉ ‚àí m + 1, v has at most œÉ ‚àí m + 1 integral non-zero entries. By Lemma 4, v has at most m fractional entries. However, if there are fractional entries present, we can only have œÉ ‚àí m  integral entries, thus, v0 ‚â§ œÉ. Intuitively, the following theorem states that if b is sampled far away from Q, then (P1 ) provides a solution to (P0 ) as well. Here B := {x ‚àà Rm : x2 ‚â§ 1} denotes the Euclidean unit ball. Theorem 6. Let A ‚àà Rm√ón , Œª ‚â• 0 and œÉ ‚â• 1 be an integer. If b is sampled uniformly at random from the convex set Q + ŒªB, then with probability at least  m Œª ‚àö Œª + œÉ mA‚àû there exists x ‚àà {0, 1}n that is optimal for (P1 ) and (P0 ). Proof. DeÔ¨Åne P := {x ‚àà [0, 1]n : x1 ‚â§ œÉ}, hence we have Q = {Ax : x ‚àà P }. Observe that all vertices of P are in {0, 1}n , and as a consequence any vertex v of Q can be written as v = Ax with x a vertex in P that is integral.  
   
  (2)  
   
  Hence, whenever an optimal solution to min{b ‚àí x2 : x ‚àà Q} is attained by a vertex of Q, the problem (P1 ) has an optimal integral vertex solution v. Since an integral solution to (P1 ) is also feasible for (P0 ), the vector v is also optimal for (P0 ). Let V be the vertex set of Q. For v ‚àà V , denote the normal cone of v by Cv := {c ‚àà Rm : c (w ‚àí v) ‚â§ 0 ‚àÄw ‚àà Q}. Fix a vertex v and assume b ‚àà v +Cv . We next show that v is an optimal solution to min{b ‚àí x22 : x ‚àà Q}. Since b = v + c with c (v ‚àí w) ‚â• 0 for all w ‚àà Q, we obtain b ‚àí w22 = v ‚àí w + c22 = v ‚àí w22 + c22 + 2c (v ‚àí w) ‚â• c22 = b ‚àí v22 , showing that v is optimal. By Eq. (2) there exists an integral x ‚àà P such that v = Ax and hence x is optimal for (P0 ). It remains to calculate the probability that b ‚àà v + Cv for some vertex v of Q. We obtain   
   
    
   
  vol (v + Cv ) ‚à© (Q + ŒªB) = vol Cv ‚à© ŒªB v‚ààV  
   
  v‚ààV  
   
   = vol  

  Cv  
   
  ‚à© ŒªB  
   
  v‚ààV  
   
  = vol(ŒªB) = Œªm vol(B).  
   
  Sparse Approximation Over the Cube  
   
  49  
   
  In the second to last equality we used that the normal cones Cv tile the space Rm . ‚àö Let Œº > 0 be a constant s.t. Q ‚äÜ ŒºB, e.g. Œº = œÉ mA‚àû . We have the containment Q + ŒªB ‚äÜ ŒºB + ŒªB = (Œº + Œª)B that allows us to estimate vol(Q + ŒªB) ‚â§ (Œª + Œº)m vol(B). The probability that b is sampled in one of the normal cones is therefore  m Œª Œªm vol(ŒªB) ‚àö ‚â• ‚â• . vol(Q + ŒªB) (Œª + Œº)m Œª + œÉ mA‚àû  ‚àö Let us brieÔ¨Çy comment on the probability quantity œÅ := (Œª/(Œª + œÉ mA‚àû ))m . If we choose Œª = 2m3/2 œÉA‚àû in Theorem 6, then œÅ ‚â• 1/2, as one can verify with Bernoulli‚Äôs inequality. Figure 1 depicts the geometry underlying the proof of Theorem 6. The vector b1 is sampled from the dotted area and hence, an optimal solution of (P1 ) may use 2 fractional entries, and thus have support œÉ + 1. On the other hand, the vector b2 is sampled from the dashed area, which leads to the solution of (P1 ) corresponding to a vertex of Q. In the second case (P1 ) has an integral solution, which automatically solves (P0 ).  
   
  Fig. 1. The sampling of the vector b from Q + ŒªB  
   
  4  
   
  Proximity Between Optimal Solutions of (P0 ) and (P1 )  
   
  In this section we illuminate the Problems (P0 ) and (P1 ) from a deterministic point of view and develop worst-case bounds for the distance of the images of corresponding optimal solutions under A. Our point of departure is an optimal solution x ÀÜ of (P1 ). The target is to show that there exists an optimal solution x ‚àí x )2 ‚â§ 2m3/2 A‚àû . Our strategy is to deÔ¨Åne a x of (P0 ) satisfying A(ÀÜ hyperplane containing AÀÜ x in the space of target vectors b that separates b from  
   
  50  
   
  S. Bruckmeier et al.  
   
  all vectors Ax with x feasible for (P0 ). The next step is to show that if we perturb x ÀÜ along the fractional variables, we will remain in this hyperplane. This has the side-eÔ¨Äect that we can Ô¨Ånd a feasible solution for (P0 ) whose image is in the vicinity of AÀÜ x. The triangle inequality and basic geometry then come into play to establish the claimed bound. x, We introduce the hyperplane tangent to the ball B(b, b ‚àí AÀÜ x2 ) in AÀÜ x) y = (b ‚àí AÀÜ x) AÀÜ x}. H := {y ‚àà Rm : (b ‚àí AÀÜ Lemma 7. We have (b ‚àí AÀÜ x) (Ax ‚àí AÀÜ x) ‚â§ 0 for any point x feasible for (P1 ). Proof. Assume that there exists a point x feasible for (P1 ) for which the x) > 0 holds. As a convex combination the point inequality (b ‚àí AÀÜ x) (Ax ‚àí AÀÜ p := A(ÀÜ x + Œµ(x ‚àí x ÀÜ)) is feasible for (P1 ) for each Œµ ‚àà [0, 1], and we can estimate the objective value as x22 + Œµ2 A(x ‚àí x ÀÜ)22 ‚àí 2Œµ(b ‚àí AÀÜ x) (Ax ‚àí AÀÜ x) < b ‚àí AÀÜ x22 b ‚àí p22 = b ‚àí AÀÜ for Œµ small enough. This contradicts the optimality of x ÀÜ. We illustrate the argument geometrically in Fig. 2.   
   
  b  
   
  AÀÜ x p  
   
  Ax  
   
  H  
   
  Fig. 2. If H does not separate b from Ax, there is a point p closer to b than AÀÜ x.  
   
  An important property is that H contains many points that we can easily generate from x ÀÜ. This is made precise below. (Recall that Ai denotes the i-th column of A). / Z} where x ÀÜ is an optimal solution Lemma 8. DeÔ¨Åne F := {i ‚àà [n] : x ÀÜi ‚àà to (P1 ). We have   AÀÜ x+ Œªi Ai : Œªi = 0 ‚äÜ H. i‚ààF  
   
  i‚ààF  
   
  Sparse Approximation Over the Cube  
   
  51  
   
  Proof. Let v := b ‚àí AÀÜ x ‚àà Rm be the normal vector of H and let   y= Œªi ei for some Œªi ‚àà R with Œªi = 0. i‚ààF  
   
  i‚ààF  
   
  Since x ÀÜi ‚àà (0, 1) for all i ‚àà F, there exists Œµ > 0 such that both points x ÀÜ + Œµy x + Œµy ‚àí x ÀÜ) = and x ÀÜ ‚àí Œµy are feasible for (P1 ). By Lemma 7, we must have v  A(ÀÜ x + y) ‚àà H.  v  AŒµy ‚â§ 0 and ‚àív  AŒµy ‚â§ 0, resulting in v  Ay = 0. Thus, A(ÀÜ With these results we are now able to show a proximity result (Theorem 2) between AÀÜ x and Ax . Here, ÀÜ x denotes the vector x ÀÜ rounded down componentwise. Proof (Theorem 2). Given an optimal solution x ÀÜ of (P1 ), let F = {i ‚àà [n] : generality, we may assume that |F| ‚â§ m and x ÀÜi ‚àà (0, 1)}. Without loss of  ÀÜi , and construct a feasible solution y for (P0 ) F = {1, 2, . . . , |F|}. Let k := i‚ààF x from x ÀÜ as follows: ‚éß 1, 1 ‚â§ i ‚â§ k ‚é™ ‚é™ ‚é™ ‚é®k ‚àí k, i = k yi := ‚é™ 0, k + 1 ‚â§ i ‚â§ |F| ‚é™ ‚é™ ‚é© x ÀÜi , i‚àà / F. The point y satisÔ¨Åes 0 ‚â§ yi ‚â§ 1 for all i ‚àà [n] and y0 = ÀÜ x1  ‚â§ œÉ. Since   yi = x ÀÜi , (3) i‚ààF  
   
  i‚ààF  
   
  Lemma 8 implies that Ay ‚àà H and hence b ‚àí Ay22 = b ‚àí AÀÜ x22 + AÀÜ x ‚àí Ay22 . Assume Ax ‚àí AÀÜ x2 > Ay ‚àí AÀÜ x2 holds for some optimal solution x for (P0 ). Since y is feasible for (P0 ), we also know b ‚àí Ay2 ‚â• b ‚àí Ax 2 . We are now prepared to estimate (using Lemma 7 in the third line) x + AÀÜ x ‚àí Ax 22 b ‚àí Ax 22 = b ‚àí AÀÜ = b ‚àí AÀÜ x22 + AÀÜ x ‚àí Ax 22 + 2(b ‚àí AÀÜ x) (AÀÜ x ‚àí Ax ) x ‚àí Ax 22 ‚â• b ‚àí AÀÜ x22 + AÀÜ > b ‚àí AÀÜ x22 + AÀÜ x ‚àí Ay22 = b ‚àí Ay22 , showing that x is not optimal. The proof is Ô¨Ånished by observing that Eq. (3) x ‚àí ÀÜ x1 , and consequently also implies ÀÜ x ‚àí y1 ‚â§ 2ÀÜ Ay ‚àí AÀÜ x2 ‚â§ 2ÀÜ x ‚àí ÀÜ x1 max Ai 2 ‚â§ 2m3/2 A‚àû . i=1,...,n  
   
    
   
  52  
   
  5  
   
  S. Bruckmeier et al.  
   
  A Deterministic Algorithm  
   
  The results presented so far give rise to a conceptually simple algorithm. Compute an optimal solution x ÀÜ to (P1 ). According to the proximity Theorem 2, we can limit our search for an optimal right-hand side vector b = Ax in the vicinity of AÀÜ x. Since b might be fractional, we cannot enumerate all possible right-hand sides. Instead, we reÔ¨Åne our approach by decomposing x = z  + f  into its integral part z  and its fractional part f  . We Ô¨Årst guess the support F of the fractional entries, which satisÔ¨Åes |F| ‚â§ m by Lemma 4. For the remaining variables, we next establish a candidate set Z  comprising the potential vectors z  in the decomposition of x . It will be essential to determine a bound on |Z  |. This is where the proximity theorem comes into play. We now enumerate the elements of Z  and extend each of them by a vector f  whose support is in the index set F that we guessed upfront. A composition of these two solutions will provide x . This section is devoted to analyze this conceptually simple algorithm and this way shed some light on some of the details required. Before we describe the decomposition x = z  + f  in more detail, we discuss the standard obstacle in convex optimization that x ÀÜ can only be approximated. To be more precise, we call a solution x ¬Ø to (P1 ) Œµ-close, if x22 ‚â§ Œµ2 . b ‚àí A¬Ø x22 ‚àí b ‚àí AÀÜ  
   
  (4)  
   
  We obtain a canonical corollary from the proximity Theorem 2. Corollary 9. Let x ¬Ø be an Œµ-close solution of (P1 ). 1. Every optimal solution x of (P0 ) satisÔ¨Åes x‚àû ‚â§ 2 m3/2 A‚àû + Œµ. Ax ‚àí A¬Ø 2. The integral part z  of x satisÔ¨Åes x‚àû ‚â§ 3 m3/2 A‚àû + Œµ. Az  ‚àí A¬Ø Proof. We start by estimating the distance from A¬Ø x to AÀÜ x for an optimal solution x ÀÜ of (P1 ). We have x + AÀÜ x ‚àí A¬Ø x22 b ‚àí A¬Ø x22 = b ‚àí AÀÜ = b ‚àí AÀÜ x22 + AÀÜ x ‚àí A¬Ø x22 + 2(b ‚àí AÀÜ x) (AÀÜ x ‚àí A¬Ø x), where the last term is non-negative by Lemma 7. Rearranging terms, we obtain x22 ‚àí b ‚àí AÀÜ x22 ‚àí 2(b ‚àí AÀÜ x) (AÀÜ x ‚àí A¬Ø x) ‚â§ Œµ2 . AÀÜ x ‚àí A¬Ø x22 = b ‚àí A¬Ø Applying the triangle inequality and combining the above estimate with Theorem 2, we have x‚àû ‚â§ Ax ‚àí A¬Ø x2 ‚â§ Ax ‚àí AÀÜ x2 + AÀÜ x ‚àí A¬Ø x2 Ax ‚àí A¬Ø ‚â§ 2 m3/2 A‚àû + Œµ.  
   
  Sparse Approximation Over the Cube  
   
  53  
   
  For Part 2, recall that x ‚àí z  = f  with f  0 ‚â§ m, implying the inequality A(x ‚àí z  )‚àû ‚â§ mA‚àû . We obtain Az  ‚àí A¬Ø x‚àû = Az  ‚àí Ax + Ax ‚àí A¬Ø x‚àû ‚â§ A(z  ‚àí x )‚àû + Ax ‚àí A¬Ø x‚àû ‚â§ mA‚àû + 2 m3/2 A‚àû + Œµ ‚â§ 3 m3/2 A‚àû + Œµ.  We next outline the decomposition x = z  + f  . As a Ô¨Årst step, we guess the support of a minimal index set of fractional entries. 2  
   
  Lemma 10. There are (2A‚àû +1)m potentially diÔ¨Äerent index sets supp(f  ). Proof. We notice that a minimal index set of fractional entries uses distinct columns from the matrix A. There are at most (2A‚àû + 1)m distinct columns of A. Since the cardinality of a minimal index set of fractional entries is bounded 2 by m, there are at most ((2A‚àû +1)m )m = (2A‚àû +1)m potentially diÔ¨Äerent index sets.  A canonical approach would be to search for the vector f  . Then we run into the problem that our objective is nonlinear, and hence f  depends on z  . This requires us to Ô¨Årst search for an optimal z  and then use continuous optimization techniques to compute f  . In order to avoid to determine a minimal index set of fractional entries, we also allow entries with index in F to be integral. Then we need to guess only sets F ‚äÜ [n] with |F| = m. We denote by A\f  the matrix A without the columns with index in supp(f  ). The next theorem shows that we can compute a small set Z  of possible vectors for z  . Theorem 11. Let x ¬Ø be an Œµ-close solution to (P1 ). If supp(f  ) is Ô¨Åxed, we can  compute a set Z ‚äÜ {0, 1}n of candidate vectors such that x = z  + f  with z  ‚àà Z  . This requires us to solve at most (6m3/2 A‚àû + 2Œµ + 1)m linear integer programming problems. Proof. We have Az  ‚àà A¬Ø x + [‚àíDŒµ , DŒµ ]m ‚à© Zm , where DŒµ = 3m3/2 A‚àû + Œµ by  x + [‚àíDŒµ , DŒµ ]m ‚à© Zm we solve the integer feasibility Corollary 9. For every b ‚àà A¬Ø problem n‚àím  yi ‚â§ œÉ ‚àí m, y ‚àà {0, 1}n‚àím . A\f  y = b , i=1  
   
  If it has a feasible solution y, we can insert zero entries according to supp(f  ) and obtain a vector z ‚àà {0, 1}n that qualiÔ¨Åes as the vector z  . The set Z  is the set of all extended vectors z.  It remains to compose each z  ‚àà Z  with a vector f  . This is accomplished by solving a series of least-square problems. The reason why we proceed in this way is that it allows us to compute the exact vector f  as opposed to an Œµ-close solution.  
   
  54  
   
  S. Bruckmeier et al.  
   
  Lemma 12 (Extension lemma). For each z ‚àà Z  an optimal solution f to min{b ‚àí Az ‚àí Af 2 : supp(f ) ‚äÜ supp(f  ), 0 ‚â§ f ‚â§ 1} can be computed in O(3m m3 ) arithmetic operations. Proof. As fi = 0 for i ‚àà / supp(f  ), we can restrict to the matrix Af  ‚àà Zm√óm and solve the equivalent problem min{b ‚àíAf  g2 : g ‚àà [0, 1]m } for b := b‚àíAz. Without the variable bounds this is a least-square problem that can be solved in O(m3 ) arithmetic operations. Let g  be an optimal solution. We guess the sets S0 := {i : gi = 0} and S1 := {i : gi = 1}, and afterwards solve the modiÔ¨Åed least-square problem min{b ‚àí Af  g2 : gi = 0 ‚àÄi ‚àà S0 , gi = 1 ‚àÄi ‚àà S1 }. If the solution g is in [0, 1]n , its extension f ‚àà [0, 1]n qualiÔ¨Åes as f  . In the end, we pick the best among all feasible extensions. As there are 3m guesses, this Ô¨Ånishes the proof.  This completes the presentation of the main steps to prove Theorem 1. In fact, in order to obtain an optimal solution to (P0 ) one proceeds as follows. We Ô¨Årst guess the set supp(f  ), determine the set Z  and compute for every z  ‚àà Z  an optimal vector f  . The best of all those solutions solves (P0 ). As a last technicality, we have to show how to Ô¨Ånd an Œµ-close solution x ¬Ø for which we fall back on [25, Chap. 8]. ‚àö Lemma Chap. 8]). We can Ô¨Ånd a mA‚àû -close solution for (P1 )   7/213 ([25, in O n ln n2 œÉb1 arithmetic operations. Proof. We apply the results presented in [25, Chap. 8] that depend on several parameters. Let P := {x ‚àà [0, 1]n : x1 ‚â§ œÉ} denote the feasible region of (P1 ) and x ÀÜ an optimal solution. We Ô¨Årst need to estimate D := max{b ‚àí Ay22 ‚àí b ‚àí AÀÜ x22 : y ‚àà P }. For any y ‚àà P we can estimate b ‚àí Ay22 ‚àí b ‚àí AÀÜ x22 = Ay22 ‚àí AÀÜ x22 + 2b A(ÀÜ x ‚àí y) ‚â§ œÉ 2 mA2‚àû + 4b1 œÉA‚àû ‚â§ 4œÉ 2 mA2‚àû (b1 + 1), resulting in D ‚â§ 4œÉ 2 mA2‚àû (b1 + 1). As the initial point in the interior of P œÉ ¬∑ 1 where 1 denotes the that is required in [25, Chap. 8] we choose w := n+œÉ all-ones vector. Next we estimate the asymmetry coeÔ¨Écient Œ±(P : w) := max{t : w + t(w ‚àí P ) ‚äÜ P }. Since [0, nœÉ ]n ‚äÜ P ‚äÜ [0, 1]n , for t =  
   
  œÉ n  
   
  we obtain  
   
  n  w + t(w ‚àí P ) ‚äÜ w + t(w ‚àí [0, 1]n ) = 0, nœÉ ‚äÜ P,  
   
  thus Œ±(P : w) ‚â•  
   
  œÉ n.  
   
  By [25, Chap. 8, Eq. 8.1.5] we can compute a feasible   solution  
   
  2n+1 x22 ‚àíb‚àíAÀÜ x22 ‚â§ Œ¥D in O(1)(2n+1)1.5 n2 ln Œ±(P x ¬Ø of (P1 ) satisfying b‚àíA¬Ø :w)Œ¥ ‚àö 1 arithmetic operations. Finally, by choosing Œ¥ = 4œÉ2 (b Ô¨Ånding a mA ‚àû1 +1)  7/2  2  close solution takes O n ln n œÉb1 arithmetic operations.   
   
  Sparse Approximation Over the Cube  
   
  6  
   
  55  
   
  Extension  
   
  A natural generalization of our problem is to consider arbitrary upper bounds ui > 0, i.e. min Ax ‚àí b2 subject to x0 ‚â§ œÉ and 0 ‚â§ xi ‚â§ ui for all i ‚àà [n]. x  
   
  (P0 )  
   
  The natural convex relaxation of (P0 ) is given by: min Ax ‚àí b2 subject to x  
   
  n  xi i=1  
   
  ui  
   
  ‚â§ œÉ and 0 ‚â§ xi ‚â§ ui for all i ‚àà [n].  
   
  (P1 )  
   
  The results of Sects. 4 and 5 extend to this generalization in a straight-forward manner. For the algorithm it implies that the number of arithmetic operations increases by an additional factor of um ‚àû . The reason is the core of our approach: The proximity bound between optimal solutions for (P0 ) and (P1 ) respectively, increases by this factor. The proximity bound must however depend on u‚àû as the following example shows: Let n and u be even, non-negative integers. Set A := 1, œÉ := n2 and b = u2 1 where 1 denotes the all-ones vector. It can easily be checked that x ÀÜ = u2 1 is  optimal for (P1 ) while u , i ‚àà [œÉ]  xi = 2 0, i ‚àà [n] \ [œÉ] is optimal for (P0 ). This shows that any approach aiming for a logarithmic dependency on u‚àû requires techniques that are diÔ¨Äerent from the ideas presented in this paper. Acknowledgements. The second and third author acknowledge support by the Einstein Foundation Berlin.  
   
  References 1. Ament, S., Gomes, C.: On the optimality of backward regression: Sparse recovery and subset selection. In: ICASSP 2021‚Äì2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), June 2021. https://doi.org/ 10.1109/icassp39728.2021.9415082 2. Beale, E.M.L., Kendall, M.G., Mann, D.W.: The discarding of variables in multivariate analysis. Biometrika 54(3‚Äì4), 357‚Äì366 (1967). https://doi.org/10.1093/ biomet/54.3-4.357 3. Candes, E., Romberg, J., Tao, T.: Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information. IEEE Trans. Inf. Theory 52(2), 489‚Äì509 (2006). https://doi.org/10.1109/TIT.2005.862083  
   
  56  
   
  S. Bruckmeier et al.  
   
  4. Candes, E., Tao, T.: Decoding by linear programming. IEEE Trans. Inf. Theory 51(12), 4203‚Äì4215 (2005). https://doi.org/10.1109/TIT.2005.858979 5. Candes, E., Tao, T.: The Dantzig selector: statistical estimation when p is much larger than n. Ann. Stat. 35(6), 2313‚Äì2351 (2007). https://doi.org/10.1214/ 009053606000001523 6. Candes, E.J., Romberg, J.K., Tao, T.: Stable signal recovery from incomplete and inaccurate measurements. Commun. Pure Appl. Math. 59(8), 1207‚Äì1223 (2006). https://doi.org/10.1002/cpa.20124 7. Chen, S.S., Donoho, D.L., Saunders, M.A.: Atomic decomposition by basis pursuit. SIAM Rev. 43(1), 129‚Äì159 (2001). https://doi.org/10.1137/S1064827596304010 8. Couvreur, C., Bresler, Y.: On the optimality of the backward greedy algorithm for the subset selection problem. SIAM J. Matrix Anal. Appl. 21(3), 797‚Äì808 (2000). https://doi.org/10.1137/S0895479898332928 9. Das, A., Kempe, D.: Algorithms for subset selection in linear regression. In: Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing. STOC 2008, pp. 45‚Äì54. Association for Computing Machinery, New York (2008). https:// doi.org/10.1145/1374376.1374384 10. Das, A., Kempe, D.: Submodular meets spectral: greedy algorithms for subset selection, sparse approximation and dictionary selection. In: Proceedings of the 28th International Conference on International Conference on Machine Learning. ICML 2011, pp. 1057‚Äì1064. Omnipress, Madison (2011). https://doi.org/10.5555/ 3104482.3104615 11. Del Pia, A., Dey, S.S., Weismantel, R.: Subset selection in sparse matrices. SIAM J. Optim. 30(2), 1173‚Äì1190 (2020). https://doi.org/10.1137/18M1219266 12. Di Lorenzo, D., Liuzzi, G., Rinaldi, F., Schoen, F., Sciandrone, M.: A concave optimization-based approach for sparse portfolio selection. Optim. Methods Softw. 27(6), 983‚Äì1000 (2012). https://doi.org/10.1080/10556788.2011.577773 13. Donoho, D.: Compressed sensing. IEEE Trans. Inf. Theory 52(4), 1289‚Äì1306 (2006). https://doi.org/10.1109/TIT.2006.871582 14. Elenberg, E.R., Khanna, R., Dimakis, A.G., Negahban, S.: Restricted strong convexity implies weak submodularity. Ann. Stat. 46(6B), 3539‚Äì3568 (2018). https:// doi.org/10.1214/17-AOS1679 15. Feng, M., Mitchell, J.J., Pang, J.S., Shen, X., Waechter, A.: Complementarity formulations of 0 -norm optimization. Pac. J. Optim. 14(2), 273‚Äì305 (2018) 16. Fung, G.M., Mangasarian, O.L.: Equivalence of minimal 0 - and 1 -norm solutions of linear equalities, inequalities and linear programs for suÔ¨Éciently small p. J. Optim. Theory Appl. 151(1), 1‚Äì10 (2011). https://doi.org/10.1007/s10957-0119871-x 17. Gamarnik, D., Zadik, I.: High dimensional regression with binary coeÔ¨Écients. estimating squared error and a phase transition. In: Proceedings of the 2017 Conference on Learning Theory. Proceedings of Machine Learning Research, vol. 65, pp. 948‚Äì953. PMLR, 07‚Äì10 July 2017 18. Gao, J., Li, D.: A polynomial case of the cardinality-constrained quadratic optimization problem. J. Glob. Optim. 56(4), 1441‚Äì1455 (2013). https://doi.org/10. 1007/s10898-012-9853-z 19. Ge, D., Jiang, X., Ye, Y.: A note on the complexity of LP minimization. Math. Program. 129, 285‚Äì299 (2011). https://doi.org/10.1007/s10107-011-0470-2 20. Gilbert, A., Indyk, P.: Sparse recovery using sparse matrices. Proc. IEEE 98(6), 937‚Äì947 (2010). https://doi.org/10.1109/JPROC.2010.2045092  
   
  Sparse Approximation Over the Cube  
   
  57  
   
  21. Gilbert, A.C., Muthukrishnan, S., Strauss, M.J.: Approximation of functions over redundant dictionaries using coherence. In: SODA, pp. 243‚Äì252. Citeseer (2003). https://doi.org/10.5555/644108.644149 22. Migot, T., Haddou, M.: A smoothing method for sparse optimization over polyhedral sets. In: Le Thi, H.A., Pham Dinh, T., Nguyen, N.T. (eds.) Modelling, Computation and Optimization in Information Systems and Management Sciences. AISC, vol. 359, pp. 369‚Äì379. Springer, Cham (2015). https://doi.org/10.1007/978-3-31918161-5 31 23. Hocking, R.R., Leslie, R.N.: Selection of the best subset in regression analysis. Technometrics 9(4), 531‚Äì540 (1967). https://doi.org/10.1080/00401706.1967. 10490502 24. Mangasarian, O.: Minimum-support solutions of polyhedral concave programs. Optimization 45(1‚Äì4), 149‚Äì162 (1999). https://doi.org/10.1080/ 02331939908844431 25. Nesterov, Y., Nemirovski, A.: Interior-point polynomial algorithms in convex programming. In: SIAM Studies in Applied Mathematics (1994) 26. Nguyen, T.: Dropping forward-backward algorithms for feature selection. CoRR abs/1910.08007 (2019) 27. Oymak, S., Thrampoulidis, C., Hassibi, B.: The squared-error of generalized lasso: a precise analysis. In: 2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 1002‚Äì1009 (2013). https://doi.org/10. 1109/Allerton.2013.6736635 28. Qian, C., Yu, Y., Zhou, Z.H.: Subset Selection by Pareto Optimization. NIPS 2015, pp. 1774‚Äì1782. MIT Press, Cambridge (2015). https://doi.org/10.5555/2969239. 2969437 29. Rinaldi, F.: Concave programming for Ô¨Ånding sparse solutions to problems with convex constraints. Optim. Methods Softw. 26(6), 971‚Äì992 (2011). https://doi. org/10.1080/10556788.2010.511668 30. Rinaldi, F., Schoen, F.: Concave programming for minimizing the zero-norm over polyhedral sets. Comput. Optim. Appl. 46, 467‚Äì486 (07 2010). https://doi.org/10. 1007/s10589-008-9202-9 31. Teng, Y., Qi, S., Xiao, D., Xu, L., Li, J., Kang, Y.: A general solution to least squares problems with box constraints and its applications. Math. Probl. Eng. 2016 (2016) 32. Tropp, J.: Greed is good: algorithmic results for sparse approximation. IEEE Trans. Inf. Theory 50(10), 2231‚Äì2242 (2004). https://doi.org/10.1109/TIT.2004.834793 33. Wainwright, M.J.: Sharp thresholds for high-dimensional and noisy sparsity recovery using 1 -constrained quadratic programming (lasso). IEEE Trans. Inf. Theor. 55(5), 2183‚Äì2202 (2009). https://doi.org/10.1109/TIT.2009.2016018 34. Zhu, J., Wen, C., Zhu, J., Zhang, H., Wang, X.: A polynomial algorithm for bestsubset selection problem. Proc. Natl. Acad. Sci. 117(52), 33117‚Äì33123 (2020). https://doi.org/10.1073/pnas.2014241117 35. C ¬∏ ivril, A.: A note on the hardness of sparse approximation. Inf. Process. Lett. 113(14), 543‚Äì545 (2013). https://doi.org/10.1016/j.ipl.2013.04.014  
   
  Recycling Inequalities for Robust Combinatorial Optimization with Budget Uncertainty Christina B¬® using1 , Timo Gersing1(B) , and Arie M.C.A. Koster2 1  
   
  Combinatorial Optimization, RWTH Aachen University, Aachen, Germany {buesing,gersing}@combi.rwth-aachen.de 2 Discrete Optimization, RWTH Aachen University, Aachen, Germany [email protected]   
   
  Abstract. Robust combinatorial optimization with budget uncertainty is one of the most popular approaches for integrating uncertainty in optimization problems. The existence of a compact reformulation for (mixed-integer) linear programs and positive complexity results give the impression that these problems are relatively easy to solve. However, the practical performance of the reformulation is actually quite poor when solving robust integer problems due to its weak linear relaxation. To overcome the problems arising from the weak formulation, we propose a procedure to derive new classes of valid inequalities for robust binary optimization problems. For this, we recycle valid inequalities of the underlying deterministic problem such that the additional variables from the robust formulation are incorporated. The valid inequalities to be recycled may either be readily available model constraints or actual cutting planes, where we can benefit from decades of research on valid inequalities for classical optimization problems. We first demonstrate the strength of the inequalities theoretically, by proving that recycling yields a facet-defining inequality in surprisingly many cases, even if the original valid inequality was not facetdefining. Afterwards, we show in a computational study that using recycled inequalities leads to a significant improvement of the computation time when solving robust optimization problems. Keywords: Robust Optimization ¬∑ Combinatorial Optimization Integer Programming ¬∑ Polyhedral Combinatorics  
   
  1  
   
  ¬∑  
   
  Introduction  
   
  Robust optimization is a widely used approach for integrating uncertainties into optimization models. The concept of budgeted uncertainty by Bertsimas and Sim [6] has received particular attention. However, despite its popularity and the amount of research devoted to solving these kind of robust optimization problems, instances of practical size often still pose a considerable challenge for c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 58‚Äì71, 2023. https://doi.org/10.1007/978-3-031-32726-1_5  
   
  Recycling Inequalities for Robust Combinatorial Optimization  
   
  59  
   
  MILP solvers [7]. In this context, we propose a new class of valid inequalities for robust combinatorial optimization problems that are easy to compute and can lead to a signiÔ¨Åcant reduction of the computation time. Without uncertainties, the socalled nominal combinatorial optimization n problem NOM is deÔ¨Åned as min{ i‚àà[n] ci xi |Ax ‚â§ b, x ‚àà {0, 1} }, with c ‚àà Rn , A ‚àà Rm√ón , and b ‚àà Rm . Here, [n] = {1, . . . , n}. In the case of uncertainty in the objective, the coeÔ¨Écients ci are replaced by uncertain coeÔ¨Écients ci from an interval [ci , ci + cÀÜi ]. We say that ci can deviate from its nominal value ci by up to the deviation cÀÜi . Since the worst-case, in which all coeÔ¨Écients ci deviate to ci + cÀÜi , is unlikely, Bertsimas and Sim [6] deÔ¨Åne an uncertainty budget Œì ‚àà [0, n] and only consider scenarios where at most Œì  coeÔ¨Écients ci deviate to ci + cÀÜi and one coeÔ¨Écient may deviate to ci + (Œì ‚àí Œì ) cÀÜi . The robust counterpart, in which we optimize against the worst-case, can be stated as     ci xi + max cÀÜi xi (Œì ‚àí Œì ) cÀÜt xt + min i‚àà[n]  
   
  S‚à™{t}‚äÜ[n]: |S|‚â§Œì ,t‚ààS / n  
   
  i‚ààS  
   
  s.t. Ax ‚â§ b, x ‚àà {0, 1} .  
   
  Dualizing the inner maximization problem [6] yields the compact robust problem  min Œì z + (ci xi + pi ) i‚àà[n] ROB n s.t. (x, p, z) ‚àà P ROB , x ‚àà {0, 1} with P ROB  
   
  ‚éß  
   
  Ax ‚â§ b ‚é™  
   
  ‚é®  
   
  = (x, p, z) pi + z ‚â• cÀÜi xi  
   
  ‚é™ ‚é©  
   
  x ‚àà [0, 1]n , p ‚àà Rn‚â•0 , z ‚àà R‚â•0  
   
  ‚àÄi ‚àà [n]  
   
  ‚é´ ‚é™ ‚é¨ ‚é™ ‚é≠  
   
  .  
   
  Unfortunately, the formulation P ROB is quite weak, often leading to much higher computation times for solving ROB compared to NOM. In fact, the relative integrality gap of the formulation P ROB may be arbitrarily large, even if the integrality gap of the corresponding nominal problem is zero. This is shown in the following example from [7]. Example the easy problem of  selecting the cheapest of n elements  1. Consider  n . The integrality gap is zero for all c x | x = 1, x ‚àà {0, 1} min i‚àà[n] i i i‚àà[n] i n c ‚àà R . However, if we consider an instance of the uncertain counterpart ROB with c ‚â° 0, cÀÜ ‚â° 1, and Œì = 1  
   
   ‚é´ ‚éß  
   
  ‚é™ ‚é™ xi = 1  
   
  ‚é™ ‚é™ ‚é™ ‚é™ ‚é¨ ‚é®   
   
  i‚àà[n] pi  
   
  min z +  
   
  pi + z ‚â• xi ‚é™ ‚àÄi ‚àà [n]‚é™ ‚é™ ‚é™ i‚àà[n]  
   
  ‚é™ ‚é™ ‚é≠ ‚é©  
   
  x ‚àà {0, 1}n , p ‚àà Rn , z ‚àà R ‚â•0  
   
  ‚â•0  
   
  60  
   
  C. B¬® using et al.  
   
    then (x, p, z) = n1 , . . . , n1 , 0, . . . , 0, n1 is the unique optimal fractional solution of value n1 , while the objective value of an optimal integer solution is 1. Hence, 1 = n, and thus unbounded. the integrality gap is 1/n The above example shows that optimal continuous solutions for ROB tend to be highly fractional, as small values of xi allow for covering all right-hand sides cÀÜi xi in the constraints pi + z ‚â• cÀÜi xi with a small value of z, while choosing p ‚â° 0. On the one hand, such solutions are exactly what we aim for when striving for robustness, as we distribute the risk as much as possible. On the other hand, highly fractional optimal solutions for the linear relaxation imply the need for much branching, and thus a high computational eÔ¨Äort when solving ROB. Bertsimas et al. [4] as well as Fischetti and Monaci [9] tested the practical performance of the compact reformulation P ROB compared to a separation approach using an alternative formulation with exponentially many inequalities, each one modeling a scenario from the uncertainty set. Unfortunately, the alternative formulation is, despite its size, as weak as P ROB and performs worse for robust integer problems (but better for continuous problems) [4,9]. Joung and Park [16] propose cuts that dominate the classic scenario inequalities and can be separated by considering the robustness term as a submodular function and greedily solving a maximization problem over the corresponding polymatroid. Atamt¬® urk [3] addresses this issue by proposing four diÔ¨Äerent strong formulations. The strongest of these preserves the integrality gap of the nominal problem, but all four formulations are very large and hence, are outperformed by P ROB [7]. The weak relaxation can be avoided by tentatively Ô¨Åxing the variable z to diÔ¨Äerent values, resulting in a series of nominal problems NOM to be solved + 1 nominal instead [2,5,19]. Lee and Kwon [17] showed that at most n‚àíŒì 2 problems have to be solved. However, the computational eÔ¨Äort is usually still higher compared to solving ROB directly if n is large [7]. Hansknecht et al. [14] improve on this with their divide & conquer approach, in which one prunes many non-optimal values for z. In [7], non-optimal values for z are pruned even more eÔ¨Éciently by exploiting structural insights and strong linearizations derived from the following bilinear formulation  
   
  ‚éß ‚é´  
   
  Ax ‚â§ b ‚é™ ‚é™  
   
  ‚é® ‚é¨  
   
  BIL ‚àÄi ‚àà [n] . P = (x, p, z) pi + xi z ‚â• cÀÜi xi  
   
  ‚é™ ‚é™ ‚é© ‚é≠  
   
  x ‚àà [0, 1]n , p ‚àà Rn‚â•0 , z ‚àà R‚â•0 This bilinear formulation strengthens the robustness constraints pi + z ‚â• cÀÜi xi by multiplying z with xi , which is valid due to xi ‚àà {0, 1}. While the bilinearity is rather hindering for practical purposes, P BIL is theoretically very strong. In fact, there exists no polyhedral formulation P for ROB with P  P BIL . Contribution. In this paper, we use the bilinear formulation P BIL as a foundation for the new class of recycled inequalities. To obtain these, we combine the strength of the bilinear inequalities with the structural properties provided by inequalities for the nominal problem NOM. By doing so, we can use inequalities for NOM  
   
  Recycling Inequalities for Robust Combinatorial Optimization  
   
  61  
   
  a second time to improve the formulation P ROB . We show that in many cases they even deÔ¨Åne facets of the convex hull of integer-feasible solutions  
   
   n  . C ROB = conv (x, p, z) ‚àà P ROB x ‚àà {0, 1} A preliminary computational study reveals that separating recycled inequalities can lead to a drastic improvement of both integrality gap and solving times. First experiments with adapted MIPLIB [12] instances (to be presented in the journal version) conÔ¨Årm these results for a broad set of robust problems. All implemented algorithms and generated test instances are published together with a package of algorithms for solving robust combinatorial optimization problems [10] and benchmark instances [11] for those very problems. Outline. In Sect. 2, we show how to derive recycled inequalities from valid inequalities for NOM. In Sect. 3, we characterize valid inequalities for which the respective recycled inequality is facet-deÔ¨Åning. We also provide examples indicating that this applies for many well-known valid inequalities for classical optimization problems. In Sect. 4, we test recycled inequalities in a computational study, highlighting their practical value.  
   
  2  
   
  Recycling Valid Inequalities  
   
  As already mentioned, the bilinear inequalities pi + xi z ‚â• cÀÜi xi play a crucial role for our recycled inequalities. To understand their strength intuitively, we recall our observations from Example 1. There, we noticed that choosing fractional values for xi is tempting, as we are then able to meet the inequalities pi +z ‚â• cÀÜi xi with a small value of z and p ‚â° 0. However, this advantage vanishes for the bilinear inequalities pi + xi z ‚â• cÀÜi xi , as we always have z ‚â• cÀÜi for xi = 0 and pi = 0. To make use of this in practice, it would be beneÔ¨Åcial to carry over the strength of the bilinear inequalities to a linear formulation. Multiplying linear inequalities with variables as an intermediate step in order to achieve a stronger linear formulation is not a new approach. For the Reformulation-Linearization-Technique by Sherali and Adams [20], one multiplies constraints with variables and linearizes the resulting products afterwards via substitution with auxiliary variables. Our approach is diÔ¨Äerent in the sense that we don‚Äôt directly linearize the bilinear inequalities, and thus don‚Äôt create auxiliary variables. Instead, we combine several of the bilinear inequalities in order to estimate the non-linear terms against a linear term, using a valid inequality for the corresponding nominal problem. From now on, let n  
   
  C NOM = conv ({x ‚àà {0, 1} |Ax ‚â§ b}) be the convex hull of all integer nominal solutions. Then we combine the bilinear inequalities and valid inequalities for C NOM as follows.  
   
  62  
   
  C. B¬® using et al.  
   
   Theorem 1. Let i‚àà[n] œÄi xi ‚â§ œÄ0 be a valid inequality for C NOM with œÄ ‚â• 0. Then the inequality   œÄ0 z + œÄi p i ‚â• œÄi cÀÜi xi (1) i‚àà[n]  
   
  is valid for C  
   
  ROB  
   
  i‚àà[n]  
   
  .  
   
  Proof. Summing the bilinear constraints pi + xi z ‚â• cÀÜi xi , each with a factor of œÄi , we obtain    œÄi p i + œÄi xi z ‚â• œÄi cÀÜi xi , i‚àà[n]  
   
  i‚àà[n]  
   
  i‚àà[n]  
   
  whichis a valid inequality for C due to œÄ ‚â• 0. Now, since z ‚â• 0 holds, we  
   
  have i‚àà[n] œÄi xi z ‚â§ œÄ0 z, which proves the statement.  As we reuse the valid inequality i‚àà[n] œÄi xi ‚â§ œÄ0 to strengthen the formula tion P ROB , we call (1) the recycled inequality of i‚àà[n] œÄi xi ‚â§ œÄ0 . In accordance  with the requirements of Theorem 1, we call i‚àà[n] œÄi xi ‚â§ œÄ0 recyclable if it is valid for C NOM and œÄ ‚â• 0. Note that we could also derive the concept of recycled inequalities on the basis of the even stronger bilinear inequalities xi (pi + z) ‚â• cÀÜi xi , resulting from multiplying both pi and z with xi . However, after  summing the bilinear inequalities with factors œÄi , this would yield the term i‚àà[n] œÄi xi pi , which we can only  estimate against i‚àà[n] œÄi pi , yielding the same result as above. To get a better understanding for recycled inequalities, let us recognize how they compare to the bilinear inequalities over the course of their construction. First, note that the sum of the bilinear inequalities is weaker than the bilinear inequalities themselves. Hence, when separating a recycled inequality to cut-oÔ¨Ä a fractional solution (Àú x, pÀú, zÀú) ‚àà P NOM , the inequality to be recycled should only xi zÀú ‚â• cÀÜi x Àúi support indices i ‚àà [n] with œÄi > 0 for which the bilinear inequality pÀúi +Àú is violated  or tight. A second potential weakening occurs  when applying the estimation i‚àà[n] œÄi xi z ‚â§ œÄ0 z. This implies that recycling i‚àà[n] œÄi xi ‚â§ œÄ0 is especially interesting if it is binding for x Àú.  Revisit Example 1, where we can recycle the valid inequality i‚àà[n] xi ‚â§ 1  implied by the constraint i‚àà[n] xi = 1. The corresponding recycled inequality    z + i‚àà[n] pi ‚â• i‚àà[n] xi yields z + i‚àà[n] pi ‚â• 1, and thus the optimal objective value of the linear relaxation is now equal to the optimal integer objective value. This intuitively highlights the strength of the recycled inequalities in the case where both properties, a binding recyclable valid inequality and the violation of supported bilinear inequalities, coincide. ROB  
   
  3  
   
  Facet-Defining Recycled Inequalities  
   
  In this section, we show that recycled inequalities often deÔ¨Åne facets of the convex hull of the robust problem C ROB . To this end, we Ô¨Årst determine the dimension of C ROB and assume for the sake of simplicity that the sets of solutions to our problems are non-empty.  
   
  Recycling Inequalities for Robust Combinatorial Optimization  
   
  63  
   
      Lemma 1. We have dim C ROB = dim C NOM + n + 1. Proof. For a polytope P ‚äÜ Rn , the number n ‚àí dim (P ) equals the maximumnumber of linearly independent equations that are met by all x ‚àà P . Let i‚àà[n] (œâi xi + œân+i pi ) + œâ2n+1 z = œâ0 be an equation that is satisÔ¨Åed by all be raised arbitrarily and C ROB = ‚àÖ, we have (x, p, z) ‚àà C ROB . Since p and z can  œân+1 = ¬∑ ¬∑ ¬∑ = œâ2n+1 = 0 and thus i‚àà[n] œâi xi = œâ0 . Hence, the equations that are met by all (x, p, z) ‚àà C ROB are exactly the equations that are met by all x ‚àà C NOM , which implies        dim C ROB = 2n + 1 ‚àí n ‚àí dim C NOM = dim C NOM + n + 1.  
   
  Knowing the dimension of C ROB , we are now able to study  facet-deÔ¨Åning recycled inequalities. For this, we only consider inequalities i‚àà[n] œÄi xi ‚â§ œÄ0 consisting of variables with uncertain objective coeÔ¨Écients, i.e., we have œÄi = 0 for all i ‚àà [n] with cÀÜi = 0. We call inequalities with this property uncertaintyexclusive inequalities. Note that these are the only interesting inequalities for recycling, because we can always drop variables xi with cÀÜi = 0. This strengthens the corresponding recycled inequality by removing œÄi pi from the left-hand side, while the right-hand side doesn‚Äôt change due to œÄi cÀÜi xi = 0. The following theorem characterizes exactly under which conditions recyclable, uncertaintyexclusive inequalities i‚àà[n] œÄi x ‚â§ œÄ0 yield facet-deÔ¨Åning recycled inequalities,  
   
      
   
  based on the face F (œÄ) = x ‚àà C NOM i‚àà[n] œÄi x = œÄ0 . The statement may seem very technical at Ô¨Årst glance, but we will see afterwards that it is quite powerful and has some surprising implications.  Theorem 2. Let i‚àà[n] œÄi xi ‚â§ œÄ0 be a recyclable, uncertainty-exclusive inequality and ej ‚àà Rn+1 be the unit-vector for j ‚àà S = {i ‚àà [n]|œÄi = 0}. Then the recyROB exist vectors cled inequality (1) is facet-defining for if and  only  j C  if there  n‚àí|S|   1 n‚àí|S|  
   
  Àú1 , 1 , . . . , x ‚äÜ F (œÄ) such that e j ‚àà S ‚à™ x Àú Àú , 1 are x Àú ,...,x linearly independent. Proof. First, note that the face of the recycled inequality is not equal to C ROB , since p and z can be raised arbitrarily.    Thus, it is facet-deÔ¨Åning if and only if there exist dim C ROB = dim C NOM + n + 1 aÔ¨Énely independent vectors (x, p, z) ‚àà C ROB that satisfy it with equality.   Regardless of œÄ, there are dim C NOM + 1 + |S|   aÔ¨Énely independent NOM ROB ) ‚äÜ (x, p, z) ‚àà C satisfying (1) with equality. For this, let x0 , . . . , xdim(C  j  NOM j C be aÔ¨Énely   NOM independent. j We choose x , cÀÜ  x , 0 for each j ‚àà 0, . . . , dim C , where cÀÜ  x refers to the component-wise multiplication,     i.e., cÀÜ  xj i = cÀÜi xji . By deÔ¨Ånition, xj , cÀÜ  xj , 0 is within C ROB and satisÔ¨Åes   (1) with equality. Additionally, we choose x0 , cÀÜ  x0 + ej , 0 for each j ‚àà S. Here, ej ‚àà Rn with some abuse of notation. Again, this vector is within C ROB and satisÔ¨Åes (1) with equality due to œÄj = 0.  
   
  64  
   
  C. B¬® using et al.  
   
  Now, the recycled inequality (1) is facet-deÔ¨Åning if and only if there exists a suitable extension of the vectors above, consisting of additional vectors x Àúj , pÀúj , zÀúj j‚àà[n‚àí|S|] that satisfy (1) with equality and are aÔ¨Énely independent to the vectors above. Such vectors need to satisfy the property  j  Àúi for all i ‚àà [n] \ S and j ‚àà [n ‚àí |S|] , (2) pÀúji = cÀÜi ‚àí zÀúj x as otherwise œÄ0 zÀúj +  
   
    
   
  œÄi pÀúji > œÄ0 zÀúj ‚àí  
   
  i‚àà[n]  
   
   i‚àà[n]  
   
  œÄi zÀúj x Àúji +  
   
   i‚àà[n]  
   
  œÄi cÀÜi x Àúji ‚â•  
   
    
   
  œÄi cÀÜi x Àúji .  
   
  i‚àà[n]  
   
   One can show that any vectors x Àúj , pÀúj , zÀúj j‚àà[n‚àí|S|] with property (2) are   
   
  aÔ¨Énely if  and only if zÀúj > 0 for all j ‚àà [n ‚àí |S|]  
   
    to1 the  ones above  j independent n‚àí|S|  
   
  Àú , 1 , . . . , x Àú , 1 are linearly independent. To show and e j ‚àà S ‚à™  x this, one subtracts x0 , cÀÜ  x0 , 0 from all other vectors, yielding vectors that are linearly independent if and only if the desired aÔ¨Éne independency holds. Writing the vectors in a matrix and performing basic column and row transformations implies the result. We omit this step here  j due to space limitations. Àúi , we also have With zÀúj > 0 and œÄi pÀúji = œÄi cÀÜi ‚àí zÀúj x     œÄi pÀúji = œÄi cÀÜi x Àúji ‚áî œÄ0 zÀúj = œÄi zÀúj x Àúji ‚áî œÄ0 = œÄi x Àúji , œÄ0 zÀúj + i‚àà[n]  
   
  i‚àà[n]  
   
  i‚àà[n]  
   
  i‚àà[n]  
   
   1   j j j and thus x Àú ,...,x Àú , pÀú , zÀú j‚àà[n‚àí|S|] fulÔ¨Åll the Àún‚àí|S| ‚äÜ F (œÄ) if and only if x recycled inequality (1) with equality. This shows the necessity of the condition.   1 Now, let x Àú ,...,x Àún‚àí|S| ‚äÜ F (œÄ) be as speciÔ¨Åed in the theorem. To show suÔ¨Éciency of the condition, we only need to construct vectors x Àúj , pÀúj , zÀúj j‚àà[n‚àí|S|]  
   
  j satisfying  (2) and zÀú > 0 for all j ‚àà [n ‚àí |S|]. Forj each j ‚àà [n ‚àí |S|], wej  jproperty j min ci |i ‚àà [n] , cÀÜi > 0} and pÀúi = max {0, cÀÜi ‚àí zÀú} x Àúi choose x Àú , pÀú , zÀú with  jzÀú =  {ÀÜ for all  i ‚àà [n]. Then x Àú , pÀúj , zÀú is by deÔ¨Ånition within C ROB and satisÔ¨Åes zÀú > 0. Since i‚àà[n] œÄi xi ‚â§ œÄ0 is uncertainty-exclusive, we have œÄi = 0 for all cÀÜi < zÀú,  j j  ci ‚àí zÀú) x Àúji for all i ‚àà [n] \ S. Therefore, x Àú , pÀú , zÀú also satisÔ¨Åes and thus pÀúji = (ÀÜ property (2), which completes the proof.  
   
  A straightforward, but powerful implication of Theorem 2 is that recycling a uncertainty-exclusive inequality yields always a facet-deÔ¨Åning inequality if dim (F (œÄ)) = n ‚àí 1 holds.  This is because there already exist n aÔ¨Énely independent vectors satisfying i‚àà[n] œÄi x = œÄ0 , which implies that there exist appro  ‚àí 1 holds if F (œÄ) priate vectors x1 , . . . , xn‚àí|S| . Note that dim (F (œÄ)) = n  is either a facet of a full-dimensional polytope C NOM or if i‚àà[n] œÄi x ‚â§ œÄ0 is   actually an equation with F (œÄ) = C NOM and dim C NOM = n ‚àí 1. This is summarized in the following corollary.  Corollary 1. Let i‚àà[n] œÄi xi ‚â§ œÄ0 be a recyclable, uncertainty-exclusive inequality. The recycled inequality (1) is facet-defining for C ROB if one of the following holds:  
   
  Recycling Inequalities for Robust Combinatorial Optimization  
   
  65  
   
  ‚Äì C NOM and F (œÄ) is a facet of C NOM ,   is full-dimensional ‚Äì dim C NOM = n ‚àí 1 and F (œÄ) = C NOM . Contrary to Ô¨Årst intuition, it is also possible to obtain facet-deÔ¨Åning inequalities by recycling weaker inequalities that are neither facet-deÔ¨Åning nor equations. This is because Theorem 2 suggests that an inequality deÔ¨Åning a low-dimensional face can also be recycled to a facet-deÔ¨Åning inequality if we have œÄi = 0 for many i ‚àà [n]. For example, consider an independent set problem on a graph  with vertices V = [n] and let Q ‚äÜ Vbe a clique. Then the clique inequality i‚ààQ xi ‚â§ 1 dominates all inequalities i‚ààQ xi ‚â§ 1 with Q  Q and is facet-deÔ¨Åning if and only if Q is a maximal cliquewith respect to inclusion [8]. However, the recycled  inequality z + i‚ààQ pi ‚â• i‚ààQ cÀÜi xi is facet-deÔ¨Åning for all cliques Q ‚äÜ Q.    
   
    1 This is because the set x Àú ,...,x Àún‚àí|S| = ej j ‚àà Q meets the criteria of Theorem 2 with S = V \ Q . Other examples include odd-hole inequalities for the independent set problem [18] and minimal cover inequalities for the knapsack problem [8]. These are in general not facet-deÔ¨Åning for their respective polytope, but yield facet-deÔ¨Åning recycled inequality for the robust counterpart. All these examples are covered by the following corollary. NOM Corollary 2. Let C NOM be a full-dimensional polyhedron such that x ‚àà C   NOM and 0 ‚â§ x ‚â§ x implies x ‚àà C . Furthermore, let i‚àà[n] œÄi xi ‚â§ œÄ0 be a recyclable, uncertainty-exclusive inequality. The recycled inequality (1) is facet defining for C ROB if i‚àà[n] œÄi xi ‚â§ œÄ0 is facet-defining for the restricted solution  
   
    space x ‚àà C NOM xi = 0 for all œÄi = 0 .  
   
  Note that the additional requirements on C NOM imply that the restricted solutionspace is of dimension n ‚àí |S|, which guarantees that we Ô¨Ånd appropriate  Àún‚àí|S| . vectors x Àú1 , . . . , x One now might raise the question whether inequalities recycled from dominated inequalities are actually of practical interest or whether they do not really matter due to the special structure of the objective function. The following example demonstrates that it can be beneÔ¨Åcial to weaken an inequality before it is recycled. Example 2. Consider the robust problem   
   
  ‚éß  
   
  3x5 + xi ‚â§ 3 ‚é™  
   
  ‚é™ ‚é™  
   
  ‚é®  i‚àà[4]  
   
  min 2z + ‚àíxi + pi  
   
  z + pi ‚â• xi ‚é™ ‚é™ i‚àà[5]  
   
  ‚é™ ‚é©  
   
  x ‚àà {0, 1}5 , p ‚àà R5 , z ‚àà R ‚â•0 ‚â•0  
   
  ‚é´ ‚é™ ‚é™ ‚é™ ‚é¨ ‚àÄi ‚àà [5]‚é™ ‚é™ ‚é™ ‚é≠  
   
  .  
   
    Choosing x = 34 , . . . , 34 , 0 , p ‚â° 0, and z = 34 yields an optimal  solution for the linear relaxation of value ‚àí 23 . Recycling constraint 3x5 + i‚àà[4] xi ‚â§ 3 yields   3z + 3p5 + i‚àà[4] pi ‚â• 3x5 + i‚àà[4] xi . After adding the recycled inequality,     an optimal choice is given by x = 34 , . . . , 34 , 0 , p = 0, . . . , 0, 14 , and z = 34 ,  
   
  66  
   
  C. B¬® using et al.  
   
  with an objective value of ‚àí 45 . Note that we now choose p5 > 0 even though x5 = 0 holds. This is because raising p5 has the same eÔ¨Äect on the recycled inequality as raising z, but is cheaper in the objective function. Since the bilinear inequality p5 + x5 z ‚â• cÀÜ5 x5 now has a slack of 14 , our observation from the last section suggests that it may be beneÔ¨Åcial to drop x5 from the valid  inequality for recycling. In fact, when recycling the dominated inequality i‚àà[4] xi ‚â§ 3   instead, we obtain 3z + i‚àà[4] pi ‚â• i‚àà[4] xi and an optimal choice is now given by x = (1, 1, 1, 0, 0), p ‚â° 0, and z = 1, which yields an objective value of ‚àí1. We can beneÔ¨Åt from this insight on dominated inequalities when recycling within a separation procedure to cut-oÔ¨Ä a fractional x, pÀú, zÀú) ‚àà P ROB .  solution (Àú ci x Àúi ‚àí pÀúi ) ‚àí œÄ0 zÀú. In The violation of a recycled inequality is given by i‚àà[n] œÄi (ÀÜ order to maximize the violation, we can drop all variables xi from the recyclable  Àúi ‚àí pÀúi < 0. We use this in our computational inequality i‚àà[n] œÄi xi ‚â§ œÄ0 with cÀÜi x study in the next section, where we show that recycled inequalities are not only interesting from a theoretical point of view, but also computationally relevant.  
   
  4  
   
  Computational Study  
   
  Due to space limitations, we present a preliminary computational study, in which we test recycled inequalities for robust counterparts of two classical combinatorial problems, namely the weighted independent set problem and the weighted bipartite matching problem. In the following, we compare (i) computation times to asses an algorithm‚Äôs performance and (ii) integrality gaps to evaluate the strength of a formulation. Since displaying these for all algorithms and instances is impractical, we give aggregated values using the shifted geometric  mean, as proposed by Achter1/k k ‚àí s for values v1 , . . . , vk ‚àà R‚â•0 berg [1]. This is deÔ¨Åned as Œ†i=1 (vi + s) and a shifting parameter s ‚àà R‚â•0 . We always use s = 1 second for aggregating computation times and s = 1% for aggregating integrality gaps. Furthermore, we use a time limit of 3600 seconds for each algorithm and instance and set the computation time to this value if an algorithm reaches the limit. Note that this is a bias in favor of algorithms that reach the time limit for many instances. All experiments have been implemented in Java 11 and are performed on a R CoreTM i7-5930K CPU @ 3.50GHz, single core of a Linux machine with an Intel with 4 GB RAM reserved for each calculation. All LPs and MILPs are solved using Gurobi version 9.5.0 [13] in single thread mode and all other settings at default, if not stated otherwise. All implemented algorithms [10] and generated test instances [11] are freely available online. 4.1  
   
  Robust Independent Set  
   
  To show the eÔ¨Äect of recycling a class of well-known valid inequalities in a separation procedure, we consider the robust maximum weighted independent set problem on a graph with nodes V and edges E  
   
  Recycling Inequalities for Robust Combinatorial Optimization  
   
  max  
   
  ‚éß ‚é™ ‚é® ‚é™ ‚é©v‚ààV  

  xv + xw ‚â§ 1   
   
  cv xv ‚àí Œì z ‚àí pv pv + z ‚â• cÀÜv xv  
   
  v‚ààV  
   
  x ‚àà {0, 1}V , p ‚àà RV , z ‚àà R ‚â•0 ‚â•0  
   
  67  
   
  ‚é´ ‚àÄ {v, w} ‚àà E ‚é™ ‚é¨ ‚àÄv ‚àà V  
   
  ‚é™ ‚é≠  
   
  .  
   
   As seen in Sect. 3, recycling a clique inequality v‚ààQ xv ‚â§ 1 yields a facetdeÔ¨Åning inequality for all cliques Q ‚äÜ V . We compare the separation of recycled clique inequalities in the root node of the branching tree against the robust default formulation P ROB , which solely uses the constraints pi + z ‚â• cÀÜi xi . For this, we use Gurobi‚Äôs callback to add the recycled inequalities as user cuts [13]. Every time Gurobi invokes the callback in the root node and reports a current ROB optimal fractional solution (Àú x, pÀú, zÀú) ‚àà P to compute cliques Q ‚äÜ V  , we try for which the recycled inequality z + v‚ààQ pv ‚â• v‚ààQ cÀÜv xv is violated. Since Àúv ‚àí pÀúv > 0 holds, a node v ‚àà V positively contributes to the violation if cÀÜv x we essentially need to solve a maximum weighted clique problem with weights Àúv ‚àí pÀúv . To separate many recycled inequalities at once, we extend each node cÀÜv x Àúv ‚àí pÀúv > 0 greedily to a clique Qv ‚äÜ V with v ‚àà Qv . For v ‚àà V with cÀÜv x this, we start with Qv = {v} and then iteratively add v  ‚àà N (Qv ) such that Àúv ‚àí pÀúv is maximal and non-negative. Finally, we return the corresponding cÀÜv x recycled inequality to Gurobi as a user cut if its violation is positive. As a basis for our test instances, we use the graphs of the second DIMACS implementation challenge on the clique problem [15]. Of the 66 DIMACS graphs, we choose the 46 graphs that have at most 500 nodes, as otherwise the nominal problem is already very hard. For each node v ‚àà V , we generate independent and uniformly distributed values cv ‚àà {900, . . . , 1000} and correlated deviations cÀÜv = Œæv cv , with Œæv ‚àà [0.45, 0.55] being an independent and uniformly distributed random variable. Since robust problems tend to be hard for Œì being somewhere around half the number of variables with xi = 1 [7], we greedily compute an  
   
  independent set S ‚äÜ V and deÔ¨Åne Œì = |S| 2 . For this, we start with S = ‚àÖ and then iteratively add nodes v ‚àà V \ N [S] such that |V \ N [S ‚à™ {v}]| is maximal, with N [S  ] being the closed neighborhood of S  . Using this procedure, we randomly generate Ô¨Åve robust independent set problems for each of the 46 DIMACS graphs, leaving us with 230 robust instances. We show computational results for the robust default formulation P ROB and the recycling of clique inequalities in Table 1. Here, we see that the shifted geometric mean of the integrality gaps is reduced absolutely by 220% from 1427.59% to 1207.59% when using recycled clique inequalities. For computing these gaps, we use the dual bound obtained by heuristically separating recycled clique inequalities for subsequent linear relaxations until no violated cuts are found. While the absolute reduction of the integrality gap is quite impressive, the relative reduction does not adequately reÔ¨Çect the strength of the recycled inequalities. This is due to the large integrality gap of the nominal problem, which constitutes a major part of the total gap. Therefore, we also test a stronger formulation for the nominal problem, in which we replace every  
   
  68  
   
  C. B¬® using et al.  
   
  Table 1. Computational results for 230 instances of the robust maximum weighted independent set problem. separate recycled robust default formulation clique inequalities nominal formulation Gurobi‚Äôs cuts tilim time int. gap tilim time int. gap edge  
   
  enabled disabled  
   
  24 40  
   
  clique  
   
  enabled disabled  
   
  61 78  
   
  26.15 1427.59% 51.01 133.62 187.21  
   
  135.30%  
   
  22 20 63 54  
   
  31.03 1207.59% 20.34 141.97 89.24  
   
  56.25%  
   
   constraint xv + xw ‚â§ 1 with v‚ààQ xv ‚â§ 1 for a clique Q ‚äÜ V with {v, w} ‚äÜ Q. This clique formulation has a much tighter linear relaxation compared to the previous edge formulation, and thus reduces the contribution of the nominal problem to the integrality gap. Indeed, Table 1 shows that separating recycled clique inequalities reduces the integrality gap by more than one half when using the clique formulation. Apart from this observation, the clique formulation is not of practical interest, as the solver performs better on the edge formulation. Using the edge formulation, we are able to solve 2 more instances when recycling clique inequalities, but observe an increase of the computation time. This seems to be due to some interference with Gurobi‚Äôs own cutting planes. When disabling Gurobi‚Äôs cutting planes, recycling is much better than using the default formulation. In fact, disabling Gurobi‚Äôs cuts and using recycled clique inequalities is the overall best performing approach, solving the most instances in the least amount of computation time. This is true for both nominal formulations and indicates that, given a careful implementation, recycling clique inequalities yields a signiÔ¨Åcant speedup compared to the robust default formulation. 4.2  
   
  Robust Bipartite Matching  
   
  We now consider the robust maximum weighted bipartite matching problem  
   
   ‚é´ ‚éß  
   
  ‚é™ ‚é™ x ‚â§ 1 ‚àÄv ‚àà V  
   
  ‚é™ ‚é™ e ‚é™ ‚é™ ‚é™ ‚é™ ‚é¨ ‚é®   
   
  e‚ààŒ¥(v)  
   
  ce xe ‚àí Œì z ‚àí pe  
   
  max ‚àÄe ‚àà E ‚é™ ‚é™  
   
  pe + z ‚â• cÀÜe xe ‚é™ ‚é™ e‚ààE ‚é™ ‚é™e‚ààE  
   
  ‚é™ ‚é™ E ‚é≠ ‚é© E  
   
  x ‚àà {0, 1} , p ‚àà R‚â•0 , z ‚àà R‚â•0 on a bipartite graph with nodes V and edges E. In contrast to the independent set problem, for which the standard nominal formulation is quite weak, we have P NOM = C NOM for the bipartite matching problem [8]. That is, the integrality gap of the robust counterpart is only due to the robust substructure, which allows us to test the strength of recycled inequalities to their limit. We randomly generate instances by Ô¨Årst dividing  n  a given set  of nodes V = [n]  n  and W = + 1, . . . , n . Afterwards, we into two partitions U = 2 2  
   
  Recycling Inequalities for Robust Combinatorial Optimization  
   
  69  
   
  Table 2. Computational results for the robust maximum weighted bipartite matching problem. recycle constraints robust default formulation recycle constraints and separate dominated nodes Gurobi‚Äôs cuts tilim time int. gap tilim time int. gap tilim time int. gap 50  
   
  enabled disabled  
   
  0 10  
   
  0.78 19.532% 3600.00  
   
  0 0  
   
  0.53 0.326% 0 0.25 0  
   
  0.64 0.319% 0.28  
   
  100  
   
  enabled disabled  
   
  5 10  
   
  603.37 22.82% 3600.00  
   
  0 0  
   
  4.76 0.319% 0 14.83 0  
   
  5.59 0.316% 15.62  
   
  150  
   
  enabled disabled  
   
  4 10  
   
  1405.15 23.66% 3600.00  
   
  0 6  
   
  122.11 0.269% 0 1809.62 7  
   
  158.81 0.265% 1873.43  
   
  sample for each node u ‚àà U a random number œÜu ‚àà [0, 1], modeling the probability with which an edge incident to u exists. Then for every w ‚àà W , we add the edge {u, w} with probability œÜu . Analogously to the independent set problem, every weight is a random number ce ‚àà {900, . . . , 1000} and the correlated deviations are cÀÜe = Œæe ce  with Œæe ‚àà [0.45, 0.55]. Finally, as the number of edges in a solution will most likely be near to n2 , we set Œì = n4 . We use this procedure to generate ten instances for diÔ¨Äerent numbers of nodes n ‚àà {50, 100, 150}. Table 2 shows computational results for the robust default formulation and two diÔ¨Äerent approaches  for using recycled inequalities. The Ô¨Årst approach recycles all constraints e‚ààŒ¥(v) xe ‚â§ 1 for v ‚àà V . The second approach additionally separates violated recycled inequalities corresponding to dominated inequalities  x ‚â§ 1 with E  ‚äÜ Œ¥ (v) for v ‚àà V in the root node of the branching tree.  e e‚ààE It is evident that recycling inequalities is signiÔ¨Åcantly better than solely using the default formulation. We observe a signiÔ¨Åcant strengthening of the formulation, leading to a reduction of the integrality gap to nearly one-hundredth for n = 150 nodes. This strength also translates to a higher number of instances solved and much lower computation times. For n = 150, recycling constraints leads to a speedup of more than 1000% with Gurobi‚Äôs cuts enabled. The reduced integrality gap obtained by  recycling dominated constraints compared to the sole recycling of constraints e‚ààŒ¥(v) xe ‚â§ 1 shows that recycling dominated inequalities can improve the strength of the linear relaxation in practice. However, as the recycled constraints already perform very well for these instances, the improvement in the linear relaxation is very small. In fact, the minor strengthening of the linear relaxation cannot compensate for the computational load imposed by the additional inequalities, which leads to higher computation times. In any case, recycling valid inequalities yields a signiÔ¨Åcant speed-up compared to the default formulation. First experiments with adapted MIPLIB [12] instances, which will be part of the full paper, conÔ¨Årm that this is also true for a broad set of diÔ¨Äerent robust problems. Here, we even observe that recycling dominated inequalities can have a strong positive eÔ¨Äect on the strength of the linear relaxation.  
   
  70  
   
  5  
   
  C. B¬® using et al.  
   
  Conclusion  
   
  In this paper, we proposed and analyzed recycled inequalities for robust combinatorial optimization problems with budget uncertainty. These can be derived in linear time from valid inequalities for the nominal problem, which gives the possibility to easily reuse model constraints and well known classical valid inequalities in order to strengthen the linear relaxation of the robust problem. We highlighted the theoretical strength of recycled inequalities by proving that they often deÔ¨Åne facets of the convex hull of the robust problem, even when the underlying valid inequality is dominated. Our preliminary computational experiments reveal that recycled inequalities are not only interesting from a theoretical point of view, but can also yield a substantial speed-up in the optimization process. They thus extend the boundaries of computational tractability for one of the most popular approach for integrating uncertainties into optimization problems. Acknowledgements. This work was partially supported by the German Federal Ministry of Education and Research (grants no. 05M16PAA) within the project ‚ÄúHealthFaCT - Health: Facility Location, Covering and Transport‚Äù, the Freigeist-Fellowship of the Volkswagen Stiftung, and the German research council (DFG) Research Training Group 2236 UnRAVeL. Code Availability. All tested algorithms have been implemented in Java and are available on GitHub, see [10]. Data Availability. All test instances used in our computational study are published and available for download, sharing, and reuse, see [11].  
   
  References 1. Achterberg, T.: Constraint integer programming. Ph.D. Thesis, Technische Universitat Berlin (2007) ¬¥ 2. Alvarez-Miranda, E., Ljubi¬¥c, I., Toth, P.: A note on the Bertsimas & Sim algorithm for robust combinatorial optimization problems. 4OR 11(4), 349‚Äì360 (2013) 3. Atamt¬® urk, A.: Strong formulations of robust mixed 0‚Äì1 programming. Math. Program. 108(2‚Äì3), 235‚Äì250 (2006) 4. Bertsimas, D., Dunning, I., Lubin, M.: Reformulation versus cutting-planes for robust optimization. CMS 13(2), 195‚Äì217 (2016) 5. Bertsimas, D., Sim, M.: Robust discrete optimization and network flows. Math. Program. 98(1‚Äì3), 49‚Äì71 (2003) 6. Bertsimas, D., Sim, M.: The price of robustness. Oper. Res. 52(1), 35‚Äì53 (2004) 7. B¬® using, C., Gersing, T., Koster, A.M.: A branch and bound algorithm for robust binary optimization with budget uncertainty. Math. Program. Comput. (2023). https://doi.org/10.1007/s12532-022-00232-2 8. Conforti, M., Cornu¬¥ejols, G., Zambelli, G., et al.: Integer Programming, vol. 271. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-11008-0 9. Fischetti, M., Monaci, M.: Cutting plane versus compact formulations for uncertain (integer) linear programs. Math. Program. Comput. 4(3), 239‚Äì273 (2012)  
   
  Recycling Inequalities for Robust Combinatorial Optimization  
   
  71  
   
  10. Gersing, T.: Algorithms for robust binary optimization, December 2022. https:// doi.org/10.5281/zenodo.7463371 11. Gersing, T., B¬® using, C., Koster, A.: Benchmark Instances for Robust Combinatorial Optimization with Budgeted Uncertainty, December 2022. https://doi.org/10. 5281/zenodo.7419028 12. Gleixner, A., et al.: MIPLIB 2017: data-driven compilation of the 6th mixed-integer programming library. Math. Program. Comput. 13(3), 443‚Äì490 (2021). https://doi. org/10.1007/s12532-020-00194-3 13. Gurobi Optimization, LLC: Gurobi optimizer reference manual, version 9.5 (2022). http://www.gurobi.com 14. Hansknecht, C., Richter, A., Stiller, S.: Fast robust shortest path computations. In: 18th Workshop on Algorithmic Approaches for Transportation Modelling, Optimization, and Systems (ATMOS 2018). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik (2018) 15. Johnson, D.S., Trick, M.A.: Cliques, coloring, and satisfiability: second DIMACS implementation challenge, 11‚Äì13 October 1993, vol. 26. American Mathematical Society (1996) 16. Joung, S., Park, S.: Robust mixed 0‚Äì1 programming and submodularity. INFORMS J. Optim. 3(2), 183‚Äì199 (2021). https://doi.org/10.1287/ijoo.2019.0042 17. Lee, T., Kwon, C.: A short note on the robust combinatorial optimization problems with cardinality constrained uncertainty. 4OR 12(4), 373‚Äì378 (2014) 18. Padberg, M.W.: On the facial structure of set packing polyhedra. Math. Program. 5(1), 199‚Äì215 (1973) 19. Park, K., Lee, K.: A note on robust combinatorial optimization problem. Manag. Sci. Financ. Eng. 13(1), 115‚Äì119 (2007) 20. Sherali, H.D., Adams, W.P.: A Reformulation-Linearization Technique for Solving Discrete and Continuous Nonconvex Problems, vol. 31. Springer, New York (2013). https://doi.org/10.1007/978-1-4757-4388-3  
   
  Inapproximability of Shortest Paths on Perfect Matching Polytopes Jean Cardinal1 1  
   
  and Raphael Steiner2(B)  
   
  Universit¬¥e Libre de Bruxelles (ULB), Brussels, Belgium [email protected]  2 ETH Zurich, Z¬® urich, Switzerland [email protected]   
   
  Abstract. We consider the computational problem of Ô¨Ånding short paths in the skeleton of the perfect matching polytope of a bipartite graph. We prove that unless P = NP, there is no polynomial-time algorithm that computes a path of constant length between two vertices at distance two of the perfect matching polytope of a bipartite graph. Conditioned on P = NP, this disproves a conjecture by Ito, Kakimura, Kamiyama, Kobayashi and Okamoto [SIAM Journal on Discrete Mathematics, 36(2), pp. 1102-1123 (2022)]. Assuming the Exponential Time Hypothesis we prove the stronger result that there exists   no polynomialtime algorithm computing a path of length at most 14 ‚àí o(1) logloglogNN between two vertices at distance two of the perfect matching polytope of an N -vertex bipartite graph. These results remain true if the bipartite graph is restricted to be of maximum degree three. The above has the following interesting implication for the performance of pivot rules for the simplex algorithm on simply-structured combinatorial polytopes: If P = NP, then for every simplex pivot rule executable in polynomial time and every constant k ‚àà N there exists a linear program on a perfect matching polytope and a starting vertex of the polytope such that the optimal solution can be reached using only two monotone non-degenerate steps from the starting vertex, yet the pivot rule will require at least k non-degenerate steps to reach the optimal solution. This result remains true in the more general setting of pivot rules for so-called circuit-augmentation algorithms. Keywords: Perfect matching polytopes ¬∑ Simplex method ¬∑ Pivot rules ¬∑ Circuit augmentations ¬∑ Combinatorial reconÔ¨Åguration  
   
  1  
   
  Introduction  
   
  The history of linear programming is intimately intertwined with that of Dantzig‚Äôs simplex algorithm. While the simplex and its many variants are among R. Steiner‚ÄìSupported by an ETH Postdoctoral Fellowship. A full version of this article can be found at https://arxiv.org/abs/2210.14608. Proofs of statements marked with  are deferred to the full version. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 72‚Äì86, 2023. https://doi.org/10.1007/978-3-031-32726-1_6  
   
  Inapproximability of Shortest Paths on Perfect Matching Polytopes  
   
  73  
   
  the most studied algorithms ever, a number of fundamental questions remain open. It is not known, for instance, whether there exists a pivot rule that makes the simplex method run in strongly polynomial time. Since the publication of the Ô¨Årst examples of linear programs that make the original simplex algorithm run in exponential time, many alternative pivot rules have been proposed, fostering a tremendous amount of work in the past 75 years, both from the combinatorial and complexity-theoretic point of views. The simplex algorithm follows a monotone path on the skeleton of the polytope deÔ¨Åning the linear program. The following natural question was recently raised by De Loera, Kafer, and Sanit` a [16]: ‚ÄúCan one hope to Ô¨Ånd a pivot rule that makes the simplex method use a shortest monotone path?‚Äù. As an answer, they proved that given an initial solution to a linear program, it is NP-hard to Ô¨Ånd a (2‚àíŒµ)-approximate shortest monotone path to an optimal solution. It implies that unless P = NP, no polynomial-time pivot rule for the simplex can be guaranteed to reach an optimal solution in a minimum number of (non-degenerate) steps. A similar result can also be deduced from two independent contributions, by Aichholzer, Cardinal, Huynh, Knauer, M¬® utze, Steiner, and Vogtenhuber [2] on the one hand, and by Ito, Kakimura, Kamiyama, Kobayashi, and Okamoto [29] on the other hand. They proved that the above result holds for perfect matching polytopes of planar and bipartite graphs, albeit with a slightly weaker inapproximability factor of 3/2 instead of 2. Ito et al. [29] conjecture that there exists a constant-factor approximation algorithm for the problem of Ô¨Ånding a shortest path between two perfect matchings on the perfect matching polytope. Our main result is a disproof of this conjecture under the P = NP assumption: Strengthening the previous inapproximability results mentioned above, we show that unless P = NP no C-approximation for a shortest path between two vertices at distance 2 of a bipartite perfect matching polytope can be found in polynomial time, for any (arbitrarily large) choice of C > 0. We also give an even stronger inapproximability result under the Exponential Time Hypothesis (ETH). The latter states that the 3-SAT problem cannot be solved in worst-case subexponential time, and is one of the main computational assumptions of the Ô¨Åne-grained complexity program [39]. As a consequence, there is not much hope of Ô¨Ånding a pivot rule for the simplex algorithm yielding good approximations of the shortest path towards an optimal solution, even when the linear program is integer and its associated matrix totally unimodular. 1.1  
   
  Our Result  
   
  We consider the complexity of computing short paths on the 0/1 polytope associated with perfect matchings of a bipartite graph. Given a balanced bipartite graph G = (V, E), where V is partitioned into two equal-size independent sets A and B, we deÔ¨Åne the perfect matching polytope PG ‚äÜ RE of G as the convex hull of the 0/1 incidence vectors of perfect matchings of G.  
   
  74  
   
  J. Cardinal and R. Steiner  
   
  It is well-known (see e.g. Chapter 18 in [38]) that for bipartite graphs G, there is a nice halfspace representation of PG . An edge-vector (xe )e‚ààE ‚àà RE is in PG if and only if the following hold.   
   
  xe = 1,  
   
  (‚àÄv ‚àà V )  
   
  (1)  
   
  xe ‚â• 0,  
   
  (‚àÄe ‚àà E).  
   
  (2)  
   
  ev  
   
  The above is a compact encoding of PG , with a number of constraints and variables of size polynomial in G. The assumption that G is bipartite is crucial here: For non-bipartite G the polytope deÔ¨Åned by the above constraints has non-integral vertices and is thus not a representation of PG [38]. The matrix of this representation of a perfect matching polytope of a bipartite graph G is simply the vertex-edge-incidence matrix of G, which is totally unimodular. The problem of maximizing a linear functional wT x subject to constraints (1) and (2) corresponds  exactly to the problem of Ô¨Ånding a perfect matching M of G whose weight e‚ààM we is maximal. Given that the simplex algorithm moves along the edges of a polytope, it is crucial for our considerations to understand adjacency of vertices on PG . The following result is well-known [13,30]. Lemma 1. For a bipartite graph G, two vertices of PG corresponding to two perfect matchings M1 and M2 are adjacent in the skeleton of PG if and only if the symmetric diÔ¨Äerence M1 ŒîM2 is a cycle in G. This cycle is said to be alternating in both matchings, and one matching can be obtained from the other by Ô¨Çipping this alternating cycle. In general, we will say that two perfect matchings are at distance at most k from each other on PG , for some positive integer k, if one can be obtained from the other by successively Ô¨Çipping at most k alternating cycles.  
   
  Fig. 1. Two perfect matchings at distance two on the perfect matching polytope, but whose symmetric diÔ¨Äerence consists of an arbitrarily large number of even cycles.  
   
  Inapproximability of Shortest Paths on Perfect Matching Polytopes  
   
  75  
   
  Note that given any two perfect matchings M1 and M2 of a bipartite graph G, it is always the case that M1 ŒîM2 is a collection of vertex-disjoint even cycles that are alternating in both matchings. The number of such cycles is therefore an upper bound on the distance between M1 and M2 on PG . Interestingly, this upper bound can be arbitrarily larger than the actual distance. Figure 1 shows a construction of a graph G with two matchings at distance two on PG , whose symmetric diÔ¨Äerence consists of an arbitrary number of cycles. Our main result is the following. Theorem 1. Let k ‚â• 2 be any Ô¨Åxed integer. Unless P = NP, there does not exist any polynomial-time algorithm solving the following problem: Input: A bipartite graph G of maximum degree 3 and a pair of perfect matchings M1 , M2 of G at distance at most 2 on the polytope PG . Output: A path from M1 to M2 in the skeleton of PG , of length at most k. More strongly, for every absolute constant Œ¥ > 0, unless the Exponential Time Hypothesis fails, no polynomial-time algorithm can solve the above problem when k is allowed to grow with the number N of vertices of G as 1  log N  k(N ) = 4 ‚àí Œ¥ log log N . A path on the perfect matching polytope of a bipartite graph G is said to be monotone with respect to some weight vector w = (we )e‚ààE ‚àà RE on the edges of G if the perfect matchings along the path have monotonically increasing total weights. Given two perfect matchings M1 and M2 at distance two on the perfect matching polytope, one can assign weights to edges so that (i) the path of length two between them is strictly monotone, and (ii) M2 is the unique matching of maximal weight (this will be formally proven later in Lemma 4). This allows us to formulate our result as one about the hardness of reaching an optimal solution from a given feasible solution of a linear program on the perfect matching polytope. Corollary 1. Unless P = NP, there does not exist any polynomial-time constant-factor approximation algorithm for the following optimization problem: Input: A bipartite graph G = (V, E) of maximum degree 3, a weight function E ‚Üí R+ , and a perfect matching M of G. Output: A shortest monotone path on PG from M to a maximum-weight perfect matching of G. Furthermore, assuming ETH, for an arbitrary but Ô¨Åxed Œ¥ > 0 no polynomial time algorithm can achieve an approximation ratio of less than 18 ‚àí Œ¥ logloglogNN , where N := |V (G)|. This corollary can be further interpreted as a statement on the existence of a polynomial-time pivot rule that would make the simplex method use an approximately shortest monotone path to a solution. Any such pivot rule could be used as an approximation algorithm for the above problem, contradicting  
   
  76  
   
  J. Cardinal and R. Steiner  
   
  the computational hypotheses. While making this statement, it is important to point out that here (and throughout this paper) we always measure the number of non-degenerate steps during an execution of a pivot rule for the simplex algorithm, and not the natural alternative, which would be the number of all steps (including both degenerate and non-degenerate steps). A degenerate step in a simplex algorithm here means a step that changes the basis of active variables, but not the value of the current feasible solution. 1.2  
   
  Pivot Rules for Circuit-Augmentation Algorithms  
   
  Our work on distances in the skeleton of PG for bipartite graphs G was originally motivated by questions regarding so-called circuit moves (or circuit augmentations), that have been recently studied in linear programming [11,15,16] as well as in the context of relaxations of the Hirsch conjecture concerning the diameter of polytopes [10,32]. A circuit move extends the simplex-paradigm of moving along an incident edge of the constraint-polyhedron, by additionally allowing to move along certain non-edge directions, called circuits. Given a linear program, the circuits in a well-deÔ¨Åned sense represent all possible edge-directions that could occur after changing the right-hand side of the LP. The following is a formal deÔ¨Ånition. Definition 1 (cf. Definition 1 in [16]). Given a polyhedron of the form P = {x ‚àà Rn |Ax = b, Bx ‚â§ d}, a circuit is a vector g ‚àà Rn \ {0} such that 1. Ag = 0, and 2. Bg is inclusion-wise support-minimal in the collection {By|Ay = 0, y = 0}. Given an LP {max cT x|x ‚àà P} for a polyhedron P, a current feasible solution x ‚àà P and a circuit g with cT g > 0, a circuit move then consists of moving to a new feasible solution x = x + t‚àó g, where t‚àó ‚â• 0 is maximal w.r.t. x + t‚àó g ‚àà P. Note that in general, an optimization algorithm based on a pivot rule for circuit moves may traverse several non-vertices of the polyhedron before reaching an optimal solution. Our interest in the perfect matching polytope for understanding the complexity of circuit-pivot algorithms came from the following statement (see the full version for a proof). Lemma 2 (). Let G be a bipartite graph. Then if x is a vertex of PG , and x = x is obtained from x by a circuit move, then x is also a vertex of PG and adjacent to x on the skeleton of PG . This lemma implies that any sequence of circuit moves, applied starting from a vertex of PG , will follow a monotone path on the skeleton of PG from vertex to vertex. Consequently, Corollary 1 also yields an inapproximability result for polynomial pivot rules for circuit augmentation, as follows.  
   
  Inapproximability of Shortest Paths on Perfect Matching Polytopes  
   
  77  
   
  Corollary 2. Unless P = NP, there does not exist a polynomial-time constantfactor approximation algorithm for the following problem. Input: A bipartite graph G of maximum degree 3, a vertex x ‚àà PG and a linear objective function. Output: A shortest sequence of circuit moves on PG from x to an optimal solution. Assuming ETH, no algorithm can achieve an approximation   polynomial-time ratio of less than 18 ‚àí Œ¥ logloglogNN , where N := |V (G)| and Œ¥ > 0 is a constant. A related inapproximability result (but for the largest improvement of the objective function via a single circuit step) was obtained by Borgwardt, Brand, Feldmann and Kouteck¬¥ y [9]. 1.3  
   
  Related Works  
   
  Our work relates to two main threads of research in combinatorics and computer science: one obviously related to the complexity of the simplex method and linear programming in general, and another more recent one, aiming at building a thorough understanding of the computational complexity of so-called combinatorial reconÔ¨Åguration problems. Complexity of the Simplex Method. In 1972, Klee and Minty showed that the original simplex method had an exponential worst-case behavior on what came to be known as Klee-Minty cubes [34]. Since then, many other variants have been shown to have exponential or superpolynomial lower bounds [4,7,19,24,31], although subexponential rules are known [26]. More dramatic complexity results have been obtained recently [1,20]. In particular, it was shown by Fearney and Savani [21] that Dantzig‚Äôs original simplex method can solve PSPACE-complete problems: Given an initial vertex, deciding whether some variable will ever be chosen by the algorithm to enter the basis is PSPACE-complete. The simplex method is also a key motivation for studying the diameter of polytopes, in particular the Hirsch conjecture, refuted in 2012 by Santos [37]. The hardness result on approximating monotone paths given by De Loera, Kafer, and Sanit` a [16] is in fact a corollary of the NP-hardness of the following problem: Given a feasible extreme point solution of the bipartite matching polytope and an objective function, decide whether there is a neighbor extreme point that is optimal. A related result for circulation polytopes was proved by Barahona and Tardos [5]. These two results, as well as the hardness results from Aichholzer et al. [2] and Ito et al. [29] rely on the NP-hardness of the Hamiltonian cycle problem. In order to deal with the approximability of the shortest path, we have to resort to more recent inapproximability results on the longest cycle problem [6].  
   
  78  
   
  J. Cardinal and R. Steiner  
   
  Reconfiguration of Matchings. The Ô¨Åeld of combinatorial reconÔ¨Åguration deals with the problems of transforming a given discrete structure, typically a feasible solution of a combinatorial optimization problem, into another one using elementary combinatorial moves [23,27,28,36]. The reachability problem, for instance, asks whether there exists such a transformation, while the shortest reconÔ¨Åguration path problem asks for the minimum number of elementary moves. A number of recent works in this vein deal with reconÔ¨Åguration of matchings in graphs [8,12,25,28,33]. Ito et al. [28] proved that the reachability problem between matchings of size at least some input number k and under single edge addition or removal was solvable in polynomial time. This was extended to an adjacency relation involving two edges by Kaminsk¬¥ƒ±, Medvedev and Milani¬¥c [33]. The problem of Ô¨Ånding the shortest reconÔ¨Åguration path under this model was shown to be NP-hard [12,25]. Another line of work involves Ô¨Çip graphs on perfect matchings in which the adjacency relation corresponds to Ô¨Çips of alternating cycles of length exactly four [8,14,17,18,35]. Note that for bipartite graphs, this Ô¨Çip graph is precisely the subgraph of the skeleton of the perfect matching polytope that consists of edges of length two. Bonamy, Bousquet, Heinrich, Ito, Kobayashi, Mary, M¬® uhlentaler and Wasa [8] proved that the reachability problem in these Ô¨Çip graphs is PSPACE-complete.  
   
  2 2.1  
   
  Proof of Theorem 1 Preliminaries  
   
  First note that perfect matchings of a bipartite graph G = (A ‚à™ B, E) with |A| = |B| can also be represented by orientations of G in which every vertex in A has outdegree one and every vertex in B has indegree one. The edges of the matching are those oriented from A to B. Alternating cycles in a perfect matching are one-to-one with directed cycles in this orientation, and Ô¨Çipping the cycle amounts to reverting the orientations of all its arcs. We will switch from one representation to another when convenient. We prove Theorem 1 by reducing from the problem of approximating the longest directed cycle in a digraph. We rely on the following two results from Bj¬® orklund, Husfeldt, and Khanna given as Theorems 1 and 2 in [6]. Theorem 2 (Bj¬® orklund, Husfeldt, Khanna [6]). Consider the problem of computing a long directed cycle in a given Hamiltonian digraph D on n vertices. 1. For every Ô¨Åxed 0 < Œµ < 1, unless P = NP, there does not exist any polynomialtime algorithm that returns a directed cycle of length at least nŒµ in D. 2. For every polynomial-time computable increasing function f : N ‚Üí N in œâ(1), unless the Exponential Time Hypothesis fails, there does not exist any polynomial-time algorithm that returns a directed cycle of length at least f (n) log n in D.  
   
  Inapproximability of Shortest Paths on Perfect Matching Polytopes  
   
  79  
   
  Note that in the two problems, the input graph is guaranteed to be Hamiltonian, yet it remains hard to explicitly construct a directed cycle of some guaranteed length. Characterising the approximability of the longest cycle problem in undirected graphs is a longstanding open question [3,22]. The second ingredient of our proof is the following lemma, perhaps of independent interest, that bounds the increase in length of a longest directed cycle after a number of cycle Ô¨Çips in a digraph. Lemma 3. Let G be an undirected graph, and let C1 , . . . , Ct be a sequence of (not necessarily distinct) cycles in G. Let D0 , D1 , . . . , Dt be a sequence of orientations of G such that for each i ‚àà [t] the cycle Ci is directed in Di‚àí1 and such that Di is obtained from Di‚àí1 by Ô¨Çipping Ci . There exists a polynomial-time algorithm that, given as input a number , the orientations D0 , . . . , Dt and a directed cycle C in Dt of length |C| > t+1 , computes a directed cycle in D0 of length at least . The bound of Lemma 3 can be shown to be essentially tight. We refer to the full version of this paper for an explicit description of a directed graph whose maximum directed cycle is of length , but after a sequence of at most t cycle Ô¨Çips, it contains a directed cycle of length at least (/2)t+1 . 2.2  
   
  Reduction  
   
  Fig. 2. Illustration of the reduction in the proof of Theorem 1. Every vertex in the given Hamiltonian digraph D (left) is replaced by the depicted gadget (right), yielding a maximum degree-three bipartite graph with a perfect matching.  
   
  We now give a proof of Theorem 1, assuming Lemma 3. Proof (Theorem 1). We consider the Ô¨Årst problem in Theorem 2: For a Ô¨Åxed Œµ > 0, given a Hamiltonian digraph D on n vertices, return a directed cycle of length at least nŒµ . We Ô¨Årst construct a digraph D from D by replacing every vertex v of D by the gadget illustrated on Fig. 2. The gadgets are obtained by applying the following transformations1 to every vertex v of D: 1  
   
  We note that the sole prupose of splitting vertices into binary trees is to restrict the maximum degree of the graph, the remainder of the proof is only based on the 4-cycles in the middle of the gadgets.  
   
  80  
   
  J. Cardinal and R. Steiner  
   
  1. The set of incoming arcs of v is decomposed into a balanced binary tree with deg‚àí D (v) leaves and a degree-one root identiÔ¨Åed to v. Each internal node of this binary tree (that is, all nodes except for the leaves and the roots of degree 1) is further split into an arc. All arcs of the tree are oriented towards the root. 2. The set of outgoing arcs are split into a tree with deg+ D (v) leaves in a similar fashion, with all arcs oriented away from the root. The roots of the in- and out-trees are both identiÔ¨Åed with v and thus equal to each other. 3. Finally, the vertex v itself is replaced by a directed 4-cycle, such that the single incoming arc from the Ô¨Årst tree and the single outgoing arc from the second tree have adjacent endpoints on the cycle.  
   
  Fig. 3. Flipping the 4-cycles of each gadget in D can be done with two successive cycle Ô¨Çips, using the Hamiltonian cycle of D.  
   
  The digraph D thus obtained is bipartite and subcubic. Furthermore, it is easy to see by construction that for every vertex v ‚àà V (D), the corresponding gadget in D has at most + 4deg‚àí D (v) + 4 + 4degD (v) ‚â§ 8(n ‚àí 1) + 4 < 8n  
   
  vertices, such that N := |V (D )| < n ¬∑ 8n = 8n2 , and D is of polynomial size. Furthermore, the orientation of D is such that every vertex in one side of the bipartition has outdegree one, and every vertex in the other has indegree one, hence it corresponds to a perfect matching M1 . By Ô¨Çipping the alternating 4-cycle in each gadget, we obtain another perfect matching M2 . We observe that M2 can be obtained from M1 in two cycle Ô¨Çips, by using the Hamiltonian cycle of D twice (see Fig. 3). Hence, while M1 ŒîM2 consists of n disjoint 4-cycles, M2 is in fact at distance two from M1 on the perfect matching polytope of D . The underlying undirected graph of D together with the two perfect matchings M1 and M2 therefore constitute an instance of the problem described in Theorem 1. We now show that any sequence of length at most k of alternating cycle Ô¨Çips  
   
  Inapproximability of Shortest Paths on Perfect Matching Polytopes  
   
  81  
   
  transforming M1 into M2 can be turned in polynomial time into a cycle of length at least nŒµ in D, for some Œµ > 0 depending solely on k. Consider a sequence of k cycles C1 , C2 , . . . , Ck in the underlying graph of D , such that C1 is alternating with respect to the matching M1 in D (and thus a directed cycle in D ); for every i = 2, . . . , k the cycle Ci is alternating with respect to the perfect matching M1 ŒîC1 Œî ¬∑ ¬∑ ¬∑ ŒîCi‚àí1 (and thus a directed cycle  , in this in the digraph obtained from D after Ô¨Çipping the cycles C1 , C2 , . . . , Ci‚àí1 order); and such that Ô¨Çipping all k alternating cycles in sequence transforms M1 into M2 . Observe that the sum of the lengths of the cycles in this sequence must be at least n, since all the orientations of the 4-cycles in the n diÔ¨Äerent gadgets in D have to be Ô¨Çipped, and since every single cycle Ci can intersect at most |Ci | diÔ¨Äerent gadget-4-cycles. Let  = n1/(k+2) . We have k   
   
  |Ci | ‚â• n = (1 ‚àí o(1))k+2 >  
   
  i=1  
   
  k   
   
  i+1 ,  
   
  i=1  
   
  hence from the pigeonhole principle, at least one cycle Ci in the sequence has length |Ci | > i+1 . From Lemma 3, applied with this value of i, we can now  compute in polynomial time a directed cycle in D of length at least nŒµ for   Œµ = 1/(k + 2). Let us call this cycle C . Note that for every gadget in D corresponding to a vertex v of D, either C  is vertex-disjoint from this gadget, or it traverses it via exactly one directed path, consisting of a leaf-to-root path in the in-tree, a directed path of length 7 touching the 4-cycle of the gadget, and then a root-to-leaf path in the out-tree. From this it follows that by contracting the edges of the gadgets, the cycle C  in D can be mapped to a cycle C in D. Note that the in- and out-degree of a vertex in D is at most n ‚àí 1, thus all the in- and out-trees in D corresponding to the gadgets have depth at most 2 log2 n . Consequently, the length of C  can be shrinked by at most a factor of 4 log2 n + 7 by contracting the gadgets. In other words, we obtain a directed  cycle C in D of length at least nŒµ /(4 log2 n + 7) = nŒµ , for Œµ = Œµ ‚àí o(1). Hence if we can obtain in polynomial time a sequence of at most k = O(1) Ô¨Çips transforming M1 into M2 , we can also Ô¨Ånd a cycle of length at least nŒµ in D for some Ô¨Åxed Œµ > 0. This establishes the Ô¨Årst statement of Theorem 1. It remains to prove the second statement. We consider the second problem in Theorem 2, in which we seek a path of length at least f (n) log n, for some computable function f (n) = œâ(1). Suppose that for some Œ¥ > 0 there is a polynomial-time algorithm that can Ô¨Ånd a sequence of at most k = k(N ) =   log N  1 Ô¨Çips transforming M1 into M2 . Note that for n large enough 4 ‚àí Œ¥ log log N  k+2‚â§  
   
  1 ‚àíŒ¥ 4  

  log 8n2 log n 1 . + 2 < (1 ‚àí Œ¥) 2 log log 8n 2 log log n  
   
  82  
   
  J. Cardinal and R. Steiner  
   
  Now, from the same reasoning as above, we can turn such an algorithm into a polynomial-time algorithm that Ô¨Ånds a directed cycle in D of length at least  
   
  n1/(k+2) n2¬∑log log n/((1‚àíŒ¥) log n) log2/(1‚àíŒ¥) n > =Œ© = f (n) log n, 4 log2 n + 7 O(log n) log n for a computable function f (n) = Œ©(log2Œ¥/(1‚àíŒ¥) n) = œâ(1). This, from Theorem 2, is impossible unless the Exponential Time Hypothesis fails.  
   
  In order to deduce Corollary 1 from Theorem 1, we need the following lemma. Lemma 4 (). Given a bipartite graph G = (V, E), let M1 and M2 be two perfect matchings in G at distance two on the perfect matching polytope, hence such that M2 = (M1 ŒîC1 )ŒîC2 for some pair C1 , C2 of cycles in G, and such that M1 ŒîC1 =: M  is also a perfect matching. Then there exists a weight function w : E ‚Üí R+ such that 1. M2 is the unique maximum-weight perfect matching of G,  2. w(M1 ) < w(M  ) < w(M2 ) (where w(M ) = e‚ààM w(e)). In other words, there exists a linear program over the perfect matching polytope of G such that the path M1 , M1 ŒîC1 = M  = M2 ŒîC2 , M2 is a strictly monotone path and M2 is the unique optimum. 2.3  
   
  Proof of Lemma 3  
   
  Proof (Lemma 3). Let the orientations D0 , D1 , . . . , Dt of G be given as input, together with a directed cycle C in Dt and a number  ‚àà N such that |C| > t+1 . Our algorithm starts by computing the sequence of cycles C1 , . . . , Ct by determining for each i ‚àà [t] the set of edges with diÔ¨Äerent orientation in Di‚àí1 and Di . Next we compute in polynomial time the subgraph H of G which is the union of the cycles C1 , . . . , Ct in G. We in particular compute a list of the vertex sets of its connected components, which we call Z1 , . . . , Zc for some number c ‚â• 1. We need the following fact, proved in the full version: Claim ‚ú¢ (). For each r ‚àà [c] the induced subdigraph D0 [Zr ] of D0 is strongly connected. Let (x0 , x1 , . . . , xk‚àí1 , xk = x0 ) be the cyclic list of vertices on the directed cycle C in Dt , with edges oriented from xi to xi+1 for all i ‚àà [k ‚àí 1]. By assumption on the input, we have k = |C| > t+1 . We Ô¨Årst check if C is vertex-disjoint from H, in which case we may return C, which is then also a directed cycle in D0 of length k > t+1 ‚â• , as desired. Otherwise, C intersects some of the components of H. We then for each vertex xi ‚àà V (C) compute a label lab(xi ) ‚àà [c + 1], deÔ¨Åned as lab(xi ) := r if xi ‚àà Zr lies in the r-th component of H, and lab(xi ) := c + 1 if xi is not a vertex of H. We next compute an auxiliary weighted directed multigraph M on the vertex set [c] as follows: For every maximal subsequence of C, of length at least  
   
  Inapproximability of Shortest Paths on Perfect Matching Polytopes  
   
  83  
   
  two, of the form xi , xi+1 , . . . , xj (addition to be understood modulo k) such that lab(xs ) = c + 1 for all s = i + 1, . . . , j ‚àí 1 (if any), we add an additional arc from lab(xi ) to lab(xj ) and give it weight j ‚àí i, the corresponding number of arcs in C. Note that the total arc weight in M is exactly |C|, while the total number of arcs is exactly |V (C) ‚à© V (H)| ‚â§ |V (H)|. The construction of M is illustrated in Fig. 4.  
   
  Fig. 4. Construction of the auxiliary directed multigraph M in the proof of Lemma 3. The cycle C is shown on the left, together with the connected components of H that it intersects. The resulting weighted directed multigraph M is shown on the right.  
   
  Furthermore, by deÔ¨Ånition every vertex in M has the same number of incoming and outgoing arcs. Hence, we may compute in polynomial time an edgedisjoint decomposition of M into directed cycles (including possible loops) in M . Let W1 , . . . , Wp for some p ‚àà N be the list of edge-disjoint directed cycles in this decomposition of M . We now create, for each Wi , a directed cycle Ki in D0 of length |Ki | ‚â• weight(Wi ), where weight(Wi ) is the total arc weight of Wi , as follows: Let (l0 , l1 , . . . , ls = l0 ) be the cyclic vertex-sequence of Wi . For each arc (lj , lj+1 ) in Wi , we consider the corresponding subsequence P (lj , lj+1 ) of C which starts in Zlj , ends in Zlj+1 , and all whose internal vertices are not contained in H. We note that since arcs outside H have the same orientation in D0 and Dt , the subsequence P (lj , lj+1 ) is a directed path or a directed cycle also in D0 which starts in Zlj and ends in Zlj+1 . We Ô¨Årst check whether there exists an index j such that P (lj , lj+1 ) is a directed cycle. In this case, necessarily Wi is a loop (i.e. s = 0) and lj = lj+1 = l0 . We thus may simply put Ki := P (lj , lj+1 ), with weight(Wi ) = |Ki | satisÔ¨Åed by deÔ¨Ånition of the weights in M . Otherwise, each of the P (lj , lj+1 ) is a directed path in D0 . We now make use of Claim ‚ú¢, which tells us that D0 [Zlj ] is strongly connected for every j = 0, 1, . . . , s ‚àí 1. We may therefore compute in polynomial time for each j = 0, 1, . . . , s‚àí1 a directed path Qj in D0 [Zlj ] (possibly consisting of a single vertex) which connects the endpoint of P (lj‚àí1 , lj ) to the starting point of P (lj , lj+1 ) (index-addition modulo s). Crucially, note that any two directed paths in the collection {P (lj , lj+1 ), Qj |j = 0, 1, . . . , s‚àí1} are vertex-disjoint except for shared common endpoints. We now compute the directed cycle Ki in D0 , which is the union of the directed paths P (lj , lj+1 ) and the directed paths Qj for j = 0, . . . , s ‚àí 1. It is clear that its length |Ki | is lower-bounded by the sum of the  
   
  84  
   
  J. Cardinal and R. Steiner  
   
  lengths of the P (lj , lj+1 ), which by deÔ¨Ånition of M equals the sum of arc-weights on Wi , i.e., we indeed have |Ki | ‚â• weight(Wi ) also in this case. After having computed the directed cycles K1 , . . . , Kp in D0 , the algorithm checks whether one of the cycles has length |Ki | ‚â• . If so, it returns the cycle Ki and the algorithm stops with the desired output. Otherwise, we have |Ki | <  for i = 1, . . . , p, which implies that t+1 < |C| = weight(M ) =  
   
  p   
   
  weight(Wi ) ‚â§  
   
  i=1  
   
  p   
   
  |Ki | ‚â§ p( ‚àí 1).  
   
  i=1  
   
  Note that p is at most as large as the number of arcs in M , which in turn is bounded by |V (H)|. We thus obtain t+1 < |V (H)| ¬∑ ( ‚àí 1) ‚â§  
   
  t   
   
  |Ci | ¬∑ ( ‚àí 1).  
   
  i=1  
   
  This yields that  
   
  t  i=1  
   
  t  
   
  |Ci | >  
   
   t+1 > i . ‚àí1 i=1  
   
  Therefore there exists i ‚àà {1, . . . , t} such that |Ci | > i . The algorithm proceeds by Ô¨Ånding one cycle Ci with this property. Note that Ci is a directed cycle in the orientation Di‚àí1 of G. Hence a recursive call of the algorithm to the input D0 , D1 , . . . , Di‚àí1 and the cycle Ci will yield a directed cycle of length at least  in D0 , as desired. This proves the correctness of the described algorithm. As all steps between two recursive calls are executable in polytime in the size of G and t, and since there will clearly be at most t ‚àí 1 recursive calls in any execution of the algorithm, the whole algorithm runs in polynomial time, as desired.  

  References 1. Adler, I., Papadimitriou, C., Rubinstein, A.: On simplex pivoting rules and complexity theory. In: Lee, J., Vygen, J. (eds.) IPCO 2014. LNCS, vol. 8494, pp. 13‚Äì24. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-07557-0 2 2. Aichholzer, O., et al.: Flip distances between graph orientations. Algorithmica 83(1), 116‚Äì143 (2021) 3. Alon, N., Yuster, R., Zwick, U.: Color-coding. J. ACM 42(4), 844‚Äì856 (1995) 4. Avis, D., Friedmann, O.: An exponential lower bound for Cunningham‚Äôs rule. Math. Program. 161(1‚Äì2), 271‚Äì305 (2017) ¬¥ Note on Weintraub‚Äôs minimum-cost circulation algo5. Barahona, F., Tardos, E.: rithm. SIAM J. Comput. 18(3), 579‚Äì583 (1989) 6. Bj¬® orklund, A., Husfeldt, T., Khanna, S.: Approximating longest directed paths and cycles. In: D¬¥ƒ±az, J., Karhum¬® aki, J., Lepist¬® o, A., Sannella, D. (eds.) ICALP 2004. LNCS, vol. 3142, pp. 222‚Äì233. Springer, Heidelberg (2004). https://doi.org/ 10.1007/978-3-540-27836-8 21  
   
  Inapproximability of Shortest Paths on Perfect Matching Polytopes  
   
  85  
   
  7. Bland, R.G.: New Ô¨Ånite pivoting rules for the simplex method. Math. Oper. Res. 2(2), 103‚Äì107 (1977) 8. Bonamy, M., et al.: The perfect matching reconÔ¨Åguration problem. In: Rossmanith, P., Heggernes, P., Katoen, J. (eds.) 44th International Symposium on Mathematical Foundations of Computer Science, MFCS 2019, August 26‚Äì30, 2019, Aachen, Germany. LIPIcs, vol. 138, pp. 80:1‚Äì80:14. Schloss Dagstuhl - Leibniz-Zentrum f¬® ur Informatik (2019) 9. Borgwardt, S., Brand, C., Feldmann, A.E., Kouteck¬¥ y, M.: A note on the approximability of deepest-descent circuit steps. Oper. Res. Lett. 49(3), 310‚Äì315 (2021) 10. Borgwardt, S., Finhold, E., Hemmecke, R.: On the circuit diameter of dual transportation polyhedra. SIAM J. Discrete Math. 29(1), 113‚Äì121 (2015) 11. Borgwardt, S., Viss, C.: A polyhedral model for enumeration and optimization over the set of circuits. Discret. Appl. Math. 308, 68‚Äì83 (2022) 12. Bousquet, N., Hatanaka, T., Ito, T., M¬® uhlenthaler, M.: Shortest reconÔ¨Åguration of matchings. In: Sau, I., Thilikos, D.M. (eds.) WG 2019. LNCS, vol. 11789, pp. 162‚Äì174. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-30786-8 13 13. Chv¬¥ atal, V.: On certain polytopes associated with graphs. J. Comb. Theory, Ser. B 18(2), 138‚Äì154 (1975) 14. CioabÀò a, S.M., Royle, G., Tan, Z.K.: On the Ô¨Çip graphs on perfect matchings of complete graphs and signed reversal graphs. Australas. J. Comb. 81, 480‚Äì497 (2021) 15. De Loera, J.A., Hemmecke, R., Lee, J.: On augmentation algorithms for linear and integer-linear programming: from Edmonds-Karp to Bland and beyond. SIAM J. Optim. 25(4), 2494‚Äì2511 (2015) 16. De Loera, J.A., Kafer, S., Sanit` a, L.: Pivot rules for circuit-augmentation algorithms in linear optimization. SIAM J. Optim. 32(3), 2156‚Äì2179 (2022) 17. Diaconis, P.W., Holmes, S.P.: Matchings and phylogenetic trees. Proc. Natl. Acad. Sci. USA 95(25), 14600‚Äì14602 (1998) 18. Diaconis, P.W., Holmes, S.P.: Random walks on trees and matchings. Electron. J. Probab. 7(6), 1‚Äì17 (2002) 19. Disser, Y., Friedmann, O., Hopp, A.V.: An exponential lower bound for Zadeh‚Äôs pivot rule. CoRR abs/1911.01074 (2019). http://arxiv.org/abs/1911.01074 20. Disser, Y., Skutella, M.: The simplex algorithm is NP-mighty. ACM Trans. Algorithms 15(1), 5:1‚Äì5:19 (2019) 21. Fearnley, J., Savani, R.: The complexity of the simplex method. In: Servedio, R.A., Rubinfeld, R. (eds.) Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14‚Äì17, 2015, pp. 201‚Äì208. ACM (2015) 22. Gabow, H.N., Nie, S.: Finding a long directed cycle. ACM Trans. Algorithms 4(1), 7:1‚Äì7:21 (2008) 23. Gima, T., Ito, T., Kobayashi, Y., Otachi, Y.: Algorithmic meta-theorems for combinatorial reconÔ¨Åguration revisited. In: Chechik, S., Navarro, G., Rotenberg, E., Herman, G. (eds.) 30th Annual European Symposium on Algorithms, ESA 2022, September 5‚Äì9, 2022, Berlin/Potsdam, Germany. LIPIcs, vol. 244, pp. 61:1‚Äì61:15. Schloss Dagstuhl - Leibniz-Zentrum f¬® ur Informatik (2022) 24. Goldfarb, D., Sit, W.Y.: Worst case behavior of the steepest edge simplex method. Discret. Appl. Math. 1(4), 277‚Äì285 (1979) 25. Gupta, M., Kumar, H., Misra, N.: On the complexity of optimal matching reconÔ¨Åguration. In: Catania, B., Kr¬¥ aloviÀác, R., Nawrocki, J., Pighizzini, G. (eds.) SOFSEM 2019. LNCS, vol. 11376, pp. 221‚Äì233. Springer, Cham (2019). https://doi.org/10. 1007/978-3-030-10801-4 18  
   
  86  
   
  J. Cardinal and R. Steiner  
   
  26. Hansen, T.D., Zwick, U.: An improved version of the random-facet pivoting rule for the simplex algorithm. In: Servedio, R.A., Rubinfeld, R. (eds.) Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14‚Äì17, 2015, pp. 209‚Äì218. ACM (2015) 27. van den Heuvel, J.: The complexity of change. In: Blackburn, S.R., Gerke, S., Wildon, M. (eds.) Surveys in Combinatorics 2013, London Mathematical Society Lecture Note Series, vol. 409, pp. 127‚Äì160. Cambridge University Press (2013) 28. Ito, T., et al.: On the complexity of reconÔ¨Åguration problems. Theor. Comput. Sci. 412(12‚Äì14), 1054‚Äì1065 (2011) 29. Ito, T., Kakimura, N., Kamiyama, N., Kobayashi, Y., Okamoto, Y.: Shortest reconÔ¨Åguration of perfect matchings via alternating cycles. SIAM J. Discret. Math. 36(2), 1102‚Äì1123 (2022) 30. Iwata, S.: On matroid intersection adjacency. Discret. Math. 242(1‚Äì3), 277‚Äì281 (2002) 31. Jeroslow, R.G.: The simplex algorithm with the pivot rule of maximizing criterion improvement. Discret. Math. 4(4), 367‚Äì377 (1973) 32. Kafer, S., Pashkovich, K., Sanit` a, L.: On the circuit diameter of some combinatorial polytopes. SIAM J. Discret. Math. 33(1), 1‚Äì25 (2019) 33. Kaminski, M., Medvedev, P., Milanic, M.: Complexity of independent set reconÔ¨Ågurability problems. Theor. Comput. Sci. 439, 9‚Äì15 (2012) 34. Klee, V., Minty, G.J.: How good is the simplex algorithm? In: Inequalities, III (Proc. Third Sympos., Univ. California, Los Angeles, Calif., 1969; dedicated to the memory of Theodore S. Motzkin), pp. 159‚Äì175. Academic Press, New York (1972) 35. Monroy, R.F., Flores-PeÀú naloza, D., Huemer, C., Hurtado, F., Wood, D.R., Urrutia, J.: On the chromatic number of some Ô¨Çip graphs. Discret. Math. Theor. Comput. Sci. 11(2), 47‚Äì56 (2009) 36. Nishimura, N.: Introduction to reconÔ¨Åguration. Algorithms 11(4), 52 (2018) 37. Santos, F.: A counterexample to the Hirsch conjecture. Ann. Math. 176(1), 383‚Äì 412 (2012) 38. Schrijver, A.: Combinatorial Optimization: Polyhedra and EÔ¨Éciency, Algorithms and Combinatorics, vol. 24. Springer (2003) 39. Williams, V.V.: On some Ô¨Åne-grained questions in algorithms and complexity. In: Proceedings of the International Congress of Mathematicians (ICM 2018), pp. 3447‚Äì3487. World ScientiÔ¨Åc (2018)  
   
  Monoidal Strengthening and Unique Lifting in MIQCPs Antonia Chmiela1 , Gonzalo MuÀú noz2(B) , and Felipe Serrano3 1  
   
  2  
   
  Zuse Institute Berlin, Berlin, Germany [email protected]  Institute of Engineering Sciences, Universidad de O‚ÄôHiggins, Rancagua, Chile [email protected]  3 I2DAMO GmbH, Berlin, Germany [email protected]   
   
  Abstract. Using the recently proposed maximal quadratic-free sets and the well-known monoidal strengthening procedure, we show how to improve intersection cuts for quadratically-constrained optimization problems by exploiting integrality requirements. We provide an explicit construction that allows an eÔ¨Écient implementation of the strengthened cuts along with computational results showing their improvements over the standard intersection cuts. We also show that, in our setting, there is unique lifting which implies that our strengthening procedure is generating the best possible cut coeÔ¨Écients for the integer variables.  
   
  Keywords: MIQCP  
   
  1  
   
  ¬∑ monoidal strengthening ¬∑ unique lifting  
   
  Introduction  
   
  In recent years, we have seen multiple eÔ¨Äorts in generating valid linear inequalities to quadratically constrained quadratic programs (QCQPs) which, using an epigraph formulation, we can assume have the form min{¬Ø cT s : s ‚àà S ‚äÜ Rp }  
   
  (1)  
   
  where S = {s ‚àà Rp : sT Qi s + bT i s + ci ‚â§ 0, i = 1, . . . , m}. One of the approaches to generate such valid inequalities has been the intersection cut paradigm [1,13, 19] which works as follows. We assume we have f ‚àà S, a basic feasible solution of a linear programming (LP) relaxation of (1). Additionally, we assume we have a simplicial conic relaxation K ‚äá S with apex f , and an S-free set C‚Äîa convex set satisfying int(C) ‚à© S = ‚àÖ‚Äîsuch that f ‚àà int(C). Using these ingredients, we can Ô¨Ånd a cutting plane separating f from S. In Fig. 1 we show a simple intersection cut in the case when all p rays of K intersect the boundary of the S-free set C. In such case, the intersection cut is simply deÔ¨Åned by the hyperplane containing all such intersection points. It is well known that one can assume C to be described as C = {s ‚àà Rp : œÜ(s ‚àí f ) ‚â§ 1} where œÜ is a sublinear function. For instance, œÜ c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 87‚Äì99, 2023. https://doi.org/10.1007/978-3-031-32726-1_7  
   
  88  
   
  A. Chmiela et al.  
   
  Fig. 1. An intersection cut (red) separating f from S (blue). The cut is computed using the intersection points of an S-free set C (green) and the rays of a simplicial cone K ‚äá S (boundary in orange) with apex f ‚àà S. Figure obtained from [8] (Color Ô¨Ågure online).  
   
  can be chosen as the gauge of C ‚àí f [17]. Further assuming w.l.o.g that the LP p relaxation is in standard form, we consider the constraint f + i=1 ri si ‚àà S with ri ‚àà Rp (e.g. the extreme rays of K) and si ‚àà R+ . Under these considerations, the intersection cut separating f is p   
   
  œÜ(ri )si ‚â• 1.  
   
  (2)  
   
  i=1  
   
  In [8,16] a method for constructing maximal quadratic-free sets (which ensures separation of any basic feasible solution f ‚àà S) and a computational implementation was developed, with positive results in a broad class of problems. One of the limitations of these cutting planes is that they do not use any integrality information: if we were to add integrality requirements to (1)‚Äîthus obtaining an MIQCP‚Äîthe intersection cuts would be completely oblivious to this. In this work, we remedy this via the monoidal strengthening framework [2]; a strengthening of intersection cuts based on integrality information. Monoidal strengthening leverages the  fact that some of the si in (2) are integer. The idea p is to take the relation f + i=1 ri si ‚àà S and modify it in the following way. Assume p to be integer. The above p relation implies that p that all si are restricted )s ‚àà S + m s . The points f + i=1 (ri + m i i i i i=1 i=1 mi si form a monoid p is, M satisÔ¨Åes 0 ‚àà M and M + M = M = {m : m = i=1 mi si , si ‚àà Z+ }, that p M . Thus, we obtain the new relation: f + i=1 (ri + mi )si ‚àà S + M . If it turns out that C is still S + M free, then we can use the function œÜ to generate a new cut. The above is summarized in the following result by Balas and Jeroslow. Theorem 1 ([2] Theorem 1). Let M be a monoid such that C is S + M -free and let I = {i ‚àà [p] : si ‚àà Z} be the index set of the integer variables. Then,   œÜ(ri )si + inf œÜ(ri + m)si ‚â• 1 i‚ààI /  
   
  i‚ààI  
   
  m‚ààM  
   
  is valid and dominates the intersection cut.  
   
  Monoidal Strengthening and Unique Lifting in MIQCPs  
   
  89  
   
  There are two main challenges in this technique. Firstly, to Ô¨Ånd a monoid M such that C stays S + M -free. Note that, equivalently, we can Ô¨Ånd a monoid M such that C ‚àí M is (possibly non-convex) S-free1 . Secondly, to eÔ¨Éciently solve the problem œà(r) := inf m‚ààM œÜ(r + m) for r the rays associated to integer non-basic variables and thus obtain a stronger cut coeÔ¨Écients. In this work we tackle both tasks. In most of this article S is deÔ¨Åned using a single quadratic inequality. As noted in [16], using linear transformations (diagonalization and homogenization), one can shift the focus from a generic quadratic set, S = {s ‚àà Rp : sT Qs + bT s + c ‚â§ 0}, to one of the following two sets: S h := {(x, y) ‚àà Rn+m : x ‚â§ y }, g  
   
  n+m  
   
  S := {(x, y) ‚àà R  
   
  (3) T  
   
  T  
   
  : x ‚â§ y , a x + d y = ‚àí1}.  
   
  (4)  
   
  where max{ a , d } = 1. Whether S gets mapped to S h or S g depends on whether the quadratic deÔ¨Åning S is homogeneous or not. Thus, one of the goals of this paper will be: using C as maximal S h - and S g -free sets of [16], to Ô¨Ånd a monoid M such that C is S h + M - or S g + M -free and, subsequently, strengthen the corresponding intersection cut. Monoidal strengthening is also related to lifting [3,9‚Äì11]. A nice property of œà is that it is subadditive2 . This implies that with œà we obtain sequence independent lifting, i.e. we can apply the strengthening to all integer variables at the same time. However, the monoidal lifting function œà is, in general, just one possible way of lifting. We can deÔ¨Åne the best possible coeÔ¨Écient that a particular integer variable can achieve, with the so-called lifting function œÄ [4,10].   1 ‚àí œÜ(s) : f + s + œÉr ‚àà S, œÉ ‚àà Z‚â•1 . (5) œÄ(r) = sup œÉ In general, œÄ is not subadditive so we do not have sequence independent lifting with it [4]. When it is subadditive, we say that there is unique lifting, because œÄ dominates any other lifting. For the case S = Zn ‚à© P it is well understood when we have unique lifting [3]. Our Ô¨Ånal goal is to show that in our setting there is unique lifting; more speciÔ¨Åcally, we show that choosing œÜ to be a minimal representation 3 of C ‚àí f , we have that œÄ = œà. Contributions. Our main contributions are: (1) we show that the monoidal strengthening framework does not produce any strengthening when S is deÔ¨Åned using a homogeneous quadratic; (2) in the non-homogeneous case, we show a family of cases where monoidal strengthening can be applied and an explicit monoid construction based on a maximal S g -free set of [16] which can be used for this strengthening; (3) we show an explicit formula for how to eÔ¨Éciently 1 2 3  
   
  With a slight abuse of notation, we refer to a non-convex set C ‚àí M as S-free whenever the convex set C ‚àí m is S-free for every m ‚àà M . A function œà is subadditive if œà(x + y) ‚â§ œà(x) + œà(y). This means that if œÅ is such that C = {s ‚àà Rp : œÅ(s ‚àí f ) ‚â§ 1} then œÅ(s) ‚â• œÜ(s).  
   
  90  
   
  A. Chmiela et al.  
   
  compute œà(r) in practice; (4) we show that in our setting there is unique lifting which, in particular, implies that œà yields the best coeÔ¨Écients in the strengthening of the intersection cut; and (5) we present extensive computational results that show the impact of this strengthening procedure. We remark that, even though our constructions are based on the structure of one quadratic, they can also be applied to MIQCPs with multiple quadratic inequalities: using our approach, it suÔ¨Éces to have one quadratic inequality being violated in order to ensure separation. In the interest of space, we do not present all details in this extended abstract. We refer the reader to our preprint available in [7].  
   
  2  
   
  Monoidal Strengthening in the Homogeneous Case  
   
  In this section, we analyze the case of S h and show that the monoidal strengthening framework does not produce any improvements when the cuts are created using maximal S h -free sets. The main reason behind this fact is that S h is a cone, and consequently every maximal S h -free set is a convex cone [6, Corollary 3]4 ; we show below why this is not a good setting for monoidal strengthening. In fact, the results in this section apply to a generic closed cone S and are stated with respect to such set. As mentioned before, for a given S-free set C, we are interested in Ô¨Ånding a monoid M such that C ‚àí M is S-free. The following result shows that in this case C ‚àí M = M . We remark that cone(¬∑) is the cone generated by a set, which may not be convex. Theorem 2. Let S, C ‚äÜ Rn where S is a closed cone and C is a convex maximal S-free set. Let M ‚äÜ Rn be a monoid such that C ‚àíM is S-free, then C ‚àíM = C. In particular, this implies that the cut obtained from monoidal strengthening would be the same as the standard intersection cut obtained through C. Proof (sketch). Since M is a monoid, 0 ‚àà M and thus C ‚äÜ C ‚àí M . It can be shown that cl cone(M ) is a convex cone such that C ‚àí cl cone(M ) is S-free. Note that C ‚àí cl cone(M ) is convex, and thus the maximality of C implies that C ‚àí cl cone(M ) ‚äÜ C. Since C ‚àí M ‚äÜ C ‚àí cl cone(M ), we conclude C ‚àí M = C. This last result shows that in the presence of a maximal S-free set C, there is not much to be gained from the monoidal strengthening framework when S is a cone. This negative property, nonetheless, can be reinterpreted as a way of detecting ‚Äúnon-maximality‚Äù of an S-free set: if one could Ô¨Ånd a monoid M such that C ‚àí M is S-free and C ‚àí M = C, then C is not maximal. We formalize this in the next result.  
   
  4  
   
  This citation deals with a particular set S, but the proof can be easily extended to any conic set S.  
   
  Monoidal Strengthening and Unique Lifting in MIQCPs  
   
  (a) Slices of S (blue) and CŒ∏ (orange)  
   
  91  
   
  (b) Slices of S (blue) and CŒ∏ (orange)  
   
  Fig. 2. Three-dimensional slices of S, CŒ∏ and CŒ∏ in Example 1 given by a = 1/10.  
   
  Proposition 1. Let S be a closed cone and let C be a full dimensional closed convex S-free cone. If there exists r ‚àà / ‚àíC such that C is S + cone(r)-free, then C is not a maximal S-free set. Furthermore, C + cone(‚àír) is S-free and strictly contains C. The next example illustrates an application of the last proposition, in a connection with the work of [6]. Example 1. Consider the set S = {(a, b, c, d) ‚àà R4 : ad = bc, a ‚â• 0}. Although this set does not fall into either the forms S h or S g which are our main objects of interest, it is still a closed conic set to which the results of this section apply. The set S is studied in [6], and appears when using lifted variables Xi,j representing bilinear terms xi xj . Let CŒ∏ = {(a, b, c, d) ‚àà R4 : cos(Œ∏)(a + d) + sin(Œ∏)(b ‚àí c) ‚â•  (a ‚àí d)2 + (b + c)2 }. In [6, Theorem 7], the authors show that CŒ∏ is maximal S-free for values of Œ∏ that satisfy cos(Œ∏) = 0 or sin(Œ∏) = 0. Using the results of this section, we can prove that if Œ∏ is such that cos(Œ∏) = 0 and sin(Œ∏) = 0, then / ‚àíCŒ∏ and CŒ∏ is not maximal S-free. More speciÔ¨Åcally, we can show that ‚àíe4 ‚àà that CŒ∏ is S + cone(‚àíe4 )-free, where e4 = (0, 0, 0, 1). Using Proposition 1, this implies that CŒ∏ = CŒ∏ + cone(e4 ) is S-free and strictly contains CŒ∏ . We leave out the details for the sake of brevity. In Fig. 2 we show a 3-dimensional slice of the 4-dimensional sets S and CŒ∏ , for Œ∏ = œÄ/4, showing how the S-free was enlarged. We remark that one can actually show that CŒ∏ is maximal S-free using the maximality criteria of [15].  
   
  92  
   
  A. Chmiela et al.  
   
  (a) S (blue) with maximal S-free set C (orange). In this case the two inequalities of C intersect S.  
   
  (b) Set of points not in S and ‚Äúto the left of the exposing points‚Äù (green). Note that the green region is not contained on the orange region: see the top left and bottom left.  
   
  Fig. 3. Constuction of the monoid for a maximal S-free set.  
   
  3  
   
  Monoidal Strengthening in the Non-homogeneous Case  
   
  In this case, the monoidal strengthening framework does produce improvements. The intuition for our construction is as follows. Consider the maximal S-free set C represented in Fig. 3a. The set is maximal, because all of its deÔ¨Åning inequalities Œ±T s ‚â§ Œ≤ have exposing points [16], that is, there exists s0 ‚àà C ‚à© S with Œ±T s0 = Œ≤ such that if Œ≥ T s ‚â§ Œ¥ is any other non-trivial valid inequality for C that is tight at s0 , then there exists a Œº > 0 such that Œ≥ = ŒºŒ± and Œ≤ = ŒºŒ¥. For example, if C is a polyhedron and s0 ‚àà C ‚à© S is an exposing point of an inequality, then that inequality is a facet and s0 is in its relative interior. Thus, in the example of Fig. 3a, the two exposing points of C are the points of the facets of C that are tangent to S. We see that a way of translate C such that the translation is S-free is by moving the apex of C to a point not in S and to the left of the exposing points (see Fig. 3b). This is the basic idea behind our monoid construction, and below we show how to formalize it. 3.1  
   
  A Technical Consideration for S g  
   
  Before motivating the construction the monoid, we need to provide some details on the construction of maximal S g -free presented in [16]. This construction starts from the maximal S h -free set CŒª = {(x, y) ‚àà Rn+m : y ‚â§ ŒªT x},  
   
  (6)  
   
  where Œª is a vector in the unit sphere. Note that CŒª can be equivalently described as CŒª = {(x, y) ‚àà Rn+m : Œ≤ T y ‚â§ ŒªT x ‚àÄŒ≤ ‚àà D1 } where D1 is the unit sphere of appropriate dimension. The proof that CŒª is maximal S h -free boils down to ÀÜ the vector (Œª, Œ≤) ÀÜ ‚àà S h ‚à© CŒª is tight for the inequality noting that for each Œ≤,  
   
  Monoidal Strengthening and Unique Lifting in MIQCPs  
   
  93  
   
  Œ≤ÀÜT y ‚â§ ŒªT x and for no other of CŒª . This means that each inequality of CŒª indexed by Œ≤ has an exposing point in S h . Moving to the non-homogeneous case, since S g = S h ‚à© H, where H = {(x, y) ‚àà Rn+m : aT x + dT y = ‚àí1}, the set C = CŒª ‚à© H is clearly S g -free5 , but it is not necessarily maximal. The maximal S g -free constructed in [16] Ô¨Årst identiÔ¨Åes the inequalities of CŒª for which an exposing point can be found in H, keeps them, and relaxes the rest. The inequalities given by Œ≤ such that Œ≤ = 1 and aT Œª + dT Œ≤ < 0 are the ones that have the desired exposing points; these exposing points are ‚àí  
   
  aT Œª  
   
  1 (Œª, Œ≤) + dT Œ≤  
   
  Maximality of the resulting set is shown using the exposing points above and, for relaxed inequalities, a diverging sequence in S g that approaches the inequality indeÔ¨Ånitely (an exposing sequence). This is due to the fact that these relaxed inequalities may have never intersect S g . In our current monoid construction, we require that all inequalities to have exposing points. This requirement translates to aT Œª + dT Œ≤ < 0 for all Œ≤ with  
   
  Œ≤ = 1. This, in turn, reduces to d < ‚àíaT Œª. Note that this condition implies that C = CŒª ‚à© H is maximal S g -free with respect to H [16]. Additionally, this implies that we can assume a = max{ a , d } = 1. We believe that when these assumptions are not fulÔ¨Ålled, monoidal strengthening cannot be applied. Proving this conjecture is part of future work. 3.2  
   
  Monoid Construction  
   
  Using the considerations of the previous section, we can formalize the notion of ‚Äúleft of the exposing points‚Äù: we Ô¨Årst consider the halfspace {(x, y) ‚àà Rn+m : (a ‚àí ŒªT aŒª)T x ‚â• 0} which contains the exposing points and the directions of lineality of CŒª ‚à© H. Secondly, when translating CŒª by a vector m we can modify m by a vector in the lineality space of CŒª without changing the translation. Thus, we restrict to vectors m that live in a subspace that contains the exposing points and is orthogonal to the lineality space of CŒª ‚à© H. This subspace is given by {Œª, a} √ó Rm where {Œª, a} denotes the span of {Œª, a}. Thus, we have the following set representing the points ‚Äúleft of the exposing points‚Äù: L = {(x, y) ‚àà {Œª, a} √ó Rm : aT x + dT y = ‚àí1, x ‚â• y , (a ‚àí ŒªT aŒª)T x ‚â• 0}. To obtain the translation we Ô¨Ånd the apex of CŒª ‚à© H in the space {Œª, a} √ó Rm . This point is given by   ‚àí1 ŒªT a ŒΩ = (x0 , 0) := a+ Œª, 0 . (7) 1 ‚àí (ŒªT a)2 1 ‚àí (ŒªT a)2 5  
   
  Note that S g is contained on a halfspace, so S g -freeness is with respect to the induced topology in H.  
   
  94  
   
  A. Chmiela et al.  
   
  Thus, L ‚àí ŒΩ is a candidate to represent the translations of C that would result in an S g -free set. Note that the assumptions d < ‚àíŒªT a and d ‚â§ 1 imply (ŒªT a)2 < 1, thus ŒΩ is well-deÔ¨Åned. Recall that the translations we consider for C are given by ‚Äúminus the monoid‚Äù and that a monoid must contain the origin, therefore our candidate for a monoid is M = {(x, y) ‚àà {Œª, a} √ó Rm : aT x + dT y = 0, x ‚àí x0 ‚â• y , T  
   
  (8)  
   
  T  
   
  (a ‚àí Œª aŒª) x ‚â§ ‚àí1} ‚à™ {(0, 0)}. Theorem 3. Let M be deÔ¨Åned as in (8) with d < ‚àíŒªT a and a = Œª = 1. The set M is a monoid. Proof (sketch). This proof is highly technical, so we just present the high-level strategy for obtaining the desired result. See [7] for the details. We equivalently show that ‚àíM is a monoid. Thus, we take two vector (xi , yi ) ‚àà ‚àíM , i = 1, 2, and show that their sum is in ‚àíM . This is trivial whenever one of the vectors is (0, 0). The linear constraints in the deÔ¨Ånition of ‚àíM are satisÔ¨Åed trivially, hence the main argument is to show that  
   
  x1 + x2 + x0 ‚â• y1 + y2 . This is equivalent to showing that the value of the following optimization problem is non-negative. min { x1 + x2 + x0 2 ‚àí y1 + y2 2 : (xi , yi ) ‚àà ‚àíM \ {(0, 0)}, i = 1, 2}  
   
  xi ,yi  
   
  2 Using that xi + x0 ‚â• yi and (a ‚àí ŒªT aŒª)T xi ‚â• 1 ‚áî ‚àíxT 0 xi ‚â• x0 , we 2 can lower bound the objective function by x1 + x2 + x0 ‚àí y1 + y2 2 ‚â• T 2 2xT 1 x2 ‚àí 2y1 y2 ‚àí x0 . Hence, to show that T min {xT 1 x2 ‚àí y1 y2 : (xi , yi ) ‚àà ‚àíM \ {(0, 0)}, i = 1, 2}  
   
  xi ,yi  
   
  (P )  
   
  is lower bounded by 21 x0 2 suÔ¨Éces. Note that we can decompose yi = œâi d + œÅi where œÅi is orthogonal to d. Furthermore, since xi ‚àà {a, Œª} , we can write xi = Œ∏i a + Œ∑i Œª. Using this together with the fact that ŒªT x0 = 0 and aT x0 = ‚àí1, the hyperplane in ‚àíM becomes 0 = aT xi +dT yi = Œ∏i +Œ∑i ŒªT a+œâi d 2 . Furthermore, 2 2 we get ‚àíxT 0 xi ‚â• x0 ‚áî Œ∏i ‚â• x0 , and expanding the nonlinear constraint we reformulate problem (P ) as min  
   
  Œ∏i ,Œ∑i ,œâi ,œÅi  
   
  s.t.  
   
  Œ∏1 Œ∏2 + Œ∑1 Œ∑2 + Œ∏1 Œ∑2 ŒªT a + Œ∏2 Œ∑1 ŒªT a ‚àí w1 w2 d 2 ‚àí œÅT 1 œÅ2 0 ‚â§ Œ∏i2 + Œ∑i2 + 2Œ∏i Œ∑i ŒªT a ‚àí 2Œ∏i + x0 2 ‚àí wi2 d 2 ‚àí œÅi 2  
   
  x0 2 ‚â§ Œ∏i  
   
  (Pexp )  
   
  d 2 œâi = ‚àíŒ∏i + Œ∑i ŒªT a The remainder of the proof focuses on showing the desired lower bound for this problem. The key elements of the proof involve: Ô¨Årst showing that the problem is simply bounded, then showing that constraint x0 2 ‚â§ Œ∏i can be assumed to be tight. This is shown leveraging results from [18]. Using this we show the desired lower bound (Pexp ) ‚â• 12 x0 2 .  
   
  Monoidal Strengthening and Unique Lifting in MIQCPs  
   
  95  
   
  The last required result for monoidal strengthening is the following. Theorem 4. Let S g and CŒª be deÔ¨Åned as in (4) and (6) respectively, and H = {(x, y) ‚àà Rn+m : aT x+dT y = ‚àí1}. Let M be deÔ¨Åned as in (8) with d < ‚àíŒªT a and a = Œª = 1. The set CŒª ‚à© H ‚àí M is S g -free.  
   
  4  
   
  Solving the Monoidal Strengthening Problem  
   
  In order to strengthen the cut using Theorem 1 and the construction in Sect. 3, we need to solve œà(r) = inf m‚ààM œÜ(r + m), where œÜ is such that CŒª ‚à© H = {s : œÜ(s ‚àí f ) ‚â§ 1}. From now on, Œª = ffxx  , where f is the point we want to separate, i.e., f ‚àà / S g . Furthermore, we restrict CŒª ‚à© H to {Œª, a} √ó Rm because any representation of CŒª ‚à© H is invariant in the directions of the lineality space of CŒª ‚à© H, namely, {Œª, a} ‚ä• √ó {0}. Thus, we deÔ¨Åne C = CŒª ‚à© H ‚à© {Œª, a} √ó Rm . Likewise, we restrict all rays to {Œª, a} √ó Rm . We work with the minimal representation of C ‚àí f ; we can prove it is given by Œ≤ T s ‚àíŒªT s supŒ≤=1 ŒªT fxy ‚àíŒ≤ T fxy , if s ‚àà H and sx ‚àà {Œª, a} œÜ(s) = (9) +‚àû, otherwise. The monoidal problem is equivalent to œà(r) = inf{œÑ : œÜ(r + m) ‚â§ œÑ, m ‚àà M }. In order to understand this problem better, we need to understand the set {s : œÜ(s) ‚â§ œÑ }. Lemma 1. Let œÜ be the minimal representation of C ‚àí f given in (9). Then {s : œÜ(s) ‚â§ œÑ } = C ‚àí ŒΩ ‚àí œÑ (f ‚àí ŒΩ), where ŒΩ is deÔ¨Åned in (7) (the apex of CŒª ‚à© H in the space {Œª, a} √ó Rm ). From this lemma, we can show that œà(r) = inf {œÑ : r + ŒΩ + œÑ (f ‚àí ŒΩ) ‚àà C ‚àí M } .  
   
  (10)  
   
  In other words, solving the monoidal strengthening problem reformulates to Ô¨Ånding the Ô¨Årst intersection point between the line l(œÑ ) = r + ŒΩ + œÑ (f ‚àí ŒΩ), and the set C ‚àí M . It can be shown that C ‚àí M = L ‚à™ C, thus, œà(r) = inf{œÑ : l(œÑ ) ‚àà L ‚à™ C} = min{œÑ1 , œÑ2 }, where œÑ1 = inf{œÑ : l(œÑ ) ‚àà L} and œÑ2 = inf{œÑ : l(œÑ ) ‚àà C}. Note that œÑ2 corresponds to the normal intersection cut coeÔ¨Écient œÜ(r). The following proposition shows how to evaluate œà(r). Proposition 2. Let œÑ¬Ø be the largest root of the univariate quadratic equation œÑ ) ‚àà L, then œà(r) = œÑ¬Ø. Otherwise,  
   
  lx (œÑ ) 2 = ly (œÑ ) 2 . If the root exists and l(¬Ø œà(r) = œÜ(r). To Ô¨Ånalize this section, we show how to use this result starting from a general quadratic constraint. Consider S to be deÔ¨Åned by a general quadratic constraint, i.e., S = {s ‚àà Rp : sT Qs + bT s + c ‚â§ 0} with Q ‚àà Rp√óp , b ‚àà Rp and c ‚àà R. Let  
   
  96  
   
  A. Chmiela et al.  
   
  s¬Ø ‚àà / S be the point we want to separate. In [8] the authors transform S using the eigenvalue decomposition Q = V ŒòV T . Let Œ∏i , i ‚àà [p], be the eigenvalues of Q, and let I+ = {i : Œ∏i > 0}, I‚àí = {i : Œ∏i < 0} and I0 = {i : Œ∏i = 0}. Furthermore, denote by vi the i-th eigenvector of Q, that is, the i-th column of V . We avoid showing the full transformation here, but an important fact is that the conditions d < ‚àíŒªT a < 1 we need for applying monoidal strengthening in S g become 1  (viT b)2 >0 (11) (V T b)I0 = 0 ‚àß c ‚àí 4 Œ∏i i‚ààI+ ‚à™I‚àí  
   
  The following result summarizes the necessary computations. Proposition 3. Suppose conditions (11) are met. The computation of œà(r) for a given ray r reduces to computing the largest root of AœÑ 2 + BœÑ + D = 0 where      

  2 b A= Œ∏i viT (¬Ø s ‚àí ŒΩ) , B = 2 Œ∏i viT (¬Ø s ‚àí ŒΩ) viT (r + ŒΩ + ) , 2Œ∏i i‚ààI+ ‚à™I‚àí i‚ààI+ ‚à™I‚àí   2  b D= Œ∏i viT (r + ŒΩ + ) +Œ∫ 2Œ∏i i‚ààI+ ‚à™I‚àí  
   
  Œ∫ T s+ j‚ààI+ Œ∏j (vj (¬Ø  
   
  ŒΩ = ‚àí  
   
  1 Œ∫=c‚àí 4  
   
   i‚ààI+ ‚à™I‚àí  
   
   b 2 2Œ∏j )) j‚ààI+  
   
  vij (vjT (¬Ø s+  
   
  b )) ‚àí 2Œ∏j  
   
   j‚ààI+ ‚à™I‚àí  
   
  vij  
   
  vjT b 2Œ∏j  
   
  (viT b)2 Œ∏i  
   
  We note that we also need to compute the cut coeÔ¨Écient œÜ(r), but this can also be done eÔ¨Éciently as shown in [8]. The expressions on the previous proposition may not provide too much insight themselves, as they are accumulating a series of transformations to bring S to S g . However, we believe their value relies in that, given an eigenvalue decomposition for a general quadratic inequality, one can simply plug-in the desired parameters and obtain a univariate quadratic that yields the strengthened coeÔ¨Écients of an intersection cut.  
   
  5  
   
  Unique Lifting  
   
  As mentioned in the introduction, monoidal strengthening is just one way of improving the cut coeÔ¨Écients of integer variables. The best possible coeÔ¨Écient that a particular integer variable can achieve is given by the lifting function œÄ deÔ¨Åned in (5). If œÄ is subadditive, then there is unique lifting [4]. This means that the lifting using œÄ can be applied simultaneously to all rays ri corresponding to integer variables and dominates any other lifting. In this section, we show that if we use œÜ the minimal representation of C ‚àí f shown in (9), we have œÄ = œà;  
   
  Monoidal Strengthening and Unique Lifting in MIQCPs  
   
  (a) œà(r) in (10) searches for the smallest step œÑ such that r + ŒΩ + œÑ (f ‚àí ŒΩ) ‚àà C ‚àí M . The latter set is depicted in green.  
   
  97  
   
  (b) œÄ1 (r) in (13) searches for the largest step œÑ such that r + ŒΩ + œÑ (f ‚àí ŒΩ) ‚àà S g ‚àí rec(C). The latter set is depicted in green  
   
  Fig. 4. Comparison of the deÔ¨Ånitions of œà and œÄ1 showing why they are equal. In both Ô¨Ågures, S g is depicted in blue and C in orange. (Color Ô¨Ågure online)  
   
  since œà is a subadditive function we obtain unique lifting and, moreover, that the procedure of Sect. 4 yields the best possible lifting coeÔ¨Écients. Let œÄ1 be the restriction of œÄ to œÉ = 1. Slightly reformulating the optimization problem, we see that œÄ1 (r) = sup {œÑ : f + s + r ‚àà S g , œÜ(s) ‚â§ 1 ‚àí œÑ }  
   
  (12)  
   
  Using Lemma 1, we can show that f + s + r ‚àà S g and œÜ(s) ‚â§ 1 ‚àí œÑ reformulate to r + ŒΩ + œÑ (f ‚àí ŒΩ) ‚àà S g ‚àí (C ‚àí ŒΩ). Since C ‚àí ŒΩ = rec(C), (12) becomes œÄ1 (r) = sup {œÑ : r + ŒΩ + œÑ (f ‚àí ŒΩ) ‚àà S g ‚àí rec(C)} .  
   
  (13)  
   
  Notice that problem (13) is very similar to the monoidal problem (10). Moreover, we can use this‚Äîplus structural results we leave out for the sake of space‚Äîto show that œà(r) = œÄ1 (r). This is almost what we want. In Fig. 4 we illustrate the deÔ¨Ånitions of both œÄ1 and œà to provide some intuition on why this result holds. To make the connection with œÄ we prove the following lemma. Lemma 2. Let M be a monoid such that C is S + M -free and œÄ1 (r) = sup{1 ‚àí œÜ(s) : f + s + r ‚àà S}. If œÄ1 is subadditive, then œÄ = œÄ1 and we have unique lifting. Since œà is subadditive and œà = œÄ1 , we directly obtain the following theorem. Theorem 5. Consider œà the monoidal strengthening function and œÄ the lifting function, both deÔ¨Åned using œÜ as in (9). Then œÄ = œà, in particular, there is unique lifting.  
   
  6  
   
  Computational Results  
   
  In this section, we show results of computational experiments testing the eÔ¨Écacy of the monoidal strengthening procedure we propose. We embedded the  
   
  98  
   
  A. Chmiela et al.  
   
  Table 1. Summary of results for branch-and-bound experiments. Rows labeled [t, 7200] consider instances where one of the settings took at least t seconds. Columns labeled relative show the relative improvement of monoidal compared to icuts. subset  
   
  instances icuts solved time  
   
  all 189 [0, 7200] 115 83 [1, 7200] 81 [10, 7200] 23 [100, 7200] [1000, 7200] 10  
   
  113 113 81 79 21 8  
   
  221.87 22.81 67.62 72.54 724.66 2475.04  
   
  nodes 5282 936 2377 2574 186545 631764  
   
  monoidal solved time 115 115 83 81 23 10  
   
  214.63 21.56 62.40 66.56 565.24 1252.96  
   
  nodes 5321 883 2184 2341 144747 307639  
   
  relative time nodes 0.97 0.95 0.92 0.92 0.78 0.51  
   
  0.97 0.94 0.92 0.91 0.78 0.49  
   
  computation of the monoidal strengthening cut coeÔ¨Écients in SCIP 8.0 [5] as a subroutine of the already implemented intersection cut generator. As the underlying LP solver, we used CPLEX 12.10.0.0. For testing, we used a Linux cluster of Intel Xeon CPU E5-2680 0 2.70 GHz with 20MB cache and 64GB main memory. The time limit in all experiments was set to two hours. The test set we consider consists of the publicly available instances of the MINLPLib [14] and QPLib [12]. We selected all non-convex instances with (mixed)-integer constriants and at least one quadratic constraint of the correct case, leaving us with 95 instances. Furthermore, we Ô¨Åltered out all instances that are either infeasible, where no dual bound was found or where monoidal strengthening could not been applied. This leaves us with a heterogeneous test set of 63 instances with 8‚Äì23826 variables and 12‚Äì24971 constraints. All experiments are run with three diÔ¨Äerent permutations for each instance. We treat every instance-permutation pair as an individual instance, since permuting the constraints and variables of a problem formulation may considerably change the solving process. We consider two diÔ¨Äerent settings that are both based on SCIP‚Äôs default settings: icuts additionally generates the original intersection cuts, whereas monoidal uses the strengthened cutting planes if possible. Furthermore, we restrict icuts and monoidal to add at most 20 intersection cuts per quadratic constraint. We found this to be the best performing setting compared to default SCIP. Summarized results can be found in Table 1. monoidal consistently outperforms icuts with respect to solving time as well as number of nodes needed. On the whole test set, the strengthened intersection cuts reduce both metrics by around 3% while solving two more instances. This improvement increases when looking at harder instances: On the hardest test set [1000, 7200] containing only instances for which at least one setting needs 1000 seconds or more, monoidal uses 49% less time and 51% less nodes. These results show that the proposed monoidal strengthening procedure signiÔ¨Åcantly improves the standard intersection cuts, which highlights the importance of exploiting integrality whenever possible. Nonetheless, our cuts are currently not able to improve the overall performance of default SCIP. One of the main reasons is that our cuts, while helping in obtaining better dual bounds, are  
   
  Monoidal Strengthening and Unique Lifting in MIQCPs  
   
  99  
   
  negatively aÔ¨Äecting the performance of SCIP‚Äôs primal heuristics. Improving this behavior is subject of ongoing work.  
   
  References 1. Balas, E.: Intersection cuts‚Äìa new type of cutting planes for integer programming. Oper. Res. 19(1), 19‚Äì39 (1971) 2. Balas, E., Jeroslow, R.G.: Strengthening cuts for mixed integer programs. Eur. J. Oper. Res. 4(4), 224‚Äì234 (1980) 3. Basu, A., Campelo, M., Conforti, M., Cornu¬¥ejols, G., Zambelli, G.: Unique lifting of integer variables in minimal inequalities. Math. Program. 141(1‚Äì2), 561‚Äì576 (2012) 4. Basu, A., Dey, S.S., Paat, J.: Nonunique lifting of integer variables in minimal inequalities. SIAM J. Discret. Math. 33(2), 755‚Äì783 (2019) 5. Bestuzheva, K., et al.: The SCIP Optimization Suite 8.0. ZIB-Report 21‚Äì41, Zuse Institute Berlin, December 2021 6. Bienstock, D., Chen, C., Munoz, G.: Outer-product-free sets for polynomial optimization and oracle-based cuts. Math. Program. 1‚Äì44 (2020) 7. Chmiela, A., MuÀú noz, G., Serrano, F.: Monoidal strengthening and unique lifting in MIQCPs (2022). https://www.gonzalomunoz.org/publications/ 8. Chmiela, A., MuÀú noz, G., Serrano, F.: On the implementation and strengthening of intersection cuts for QCQPs. Math. Program. pp. 1‚Äì38 (2022) 9. Conforti, M., Cornu¬¥ejols, G., Zambelli, G.: A geometric perspective on lifting. Oper. Res. 59(3), 569‚Äì577 (2011) 10. Dey, S.S., Wolsey, L.A.: Two row mixed-integer cuts via lifting. Math. Program. 124(1‚Äì2), 143‚Äì174 (2010) ¬¥ 11. Fukasawa, R., Poirrier, L., Xavier, A.S.: The (not so) trivial lifting in two dimensions. Math. Program. Comput. 11(2), 211‚Äì235 (2018). https://doi.org/10.1007/ s12532-018-0146-5 12. Furini, F., et al.: A library of quadratic programming instances. Programming Computation, QPLIB (2018) 13. Glover, F.: Convexity cuts and cut search. Oper. Res. 21(1), 123‚Äì134 (1973) 14. MINLP library (2010). http://www.minlplib.org/ 15. MuÀú noz, G., Serrano, F.: Maximal quadratic-free sets. In: Bienstock, D., Zambelli, G. (eds.) IPCO 2020. LNCS, vol. 12125, pp. 307‚Äì321. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-45771-6 24 16. MuÀú noz, G., Serrano, F.: Maximal quadratic-free sets. Math. Program. 1‚Äì42 (2021) 17. Rockafellar, R.T.: Convex Analysis. Princeton University Press, Princeton (1970) 18. Santana, A., Dey, S.S.: The convex hull of a quadratic constraint over a polytope. SIAM J. Optim. 30(4), 2983‚Äì2997 (2020) 19. Tuy, H.: Concave programming with linear constraints. In: Doklady Akademii Nauk, vol. 159, pp. 32‚Äì35. Russian Academy of Sciences (1964)  
   
  From Approximate to Exact Integer Programming Daniel Dadush1 , Friedrich Eisenbrand2 , and Thomas Rothvoss3(B) 1 CWI, Amsterdam, The Netherlands  
   
  [email protected]   
   
  2 EPFL, Lausanne, Switzerland  
   
  [email protected]   
   
  3 University of Washington, Seattle, USA  
   
  [email protected]   
   
  Abstract. Approximate integer programming is the following: For a given convex body K ‚äÜ Rn , either determine whether K ‚à© Zn is empty, or find an integer point in the convex body 2 ¬∑ (K ‚àí c) + c which is K , scaled by 2 from its center of gravity c. Approximate integer programming can be solved in time 2O(n) while the fastest known methods for exact integer programming run in time 2O(n) ¬∑ n n . So far, there are no efficient methods for integer programming known that are based on approximate integer programming. Our main contribution are two such methods, each yielding novel complexity results. First, we show that an integer point x ‚àó ‚àà (K ‚à©Zn ) can be found in time 2O(n) , provided that the remainders of each component x i‚àó mod  for some arbitrarily fixed  ‚â• 5(n + 1) of x ‚àó are given. The algorithm is based on a cutting-plane technique, iteratively halving the volume of the feasible set. The cutting planes are determined via approximate integer programming. Enumeration of the possible remainders gives a 2O(n) n n algorithm for general integer programming. This matches the current best bound of an algorithm by Dadush (2012) that is considerably more involved. Our algorithm also relies on a new asymmetric approximate Carath√©odory theorem that might be of interest on its own. Our second method concerns integer programming problems in standard equation form Ax = b, 0 ‚â§ x ‚â§ u, x ‚àà Zn . Such a problem can be reduced to  the solution of i O(log u i + 1) approximate integer programming problems. This implies, for example that knapsack or subset-sum problems with polynomial variable range 0 ‚â§ x i ‚â§ p(n) can be solved in time (log n)O(n) . For these problems, the best running time so far was n n ¬∑ 2O(n) .  
   
  A full version of this paper can be found under https://arxiv.org/abs/2211.03859. D. Dadush‚ÄîSupported by ERC Starting Grant no. 805241-QIP. F. Eisenbrand‚ÄîSupported by the Swiss National Science Foundation (SNSF) grant 185030 and 207365. T. Rothvoss‚ÄîSupported by NSF CAREER grant 1651861 and a David & Lucile Packard Foundation Fellowship. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 100‚Äì114, 2023. https://doi.org/10.1007/978-3-031-32726-1_8  
   
  From Approximate to Exact Integer Programming  
   
  101  
   
  1 Introduction Many combinatorial optimization problems as well as many problems from the algorithmic geometry of numbers can be formulated as an integer linear program max{‚å©c, x‚å™ | Ax ‚â§ b, x ‚àà Zn }  
   
  (1)  
   
  where A ‚àà Zm√ón , b ‚àà Zm and c ‚àà Zn , see, e.g. [16,27,30]. Lenstra [23] has shown that integer programming can be solved in polynomial time, if the number of variables 2 is fixed. A careful analysis of his algorithm yields a running time of 2O(n ) times a polynomial in the binary encoding length of the input of the integer program. Kannan [19] has improved this to n O(n) , where, from now on we ignore the extra factor that depends polynomially on the input length. The current best algorithm is the one of Dadush [10] with a running time of 2O(n) ¬∑ n n . The question whether there exists a singly exponential time, i.e., a 2O(n) -algorithm for integer programming is one of the most prominent open problems in the area of algorithms and complexity. Integer programming can be described in the following more general form. Here, a convex body is synonymous for a full-dimensional compact and convex set. Integer Programming (IP) Given a convex body K ‚äÜ Rn , find an integer solution x ‚àó ‚àà K ‚à© Zn or assert that K ‚à© Zn = . The convex body K must be well described in the sense that there is access to a separation oracle, see [16]. Furthermore, one assumes that K contains a ball of radius r > 0 and that it is contained in some ball of radius R. In this setting, the current best running times hold as well. The additional polynomial factor in the input encoding length becomes a polynomial factor in log(R/r ) and the dimension n. Central to this paper is Approximate integer programming which is as follows. Approximate Integer Programming (Approx-IP) Given a convex body K ‚äÜ Rn , let c ‚àà Rn be its center of gravity. Either find an integer vector x ‚àó ‚àà (2 ¬∑ (K ‚àí c) + c) ‚à© Zn , or assert that K ‚à© Zn = . The convex body 2 ¬∑ (K ‚àí c) + c is K scaled by a factor of 2 from its center of gravity. The algorithm of Dadush [11] solves approximate integer programming in singly exponential time 2O(n) . Despite its clear relation to exact integer programming, there is no reduction from exact to approximate known so far. Our guiding question is the following: Can approximate integer programming be used to solve the exact version of (specific) integer programming problems? 1.1 Contributions of This Paper We present two different algorithms to reduce the exact integer programming problem (IP) to the approximate version (A PPROX -IP).  
   
  102  
   
  D. Dadush et al.  
   
  a) Our first method is a randomized cutting-plane algorithm that, in time 2O(n) and for any  ‚â• 5(n + 1) finds a point in K ‚à© (Zn /) with high probability, if K contains an integer point. This algorithm uses an oracle for (A PPROX -IP) on K intersected with one side of a hyperplane that is close to the center of gravity. Thereby, the algorithm collects  integer points close to K . The collection is such that the convex combination with uniform weights 1/ of these points lies in K . If, during an iteration, no point is found, the volume of K is roughly halved and eventually K lies on a lower-dimensional subspace on which one can recurse. b) If equipped with the component-wise remainders v ‚â° x ‚àó (mod ) of a solution x ‚àó of (IP), one can use the algorithm to find a point in (K ‚àí v) ‚à© Zn and combine it with the remainders to a full solution of (IP), using that (K ‚àí v) ‚à© Zn = . This runs in singly exponential randomized time 2O(n) . Via enumeration of all remainders, one obtains an algorithm for (IP) that runs in time 2O(n) ¬∑ n n . This matches the best-known running time for general integer programming [11], which is considerably involved. c) Our analysis depends on a new approximate Carath√©odory theorem that we develop in Sect. 4. While approximate Carath√©odory theorems are known for centrally symmetric convex bodies [4,26,28], our version is for general convex sets and might be of interest on its own. d) Our second method is for integer programming problems Ax = b, x ‚àà Zn , 0 ‚â§ x ‚â§ u in equation standard form. We show that such a problem can be reduced  to 2O(n) ¬∑ ( i log(u i + 1)) instances of (A PPROX -IP). This yields a running time of (log n)O(n) for such IPs, in which the variables are bounded by a polynomial in the dimension. The so-far best running time for such instances 2O(n) ¬∑n n . Well known benchmark problems in this setting are knapsack and subset-sum with polynomial upper bounds on the variables, see Sect. 5. 1.2 Related Work If the convex body K is an ellipsoid, then the integer programming problem (IP) is the well known closest vector problem (CVP) which can be solved in time 2O(n) with an algorithm by Micciancio and Voulgaris [25]. Bl√∂mer and Naewe [7] previously observed that the sampling technique of Ajtai et al. [1] can be modified in such a way as to solve the closest vector approximately. More precisely, they showed that a (1 + )-approximation of the closest vector problem can be found in time O(2 + 1/)n time. This was later generalized to arbitrary convex sets by Dadush [11]. This algorithm either asserts that the convex body K does not contain any integer points, or it finds an integer point in the body stemming from K is scaled by (1+) from its center of gravity. Also the running time of this randomized algorithm is O(2 + 1/)n . In our paper, we restrict to the case  = 1 which can be solved in singly exponential time. The technique of reflection sets was also used by Eisenbrand et al. [13] to solve (CVP) in the ‚àû -norm approximately in time O(2 + log(1/))n . In the setting in which integer programming can be attacked with dynamic programming, tight upper and lower bounds on the complexity are known [14,17,20]. Our n n ¬∑ 2O(n) algorithm could be made more efficient by constraining the possible remainders of a solution (mod ) efficiently. This barrier is different than the one in  
   
  From Approximate to Exact Integer Programming  
   
  103  
   
  classical integer-programming methods that are based on branching on flat directions [16,23] as they result in a branching tree of size n O(n) . The subset-sum problem is as follows. Given a set Z ‚äÜ N of n positive integers and  a target value t ‚àà N, determine whether there exists a subset S ‚äÜ Z with s‚ààS s = t . Subset sum is a classical NP-complete problem that serves as a benchmark in algorithm design. The problem can be solved in pseudopolynomial time [5] by dynamic programming. The current fastest pseudopolynomial-time algorithm is the one of Bringmann [8] that runs in time O(n + t ) up to polylogarithmic factors. There exist instances of subset-sum whose set of feasible solutions, interpreted as 0/1 incidence vectors, require numbers of value n n in the input, see [2]. Lagarias and Odlyzko [21] have shown that instances of subset sum in which each number of the input Z is 2 drawn uniformly at random from {1, . . . , 2O(n ) } can be solved in polynomial time with high probability. The algorithm of Lagarias and Odlyzko is based on the LLLalgorithm [22] for lattice basis reduction.  
   
  2 Preliminaries A lattice Œõ is the set of integer combinations of linearly independent vectors, i.e. Œõ := Œõ(B ) := {B x | x ‚àà Zr } where B ‚àà Rn√ór has linearly independent columns. The determinant is the volume of the r -dimensional parallelepiped spanned by the columns of the basis B , i.e. det(Œõ) := detr (B T B ). We say that Œõ has full rank if n = r . In that case the determinant is simply det(Œõ)  = | detn (B )|. For a full rank lattice Œõ, we denote the dual lattice by Œõ‚àó = {y ‚àà Rn | x, y ‚àà Z ‚àÄx ‚àà Œõ}. Note that det(Œõ‚àó ) ¬∑ det(Œõ) = 1. For an introduction to lattices, we refer to [24]. A set Q ‚äÜ Rn is called a convex body if it is convex, compact and has a non-empty interior. A set Q is symmetric if Q = ‚àíQ. Recall that any symmetric convex body Q naturally induces a norm  ¬∑ Q of the form xQ = min{s ‚â• 0 | x ‚àà sQ}. For a full rank lattice Œõ ‚äÜ Rn and a symmetric convex body Q ‚äÜ Rn we denote Œª1 (Œõ,Q) := min{xQ | x ‚àà Œõ \ {0}} as the length of the shortest vector with respect to the norm induced by n Q. We denote the Euclidean ball by B 2n := {x ‚àà Rn | x2 ‚â§ 1} and the ‚àû -ball by B ‚àû := n n n [‚àí1, 1] . An (origin centered) ellipsoid is of the form E = A(B 2 ) where A : R ‚Üí Rn is an invertible linear map. For any  such ellipsoid E there is a unique positive definite matrix M ‚àà Rn√ón so that x = x T M x. The barycenter (or centroid) of a convex  E 1 body Q is the point Voln (Q) Q x d x. We will use the following version of (A PPROX -IP) that runs in time 2O(n) , provided that the symmetrizer for the used center c is large enough. This is the case for c being the center of gravity, see Theorem 3. Note that the center of gravity of a convex body can be (approximately) computed in randomized polynomial time [6,12].  
   
  Theorem 1 (Dadush [11]). There is a 2O(n) -time algorithm A PX IP(K , c, Œõ) that takes as input a convex set K ‚äÜ Rn , a point c ‚àà K and a lattice Œõ ‚äÜ Rn . Assuming that Voln ((K ‚àí c) ‚à© (c ‚àí K )) ‚â• 2‚àíŒò(n) Voln (K ) the algorithm either returns a point x ‚àà (c + 2(K ‚àí c)) ‚à© Œõ or returns EMPTY if K ‚à© Œõ = . One of the classical results in the geometry of numbers is Minkowski‚Äôs Theorem which we will use in the following form:  
   
  104  
   
  D. Dadush et al.  
   
  Theorem 2 (Minkowski‚Äôs Theorem). For a full rank lattice Œõ ‚äÜ Rn and a symmetric convex body Q ‚äÜ Rn one has  det(Œõ) 1/n Œª1 (Œõ,Q) ‚â§ 2 ¬∑ Voln (Q) We will use the following bound on the density of sublattices which is an immediate consequence of Minkowski‚Äôs Second Theorem. Here we abbreviate Œª1 (Œõ) := Œª1 (Œõ, B 2n ). Lemma 1. Let Œõ ‚äÜ Rn be a full rank lattice. Then for any k-dimensional sublattice 1 (Œõ) k Àú ‚äÜ Œõ one has det(Œõ) Àú ‚â• ( Œª Œõ ) . k  
   
  Finally, we revisit a few facts from convex geometry. Details and proofs can be found in the excellent textbook by Artstein-Avidan, Giannopoulos and Milman [3]. Lemma 2 (Gr√ºnbaum‚Äôs Lemma). Let K ‚äÜ Rn be any convex body and let ‚å©a, x‚å™ = Œ≤ be any hyperplane through the barycenter of K . Then 1e Voln (K ) ‚â§ Voln ({x ‚àà K | ‚å©a, x‚å™ ‚â§ Œ≤}) ‚â§ (1 ‚àí 1e )Voln (K ). For a convex body K , there are two natural symmetric convex bodies that approximate K in many ways: the ‚Äúinner symmetrizer‚Äù K ‚à© (‚àíK ) (provided 0 ‚àà K ) and the ‚Äúouter symmetrizer‚Äù in form of the difference body K ‚àí K . The following is a consequence of a more general inequality of Milman and Pajor. Theorem 3. Let K ‚äÜ Rn be any convex body with barycenter 0. Then Voln (K ‚à©(‚àíK )) ‚â• 2‚àín Voln (K ). In particular Theorem 3 implies that choosing c as the barycenter of K in Theorem 1 results in a 2O(n) running time‚Äîhowever this will not be the choice that we will later make for c. Also the size of the difference body can be bounded: Theorem 4 (Inequality of Rogers and Shephard). For any convex body K ‚äÜ Rn one has Voln (K ‚àí K ) ‚â§ 4n Voln (K ).   Recall that for a convex body Q with 0 ‚àà int(Q), the polar is Q ‚ó¶ = {y ‚àà Rn | x, y ‚â§ 1 ‚àÄx ‚àà Q}. We will use the following relation between volume of a symmetric convex body and the volume of the polar; to be precise we will use the lower bound (which is due to Bourgain and Milman). Theorem 5 (Blaschke-Santal√≥-Bourgain-Milman). For any symmetric convex body Q ‚äÜ Rn one has Voln (Q) ¬∑ Voln (Q ‚ó¶ ) Cn ‚â§ ‚â§1 Voln (B 2n )2 where C > 0 is a universal constant. We will also rely on the result of Frank and Tardos to reduce the bit complexity of constraints: Theorem 6 (Frank, Tardos [15]). There is a polynomial time algorithm that takes Àú ‚àà Zn+1 with a Àú ‚â§ Àú b) Àú ‚àû , |b| (a, b) ‚àà Qn+1 and Œî ‚àà N+ as input and produces a pair (a, O(n 3 ) O(n 2 ) Àú x‚å™ = bÀú and ‚å©a, x‚å™ ‚â§ b ‚áî ‚å©a, Àú x‚å™ ‚â§ bÀú for all ¬∑Œî so that ‚å©a, x‚å™ = b ‚áî ‚å©a, 2 x ‚àà {‚àíŒî, . . . , Œî}n .  
   
  From Approximate to Exact Integer Programming  
   
  105  
   
  3 The Cut-Or-Average Algorithm First, we discuss our C UT-O R-AVERAGE algorithm that on input of a convex set K , a lattice Œõ and integer  ‚â• 5(n +1), either finds a point x ‚àà Œõ  ‚à©K or decides that K ‚à©Œõ = O(n) n  
   
  in time 2 . Note that for any polyhedron K = {x ‚àà R | Ax ‚â§ b} with rational A, b and lattice Œõ with basis B one can compute a value of Œî so that log(Œî) is polynomial in the encoding length of A, b and B and K ‚à©Œõ = if and only if K ‚à©[‚àíŒî, Œî]n ‚à©Œõ = . See Schrijver [31] for details. In other words, w.l.o.g. we may assume that our convex set is bounded. The pseudo code of the algorithm can be found in Fig. 1. An intuitive description of the algorithm is as follows: we compute the barycenter c of K and an ellipsoid E that approximates K up to a factor of R = n + 1. Then we iteratively use the oracle for approximate integer programming from Theorem 1 to find a convex combination z of lattice points in a 3-scaling of K until z is close to the barycenter c. If this succeeds, then we can directly use an asymmetric version of the Approximate Carath√©odory Theorem (Lemma 9) to find an unweighted average of  lattice points that lies in K ; this would be a vector of the form x ‚àà Œõ  ‚à© K . If the algorithm fails to approximately express c as a convex combination of lattice points, then we will have found a hyperplane H going almost through the barycenter c so that K ‚à© H‚â• does not contain a lattice point. Then the algorithm continues searching in K ‚à© H‚â§ (Fig. 2). This case might happen repeatedly, but after polynomial number of times, the volume of K will have dropped below a threshold so that we may recurse on a single (n ‚àí 1)-dimensional subproblem. We will now give the detailed analysis. Note that in order to obtain a clean exposition we did not aim to optimize any constant. However by merely tweaking the parameters one could make the choice of  = (1 + Œµ)n work for any constant Œµ > 0. 3.1 Bounding the Number of Iterations We begin the analysis with a few estimates that will help us to bound the number of iterations. Lemma 3. Any point x found in line (7) lies in a 3-scaling of K around c, i.e. x ‚àà c + 3(K ‚àí c) assuming 0 < œÅ ‚â§ 1. Proof. We verify that x ‚àà (c ‚àí œÅd ) + 2(K ‚àí (c ‚àí œÅd )) = c + 2(K ‚àí c) + œÅd ‚äÜ c + 3(K ‚àí c) using that œÅd E = œÅ ‚â§ 1. Next we bound the distance of z to the barycenter: Lemma 4. At the beginning of the kth iterations of the WHILE loop on line (5), one 2 has c ‚àí z2E ‚â§ 9Rk . Proof. We prove the statement by induction on k. At k = 1, by construction on line (4), z ‚àà c + 2(K ‚àí c) ‚äÜ c + 2R E . Thus c ‚àí z2E ‚â§ (2R)2 ‚â§ 9R 2 , as needed.  
   
  106  
   
  D. Dadush et al.  
   
  Fig. 1. The Cut-Or-Average algorithm.  
   
  œÅ Fig. 2. Visualization of the inner WHILE loop where Q := K ‚à© {x ‚àà Rn | ‚å©a, x‚å™ ‚â• a, c + 2 d }.  
   
  Now assume k ‚â• 2. Let z, z  denote the values of z during iteration k ‚àí 1 before and after the execution of line (9) respectively, and let x be the vector found on line (7) during iteration k ‚àí1. Note that z  = (1‚àí k1 )z + k1 x. By the induction hypothesis, we have that z ‚àí c2E ‚â§ 9R 2 /(k ‚àí 1). Our goal is to show that z  ‚àí c2E ‚â§ 9R 2 /k. Letting d denote the normalized version of z ‚àí c, we see that d E = 1 and hence d ‚àà K ‚àí c. By construction ‚å©a, x ‚àí c‚å™ ‚â• 0 and from Lemma 3 we have x ‚àà c + 3(K ‚àí c) which implies  
   
  From Approximate to Exact Integer Programming  
   
  107  
   
  x ‚àí cE ‚â§ 3R. The desired bound on the E -norm of z  ‚àí c follows from the following calculation: 2  1 1 z  ‚àí c2E = 1 ‚àí (z ‚àí c) + (x ‚àí c) E k k   1 1 2 1 1 2 ‚å©a, x ‚àí c‚å™ + 2 x ‚àí c2E = 1‚àí z ‚àí cE ‚àí 2 1 ‚àí k k k k  1 2 1 ‚â§ 1‚àí z ‚àí c2E + 2 x ‚àí c2E k k  1 1 2 1 9R 2 + 2 ¬∑ 9R 2 = . ‚â§ 1‚àí k k ‚àí1 k k In particular Lemma 4 implies an upper bound on the number of iterations of the inner WHILE loop: Corollary 1. The WHILE loop on line (5) never takes more than 36R 2 iterations. Proof. By Lemma 4, for k := 36R 2 one has c ‚àí z2E ‚â§  
   
  9R 2 k  
   
  ‚â§ 14 .  
   
  Next, we prove that every time we replace K by K  ‚äÇ K in line (8), its volume drops by a constant factor. œÅ  
   
  Lemma 5. In step (8) one has Voln (K  ) ‚â§ (1 ‚àí 1e ) ¬∑ (1 + 2 )n ¬∑ Voln (K ) for any œÅ ‚â• 0. In 1 one has Voln (K  ) ‚â§ 34 Voln (K ). particular for 0 ‚â§ œÅ ‚â§ 4n Proof. The claim is invariant under affine linear transformations, hence we may assume w.l.o.g. that E = B 2n , M = I n and c = 0. Note that then B 2n ‚äÜ K ‚äÜ RB 2n . Let us abbreviate K ‚â§t := {x ‚àà K | ‚å©d , x‚å™ ‚â§ t }. In this notation K  = K ‚â§œÅ/2 . Recall that Gr√ºnbaum‚Äôs Lemma (Lemma 2) guarantees that  
   
  1 e  
   
  ‚â§  
   
  Voln (K ‚â§0 ) Voln (K )  
   
  ‚â§ 1 ‚àí 1e . Moreover, it is well  
   
  known that the function t ‚Üí Voln (K ‚â§t )1/n is concave on its support, see again [3]. Then Voln (K ‚â§0 )1/n ‚â•  
   
    
   
   œÅ/2 1 ¬∑ Voln (K ‚â§œÅ/2 )1/n + ¬∑ Voln (K ‚â§‚àí1 )1/n   1 + œÅ/2 1 + œÅ/2  
   
  1 ¬∑ Voln (K ‚â§œÅ/2 )1/n ‚â• 1 + œÅ/2   
   
  and so  
   
    
   
  1‚àí  
   
  ‚â•0  
   
   1 n 1 ¬∑ Voln (K ) ‚â• Voln (K ‚â§0 ) ‚â• ¬∑ Voln (K ‚â§œÅ/2 ) e 1 + œÅ/2 œÅ  
   
  Rearranging gives the first claim in the form Voln (K ‚â§œÅ/2 ) ‚â§ (1 ‚àí 1e ) ¬∑ (1 + 2 )n ¬∑ Voln (K ). œÅ œÅ 1 For the 2nd part we verify that for œÅ ‚â§ 4n one has (1‚àí 1e )¬∑(1+ 2 )n ‚â§ (1‚àí 1e )¬∑exp( 2 ) ‚â§ 34 . Lemma 6. Consider a call of C UT-O R-AVERAGE on (K , Œõ) where K ‚äÜ r B 2n for some r > 0. Then the total number of iterations of the outer WHILE loop over all recursion levels is bounded by O(n 2 log( Œª1nr(Œõ) )).  
   
  108  
   
  D. Dadush et al.  
   
  Proof. Consider any recursive run of the algorithm. The convex set will be of the Àú := Œõ ‚à© U where U is a subform KÀú := K ‚à© U and the lattice will be of the form Œõ Àú as n-dimensional Àú space and we denote nÀú := dim(U ). We think of KÀú and Œõ objects. Let KÀú t ‚äÜ KÀú be the convex body after t iterations of the outer WHILE loop. Recall that VolnÀú (KÀú t ) ‚â§ ( 34 )t ¬∑ VolnÀú (KÀú ) by Lemma 5 and VolnÀú (KÀú ) ‚â§ r nÀú VolnÀú (B 2nÀú ). Our goal is to show Àú ‚àó with y Àú Àú ‚ó¶ ‚â§ 1 that for t large enough, there is a non-zero lattice vector y ‚àà Œõ (K t ‚àíK t )  
   
  2  
   
  which then causes the algorithm to recurse. To prove existence of such a vector y, we use Minkowski‚Äôs Theorem (Theorem 2) followed by the Blaschke-Santal√≥-BourgainMilman Theorem (Theorem 5) to obtain Àú ‚àó , (KÀú t ‚àí KÀú t )‚ó¶ ) Œª1 (Œõ  
   
  1/nÀú Àú ‚àó) det(Œõ ‚ó¶ VolnÀú ((KÀú t ‚àí KÀú t ) )  Vol (KÀú ‚àí KÀú ) 1/nÀú Thm 5 t t nÀú ‚â§ 2C ¬∑ Àú ¬∑ VolnÀú (B nÀú )2 det(Œõ) 2  1/nÀú Thm 4 VolnÀú (KÀú t ) nÀú  ¬∑C ‚â§ 2¬∑4¬∑ Àú ¬∑ VolnÀú (B nÀú ) 2 det(Œõ) 2  
   
  Thm 2  
   
    
   
  ‚â§  
   
  2¬∑  
   
  ‚â§  
   
   nÀú ¬∑ r (3/4)t /nÀú ¬∑ (3/4)t /nÀú 4C nÀú ¬∑ r ¬∑ ‚â§ 4C ¬∑ Àú 1/nÀú Œª1 (Œõ) det(Œõ)  
   
  nÀú Here we use the convenient estimate of VolnÀú (B 2nÀú ) ‚â• VolnÀú ( 1 B ‚àû ) = ( 2 )nÀú . Moreover, nÀú  
   
  nÀú  
   
  Àú 1 (Œõ) n Àú ‚â• ( Œª we have used that by Lemma 1 one has det(Œõ) ) Àú . Then t = Œò(nÀú log( Œª1nr (Œõ) )) nÀú 1 Àú ‚àó , (KÀú t ‚àí KÀú t )‚ó¶ ) ‚â§ and the algorithm recurses. Hence the iterations suffice until Œª1 (Œõ 2  
   
  total number of iterations of the outer WHILE loop over all recursion levels can be bounded by O(n 2 log( Œª1nr(Œõ) )). The iteration bound of Lemma 6 can be improved by amortizing the volume reduction over the different recursion levels following the approach of Jiang [18]. We refrain from that to keep our approach simple. 3.2 Correctness and Efficiency of Subroutines Next, we verify that the subroutines are used correctly. The proofs in this section are deferred to the full version of this paper. Lemma 7. For any convex body K ‚äÜ Rn one can compute the barycenter c and a 0centered ellipsoid E in randomized polynomial time so that c + E ‚äÜ K ‚äÜ c + (n + 1)E . In order for the call of A PX IP in step (7) to be efficient, we need that the symmetrizer of the set is large enough volume-wise, see Theorem 1. In particular for any Àú ‚à© (cÀú ‚àí Q)) ‚â• parameters 2‚àíŒò(n) ‚â§ œÅ ‚â§ 0.99 and R ‚â§ 2O(n) we will have Voln ((Q ‚àí c) ‚àíŒò(n) 2 Voln (Q) which suffices for our purpose.  œÅ  Lemma 8. In step (7), the set Q := {x ‚àà K | ‚å©a, x‚å™ ‚â• a, c + 2 d } and the point cÀú := c+œÅd œÅ Àú ‚à© (cÀú ‚àíQ)) ‚â• (1 ‚àí œÅ)n ¬∑ 2R ¬∑ 2‚àín ¬∑ Voln (Q). satisfy Voln ((Q ‚àí c)  
   
  From Approximate to Exact Integer Programming  
   
  109  
   
  3.3 Conclusion on the Cut-Or-Average Algorithm From the discussion above, we can summarize the performance of the algorithm in Fig. 1 as follows: Theorem 7. Given a full rank matrix B ‚àà Qn√ón and parameters r > 0 and  ‚â• 5(n + 1) with  ‚àà N and a separation oracle for a closed convex set K ‚äÜ r B 2n , there is a randomized algorithm that with high probability finds a point x ‚àà K ‚à© 1 Œõ(B ) or decides that K ‚à©Œõ(B ) = . Here the running time is 2O(n) times a polynomial in log(r ) and the encoding length of B . This can be easily turned into an algorithm to solve integer linear programming: Theorem 8. Given a full rank matrix B ‚àà Qn√ón , a parameter r > 0 and a separation oracle for a closed convex set K ‚äÜ r B 2n , there is a randomized algorithm that with high probability finds a point x ‚àà K ‚à© Œõ(B ) or decides that there is none. The running time is 2O(n) n n times a polynomial in log(r ) and the encoding length of B . Proof. Suppose that K ‚à© Œõ = and fix an (unknown) solution x ‚àó ‚àà K ‚à© Œõ. We set  := 5(n + 1). We iterate through all v ‚àà {0, . . . ,  ‚àí 1}n and run Theorem 7 on the set K and the shifted lattice v + Œõ. For the outcome of v with x ‚àó ‚â° v mod  one has K ‚à© (v + Œõ) = and so the algorithm will discover a point x ‚àà K ‚à© (v + Œõ).  
   
  4 An Asymmetric Approximate Carath√©odory Theorem The Approximate Carath√©odory Theorem states the following. Given any point-set X ‚äÜ B 2n in the unit ball with 0 ‚àà conv(X ) and a parameter k ‚àà N, there exist u 1 , . . . , u k ‚àà X (possibly with repetition) such that 1    k u i ‚â§ O 1/ k . k i =1 2  
   
  The theorem is proved, for example, by Novikoff [28] in the context of the perceptron algorithm. An p -version was provided by Barman [4] to find Nash equilibria. Deterministic and nearly-linear time methods to find the convex combination were recently described in [26]. In the following, we provide a generalization to asymmetric convex bodies and the dependence on k will be weaker but sufficient for our analysis of our C UT- OR-AVERAGE algorithm from Sect. 3. Recall that with a symmetric convex body K , we one can associate the Minkowski norm  ¬∑ K with xK = inf{s ‚â• 0 | x ‚àà sK }. In the following we will use the same definition also for an arbitrary convex set K with 0 ‚àà K . Symmetry is not given but one still has x + yK ‚â§ xK + yK for all x, y ‚àà Rn and Œ±xK = Œ±xK for Œ± ‚àà R‚â•0 . Using this notation we can prove the main result of this section. Lemma 9. Given a point-set X ‚äÜ K contained in a convex set K ‚äÜ Rn with 0 ‚àà conv(X ) and a parameter k ‚àà N, there exist u 1 , . . . , u k ‚àà X (possibly with repetition) so that 1  k u i ‚â§ min{|X |, n + 1}/k. k i =1 K  
   
  110  
   
  D. Dadush et al.  
   
  Moreover, given X as input, the points u 1 , . . . , u k can be found in time polynomial in |X |, k and n. Proof. Let  = min{|X |, n + 1}. The claim is true whenever k ‚â§  since then we may simply pick an arbitrary point in X . Hence from now on we assume k > . By Carath√©odory‚Äôs theorem, there exists a convex combination of zero, using    elements of X . We write 0 = i =1 Œªi v i where v i ‚ààX , Œªi ‚â• 0 for i ‚àà [] and i =1 Œªi = 1.  Consider the numbers L i = (k ‚àí )Œªi + 1. Clearly, i =1 L i = k. This implies that there  exists an integer vector Œº ‚àà N with Œº ‚â• (k ‚àí )Œª and i =1 Œºi = k. It remains to show that we have 1   Œºi v i ‚â§ /k. k i =1 K  
   
  In fact, one has       Œºi v i = (Œºi ‚àí (k ‚àí )Œªi ) v i + (k ‚àí ) Œªi v i    
   
  K K    
   
  i =1 i =1 i =1 ‚â•0  
   
  ‚â§  
   
  ‚â•0  
   
    (Œºi ‚àí (k ‚àí )Œªi ) v i K +(k ‚àí ) Œªi v i ‚â§ .  
   
    K i =1 i =1  
   
    ‚â§1    
   
  =0  
   
  For the moreover part, note that the coefficients Œª1 , . . . , Œª are the extreme points of a linear program which can be found in polynomial time. Finally, the linear system  Œº ‚â• (k ‚àí )Œª, i =1 Œºi = k has a totally unimodular constraint matrix and the right hand side is integral, hence any extreme point solution is integral as well, see e.g. [31]. Lemma 10. For any integer  ‚â• 5(n + 1), the convex combination Œº computed in line  (10) satisfies x‚ààX Œºx x ‚àà K . Proof. We may translate the sets X and K so that c = 0 without affecting the claim. Recall that z ‚àà conv(X ). By Carath√©odory‚Äôs Theorem there are v 1 , . . . , v m ‚àà X with m ‚â§ n + 1 so that z ‚àà conv{v 1 , . . . , v m } and so 0 ‚àà conv{v 1 ‚àí z, . . . , v m ‚àí z}. We have v i ‚àà 3K by Lemma 3 and ‚àíz ‚àà 14 E ‚äÜ 14 K as well as z ‚àà 14 K . Hence v i ‚àí zK ‚â§ v i K +‚àí zK ‚â§ m Zm 13 ‚â•0 4 . We apply Lemma 9 and obtain a convex combination Œº ‚àà  with  i =1 Œºi (v i ‚àí m z) 13 K ‚â§  . Then 4  
   
    m m 13 m 1 + ‚â§1 Œºi v i ‚â§ Œºi (v i ‚àí z) + zK ‚â§  
   
    K K 4  4 i =1 i =1 ‚â§1/4  
   
  if  ‚â•  
   
  13 3 m. This is satisfies if  ‚â• 5(n + 1).  
   
  5 IPs with Polynomial Variable Range Now we come to our second method that reduces (IP) to (A PPROX -IP) that applies to integer programming in standard equation form Ax = b, x ‚àà Zn , 0 ‚â§ x i ‚â§ u i , i = 1, . . . , n,  
   
  (2)  
   
  From Approximate to Exact Integer Programming  
   
  111  
   
  Here, A ‚àà Zm√ón , b ‚àà Zm , and the u i ‚àà N+ are positive integers that bound the variables from above. Our main goal is to prove the following theorem. Theorem 9. The integer feasibility problem in standard equation form (see (2)) can  be solved in time 2O(n) ni=1 log2 (u i + 1). We now describe the algorithm. It is again based on the approximate integer programming technique of Dadush [11]. We exploit it to solve integer programming exactly via the technique of reflection sets developed by Cook et al. [9]. For each i = 1, . . . , n we consider the two families of hyperplanes that slice the feasible region with the shifted lower and upper bounds respectively x i = 2 j ‚àí1 and x i = u i ‚àí 2 j ‚àí1 , 0 ‚â§ j ‚â§ log2 (u i ).  
   
  (3)  
   
  Following [9], we consider two points w, v that lie in the region between two consecutive planes x i = 2 j ‚àí1 and x i = 2 j for some j . Suppose that w i ‚â§ v i holds. Let s be the point such that w = 1/2(s + v). The line-segment s, v is the line segment w, v scaled by a factor of 2 from v. Let us consider what can be said about the i -th component of s. Clearly s i ‚â• 2 j ‚àí1 ‚àí (2 j ‚àí 2 j ‚àí1 ) = 0. Similarly, if w and v lie in the region in-between x i = 0 and x i = 1/2, then s i ‚â• ‚àí1/2. We conclude with the following observation. Lemma 11. Consider the hyperplane arrangement defined by the equations (3) as well as by x i = 0 and x i = u i for 1 ‚â§ i ‚â§ n. Let K ‚äÜ Rn a cell of this hyperplane arrangement and v ‚àà K . If K  is the result of scaling K by a factor of 2 from v, i.e. K  = {v + 2(w ‚àí v) | w ‚àà K }, then K  satisfies the inequalities ‚àí1/2 ‚â§ x i ‚â§ u i + 1/2 for all 1 ‚â§ i ‚â§ n. We use this observation to prove Theorem 9: Proof (Proof of Theorem 9). The task of (2) is to find an integer point in the affine subspace defined by the system of equations Ax = b that satisfies the bound constraints 0 ‚â§ x i ‚â§ u i . We first partition the feasible region with the hyperplanes (3) as well as x i = 0 and x i = u i for each i . We then apply the approximate integer programming algorithm with approximation factor 2 on each convex set P K = {x ‚àà Rn | Ax = b} ‚à© K where K ranges over all cells of the arrangement. In 2O(n) time, the algorithm either finds an integer point in the convex set C K that results from P K by scaling it with a factor of 2 from its center of gravity, or it asserts that P K does not contain an integer point. Clearly, C K ‚äÜ {x ‚àà Rn | Ax = b} and if the algorithm returns an integer point x ‚àó , then, by Lemma 11, this integer point also satisfies the bounds 0 ‚â§ x i ‚â§ u i . The running time of the algorithm is equal to the number of cells times 2O(n) which is  2O(n) ni=1 log2 (u i + 1). IPs in Inequality Form We can also use Theorem 9 to solve integer linear programs in inequality form. Here the efficiency is strongly dependent on the number of inequalities.  
   
  112  
   
  D. Dadush et al.  
   
  Theorem 10. Let A ‚àà Qm√ón , b ‚àà Qm , c ‚àà Qn and u ‚àà Nn+ . Then the integer linear program   max ‚å©c, x‚å™ | Ax ‚â§ b, 0 ‚â§ x ‚â§ u, x ‚àà Zn can be solved in time n O(m) ¬∑ (2 log(1 + Œî))O(n+m) where Œî := max{u i | i = 1, . . . , n}. Proof. Via binary search it suffices to solve the feasibility problem ‚å©c, x‚å™ ‚â• Œ≥, Ax ‚â§ b, 0 ‚â§ x ‚â§ u, x ‚àà Zn  
   
  (4)  
   
  in the same claimed running time. We apply the result of Frank and Tardos (Theorem 6) and replace c, Œ≥, A, b by integer-valued objects of bounded ¬∑‚àû -norm so that the feasible region of (4) remains the same. Hence we may indeed assume that c ‚àà Zn , 3 2 Œ≥ ‚àà Z, A ‚àà Zm√ón and b ‚àà Zm with c‚àû , |Œ≥|, A‚àû , b‚àû ‚â§ 2O(n ) ¬∑ ŒîO(n ) . Any feasible solution x to (4) has a slack bounded by Œ≥‚àí‚å©c, x‚å™ ‚â§ |Œ≥|+c‚àû ¬∑n ¬∑Œî ‚â§ N where we may 3 2 choose N := 2O(n ) ŒîO(n ) . Similarly b i ‚àí A i x ‚â§ N for all i ‚àà [n]. We can then introduce slack variables y ‚àà Z‚â•0 and z ‚àà Zm ‚â•0 and consider the system ‚å©c, x‚å™ + y = Œ≥, Ax + z = b, 0 ‚â§ x ‚â§ u, 0 ‚â§ y ‚â§ N , 0 ‚â§ z j ‚â§ N ‚àÄ j ‚àà [m], (x, y, z) ‚àà Zn+1+m  
   
  (5)  
   
  in equality form which is feasible if and only if (4) is feasible. Then Theorem 9 shows that such an integer linear program can be solved in time 2O(n+m) ¬∑  
   
   n i =1  
   
  ln(1 + u i ) ¬∑ (ln(1 + N ))m+1 ‚â§ n O(m) ¬∑ (2 log(1 + Œî))O(n+m) .  
   
  Subset Sum and Knapsack The subset-sum problem (with multiplicities) is an integer program of the form (2) with one linear constraint. Polak and Rohwedder [29] have shown that subset-sum  with multiplicities‚Äîthat means ni=1 x i z i = t , 0 ‚â§ x i ‚â§ u i ‚àÄi ‚àà [n], x ‚àà Zn ‚Äîcan be 5/3 solved in time O(n+z max ) times a polylogarithmic factor where z max := maxi =1,...,n z i . The algorithm of Frank and Tardos [15] (Theorem 6) finds an equivalent instance in 3  
   
  2  
   
  O(n ) . All-together, if each multiplicity is bounded which z max is bounded by 2O(n ) u max by a polynomial p(n), then the state-of-the-art for subset-sum with multiplicities is straightforward enumeration resulting in a running time n O(n) which is the current best running time for integer programming. We can significantly improve the running time in this regime. This is a direct consequence of Theorem 10.  Corollary 2. The subset sum problem with multiplicities of the form ni=1 x i z i = t , 0 ‚â§ x ‚â§ u, x ‚àà Zn can be solved in time 2O(n) ¬∑ (log(1 + u‚àû ))n . In particular if each multiplicity is bounded by a polynomial p(n), then it can be solved in time (log n)O(n) .  
   
  Knapsack with multiplicities is the following integer programming problem   max ‚å©c, x‚å™ | x ‚àà Zn‚â•0 , ‚å©a, x‚å™ ‚â§ Œ≤, 0 ‚â§ x ‚â§ u ,  
   
  (6)  
   
  From Approximate to Exact Integer Programming  
   
  113  
   
  where c, a, u ‚àà Zn‚â•0 are integer vectors. Again, via the preprocessing algorithm of Frank and Tardos [15] (Theorem 6) one can assume that c‚àû as well as a‚àû are 3 O(n 2 ) . If each u i is bounded by a polynomial in the dimenbounded by 2O(n ) u max sion, then the state-of-the-art for this problem is again straightforward enumeration which leads to a running time of n O(n) . Also in this regime, we can significantly improve the running time which is an immediate consequence of Theorem 10. Corollary 3. A knapsack problem (6) can be solved in time 2O(n) ¬∑ (log(1 + u‚àû ))n . In particular if u‚àû is bounded by a polynomial p(n) in the dimension, it can be solved in time (log n)O(n) .  
   
  References 1. Mikl√≥s Ajtai, R.K., Sivakumar, D.: A sieve algorithm for the shortest lattice vector problem. In: Proceedings of the Thirty-Third Annual ACM Symposium on Theory of Computing, pp. 601‚Äì610 (2001) Àú V.H.: Anti-hadamard matrices, coin weighing, threshold gates, and indecom2. Alon, N., Vu, posable hypergraphs. J. Comb. Theory Ser. A 79(1), 133‚Äì160 (1997) 3. Artstein-Avidan, S., Giannopoulos, A., Milman, V.D.: Asymptotic geometric analysis. Part I, Volume 202 of Mathematical Surveys and Monographs. American Mathematical Society, Providence, RI (2015) 4. Barman, S.: Approximating nash equilibria and dense bipartite subgraphs via an approximate version of Caratheodory‚Äôs theorem. In: Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing, pp. 361‚Äì369 (2015) 5. Bellman, R.: Dynamic programming. Science 153(3731), 34‚Äì37 (1966) 6. Bertsimas, D., Vempala, S.: Solving convex programs by random walks. J. ACM (JACM) 51(4), 540‚Äì556 (2004) 7. Bl√∂mer, J., Naewe, S.: Sampling methods for shortest vectors, closest vectors and successive minima. Theor. Comput. Sci. 410(18), 1648‚Äì1665 (2009) 8. Bringmann, K.: A near-linear pseudopolynomial time algorithm for subset sum. In: Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 1073‚Äì1084. SIAM (2017) 9. Cook, W., Hartmann, M., Kannan, R., McDiarmid, C.: On integer points in polyhedra. Combinatorica 12(1), 27‚Äì37 (1992) 10. Dadush, D.: Integer Programming, Lattice Algorithms, and Deterministic Volume Estimation. Georgia Institute of Technology (2012) 11. Dadush, D.: A randomized sieving algorithm for approximate integer programming. Algorithmica 70(2), 208‚Äì244 (2014) 12. Dyer, M., Frieze, A., Kannan, R.: A random polynomial-time algorithm for approximating the volume of convex bodies. J. ACM 38(1), 1‚Äì17 (1991) 13. Eisenbrand, F., H√§hnle, N., Niemeier, M.: Covering cubes and the closest vector problem. In: Proceedings of the Twenty-Seventh Annual Symposium on Computational Geometry, pp. 417‚Äì423 (2011) 14. Eisenbrand, F., Weismantel, R.: Proximity results and faster algorithms for integer programming using the Steinitz lemma. ACM Trans. Algorithms (TALG) 16(1), 1‚Äì14 (2019) 15. Frank, A., Tardos, √â.: An application of simultaneous diophantine approximation in combinatorial optimization. Combinatorica 7(1), 49‚Äì65 (1987) 16. Gr√∂tschel, M., Lov√°sz, L., Schrijver, A.: Geometric Algorithms and Combinatorial Optimization. Algorithms and Combinatorics, vol. 2. Springer, Heidelberg (1988). https://doi. org/10.1007/978-3-642-78240-4  
   
  114  
   
  D. Dadush et al.  
   
  17. Jansen, K., Rohwedder, L.: On integer programming and convolution. In: 10th Innovations in Theoretical Computer Science Conference (ITCS 2019). Schloss Dagstuhl-LeibnizZentrum fuer Informatik (2018) 18. Jiang, H.: Minimizing convex functions with integral minimizers. In: SODA, pp. 976‚Äì985. SIAM (2021) 19. Kannan, R.: Minkowski‚Äôs convex body theorem and integer programming. Math. Oper. Res. 12(3), 415‚Äì440 (1987) 20. Knop, D., Pilipczuk, M., Wrochna, M.: Tight complexity lower bounds for integer linear programming with few constraints. ACM Trans. Comput. Theory (TOCT) 12(3), 1‚Äì19 (2020) 21. Lagarias, J.C., Odlyzko, A.M.: Solving low-density subset sum problems. J. ACM (JACM) 32(1):229‚Äì246 (1985) 22. Lenstra, A.K., Lenstra, H.W., Lov√°sz, L.: Factoring polynomials with rational coefficients. Mathematische annalen 261(ARTICLE), 515‚Äì534 (1982) 23. Lenstra Jr., H.W.: Integer programming with a fixed number of variables. Math. Oper. Res. 8(4), 538‚Äì548 (1983) 24. Micciancio, D., Goldwasser, S.: Complexity of lattice problems - a cryptograhic perspective. The Kluwer International Series in Engineering and Computer Science, vol. 671. Springer, New York (2002). https://doi.org/10.1007/978-1-4615-0897-7 25. Micciancio, D., Voulgaris, P.: A deterministic single exponential time algorithm for most lattice problems based on voronoi cell computations. In: Proceedings of the Forty-Second ACM Symposium on Theory of Computing, pp. 351‚Äì358 (2010) 26. Mirrokni, V., Leme, R.P., Vladu, A., Wong, S.C.: Tight bounds for approximate Carath√©odory and beyond. In: International Conference on Machine Learning, pp. 2440‚Äì2448. PMLR (2017) 27. Nemhauser, G.L., Wolsey, L.A.: Integer programming. In: Nemhauser, G.L., et al. (eds.) Optimization. Handbooks in Operations Research and Management Science, chapter VI, vol. 1, pp. 447‚Äì527. Elsevier (1989) 28. Novikoff, A.B.: On convergence proofs for perceptrons. Technical report, Office of Naval Research, Washington, D.C. (1963) 29. Polak, A., Rohwedder, L., WÀõegrzycki, K.: Knapsack and subset sum with small items. In: 48th International Colloquium on Automata, Languages, and Programming (ICALP 2021), number CONF, pp. 106‚Äì1. Schloss Dagstuhl-Leibniz-Zentrum f√ºr Informatik (2021) 30. Schrijver, A.: Polyhedral combinatorics. In: Graham, R., Gr√∂tschel, M., Lov√°sz, L. (eds.) Handbook of Combinatorics, chapter 30, vol. 2, pp. 1649‚Äì1704. Elsevier (1995) 31. Schrijver, A.: Theory of Linear and Integer Programming. Wiley-Interscience Series in Discrete Mathematics and Optimization. Wiley, Hoboken (1999)  
   
  Optimizing Low Dimensional Functions over the Integers Daniel Dadush1 , Arthur L¬¥eonard2 , Lars Rohwedder3(B) , and Jos¬¥e Verschae4 1  
   
  4  
   
  CWI, Amsterdam, Netherlands [email protected]  2 ENS, Paris, France [email protected]  3 Maastricht University, Maastricht, Netherlands [email protected]  PontiÔ¨Åcia Universidad Cat¬¥ olica de Chile, Santiago, Chile [email protected]   
   
  Abstract. We consider box-constrained integer programs with objective g(W x) + cT x, where g is a ‚Äúcomplicated‚Äù function with an m dimensional domain. Here we assume we have n  m variables and that W ‚àà Zm√ón is an integer matrix with coeÔ¨Écients of absolute value at most Œî. We design an algorithm for this problem using only the mild assumption that the objective can be optimized eÔ¨Éciently when all but m variables are Ô¨Åxed, 2 yielding a running time of nm (mŒî)O(m ) . Moreover, we can avoid the term m n in several special cases, in particular when c = 0. Our approach can be applied in a variety of settings, generalizing several recent results. An important application are convex objectives of low domain dimension, where we imply a recent result by Hunkenschr¬® oder et al. [SIOPT‚Äô22] for the 0-1-hypercube and sharp or separable convex g, assuming W is given explicitly. By avoiding the direct use of proximity results, which only holds when g is separable or sharp, we match their running time and generalize it for arbitrary convex functions. In the case where the objective is only accessible by an oracle and W is unknown, we further show that their proximity framework can be implemented in 2 3 n(mŒî)O(m ) -time instead of n(mŒî)O(m ) . Lastly, we extend the result by Eisenbrand and Weismantel [SODA‚Äô17, TALG‚Äô20] for integer programs with few constraints to a mixed-integer linear program setting where integer variables appear in only a small number of diÔ¨Äerent constraints.  
   
  1  
   
  Introduction  
   
  Integer programming has played a crucial role in many areas of computer science, operations research, and more recently, data science. Its modelling power allows The Ô¨Årst author has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme: grant agreement QIP805241. The fourth author was partially funded by Fondecyt grant Nr. 1221460 and Centro de Modelamiento Matem¬¥ atico (CMM), FB210005, BASAL funds, ANID-Chile. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 115‚Äì126, 2023. https://doi.org/10.1007/978-3-031-32726-1_9  
   
  116  
   
  D. Dadush et al.  
   
  to capture a large diversity of settings. However, its general intractability makes it challenging to derive a general algorithmic theory, and hence the focus has been to consider meaningful special cases. The main theoretical result in this area has been the algorithm by Lenstra [10], and the improvement by Kannan [7], which show that integer programs are tractable as long as the dimension is constant. In recent years, a surge of interest appeared regarding eÔ¨Écient algorithms for integer programs under other assumptions. More recently, the seminal work by Eisenbrand and Weismantel [4] for integer programs with a constant number of constraints and bounded matrix coeÔ¨Écients sparked a new trend of improved algorithms and lower bounds; see, e.g., [2,6,9]. In this paper, we study a new general framework that encompasses and further extends many of the settings found in the literature. Consider the problem of optimizing a low dimensional objective function over a high dimensional space Zn . Formally, the problem is deÔ¨Åned as min cT x + g(W x) i ‚â§ xi ‚â§ ui  
   
  for all i ‚àà {1, 2, . . . , n},  
   
  (1)  
   
  n  
   
  x‚ààZ . We assume that W ‚àà Zm√ón has entries of absolute value at most Œî. Here, W can be interpreted as a projection matrix to a space of low dimension m  n, where then the function g : Rm ‚Üí R ‚à™ {‚àû} is applied to the projection. We can think of W as extracting a relatively small set of features from x. The vectors  ‚àà (Z ‚à™ {‚àí‚àû})n and u ‚àà (Z ‚à™ {‚àû})n are arbitrary variable bounds and c represents a linear cost function. Crucially, we only make a very mild assumption on g, namely that we can Àô = [n] with solve (1) when all but m of the variables are Ô¨Åxed: given any I ‚à™J J |I| = m and any Ô¨Åxing z ‚àà Z of the J-variables, we require that min  
   
  cI x + g(WI x + WJ z)  
   
  s.t.  
   
  i ‚â§ xi ‚â§ ui  
   
  for all i ‚àà I,  
   
  (2)  
   
  I  
   
  x‚ààZ , can be solved eÔ¨Éciently. Here cI = (ci )i‚ààI is the vector c restricted to indices in I and similarly WI (resp. WJ ) is the matrix W restricted to columns indexed by I (resp. J). The requirement is intuitively necessary, because the only plausible approach to eÔ¨Éciently solve the very general setting (1) is to exploit that the function g is low dimensional. If we cannot even optimize over it in a low dimensional coordinate subspace of Zn , then there is no hope to optimize over it on all of Zn . Perhaps the most natural such setting is when g is convex and can be accessed through gradient and function evaluation queries. Then (2) can be solved in time that is exponential only in m, but polynomial in the other input parameters, by the Lenstra-Kannan algorithm [7,10]. If g is indeed convex, the Lenstra-Kannan algorithm can also be used to O(m) ¬∑input O(1) , where input denotes the encoding directly solve (1) in time ŒîŒî size of the input. Indeed, we can merge variables with the same columns in  
   
  Optimizing Low Dimensional Functions  
   
  117  
   
  W , which reduces the dimension of the problem to n = (2Œî + 1)m . Notice that the linear part of the objective may not remain linear, but it does remain convex. Thus, we can apply Lenstra-Kannan to solve the problem in the claimed running time. This indicates that the problem is tractable for small values of Œî and m. Our main result is an algorithm that avoids the double exponential running time. Theorem 1. For any function g, problem (1) can be solved in time 2  
   
  nm ¬∑ (mŒî)O(m ) ¬∑ Q , where Q is the query time of the oracle for (2). In particular, for a convex function g, the term Q can be replaced by input O(1) . We notice that in this theorem the bound of Q for the convex case follows by using the Lenstra-Kannan algorithm to solve the small dimensional subproblem (2). In this case, the mO(m) factor in the running time of the Lenstra-Kannan 2 algorithm can be omitted as it is upper bounded by (mŒî)O(m ) . Regarding the nm term, as we explain below, it can be made lower order in interesting concrete settings. We also remark that a term of the form Œîm cannot be avoided due to reductions from integer linear programming (see Sect. 1.1) and lower bounds for that problem [6]. 1.1  
   
  Applications  
   
  Low dimensional convex functions. The main inspiration for this work is a recent study by Hunkenschr¬® oder, Pokutta, and Weismantel [5], who consider the problem (3) min n g(W x) , x‚àà{0,1}  
   
  m√ón  
   
  where W ‚àà Z with entries of absolute value at most Œî, g : Rm ‚Üí R is a ‚Äúnice‚Äù sharp or separable convex function, and the algorithm can make function and gradient evaluations to the objective g(W x). They further distinguish between the case where W is given explicitly and where W is unknown to the 2 algorithm. Assuming g is separable, they provide an n(mŒî)O(m ) -time algorithm 3 when W is known, and an n(mLŒî)O(m ) -time algorithm when W is unknown and g is assumed to have L-Lipschitz gradients1 . They show similar results when g is suitably ‚Äúsharp‚Äù, though we omit the statements for concision. As a direct application of Theorem 1, we extend the result of [5] to arbitrary convex functions when W is known. Corollary 1. When W is known and g is an arbitrary convex function, problem (3) can be solved in time O(nm) + (mŒî)O(m 1  
   
  2  
   
  )  
   
  .  
   
  They further require g to have an integer valued gradient on integer inputs.  
   
  118  
   
  D. Dadush et al.  
   
  The reduction to Theorem 1 is as follows: we Ô¨Årst abandon the restriction of x ‚àà {0, 1}n in favor of the general bounded integer variables. Then, since any two variables with the same column in the projection matrix W can be merged to one (by adapting the box-constraints), we may assume without loss of generality that n ‚â§ (2Œî + 1)m . One of the main motivations in the work by Hunkenschr¬® oder et al. [5] is to solve certain types of regression problems. For example, they examine an integer compressed sensing problem, where one receives a small number m of linear measurements of a high dimensional integral signal x‚àó ‚àà {0, 1}n which one would like to (approximately) reconstruct. The received measurements are of the form b = W x‚àó , where W ‚àà Zm√ón is an unknown matrix with coeÔ¨Écients of size at most Œî. As an approximation to x‚àó , they compute the minimizer of min{ b ‚àí W x 2 : x ‚àà {0, 1}n }, under the assumption that one can only access W indirectly via gradient and function evaluation queries to f (x) = b ‚àí W x 2 . As we will explain later, in the compressed sensing and related settings, one can essentially avoid any overhead from not knowing W . While we focus above on the case where W is known, using orthogonal techniques, we can also improve the running times in the unknown W setting by modifying the Hunkenschr¬® oder et al. framework. We defer further discussion of their framework and our related improvements to Sect. 4. Mixed-Integer Linear Programming. Eisenbrand and Weismantel [4] studied the complexity of integer programs of the form min cT x s.t. Ax = b, i ‚â§ xi ‚â§ ui x ‚àà Zn .  
   
  (4) for all i ‚àà {1, . . . , n},  
   
  SpeciÔ¨Åcally, they considered the setting where A has few rows and then used 2 the Steinitz Lemma to obtain an algorithm with running time (mŒî)O(m ) ¬∑ n, where Œî is the maximum absolute value in A. This has inspired a line of work for similar settings, see for example [3,6,8,9]. Our setting is a generalization of theirs: take A = W and let  0 if Ax = b, g(Ax) = ‚àû otherwise. Here subproblem (2) corresponds to solving integer programming in m dimensions, which can be done using Lenstra-Kannan. Alternatively, one could model the problem as minimizing the convex function g(Ax) = Ax ‚àí b for some suitable norm. Moreover, our model generalizes beyond the scope of Eisenbrand and  
   
  Optimizing Low Dimensional Functions  
   
  119  
   
  Weismantel‚Äôs work to mixed-integer linear programming. Consider the problem min cT x + dT y s.t. Ax + By = b, i ‚â§ xi ‚â§ ui , x ‚àà Zn ,  
   
  for all i ‚àà {1, . . . , n}  
   
  (5)  
   
  y ‚àà P ‚äÜ Rh . Here P is some polytope that can impose additional constraints on the continuous variables. We can encode this problem in (1) by setting W = A and  min{dT y : By = b ‚àí Ax, y ‚àà P } if this minimum exists, g(Ax) = ‚àû otherwise. Notice that the oracle problem (2) in this case forms a mixed-integer linear program itself, but with only m many integer variables; hence it can be solved eÔ¨Éciently with the algorithm by Lenstra-Kannan. Corollary 2. Assuming P can be eÔ¨Éciently separated over, problem (5) can be solved in time 2 nm ¬∑ (mŒî)O(m ) ¬∑ input O(1) . We emphasize here that Œî is only a bound on the entries of A, but not necessarily on those of B. Compared to the algorithm for the pure integer setting in [4], our running time has an extra factor of nm , which however vanishes in some settings: for example, when c = 0 or ui = ‚àû for all i. In those cases we can again merge variables that share the same column in W . The only other example we are aware of that extends Eisenbrand and Weismantel‚Äôs setting to mixed-integer linear programming is the work by Brand, Kouteck¬¥ y, and Ordyniak [2]. Their setting can be considered orthogonal to ours. On the one hand, they study a much more general structure of bounded treedepth programs, of which integer programs with a bounded number of constraints are the simplest special case. On the other hand, they impose these structural restrictions also on the continuous variables (and additionally bounds on their coeÔ¨Écients), whereas we impose essentially no restrictions on the structure of continuous variables or their coeÔ¨Écients. To appreciate this, let us remark a pleasing aspect of the (straight-forward) extension of Lenstra-Kannan to mixed-integer linear programs: it combines the tractability of integer programs in Ô¨Åxed dimension with the tractability of linear programs in any dimension, achieving essentially a generalization of both. Eisenbrand and Weismantel‚Äôs algorithm, on the other hand, concerns the tractability of integer programs with a Ô¨Åxed number of constraints (adding the necessary assumption that Œî is bounded). In a similar spirit to the aforementioned generalization, our algorithm combines this with the tractability of (arbitrary) linear programs.  
   
  120  
   
  D. Dadush et al.  
   
  Integer linear programming with few complex variables. Recall the integer programming setting (4) studied by Eisenbrand and Weismantel, for which they 2 gave an algorithm with running time (mŒî)O(m ) ¬∑ n (with Œî being the maximum absolute value in A). The interesting parameter regime for this algorithm is therefore when m and Œî are very small. Already for m = 1 this formulation easily captures the Knapsack problem, which is weakly NP-hard and therefore we cannot hope to reduce the dependency on Œî to, say, log(Œî) while still maintaining polynomial dependency on n. In Lenstra-Kannan, on the other hand, the dependency on the coeÔ¨Écients of the matrix is polynomial in the encoding size, i.e., in log(Œî), but the dependency on n is exponential. These two rather orthogonal results can be combined using Theorem 1. Corollary 3. Consider the integer programming problem in (4) and partition the columns of A into ‚Äúsimple‚Äù columns where the entries are bounded by Œî in absolute value and ‚Äúcomplex‚Äù columns where they are arbitrary. Suppose that there are only k many complex columns. Then we can solve (4) in time 2  
   
  nm ¬∑ (mŒî)m ¬∑ k O(k) ¬∑ input O(1) . For this we proceed as follows. Let S and C be the index sets of the simple and complex columns and accordingly let AS and AC be the matrix A restricted to these column sets. We deÔ¨Åne Problem (1) only on xS , the variables for the simple columns. Then let  min{cTC xC : AC xC = b ‚àí AS xS } g(AS xS ) = ‚àû  
   
  if this minimum exists, otherwise.  
   
  The resulting subproblem (2) is then an integer program with m + k variables that can be solved using Lenstra-Kannan. We note that one could even add to (4) arbitrary additional constraints on the complex columns and still solve the problem in the same way. Variable-sized Knapsack. Antoniadis et al. [1] introduce a variant of the Knapsack problem with a non-linear cost function associated with the used capacity. They show that the case where this function is concave is polynomial time solvable and describe an FPTAS for the convex case. Our result can be used to devise a pseudopolynomial time algorithm for the convex case: the problem can be expressed as   n   n  pi xi ‚àí g wi xi : xi ‚àà {0, 1, . . . , ui } for all i . (6) max i=1  
   
  i=1  
   
  where pi is the proÔ¨Åt of item i, wi the weight, and ui ‚àà Z‚â•0 ‚à™ {‚àû} is a bound on the number of items of this type. Straightforward generalizations to multidimensional knapsack follow in a similar way.  
   
  Optimizing Low Dimensional Functions  
   
  121  
   
  Corollary 4. For a convex function g, problem (6) can be solved in time (n + wmax )O(1) . Here, the oracle problem (2) reduces to a simple binary search. In general our result Ô¨Åts well to problems with a similar spirit, where the constraints are not hard, but they induce some penalty. 1.2  
   
  Overview of Techniques  
   
  The related results for more restrictive cases in [4] and [5] are based on proximity: the continuous relaxation of the problem, where the integer requirement is omitted, is solved and if one can show that the solution for the relaxation and the actual solution diÔ¨Äer only slightly, then this can be exploited in reducing the search space. The precise proximity theorem in [4] is as follows. Theorem 2 (Eisenbrand and Weismantel [4]). Let z be an optimal vertex solution to the linear program   max cT x : Ax = b and i ‚â§ xi ‚â§ ui for all i , where A ‚àà Zm√ón has entries of size at most Œî. If there exists an integer solution, then there is also an optimal integer solution x‚àó with  
   
  x‚àó ‚àí z 1 ‚â§ m(2mŒî + 1)m . Hunkenschr¬® oder et al. [5] consider the optimal solution to the continuous relaxation of (3). In the special cases of separable convex and strict convex functions they show that a similar proximity holds, which is a crucial ingredient in their algorithm. Already for general convex functions, however, the proximity bound can be very large, as shown in an example in [5]. This forms a serious obstacle towards our main result. We manage to circumvent this and still rely on proximity by applying it in a diÔ¨Äerent way. Consider for sake of illustration that we were able to determine the value of b‚àó = W x‚àó , where x‚àó is the optimal solution of (1). Then it would be easy to recover x‚àó (solving our problem) by applying the integer linear programming algorithm by Eisenbrand and Weismantel [4]. The algorithm works by computing the continuous solution z to W x = b‚àó and then using that z ‚àí x‚àó 1 is bounded by Theorem 2. Indeed, this bound still holds in our case when Ô¨Åxing b‚àó . However, it is not clear how to compute or guess b‚àó , nor how to compute z without knowing b‚àó . Let us now consider the case that the domain of each variable is Z‚â•0 , which is slightly simpler than the bounded case. Here we may assume that z has only m non-zero components, which we can guess from nm candidates. We still do not know b‚àó or z, but we trivially know z on the n ‚àí m zero components. Intuitively, this is enough to apply proximity to recover x‚àó on the zero components of z. Moreover, recovering x‚àó on the non-zero components of z is only an mdimensional problem, where we can apply the oracle problem (2).  
   
  122  
   
  D. Dadush et al.  
   
  For our general result with arbitrarily bounded variable domains, there is another obstacle: if we try to generalize the previous line of arguments, it is still true that there are only m ‚Äúspecial‚Äù variables in z, namely variables that are not tight on either of their bounds. For the remaining variables, however, it is not immediately obvious whether they equal the lower bound or the upper bound and if we do not know this, it is unclear how to determine x‚àó on these tight variables. We overcome this by guessing enough information about the dual so that we can use complementary slackness to infer which bound that the tight variables attain.  
   
  2  
   
  Non-negative Variables  
   
  For simplicity, in this section we Ô¨Årst prove our main result for the variable domain Z‚â•0 , that is, i = 0 and ui = ‚àû for all i. Consider the optimal solution x‚àó to (1) and deÔ¨Åne b‚àó = W x‚àó . Furthermore, let z be an optimal vertex solution to min{cT x : W x = b‚àó , x ‚àà Rn‚â•0 }. We emphasize that z is not necessarily integral. By Theorem 2 there is an optimal integer solution x to min{cT x : W x = b‚àó , x ‚àà Zn‚â•0 } with x ‚àí z 1 ‚â§ O(mŒî)m . We can assume without loss of generality that x = x‚àó . Since z is a vertex solution, it has at least n ‚àí m zero components T . It follows that  
   
  x‚àóT 1 = x‚àóT ‚àí zT 1 ‚â§ x‚àó ‚àí z 1 ‚â§ O(mŒî)m . Thus,  
   
  WT x‚àóT 1 ‚â§ mŒî ¬∑ x‚àóT 1 ‚â§ O(mŒî)m+1 .  
   
  We now guess the indices of variables in T from the nm many candidates and we guess the value of b(T ) := WT x‚àóT from the O(mŒî)(m+1)m many candidates. It is now easy to recover x‚àóT (or an equivalent solution) by solving  
   
  min cTT xT : WT xT = b(T ) and xi ‚àà Z‚â•0 for all i ‚àà T . Here we use the algorithm by Eisenbrand and Weismantel [4] or the improvement in [6]. This requires time (mŒî)O(m) ¬∑ n, which is insigniÔ¨Åcant compared to the number of guesses above. The algorithm assumes a solution to the LP relaxation is given, which, however, only serves the purpose of having a vector close to the optimal solution (in 1 -norm). For this purpose we can also simply take zT (the zero vector). Let L be the set of indices not in T . To recover x‚àóL we need to solve   min cTL x‚àóL + g(WL x‚àóL + WT x‚àóT ) : x‚àói ‚àà Z‚â•0 for all i ‚àà L . This corresponds to an oracle query of the form (2). For each guess of T and b(T ) we compute a solution in this way and return the best among them. The running time, which is dominated by the number of guesses, is therefore 2  
   
  nm ¬∑ (mŒî)O(m ) ¬∑ Q .  
   
  (7)  
   
  In fact, the nm term here can be omitted, since one may assume without loss of generality that no two columns of W are equal and therefore n ‚â§ (2Œî + 1)m .  
   
  Optimizing Low Dimensional Functions  
   
  3  
   
  123  
   
  Bounded Variables  
   
  Let again x‚àó denote an optimal solution to (1) and b‚àó = W x‚àó . Let z be an optimal solution to   min cT x : W x = b‚àó and i ‚â§ xi ‚â§ ui for all i . We assume that c is augmented slightly by adding Œµi to the ith component for all i for some very small Œµ, which essentially implements a lexicographic tiebreaking rule between solutions. Here Œµ can be treated symbolically. We note that the dual of this linear program is  
   
  max b‚àóT y + T s ‚àí uT su : c ‚àí W T y = s ‚àí su and si , sui ‚àà Rn‚â•0 , y ‚àà Rm . Let y, s , su be an optimal vertex solution to the dual. Then there are m linearly independent rows (W T )i with si = sui = 0 (otherwise it would not be a vertex solution). We guess these rows among the nm candidates, which fully determines y and in particular c ‚àí W T y. We may assume that (c ‚àí W T y)i = 0 for the other n ‚àí m rows, which follows from the perturbation with Œµ. If (c ‚àí W T y)i > 0 for some i we know that sui > 0 and likewise if (c ‚àí W T y)i < 0, then si > 0. By complementary slackness we can determine for these rows that zi = ui (respectively, zi = i ). It follows that for n ‚àí m variables T we now determined its value in z. Let L denote the m other variables. We now proceed similar to the previous section. We again have that  
   
  x‚àóT ‚àí zT 1 ‚â§ O(mŒî)m . This implies that  
   
  WT x‚àóT ‚àí WT zT 1 ‚â§ mŒî ¬∑ x‚àóT ‚â§ O(mŒî)m+1 . Since we know the value of WT zT , we can guess b(T ) = WT x‚àóT among the O(mŒî)(m+1)m many candidates. Then we recover x‚àóT using the algorithm by Eisenbrand and Weismantel [4] (where we can use zT instead of an LP solution) and x‚àóL by applying (2) to min{cTL x‚àóL + g(WL x‚àóL + WT x‚àóT ) : x‚àói ‚àà {i , i + 1, . . . , ui } for all i ‚àà L}. Finally, we return the best solution computed for any guess.  
   
  4  
   
  Overview of Hunkenschr¬® oder Et Al. [5] and Related Improvements  
   
  We now explain the high-level algorithm of Hunkenschr¬® oder et al. [5] in more detail, as well as some improvements to their framework in the unknown W case. Their algorithm starts with an optimal solution z to the continuous relaxation min{g(W x) : x ‚àà [0, 1]n } having at most m fractional components (which is  
   
  124  
   
  D. Dadush et al.  
   
  easy to show to always exists). Here, z is assumed to be given by an oracle. For the cases they consider, e.g., the separable case, they prove that there is a ‚Äúnearby‚Äù optimal integral solution x‚àó satisfying x‚àó ‚àíz 1 ‚â§ (mŒî)O(m) . Function g being separable means that it can be decomposed into a sum of functions each depending only on a single dimension, that is, g(W x) = g1 ((W x)1 ) + . . . + gm ((W x)m ). Using the proximity result, they guess b‚àó = W x‚àó ‚àà Zm , where the 2 number of guesses is bounded by (mŒî)O(m ) (modulo an n factor, this is the dominant term in the complexity), noting that W (x‚àó ‚àí z) ‚àû ‚â§ (mŒî) x‚àó ‚àí z 1 . They then recover an optimal solution by solving the integer program W x = b‚àó , x ‚àà {0, 1}n . Note that this version of the algorithm requires W to be known. When W is unknown, they show that one can replace W by a proxy matrix W  , whose rows correspond to linearly independent gradients of f (x) := g(W x) seen so far by the algorithm. Their Ô¨Årst observation is that the gradients of ‚àáf (x) = W T ‚àág(W x) are linear combinations of the rows of W . Their second crucial observation is that for b‚àó = W  x‚àó , any integer solution to W  x = b‚àó , x ‚àà {0, 1}n , is either optimal or has a gradient ‚àáf (x) outside the row span of W  , in which case we can add an extra row to W  . Thus, one can iterate the guessing procedure with W replaced by W  at most m times before Ô¨Ånding an optimal solution. The blowup in complexity in this setting comes from a lack of control over the coeÔ¨Écients appearing in W  . Indeed, this is precisely why they require that g has an L-Lipschitz gradient and integral gradients on integral inputs. We remark that this idea can be implemented more eÔ¨Éciently without suffering from the worse parameters of W  . First, we observe that the cardinality of the set BN = {W  x : x ‚àà {0, 1}n , z ‚àí x 1 ‚â§ N } can be bounded solely in N and the parameters of W . This is because each row of W  is a linear combination of rows of W . Hence, W x = W x implies W  x = W  x and therefore |BN | ‚â§ O(N Œî)m . Next, notice that BN can be enumerated in time polynomial in n and |BN |: this follows from an induction over n. To this end, for all n ‚â§ n + 1 we deÔ¨Åne (n )  
   
  BN  
   
  = {W  x : x ‚àà {0, 1}n , z ‚àí x 1 ‚â§ N, and xi = zi  for all i ‚â• n } , (n+1)  
   
  (n )  
   
  1 = BN . We now iteratively generate the sets BN  by using BN where BN  =  {W z} and the recurrence  (n ) (n ) BN  ‚à™ (BN  ‚àí1 + Wn  ) if zn  = 0, (n +1) BN  =  (n ) (n ) BN  ‚à™ (BN  ‚àí1 ‚àí Wn  ) if zn  = 1. (n )  
   
  Here Wn  is the n th column of W  . We note that every vector in BN  ‚àí1 is generated from some x with xn = 0 iÔ¨Ä zn  = 0. Hence, when adding (resp. removing) Wn  there is again a legal x generating this vector (where z ‚àí x 1 has increased by one). As in the algorithm of Hunkenschr¬® oder et al., we now start with W  having only the single row ‚àáf (z). Then for every element of BN we consider a  
   
  Optimizing Low Dimensional Functions  
   
  125  
   
  corresponding integer solution x (note that such an x can easily be recovered in the above recurrence) and check if ‚àáf (x) and the rows of W  are linearly independent. If so, we add the gradient as a new row to W  . We repeat for at most m iterations until no new row is added. Then we return the best solution x‚àó seen during this process. Theorem 3. Let g : Rm ‚Üí R be a convex function, let f (x) := g(W x) be accessible via a function value and gradient oracle, where W ‚àà Zm√ón is an unknown matrix with entries of absolute value at most Œî. Then given an optimal solution z to the continuous relaxation min{f (x) : x ‚àà [0, 1]n } with at most m fractional entries, one can compute an optimal integral solution in time n(N Œî)O(m) . Here N is the minimum z ‚àí x‚àó 1 over all optimal integer solutions x‚àó . In particular, 2 when g is separable convex, the running time becomes n(mŒî)O(m ) .  
   
  5  
   
  Conclusion and Open Questions  
   
  In this paper we have demonstrated that the task of optimizing low dimensional functions over a projection as introduced by Hunkenschr¬® oder et al. [5] remains tractable even in much more general settings than originally considered. This creates a bridge also to other lines of work in integer optimization, such as integer programs with few constraints [4]. Our main result leaves open a few questions about the complexity of algorithms for problem (1) or the central case of g being a convex function. As mentioned before, one cannot hope to avoid a term of Œîm in the running time because of known conditional lower bounds. The necessity of the nm term or the m2 exponent, however, appears less clear. The algorithm for integer programming by Eisenbrand and Weismantel [4], a special case of our setting (see applications), does not require the nm term and in many cases we can avoid it as well by merging duplicate columns of W . It would be nice if this term could be removed in general, or at least in the convex case. Related to the m2 exponent, there is already a notorious question arising from [4]. There, Eisenbrand and Weismantel gave an improved algorithm with exponent O(m) instead of O(m2 ) for the case that there are no upper variable bounds, but with bounds they require O(m2 ). It remains unclear whether this is necessary. In our case even without upper bounds our algorithm need the exponent O(m2 ). In fact, this exponent arises in several places: when guessing the support (assuming n ‚âà Œîm ) and when guessing the projection of the tight variables b(T ) .  
   
  References 1. Antoniadis, A., Huang, C.-C., Ott, S., Verschae, J.: How to pack your items when you have to buy your knapsack. In: Proceedings of MFCS, pp. 62‚Äì73 (2013) 2. Brand, C., Kouteck¬¥ y, M., Ordyniak, S.: Parameterized algorithms for MILPs with small treedepth. In: Proceedings of AAAI, pp. 12249‚Äì12257 (2021)  
   
  126  
   
  D. Dadush et al.  
   
  3. Cslovjecsek, J., Eisenbrand, F., Hunkenschr¬® oder, C., Rohwedder, L., Weismantel, R.: Block-structured integer and linear programming in strongly polynomial and near linear time. In: Proceedings of SODA, pp. 1666‚Äì1681 (2021) 4. Eisenbrand, F., Weismantel, R.: Proximity results and faster algorithms for integer programming using the Steinitz lemma. ACM Trans. Algorithms 16(1), 5:1-5:14 (2020) 5. Hunkenschr¬® oder, C., Pokutta, S., Weismantel, R.: Optimizing a low-dimensional convex function over a high-dimensional cube. SIAM J. Optim. 2022, to appear 6. Jansen, K., Rohwedder, L.: On integer programming, discrepancy, and convolution. Math. Oper. Res. (2022, to appear) 7. Kannan, R.: Improved algorithms for integer programming and related lattice problems. In: Proceedings of STOC, pp. 193‚Äì206 (1983) 8. Klein, K.-M.: About the complexity of two-stage stochastic IPs. Math. Program. 192(1), 319‚Äì337 (2022) 9. Knop, D., Pilipczuk, M., Wrochna, M.: Tight complexity lower bounds for integer linear programming with few constraints. ACM Trans. Comput. Theory 12(3), 191‚Äì1919 (2020) 10. Lenstra, H.W., Jr.: Integer programming with a Ô¨Åxed number of variables. Math. Oper. Res. 8(4), 538‚Äì548 (1983)  
   
  Configuration Balancing for Stochastic Requests Franziska Eberle1(B) , Anupam Gupta2(B) , Nicole Megow3(B) , Benjamin Moseley2(B) , and Rudy Zhou2(B) 1  
   
  London School of Economics and Political Science, London, UK [email protected]  2 Carnegie Mellon University, Pittsburgh, PA, USA [email protected]  , {moseleyb,rbz}@andrew.cmu.edu 3 University of Bremen, Bremen, Germany [email protected]   
   
  Abstract. The conÔ¨Åguration balancing problem with stochastic requests generalizes well-studied resource allocation problems such as load balancing and virtual circuit routing. There are given m resources and n requests; each request has multiple possible configurations, each of which increases the load of each resource by some amount. The goal is to select one conÔ¨Åguration for each request to minimize the makespan: the load of the most-loaded resource. In the stochastic setting, the amount by which a conÔ¨Åguration increases the resource load is uncertain until the conÔ¨Åguration is chosen, but we are given a probability distribution. We develop both oÔ¨Ñine and online algorithms for conÔ¨Åguration balancing with stochastic requests. When the requests are known oÔ¨Ñine, we give a non-adaptive policy for conÔ¨Åguration balancing with stochastic requests that O( logloglogmm )-approximates the optimal adaptive policy, which matches a known lower bound for the special case of load balancing on identical machines. When requests arrive online in a list, we give a non-adaptive policy that is O(log m) competitive. Again, this result is asymptotically tight due to information-theoretic lower bounds for special cases (e.g., for load balancing on unrelated machines). Finally, we show how to leverage adaptivity in the special case of load balancing on related machines to obtain a constant-factor approximation oÔ¨Ñine and an O(log log m)-approximation online. A crucial technical ingredient in all of our results is a new structural characterization of the optimal adaptive policy that allows us to limit the correlations between its decisions. Keywords: stochastic scheduling  
   
  1  
   
  ¬∑ stochastic routing ¬∑ load balancing  
   
  Introduction  
   
  This paper considers the conÔ¨Åguration balancing problem: there are m resources and n requests. Request j has qj conÔ¨Ågurations xj (1), . . . , xj (qj ) ‚àà Rm ‚â•0 . We F. Eberle‚ÄîSupported by the Dutch Research Council (NWO), Netherlands Vidi grant 016.Vidi.189.087. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 127‚Äì141, 2023. https://doi.org/10.1007/978-3-031-32726-1_10  
   
  128  
   
  F. Eberle et al.  
   
  must choose one conÔ¨Åguration cj ‚àà [qj ] per request, which adds xj (cj ) to the load vector on the resources. The goal is to minimize the makespan, i.e., the load of the most-loaded resource. ConÔ¨Åguration balancing captures many natural resource allocation problems where requests compete for a Ô¨Ånite pool of resources and the task is to Ô¨Ånd a ‚Äúfair‚Äù allocation in which no resource is over-burdened. Two well-studied problems of this form arise in scheduling and routing. (i) In load balancing a.k.a. makespan minimization, there are m (unrelated) machines and n jobs. Scheduling job j on machine i increases the load of i by pij ‚â• 0. The goal is to schedule each job on some machine to minimize the makespan, i.e., the load of the most-loaded machine. (ii) In virtual circuit routing or congestion minimization, there is a directed graph G = (V, E) on m edges with edge capacities ce > 0 for e ‚àà E, and n requests, each request consisting of a source-sink pair (sj , tj ) in G and a demand dj ‚â• 0. The goal is to route each request j from sj to tj via some directed path, increasing the load/congestion of each edge e on the path by dj/ce , while the objective is to minimize the load of the most-loaded edge. ConÔ¨Åguration balancing captures both problems by taking the m resources to be the m machines or edges, respectively; each conÔ¨Åguration now corresponds to assigning a job to some machine or routing a request along some path. Typically, job sizes or request demands are not known exactly when solving resource allocation problems in practice. This motivates the study of algorithms under uncertainty, where an algorithm must make decisions given only partial/uncertain information about the input. Uncertainty can be modeled in diÔ¨Äerent ways. In exceptional cases, a non-clairvoyant algorithm that has no knowledge about the loads of requests may perform surprisingly well; an example is Graham‚Äôs greedy list scheduling for load balancing on identical machines [15]. In general, a non-clairvoyant algorithm cannot perform well. Hence, we consider a stochastic model, where the unknown input follows some known distribution but the actual realization is a priori unknown. Such a model is natural when there is historical data available from which such distributions can be deduced. In the conÔ¨Åguration balancing with stochastic requests problem, we assume that each conÔ¨Åguration c of request j is a random vector Xj (c) with known distribution Dj (c) supported on Rm ‚â•0 such that the Xj (c)‚Äôs are independent across diÔ¨Äerent requests j. The actual realized vector of a conÔ¨Åguration c of request j is only observed after irrevocably selecting this particular conÔ¨Åguration for request objective is to minimize the expected maximum load  j. The n (makespan) E maxi j=1 Xij (cj ) , where cj is the conÔ¨Åguration chosen for request j. We assume that we have oracle access to the Dj (c)‚Äôs; in particular we assume that in constant time, we can compute any needed statistic of the distribution Dj (c). Further, we distinguish whether there is an additional dimension of uncertainty or not, namely the knowledge about the request set. In the oÔ¨Ñine setting, the set of requests and the distributions of the conÔ¨Ågurations of each request are known up-front, and they can be selected and assigned to the resources irre-  
   
  ConÔ¨Åguration Balancing for Stochastic Requests  
   
  129  
   
  vocably in any order. In the online setting, requests are not known in advance and they are revealed one-by-one (online-list model). The algorithm learns the stochastic information on conÔ¨Ågurations of a request upon its arrival, and must select one of them without knowledge of future arrivals. After a conÔ¨Åguration is chosen irrevocably, the next request arrives. In general, we allow an algorithm to base the next decision on knowledge about the realized vectors of all previously selected request conÔ¨Ågurations. We call such policies adaptive. Conversely, a non-adaptive policy is one that Ô¨Åxes the particular conÔ¨Åguration chosen for a request without using any knowledge of the realized conÔ¨Åguration vectors. The goal of this paper is to investigate the power of adaptive and nonadaptive policies for online and oÔ¨Ñine conÔ¨Åguration balancing with stochastic requests. We quantify the performance of an algorithm by bounding the worst-case ratio of the achieved expected makespan and the minimal expected makespan achieved by an optimal oÔ¨Ñine adaptive policy. We say that an algorithm Alg Œ±-approximates an algorithm Alg‚Äô if, for any input instance, the expected makespan of Alg is at most a factor Œ± larger than the expected makespan of Alg‚Äô; we refer to Œ± also as approximation ratio. For online algorithms, the term competitive ratio refers to their approximation ratio. 1.1  
   
  Our Results  
   
  Main Result. As our Ô¨Årst main result, we present non-adaptive algorithms for oÔ¨Ñine and online conÔ¨Åguration balancing with stochastic requests. Theorem 1. For conÔ¨Åguration balancing with stochastic requests there is a randomized  oÔ¨Ñine algorithm that computes a non-adaptive policy that is a  Œò logloglogmm -approximation and an eÔ¨Écient deterministic online algorithm that is a Œò(log m)-approximation when comparing to the optimal oÔ¨Ñine adaptive policy. Both algorithms run in polynomial time in the number of resources and the total number of conÔ¨Ågurations over all requests. The oÔ¨Ñine analysis relies on a linear programming (LP)relaxation  of conÔ¨Åguration balancing, which has a known integrality gap of Œò logloglogmm , even for virtual circuit routing [25], implying that the analysis is tight. In the online setting, our analysis employs a potential function to greedily determine which conÔ¨Åguration to choose for each request. In particular, we generalize the idea by [3] to the setting of conÔ¨Åguration balancing with stochastic requests and match a known lower bound for online deterministic load balancing on unrelated machines by [5]. If the conÔ¨Ågurations are not given explicitly as part of the input or the number of conÔ¨Ågurations is large, then eÔ¨Éciently solving the problem requires us to be able to optimize over conÔ¨Ågurations in polynomial time. Applications. These results would hold for both load balancing on unrelated machines and virtual circuit routing if we could guarantee that either the con-  
   
  130  
   
  F. Eberle et al.  
   
  Ô¨Ågurations are given explicitly or the respective subproblems can be solved eÔ¨Éciently. We can ensure this in both cases. For stochastic load balancing on unrelated machines, the resources are the m machines, and each job has m possible conÔ¨Ågurations ‚Äì one corresponding to assigning that job to each machine. Thus, we can eÔ¨Éciently represent all conÔ¨Ågurations. Further, here the LP relaxation of conÔ¨Åguration balancing used in Theorem 1 is equivalent to the LP relaxation of the generalized assignment problem (GAP) solved in [33], which gives a deterministic rounding algorithm. Hence, Theorem 1 implies the following theorem. We omit the proof in this extended abstract; see [14] for proof. Theorem 2. There exist eÔ¨Écient deterministic algorithms that compute a nonadaptive policy for loadbalancing on unrelated machines with stochastic jobs that achieve an Œò logloglogmm -approximation oÔ¨Ñine and an Œò(log m)-approximation online when comparing to the optimal oÔ¨Ñine adaptive policy.   These results are asymptotically tight due to the lower bound of Œ© logloglogmm on the adaptivity gap [17] and the lower bound of Œ©(log m) on the competitive ratio of any deterministic online algorithm, even for deterministic requests [5].   This implies that the adaptivity gap for stochastic load balancing is Œò logloglogmm . For virtual circuit routing, the resources are the m edges and each request has a conÔ¨Åguration for each possible routing path. Thus, eÔ¨Éciently solving the subproblems requires more work as the conÔ¨Ågurations are only given implicitly and there can be exponentially many. For the oÔ¨Ñine setting, since the LP relaxation has (possibly) exponentially many variables, we design an eÔ¨Écient separation oracle for the dual LP in order to eÔ¨Éciently solve the primal. For the online setting, we carefully select a subset of polynomially many conÔ¨Ågurations that contain the conÔ¨Åguration chosen by the greedy algorithm, even when presented with all conÔ¨Ågurations. Thus, Theorem 1 implies that stochastic requests are not harder to approximate than deterministic requests. We omit the proof in this extended abstract; see [14] for proof. Theorem 3. For routing with stochastic requests, there exist an eÔ¨Écient ran  domized oÔ¨Ñine algorithm computing a non-adaptive policy that is a Œò logloglogmm approximation and an eÔ¨Écient deterministic online algorithm that computes an Œò(log m)-approximation when comparing to the optimal oÔ¨Ñine adaptive policy. Adaptive Policies for Related Machines. When each request j has m conÔ¨ÅguraX tions and conÔ¨Åguration c ‚àà [m] can be written as Xj (c) = sij ec , where ec ‚àà Rm is the cth standard unit vector, the problem is also known as load balancing on related machines. We say that Xj is the size of request (or job) j and si is the speed of resource (or machine) i. In this  special case, we show how to leverage adaptivity to overcome the Œ© logloglogmm lower bound on the adaptivity gap. Interestingly, our adaptive algorithms begin with a similar non-adaptive assignment of jobs to machines, but we deviate from the assignment adaptively to obtain our improved algorithms.  
   
  ConÔ¨Åguration Balancing for Stochastic Requests  
   
  131  
   
  Theorem 4. For load balancing on related machines with stochastic jobs, there exist eÔ¨Écient deterministic algorithms that compute an adaptive oÔ¨Ñine O(1)approximation and an adaptive online O(log log m)-approximation when comparing to the optimal oÔ¨Ñine adaptive policy. It remains an interesting open question whether the online setting admits an O(1)-competitive algorithm. 1.2  
   
  Technical Overview  
   
  We illustrate the main idea behind our non-adaptive policies, which compare to the optimal oÔ¨Ñine adaptive policy. Throughout this paper, we let Opt denote the optimal adaptive policy as well as its makespan. As in many other stochastic optimization problems, our goal is to give a good deterministic proxy for the makespan of a policy. Then, our algorithm will optimize over this deterministic proxy to obtain a good solution. First, we observe that if all conÔ¨Ågurations were bounded with respect to E[Opt] in every entry, then selecting conÔ¨Ågurations such   that each resource has expected load O(E[Opt]) gives the desired O logloglogmm approximation by standard concentration inequalities for independent sums with bounded increments. Thus, in this case the expected load on each resource is a good proxy. However, in general, we have no upper bound on Xij (c), so we cannot argue as above. We turn these unbounded random variables (RVs) into bounded ones in a standard way by splitting each request into truncated and exceptional parts. DeÔ¨Ånition 1 (Truncated and Exceptional Parts). Fix œÑ ‚â• 0 as threshold. For a RV X, its truncated part (w.r.t. threshold œÑ ) is X T := X ¬∑ 1X 0 is the truncationthresh old. There are m jobs: a stochastic one with processing time Xj ‚àº œÑ ¬∑Ber œÑ1 and 1 m ‚àí 1 deterministic jobs with processing time Xj ‚â° m . The optimal adaptive policy Ô¨Årst schedules the stochastic job on the fast machine. If its realized size is 0, then it schedules all deterministic jobs on the fast machine as well. Otherwise the realized size is œÑ and it schedules deterministic job on each slow machine,   one 1 + ¬∑ implying E[Opt] = 1 ‚àí œÑ1 m‚àí1 m  However, the total expected œÑ EœÑ = Œò(1).  ¬∑ 1j‚Üíi = œÑ1 (mœÑ ) = m, where j ‚Üí i exceptional load (w.r.t. œÑ ) is i,j E Xij denotes that job j is assigned to machine i, i.e., conÔ¨Åguration i is chosen for j. In the example, the optimal adaptive policy accrues a lot of exceptional load, but this does not have a large eÔ¨Äect on the makespan. Concretely, (1) can be loose by a Œ©(m)-factor for adaptive policies. Thus, it seems that the total exceptional load is a bad proxy in terms of lower-bounding Opt. However, we show that, by comparing our algorithm to a near-optimal adaptive policy rather than the optimal one, the total exceptional load becomes a good proxy in the following sense. This is the main technical contribution of our work, and it underlies all of our algorithmic techniques. Theorem 5. For conÔ¨Åguration balancing with stochastic requests, there exists an adaptive policy with expected maximum load and total expected exceptional load at most 2 ¬∑ E[Opt] with respect to any truncation threshold   œÑ ‚â• 2 ¬∑ E[Opt]. Further, any conÔ¨Åguration c selected by this policy satisÔ¨Åes E maxi Xi (c) ‚â§ œÑ . The proof of the above relies on carefully modifying the ‚Äúdecision tree‚Äù representing the optimal adaptive policy; see [14] for proof. In light of Theorem 5, the deterministic proxies we consider are the expected truncated load on each resource and the total expected exceptional load. All of our algorithms then proceed by ensuring that both quantities are bounded with respect to E[Opt]. In the oÔ¨Ñine case, we round a natural assignment-type linear program (LP), and in the online case, we use a potential-function argument. All of these algorithms actually output non-adaptive policies. For the special case of related-machines load balancing, we also compute a non-adaptive assignment but instead of following it exactly, we deviate using adaptivity and give improved solutions. 1.3  
   
  Related Work  
   
  While stochastic optimization problems have long been studied [6,11], approximation algorithms for them are more recent [13,29]. By now, multi-stage stochastic problems (where uncertain information is revealed in stages) are well-understood [9,19,34]. In contrast, more dynamic models, where the exact value of an unknown parameter becomes known at times depending on the algorithms  
   
  ConÔ¨Åguration Balancing for Stochastic Requests  
   
  133  
   
  decisions (serving a request) still remain poorly understood. Some exceptions come from stochastic knapsack [8,12,16,28] as well as stochastic scheduling and routing which we discuss below. Scheduling. For load balancing with deterministic sizes, a 2-approximation in the most general unrelated-machines oÔ¨Ñine setting [26] is known. For identical machines (pij = pj for all jobs j), the greedy algorithm (called list scheduling)  1 -approximation algorithm [15]. This guarantee holds even when the is a 2 ‚àí m   1 jobs arrive online and nothing is known about job sizes. This implies a 2 ‚àí m approximate adaptive policy for stochastic load balancing on identical machines. Apart from this, prior work on stochastic scheduling has focused on approximating the optimal non-adaptive policy. There are non-adaptive O(1)approximations known for identical machines [24], unrelated machines [17], the q -norm objective [30], and monotone, symmetric norms [21]. In contrast, our work focuses on approximating the stronger optimal adaptive policy. The adaptivity gap (the ratio between the expected makespan of  the optimal adaptive and non-adaptive policies) can be Œ© logloglogmm even for the simplest case of identical machines [17]. Thus, previous work on approximating the optimal non-adaptive policy does not immediately give any non-trivial approximation guarantees for our setting. The only previous work on adaptive stochastic policies for load-balancing (beyond the highly-adaptive list scheduling) is by [32]. They propose scheduling policies whose degree of adaptivity can be controlled by parameters and show an approximation factor of O(log log m) for scheduling on identical machines. Online load balancing with deterministic jobs is also well studied [4]. On identical machines, the aforementioned list scheduling algorithm [15] is 2 ‚àí  1 -competitive. For unrelated machines, there is a deterministic O(log m)m competitive algorithm [3] and this is best possible [5]. When the machines are uniformly related, [7] design an O(1)-competitive algorithm for minimizing the makespan. [22,23] study the multi-dimensional generalization to vector scheduling under the makespan and the q -norm objective. To the best of our knowledge, conÔ¨Åguration balancing has not been explicitly deÔ¨Åned before. The techniques of [3] give an O(log m)-competitive algorithm for deterministic requests. It is also studied for packing integer programs [1,2,18]. Routing. For oblivious routing with stochastic demands, [20] give an algorithm which is an O(log2 n)-approximation with high probability. Here, ‚Äúoblivious‚Äù refers to the requirement that the chosen path between a source-sink pair must not depend on the current congestion of the network. In particular, after specifying a set of paths for each possible source-sink pair, a demand matrix is drawn from an a-priori known distribution and each demand needs to be routed along one of the predeÔ¨Åned paths. The obliviousness requirement is very diÔ¨Äerent from our setting and makes the two models essentially incomparable.   When dj = 1 for each source-sink pair, there is an O logloglogmm -approximation algorithm by [31], which is best possible, unless NP ‚äÜ ZPTIME(nlog log n ) [10].  
   
  134  
   
  F. Eberle et al.  
   
  In the online setting, when the source-sink pairs arrive online in a list and have to be routed before the next pair arrives, [3] give a lower bound of Œ©(log n) on the competitive ratio of any deterministic online algorithm in directed graphs, where n is the number of vertices. They also give a matching upper bound. For more details on online routing we refer to the survey [27].  
   
  2  
   
  Configuration Balancing with Stochastic Requests  
   
  In this section, we prove our main results for the most  general problem we consider: conÔ¨Åguration balancing. We give an O logloglogmm -approximation oÔ¨Ñine and an O(log m)-approximation online; both algorithms are non-adaptive. Before describing the algorithms, we give our main structural theorem that enables all of our results. Roughly, we show that instead of comparing to the optimal adaptive policy, by losing only a constant factor in the approximation ratio, we can compare to a near-optimal policy that behaves like a non-adaptive one (w.r.t. the proxy objectives we consider, namely, the total expected exceptional load). 2.1  
   
  Structural Theorem  
   
  In this section, we show that there exists a near-optimal policy as guaranteed by Theorem 5. To this end, we modify the optimal policy by ‚Äúrestarting‚Äù whenever an exceptional request is encountered. Additionally, we ensure   that this modiÔ¨Åed policy never selects a conÔ¨Åguration c for a request j with E maxi Xij (c) > œÑ . We let J denote the set of requests. For any subset J  ‚äÜ J, we let Opt(J  ) denote the optimal adaptive policy (and its maximum load) on the set of requests J  . Note that Opt(‚àÖ) = 0. Our (existential) algorithm to construct such a policy will begin by running the optimal policy Opt(J) on all requests. However, once an exceptional request is encountered or the next decision will choose a conÔ¨Åguration with too large expected maximum, we cancel Opt(J) and instead recurse on all remaining requests, ignoring all previously-accrued loads; see Algorithm 1. The idea of our analysis is that we recurse with small probability; see [14]. Theorem 5. For conÔ¨Åguration balancing with stochastic requests, there exists an adaptive policy with expected maximum load and total expected exceptional load at most 2 ¬∑ E[Opt] with respect to any truncation threshold   œÑ ‚â• 2 ¬∑ E[Opt]. Further, any conÔ¨Åguration c selected by this policy satisÔ¨Åes E maxi Xi (c) ‚â§ œÑ . Having this near-optimal policy at hand, the upshot is that we can bound our subsequent algorithms with respect to the following LP relaxation (LPC ) for conÔ¨Åguration balancing with stochastic requests. The variable ycj denotes selecting conÔ¨Åguration c for request j. We take our threshold between the truncated and exceptional parts to be œÑ . Using the natural setting of the y-variables as the probabilities of the policy from Theorem 5, it is straight-forward to show that  
   
  ConÔ¨Åguration Balancing for Stochastic Requests  
   
  135  
   
  Algorithm 1: Policy S(J) R‚ÜêJ if R = ‚àÖ then return empty policy  
   
  1  
   
  2  
   
  // remaining requests // Ô¨Ånish  
   
  while R = ‚àÖ do j ‚Üê Ô¨Årst / next request considered by Opt(J) cj ‚ÜêconÔ¨Åguration chosen for request j by Opt(J) if E maxi Xij (cj ) > œÑ then // maximum too large break else choose cj for request j // S(J) follows Opt(J) R ‚Üê R \ {j} // update remaining requests if maxi Xij (cj ) ‚â• œÑ then // exceptional conÔ¨Åguration observed break run S(R)  
   
  // recurse with remaining requests  
   
  the following LP relaxation has a feasible solution, formalized in Lemma 1. qj ‚àÄ j ‚àà [n] c=1 ycj = 1 n qj T E[X (c)] ¬∑ y ‚â§ œÑ ‚àÄ i ‚àà [m] cj ij c=1 n j=1 qj E E[max X (c)] ¬∑ y ‚â§ œÑ i cj ij c=1 j=1 ycj = 0 ‚àÄ j ‚àà [n], ‚àÄ c ‚àà [qj ] : E[maxi Xij (c)] > œÑ ycj ‚â• 0 ‚àÄ j ‚àà [n], ‚àÄ c ‚àà [qj ] (LPC ) Lemma 1. (LPC ) has a feasible solution for any œÑ ‚â• 2 ¬∑ E[Opt]. 2.2  
   
  OÔ¨Ñine Setting  
   
  Our oÔ¨Ñine algorithm is the natural randomized rounding of (LPC ). For the truncated parts, the following inequality bounds their contribution to the makespan. Lemma 2. Let S1 , . . . , Sm be sums of independent RVs bounded in  [0, œÑ ] for some œÑ > 0 such that E[Si ] ‚â§ œÑ for all i ‚àà [m]. Then, E[maxi Si ] = O logloglogmm œÑ . To bound the contribution of the exceptional parts, we use (1), i.e., the total expected exceptional load. Using binary search for the correct choice of œÑ and re-scaling the instance down by the current value of œÑ , it suÔ¨Éces to give an eÔ¨Écient algorithm that either   ‚Äì outputs a non-adaptive policy with expected makespan O logloglogmm , or ‚Äì certiÔ¨Åes that E[Opt] > 1.   This is because for œÑ ‚àà E[Opt], 2 ¬∑ E[Opt] , the re-scaling guarantees E[Opt] ‚àà [ 12 , 1) on the scaled instance, in which case the algorithm achieves     expected makespan O logloglogmm = O logloglogmm ¬∑ E[Opt].  
   
  136  
   
  F. Eberle et al.  
   
  Algorithm 2: OÔ¨Ñine ConÔ¨Åguration Balancing with Stochastic Requests try to solve (LPC ) with œÑ = 2 if (LPC ) is feasible then let y ‚àó be the outputted feasible solution for each request j do ‚àó independently sample c ‚àà [qj ] with probability ycj choose sampled c as cj else return ‚ÄúE[Opt] > 1‚Äù  
   
  To that end, we use the natural independent randomized rounding of (LPC ). That is, if (LPC ) has a feasible solution y ‚àó , for request j, we choose conÔ¨Åguration c ‚àó ; see Algorithm 2. as conÔ¨Åguration cj independently with probability ycj If the conÔ¨Ågurations are given explicitly as part of the input, then (LPC ) can be solved in polynomial  time and, thus, Algorithm 2 runs in polynomial  time. Hence, the O logloglogmm -approximate non-adaptive policy for conÔ¨Åguration balancing with stochastic requests (Theorem 1) follows from the next lemma. Lemma 3. If (LPC ) can be solved in polynomial time, Algorithm 2 is a polynomial-time randomized algorithm  that either outputs a non-adaptive pol icy with expected makespan O logloglogmm , or certiÔ¨Åes correctly that E[Opt] > 1. 2.3  
   
  Online Setting  
   
  We now consider online conÔ¨Åguration balancing where n stochastic requests arrive online one-by-one, and for each request, one conÔ¨Åguration has to be irrevocably selected before the next request appears. We present a non-adaptive online algorithm that achieves a competitive ratio of O(log m), which is best possible due to the lower bound of Œ©(log m) [5]. By a standard guess-and-double scheme, we may assume that we have a good guess of E[Opt]. We omit the proof, which is analogous to its virtual-circuitrouting counterpart in [3]. Lemma 4. Given an instance of online conÔ¨Åguration balancing with stochastic requests, suppose there exists an online algorithm that, given parameter Œª > 0, never creates an expected makespan more than Œ± ¬∑ Œª, possibly terminating before handling all requests. Further, if the algorithm terminates prematurely, then it certiÔ¨Åes that E[Opt] > Œª. Then, there exists an O(Œ±)-competitive algorithm for online conÔ¨Åguration balancing with stochastic requests. Further, the resulting algorithm preserves non-adaptivity. We will build on the same technical tools as in the oÔ¨Ñine case. In particular, we wish to compute a non-adaptive assignment online with small expected truncated load on each resource and small total expected exceptional load. To achieve this, we generalize the greedy potential function approach of [3]. Our two new ingredients are to treat the exceptional parts of a request‚Äôs conÔ¨Åguration as a  
   
  ConÔ¨Åguration Balancing for Stochastic Requests  
   
  137  
   
  resource requirement for an additional, artiÔ¨Åcial resource and to compare the potential of our solution directly with a fractional solution to (LPC ). Now we describe our potential function, which is based on an exponential/soft-max function. Let Œª denote the current guess of the optimum as required by Lemma 4. We take œÑ = 2Œª as our truncation threshold. Given load vector L ‚àà Rm+1 , our potential function is œÜ(L) =  
   
  m  
   
  (3/2)Li /œÑ .  
   
  i=0  
   
  For i ‚àà [m], we ensure the ith entry of L is the expected truncated load on resource i and use the 0th entry as a virtual resource that is the total expected exceptional load. For any request j, let Lj be the expected load vector after handling the Ô¨Årst j requests, with Lij denoting its ith entry. Let Li0 := 0 for all i. Upon arrival of request j, our algorithm tries to choose the conÔ¨Åguration cj ‚àà [qj ] that minimizes the increase in potential; see Algorithm 3. Algorithm 3: Online ConÔ¨Åguration Balancing with Stochastic Requests  ‚Üê log3/2 (2m + 2) Œª ‚Üê current guess of E[Opt] œÑ ‚Üê 2Œª truncation threshold upon arrival of requestj do 1  
   
  E  
   
  cj ‚Üê arg minc‚àà[qj ] (3/2)(L0j‚àí1 +E[maxi‚àà[m] Xij (c)])/œÑ +  m T 3/2)(Lij‚àí1 +E[Xij (c)])/œÑ ‚àí œÜ(L ( j‚àí1 ) i=1  
   
  T E if Lij‚àí1 + E[Xij (cj )] ‚â§ œÑ for all i ‚àà [m] and L0j‚àí1 + E[maxi‚àà[m] Xij (cj )] ‚â§ œÑ then choose cj for j T (cj )] for all i ‚àà [m] Lij ‚Üê Lij‚àí1 + E[Xij E (cj )] L0j ‚Üê L0j‚àí1 + E[maxi‚àà[m] Xij else return ‚ÄúE[Opt] > Œª‚Äù  
   
  To analyze this algorithm, we compare its makespan with a solution to (LPC ).   This LP has an integrality gap of Œ© logloglogmm , which follows immediately from the path assignment LP for virtual circuit routing [25]. Hence, a straightforward analysis of Algorithm 3 comparing to a rounded solution to (LPC ) gives an assignment with  expected truncated load per machine and total expected exceptional load O log m ¬∑ logloglogmm ) ¬∑ E[Opt]. To get a tight competitive ratio of O(log m), we avoid the integrality gap by comparing to a fractional solution to (LPC ), and we use a slightly diÔ¨Äerent inequality than Lemma 2 for the regime where the mean of the sums is larger than the increments by at most a O(log m)-factor. Lemma 5. Let S1 , . . . , Sm be sums of independent RVs bounded in [0, œÑ ] for œÑ > 0 such that E[Si ] ‚â§ O(log m)œÑ for all 1 ‚â§ i ‚â§ m. Then, E[maxi Si ] ‚â§ O(log m)œÑ .  
   
  138  
   
  F. Eberle et al.  
   
  We give the guarantee for Algorithm 3, which implies the O(log m)competitive algorithm for online conÔ¨Åguration balancing with stochastic requests. Lemma 6. Suppose the minimizing conÔ¨Åguration in Line 1 can be found in polynomial time. Then Algorithm 3 runs in polynomial time; it is deterministic, non-adaptive and correctly solves the subproblem of Lemma 4 for Œ± = O(log m).  
   
  3  
   
  Load Balancing on Related Machines  
   
  In this section, we improve on Theorem 2 in the special case of related machines, where each machine i has a speed parameter si > 0 and each job j an indeX pendent size Xj such that Xij = sij . Recall that we gave a non-adaptive  log m  O log log m -approximation for unrelated machines. However, the adaptivity gap   is Œ© logloglogmm even for load balancing on identical machines where every machine has the same speed. Thus, to improve on Theorem 2, we need to use adaptivity. The starting point of our improved algorithms is the same non-adaptive assignment for unrelated-machines load balancing. However, instead of nonadaptively assigning job j to the speciÔ¨Åed machine i, we adaptively assign j to the least loaded machine with similar speed to i. We formalize this idea and brieÔ¨Çy explain the algorithms for oÔ¨Ñine and online load balancing on related machines. Machine Smoothing. In this part, we deÔ¨Åne a notion of smoothed machines. We show that by losing a constant factor in the approximation ratio, we may assume that the machines are partitioned into at most O(log m) groups such that machines within a group have the same speed and the size of the groups shrinks geometrically. Thus, by ‚Äúmachines with similar speed to i,‚Äù we mean machines in the same group. Formally, we transform an instance I of load balancing on m related machines with stochastic jobs into an instance Is with so-called ‚Äúsmoothed machines‚Äù and the same set of jobs with the following three properties: (i) The machines are partitioned into m = O(log m) groups such that group k consists of mk machines with speed exactly sk such that s1 < s2 < ¬∑ ¬∑ ¬∑ < sm . (ii) For all groups 1 ‚â§ k < m , we have mk ‚â• 32 mk+1 . (iii) Opt(Is ) = O(Opt(I)). To this end, we suitably decrease machine speeds and delete machines from the original instance I; see [14] for the algorithm and the technical details. Lemma 7. There is an eÔ¨Écient algorithm that, given an instance I of load balancing with m related machines and stochastic jobs, computes an instance Is of smoothed machines with the same set of jobs satisfying Properties (i) to (iii).  
   
  ConÔ¨Åguration Balancing for Stochastic Requests  
   
  139  
   
  A similar idea for machine smoothing has been employed by Im et al. [23] for deterministic load balancing on related machines. In their approach, they ensure that the total processing power of the machines in a group decreases geometrically rather than the number of machines. OÔ¨Ñine Setting. We run Algorithm 2 on the conÔ¨Åguration balancing instance deÔ¨Åned by the load balancing instance with smoothed machines. Given a job-tomachine assignment, we list schedule the jobs assigned to a particular group on the machines of this group. In the proof of Theorem 4, we rely on the following strong bound on the expected maximum of the truncated load; see [14]. Lemma 8. Let c1 , . . . , cm ‚àà N‚â•1 be constants such that ci ‚â• 32 ci+1 for all 1 ‚â§ variables bounded in [0, œÑ ] i ‚â§ m. Let S1 , . . . , Sm be sums of independent random   such that E[Si ] ‚â§ ci œÑ for all 1 ‚â§ i ‚â§ m. Then, E maxi Scii ‚â§ O(œÑ ). Online Setting. We apply a similar framework as above. Note that our online conÔ¨Åguration balancing algorithm loses a logarithmic factor in the number of resources, so to obtain a O(log log m)-approximation, we aggregate each group (in the smoothed-machines instance) as a single resource. Intuitively, this deÔ¨Ånition captures the fact that we will average all jobs assigned to a group over the machines in this group. Thus, our conÔ¨Åguration balancing instance will have only O(log m) resources and applying Theorem 1 proves Theorem 4; see [14].  
   
  Conclusion We considered the conÔ¨Åguration balancing problem under uncertainty. In contrast to the (often overly optimistic) clairvoyant settings and the (often overly pessimistic) non-clairvoyant settings, we consider the stochastic setting where each request j presents a set of random vectors, and we need to (adaptively) pick one of these vectors, to minimize the expected maximum load over the m resources. We give logarithmic bounds for several general settings (which are existentially tight), and a much better O(1) oÔ¨Ñine and O(log log m) online bound for the related machines setting. Closing the gap for online related-machines load balancing remains an intriguing open problem. More generally, getting a better understanding of both adaptive and non-adaptive algorithms for stochastic packing and scheduling problems remains an exciting direction for research.  
   
  References 1. Agrawal, S., Devanur, N.R.: Fast algorithms for online stochastic convex programming. In: Proceedings of SODA, pp. 1405‚Äì1424 (2015) 2. Agrawal, S., Wang, Z., Ye, Y.: A dynamic near-optimal algorithm for online linear programming. Oper. Res. 62(4), 876‚Äì890 (2014) 3. Aspnes, J., Azar, Y., Fiat, A., Plotkin, S.A., Waarts, O.: On-line routing of virtual circuits with applications to load balancing and machine scheduling. J. ACM 44(3), 486‚Äì504 (1997)  
   
  140  
   
  F. Eberle et al.  
   
  4. Azar, Y.: On-line load balancing. In: Fiat, A., Woeginger, G.J. (eds.) Online Algorithms. LNCS, vol. 1442, pp. 178‚Äì195. Springer, Heidelberg (1998). https://doi. org/10.1007/BFb0029569 5. Azar, Y., Naor, J., Rom, R.: The competitiveness of on-line assignments. J. Algorithms 18(2), 221‚Äì237 (1995) 6. Beale, E.M.L.: On minimizing a convex function subject to linear inequalities. J. Roy. Stat. Soc. Ser. B. Methodol. 17, 173‚Äì184; discussion, 194‚Äì203 (1955) 7. Berman, P., Charikar, M., Karpinski, M.: On-line load balancing for related machines. J. Algorithms 35(1), 108‚Äì121 (2000) 8. Bhalgat, A., Goel, A., Khanna, S.: Improved approximation results for stochastic knapsack problems. In: Proceedings of SODA, pp. 1647‚Äì1665. SIAM (2011) 9. Charikar, M., Chekuri, C., P¬¥ al, M.: Sampling bounds for stochastic optimization. In: Chekuri, C., Jansen, K., Rolim, J.D.P., Trevisan, L. (eds.) APPROX/RANDOM -2005. LNCS, vol. 3624, pp. 257‚Äì269. Springer, Heidelberg (2005). https://doi.org/10.1007/11538462 22 10. Chuzhoy, J., Guruswami, V., Khanna, S., Talwar, K.: Hardness of routing with congestion in directed graphs. In: Proceedings of STOC, pp. 165‚Äì178. ACM (2007) 11. Dantzig, G.B.: Linear programming under uncertainty. Manag. Sci. 1, 197‚Äì206 (1955) 12. Dean, B.C., Goemans, M.X., Vondr¬¥ ak, J.: Approximating the stochastic knapsack problem: the beneÔ¨Åt of adaptivity. Math. Oper. Res. 33(4), 945‚Äì964 (2008) 13. Dye, S., Stougie, L., Tomasgard, A.: The stochastic single resource service-provision problem. Naval Res. Logist. 50(8), 869‚Äì887 (2003) 14. Eberle, F., Gupta, A., Megow, N., Moseley, B., Zhou, R.: ConÔ¨Åguration balancing for stochastic requests. CoRR, abs/2208.13702 (2022) 15. Graham, R.L.: Bounds on multiprocessing timing anomalies. SIAM J. Appl. Math. 17(2), 416‚Äì429 (1969) 16. Gupta, A., Krishnaswamy, R., Molinaro, M., Ravi, R.: Approximation algorithms for correlated knapsacks and non-martingale bandits. In: Ostrovsky, R. (ed.) Proceedings of FOCS, pp. 827‚Äì836. IEEE Computer Society (2011) 17. Gupta, A., Kumar, A., Nagarajan, V., Shen, X.: Stochastic load balancing on unrelated machines. Math. Oper. Res. 46(1), 115‚Äì133 (2021) 18. Gupta, A., Molinaro, M.: How the experts algorithm can help solve LPS online. Math. Oper. Res. 41(4), 1404‚Äì1431 (2016) 19. Gupta, A., P¬¥ al, M., Ravi, R., Sinha, A.: Sampling and cost-sharing: approximation algorithms for stochastic optimization problems. SIAM J. Comput. 40(5), 1361‚Äì 1401 (2011) 20. Hajiaghayi, M.T., Kim, J.H., Leighton, T., R¬® acke, H.: Oblivious routing in directed graphs with random demands. In: Proceedings of STOC, pp. 193‚Äì201. ACM (2005) 21. Ibrahimpur, S., Swamy, C.: Approximation algorithms for stochastic minimumnorm combinatorial optimization. In: Proceedings of FOCS, pp. 966‚Äì977. IEEE (2020) 22. Im, S., Kell, N., Kulkarni, J., Panigrahi, D.: Tight bounds for online vector scheduling. SIAM J. Comput. 48(1), 93‚Äì121 (2019) 23. Im, S., Kell, N., Panigrahi, D., Shadloo, M.: Online load balancing on related machines. In: Proceedings of STOC, pp. 30‚Äì43. ACM (2018) ¬¥ Allocating bandwidth for bursty connec24. Kleinberg, J.M., Rabani, Y., Tardos, E.: tions. SIAM J. Comput. 30(1), 191‚Äì217 (2000) 25. Leighton, T., Rao, S., Srinivasan, A.: Multicommodity Ô¨Çow and circuit switching. In: HICSS (7), pp. 459‚Äì465. IEEE Computer Society (1998)  
   
  ConÔ¨Åguration Balancing for Stochastic Requests  
   
  141  
   
  ¬¥ Approximation algorithms for scheduling 26. Lenstra, J.K., Shmoys, D.B., Tardos, E.: unrelated parallel machines. Math. Program. 46, 259‚Äì271 (1990) 27. Leonardi, S.: On-line network routing. In: Fiat, A., Woeginger, G.J. (eds.) Online Algorithms. LNCS, vol. 1442, pp. 242‚Äì267. Springer, Heidelberg (1998). https:// doi.org/10.1007/BFb0029572 28. Ma, W.: Improvements and generalizations of stochastic knapsack and Markovian bandits approximation algorithms. Math. Oper. Res. 43(3), 789‚Äì812 (2018) 29. M¬® ohring, R.H., Schulz, A.S., Uetz, M.: Approximation in stochastic scheduling: the power of LP-based priority policies. J. ACM 46(6), 924‚Äì942 (1999) 30. Molinaro, M.: Stochastic p load balancing and moment problems via the l-function method. In: Proceedings of SODA, pp. 343‚Äì354. SIAM (2019) 31. Raghavan, P., Thompson, C.D.: Randomized rounding: a technique for provably good algorithms and algorithmic proofs. Combinatorica 7(4), 365‚Äì374 (1987) 32. Sagnol, G., Schmidt, D., Waldschmidt, G.: Restricted adaptivity in stochastic scheduling. In: Proceedings of ESA. LIPIcs, vol. 204, pp. 79:1‚Äì79:14. Schloss Dagstuhl - Leibniz-Zentrum f¬® ur Informatik (2021) ¬¥ An approximation algorithm for the generalized assign33. Shmoys, D.B., Tardos, E.: ment problem. Math. Program. 62, 461‚Äì474 (1993) 34. Swamy, C., Shmoys, D.B.: Sampling-based approximation algorithms for multistage stochastic optimization. SIAM J. Comput. 41(4), 975‚Äì1004 (2012)  
   
  An Update-and-Stabilize Framework for the Minimum-Norm-Point Problem Satoru Fujishige1 , Tomonari Kitahara2 , and L√°szl√≥ A. V√©gh3(B) 1  
   
  3  
   
  Research Institute for Mathematical Sciences, Kyoto University, Kyoto 606-8502, Japan [email protected]  2 Faculty of Economics, Kyushu University, Fukuoka 819-0395, Japan [email protected]  Department of Mathematics, London School of Economics and Political Science, London WC2A 2AE, UK [email protected]   
   
  Abstract. We consider the minimum-norm-point (MNP) problem of polyhedra, a well-studied problem that encompasses linear programming. Inspired by Wolfe‚Äôs classical MNP algorithm, we present a general algorithmic framework that performs Ô¨Årst order update steps, combined with iterations that aim to ‚Äòstabilize‚Äô the current iterate with additional projections, i.e., Ô¨Ånding a locally optimal solution whilst keeping the current tight inequalities. We bound the number of iterations polynomially in the dimension and in the associated circuit imbalance measure. In particular, the algorithm is strongly polynomial for network Ô¨Çow instances. The conic version of Wolfe‚Äôs algorithm is a special instantiation of our framework; as a consequence, we obtain convergence bounds for this algorithm. Our preliminary computational experiments show a signiÔ¨Åcant improvement over standard Ô¨Årst-order methods.  
   
  1  
   
  Introduction  
   
  We study the minimum-norm-point (MNP) problem Minimize 12 ||Ax ‚àí b||2 subject to 0 ‚â§ x ‚â§ u , x ‚àà RN ,  
   
  (P)  
   
  where m and n are positive integers, M = {1, ¬∑ ¬∑ ¬∑ , m} and N = {1, ¬∑ ¬∑ ¬∑ , n}, A ‚àà RM √óN is a matrix with rank rk(A) = m, b ‚àà RM , and u ‚àà (R ‚à™ {‚àû})N . This is an extended abstract. The full version including all omitted proofs is available on arXiv:2211.02560. SF‚Äôs research is supported by JSPS KAKENHI Grant Numbers JP19K11839 and 22K11922 and by the Research Institute for Mathematical Sciences, an International Joint Usage/Research Center located in Kyoto University. TK is supported by JSPS KAKENHI Grant Number JP19K11830. LAV‚Äôs research is supported by the European Research Council (ERC) under the European Union‚Äôs Horizon 2020 research and innovation programme (grant agreement no. 757481‚ÄìScaleOpt). c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 142‚Äì156, 2023. https://doi.org/10.1007/978-3-031-32726-1_11  
   
  An Update-and-Stabilize Framework for the Minimum-Norm-Point Problem  
   
  143  
   
  We will use the notation B(u) := {x ‚àà RN | 0 ‚â§ x ‚â§ u} for the feasible set. The problem (P) generalizes the linear programming (LP) feasibility problem: the optimum value is 0 if and only if Ax = b, x ‚àà B(u) is feasible. We say that (P) is an uncapacitated instance if u(i) = ‚àû for all i ‚àà N . The formulation (P) belongs to a family of problems for which Necoara, Nesterov, and Glineur [16] showed linear convergence bounds of Ô¨Årst order methods. That is, the number of iterations needed to Ô¨Ånd an Œµ-approximate solution depends linearly on log(1/Œµ). Such convergence has been known for strongly convex functions, but this property does not hold for (P). However, [16] shows that restricted variants of strong convexity also suÔ¨Éce. For problems of the form (P), the required property follows using HoÔ¨Äman-proximity bounds [13]; see [19] and the references therein for recent results on HoÔ¨Äman-proximity. We propose a new algorithmic framework for the minimum-norm-point problem (P) that uses stabilizing steps between Ô¨Årst order updates. Our algorithm terminates with an exact optimal solution in a Ô¨Ånite number of iterations. Moreover, we show poly(n, Œ∫) running time bounds for multiple instantiations of the framework, where Œ∫ is the circuit imbalance measure associated with the matrix (A | Im ) (see Sect. 2). This gives strongly polynomial bounds whenever Œ∫ is constant; in particular, Œ∫ = 1 for network Ô¨Çow feasibility. We note that if A ‚àà ZM √óN , then Œ∫ ‚â§ Œî(A) for the maximum subdeterminant Œî(A). Still, Œ∫ can be exponential in the encoding length of the matrix. The stabilizing step is inspired by Wolfe‚Äôs classical minimum-norm-point algorithm [24]. This considers the variant of (P) where the box constraint  x ‚àà B(u) is replaced by i‚ààN xi = 1, x ‚â• 0. Wolfe‚Äôs algorithm is reminiscent of the simplex method. It comprises major and minor cycles, and at the end of every major cycle, the algorithm maintains a corral solution: for a linearly independent set of columns, the current point is the nearest point to b in the aÔ¨Éne hull of these columns, while it also falls inside their convex hull. Wolfe‚Äôs algorithm has been successfully employed as a subroutine in various optimization problems, e.g., submodular function minimization [11], see also [1,8,10]. Beyond the trivial 2n bound, the convergence analysis remained elusive; the Ô¨Årst bound with 1/Œµ-dependence was given by Chakrabarty et al. [2] in 2014. Lacoste-Julien and Jaggi [14] gave a log(1/Œµ) bound, parametrized by the pyramidal width of the polyhedron. Recently, De Loera et al. [5] showed an example of exponential time behaviour of Wolfe‚Äôs algorithm for the min-norm insertion rule (the analogue of a pivot rule); no exponential example for other insertion rules such as the linopt rule used in the application for submodular minimization. Wolfe‚Äôs algorithm works with a polytope in V -representation. Concurrently with Wolfe‚Äôs work, Wilhelmsen [23] proposed an equivalent algorithm for uncapacitated (conic) instances of (P), i.e., for a polytope in H-representation. This algorithm can be seen as a special instantiation of our framework, and we show an O(n4 Œ∫2 A2 log(n + Œ∫)) iteration bound. A signiÔ¨Åcant diÔ¨Äerence compared to the Wolfe and Wolfe‚ÄìWilhelmsen algorithms is that the supports of our iterates are not required to be independent. This provides much additional Ô¨Çexibility: our algorithm can be combined with  
   
  144  
   
  S. Fujishige et al.  
   
  a variety of Ô¨Årst order methods. This feature also yields a signiÔ¨Åcant advantage in our computational experiments. Overview of the Algorithm. A key concept in our algorithm is the centroid mapping, deÔ¨Åned as follows. For disjoint subsets I0 , I1 ‚äÜ N , we let L(I0 , I1 ) denote the aÔ¨Éne subspace of RN where x(i) = 0 for i ‚àà I0 and x(i) = u(i) for i ‚àà I1 . For x ‚àà B(u), let I0 (x) and I1 (x) denote the subsets of coordinates i with x(i) = 0 and x(i) = u(i), respectively. A centroid mapping Œ® : B(u) ‚Üí RN is a mapping with the property that Œ® (x) ‚àà arg miny { 12 Ay ‚àí b2 | y ‚àà L(I0 (x), I1 (x))}. This mapping may not be unique, since the columns of A corresponding to {i ‚àà N | 0 < x(i) < u(i)} = N \ (I0 (x) ‚à™ I1 (x)) may not be independent: the optimal centroid set is itself an aÔ¨Éne subspace. The point x ‚àà B(u) is stable if Œ® (x) = x. Stable points can be seen as the analogues of corral solutions in Wolfe‚Äôs algorithm. Every major cycle starts with an update step and ends with a stable point. The update step could be any Ô¨Årst-order step satisfying some natural requirements, such as variants of Frank‚ÄìWolfe, projected gradient, or Wolfe updates. As long as the current iterate is not optimal, this update strictly improves the objective. Finite convergence follows by the fact that there can be at most 3n stable points. After the update step, we start a sequence of minor cycles. From the current iterate x ‚àà B(u), we move to Œ® (x) in case Œ® (x) ‚àà B(u), or to the intersection of the boundary of B(u) and the line segment [x, Œ® (x)] otherwise. The minor cycles Ô¨Ånish once x = Œ® (x) is a stable point. The objective 12 Ax ‚àí b2 is decreasing in every minor cycle, and at least one new coordinate i ‚àà N is set to 0 or to u(i). Thus, the number of minor cycles in any major cycle is at most n. One can use various centroid mappings, with only a mild requirement on Œ® (see Sect. 2.2). We present a poly(n, Œ∫) convergence analysis in the uncapacitated case for projected gradient and Wolfe updates. We expect that similar arguments extend to the capacitated case. The proof has two key ingredients. First, we show linear convergence of the Ô¨Årst-order update steps (Theorem 3). Such a bound follows already from [16]; we present a simple self-contained proof exploiting properties of stable points and the uncapacitated setting. The second step of the analysis shows that in every poly(n, Œ∫) iterations, we can identify a new variable that will never become zero in subsequent iterations (Theorem 2). The proof relies on proximity arguments: we show that for any iterate x and any subsequent iterate x , the distance x ‚àí x  can be upper bounded in terms of n, Œ∫, and the optimality gap at x. In Sect. 5, we present preliminary computational experiments using randomly generated problem instances of various sizes. We compare the performance of diÔ¨Äerent variants of our algorithm to standard gradient methods. The algorithm performs much better with projected gradient updates than with Wolfe updates. We compare an ‚Äòoblivious‚Äô centroid mapping and one that chooses Œ® (x) as the a nearest point to x in the centroid set in the ‚Äòlocal norm‚Äô (see Sect. 2.2). The latter one appears to be signiÔ¨Åcantly better. For choices of parameters n ‚â• 2m, our method with projected gradient updates and local norm mapping outperforms  
   
  An Update-and-Stabilize Framework for the Minimum-Norm-Point Problem  
   
  145  
   
  the accelerated gradient method‚Äîthe best among classical methods‚Äîby a factor 10 or more in computational time. Related Work. Arguments that show strongly polynomial convergence by gradually revealing the support of an optimal solution are prevalent in combinatorial optimization. These date back to Tardos‚Äôs [21] groundbreaking work giving the Ô¨Årst strongly polynomial algorithm for minimum-cost Ô¨Çows. Our proof is closer to the dual ‚Äòabundant arc‚Äô algorithms in [9,17]. Tardos generalized the above result for general LP‚Äôs, giving a running time dependence poly(n, log Œî(A)), where Œî(A) is the largest subdeterminant of A. This framework was recently strengthened in [4] to poly(n, log Œ∫(A)) running time for the circuit imbalance measure Œ∫(A). We note that the above algorithms‚Äîalong with many other strongly polynomial algorithms in combinatorial optimization‚Äîmodify the problem directly once new information is learned about the optimal support. In contrast, our algorithm does not require any such modiÔ¨Åcations, nor a knowledge or estimate on the condition number Œ∫. Strongly polynomial algorithms with poly(n, log Œ∫(A)) running time bounds can also be obtained using layered least squares interior point methods. This line of work was initiated by Vavasis and Ye [22] using a related condition measure œá(A). ¬Ø An improved version that also established the relation between œá(A) ¬Ø and Œ∫(A) was recently given by Dadush et al. [3]. We refer the reader to the survey [6] for properties and further applications of circuit imbalances. Further Related Work. There are similarities between our algorithm and the Iteratively Reweighted Least Squares (IRLS) method that has been intensively studied since the 1960‚Äôs [15,18]. For some p ‚àà [0, ‚àû], A ‚àà RM √óN , b ‚àà RM , the goal is to approximately solve  p : Ax = b}. At each iteration, a weighted  min{x minimum-norm point min{ w(t) , x : Ax = b} is solved, where the weights w(t) are iteratively updated. The LP-feasibility problem Ax = b, 0 ‚â§ x ‚â§ 1 for Ô¨Ånite upper bounds u = 1 can be phrased as an ‚àû -minimization problem min{x‚àû : Ax = b ‚àí A1/2}. Ene and Vladu [7] gave an eÔ¨Écient variant of IRLS for 1 and ‚àû -minimization; see their paper for further references. Some variants of our algorithm solve a weighted least squares problem with changing weights in the stabilizing steps. There are however signiÔ¨Åcant diÔ¨Äerences between IRLS and our method. The underlying optimization problems are diÔ¨Äerent, and IRLS does not Ô¨Ånd an exact optimal solution in Ô¨Ånite time. Applied to LP in the ‚àû formulation, IRLS satisÔ¨Åes Ax = b throughout while violating the box constraints 0 ‚â§ x ‚â§ u. In contrast, iterates of our algorithm violate Ax = b but maintain 0 ‚â§ x ‚â§ u. The role of the least squares subroutines is also rather diÔ¨Äerent in the two settings.  
   
  2  
   
  Preliminaries  
   
  Notation. We use N ‚äï M for disjoint union (or direct sum) of the copies of the two sets. For a matrix A ‚àà RM √óN , i ‚àà M and j ‚àà N , we denote the ith row  
   
  146  
   
  S. Fujishige et al.  
   
  of A by Ai and jth column by Aj . Also for any matrix X denote by X  the matrix transpose of X. We let  ¬∑ p denote the p vector norm; we use  ¬∑  to denote the Euclidean norm  ¬∑ 2 . For a matrix A ‚àà RM √óN , we let A denote norm. the spectral norm, that is, the 2 ‚Üí 2 operator  For any x, y ‚àà RM we deÔ¨Åne x, y = i‚ààM x(i)y(i). We will use this notation also in other dimensions. We let [x, y] := {Œªx + (1 ‚àí Œª)y | Œª ‚àà [0, 1]} denote the line segment between the vectors x and y. Elementary Vectors and Circuits. For a linear space W  RN , g ‚àà W is an elementary vector if g is a support minimal nonzero vector in W , that is, no h ‚àà W \{0} exists such that supp(h)  supp(g), where supp denotes the support of a vector. We let F(W ) ‚äÜ W denote the set of elementary vectors. A circuit in W is the support of some elementary vector; these are precisely the circuits in the associated linear matroid M(W ). The subspaces W = {0} and W = RN are called trivial subspaces, all other subspaces are nontrivial. We deÔ¨Åne the circuit imbalance measure     g(j)    | g ‚àà F(W ), i, j ‚àà supp(g) Œ∫(W ) := max  g(i)  for nontrivial subspaces and Œ∫(W ) = 1 for trivial subspaces. For a matrix A ‚àà RM √óN , we use Œ∫(A) to denote Œ∫(ker(A)). Recall that a matrix is totally unimodular (TU) if the determinant of every square submatrix is 0, +1, or ‚àí1. A result by Cederbaum from 1957 shows that Œ∫(W ) = 1 if and only if there exists a TU matrix A ‚àà RM √óN such that W = ker(A). We also note that if A ‚àà ZM √óN , then Œ∫(A) ‚â§ Œî(A). We say that the vector y ‚àà RN conforms to x ‚àà RN if x(i)y(i) > 0 whenever y(i) = 0. Given a subspace W ‚äÜ RN , a conformal circuit decomposition of a vector v ‚àà W is a decomposition v=  
   
     
   
  hk ,  
   
  k=1  
   
  where  ‚â§ n and h1 , h2 , . . . , h ‚àà F(W ) are elementary vectors that conform to v. A fundamental result on elementary vectors asserts that for every subspace W ‚äÜ RN , every v ‚àà W admits a conformal circuit decomposition, see e.g. [12,20]. Note that there may be multiple conformal circuit decompositions of a vector. Given A ‚àà RM √óN , we deÔ¨Åne the extended subspace XA ‚äÇ RN ‚äïM as XA := ker(A | ‚àíIM ). Hence, for every v ‚àà RN , (v, Av) ‚àà XA . For v ‚àà RN , the generalized  path-circuit decomposition of v with respect to A is a decomposition v = k=1 hk , where  ‚â§ n, and for each 1 ‚â§ k ‚â§ , (hk , Ahk ) ‚àà RN ‚äïM is an elementary vector in XA that conforms to (v, Av). Moreover, hk is an inner vector in the decomposition if Ahk = 0 and an outer vector otherwise. We say that v ‚àà RN is cycle-free with respect to A, if all generalized pathcircuit decompositions of v contain outer vectors only. The following lemma will play a key role in analyzing our algorithms.  
   
  An Update-and-Stabilize Framework for the Minimum-Norm-Point Problem  
   
  147  
   
  Lemma 1. For any A ‚àà RM √óN , let v ‚àà RN be cycle-free with respect to A. Then, v‚àû ‚â§ Œ∫(XA )Av1 and v2 ‚â§ nŒ∫(XA )Av2 . Remark 1. We note that a similar argument shows that A ‚â§ min{nŒ∫(XA ), ‚àö nœÑ (A)Œ∫(XA )}, where œÑ (A) is the maximum size of supp(Ah) for an elementary vector (h, Ah) ‚àà XA . Example 1. If A ‚àà RM √óN is the node-arc incidence matrix of a directed graph D = (M, N ). The system Ax = b, x ‚àà B(u) corresponds to a network Ô¨Çow feasibility problem. Here, b(i) is the demand of node i ‚àà M , i.e., the inÔ¨Çow minus the outÔ¨Çow at i is required to be b(i). Recall that A is a TU matrix; consequently, (A| ‚àí IM ) is also TU, and Œ∫(XA ) = 1. Our algorithm is strongly polynomial in this setting. Note that inner vectors correspond to cycles and outer vectors to paths; this motivates the term ‚Äògeneralized path-circuit decomposition‚Äô. We also ‚àö note œÑ (A) = 2, and thus A ‚â§ 2 n in this case. 2.1  
   
  Optimal Solutions and Proximity  
   
  We deÔ¨Åne the set Z(A, u) := {Ax | x ‚àà B(u)} . Thus, Problem (P) is to Ô¨Ånd the point in Z(A, u) that is nearest to b with respect to the Euclidean norm. We note that if the upper bounds u are Ô¨Ånite, Z(A, u) is called a zonotope. Throughout, we let p‚àó denote the optimum value of (P). Note that whereas the optimal solution x‚àó may not be unique, the vector b‚àó := Ax‚àó is unique by strong convexity; we have p‚àó = 12 b ‚àí b‚àó 2 . We use Œ∑(x) := 12 Ax ‚àí b2 ‚àí p‚àó to denote the optimality gap for x ‚àà B(u). The point x ‚àà B(u) is an Œµapproximate solution if Œ∑(x) ‚â§ Œµ. For a point x ‚àà B(u), let I0 (x) := {i ‚àà N : x(i) = 0}, I1 (x) := {i ‚àà N : x(i) = u(i)}, and J(x) := N \ (I0 (x) ‚à™ I1 (x)). The gradient of the objective 1 2 2 Ax ‚àí b in (P) can be written as g x := A (Ax ‚àí b) . We recall the Ô¨Årst order optimality conditions: x ‚àà B(u) is an optimal solution to (P) if and only if g x (i) = 0 for all i ‚àà J(x), g x (i) ‚â• 0 for all i ‚àà I0 (x), and g x (i) ‚â§ 0 for all i ‚àà I1 (x). Using Lemma 1, we can show: Lemma 2. For any x ‚àà B(u), there exists an optimal solution x‚àó to (P) such that x ‚àí x‚àó ‚àû ‚â§ Œ∫(XA )Ax ‚àí b‚àó 1 , and hence, x ‚àí x‚àó 2 ‚â§ nŒ∫(XA )Ax ‚àí b‚àó 2 .  
   
  148  
   
  2.2  
   
  S. Fujishige et al.  
   
  The Centroid Mapping  
   
  Let us denote by 3N the set of all ordered pairs (I0 , I1 ) of disjoint subsets I0 , I1 ‚äÜ N , and let I‚àó := {i ‚àà N | u(i) < ‚àû}. For any (I0 , I1 ) ‚àà 3N with I1 ‚äÜ I‚àó , we let L(I0 , I1 ) := {x ‚àà RN | ‚àÄi ‚àà I0 : x(i) = 0, ‚àÄi ‚àà I1 : x(i) = u(i) } . We call {Ax | x ‚àà B(u) ‚à© L(I0 , I1 )} ‚äÜ Z(A, u) a pseudoface of the Z(A, u). We note that every face of Z(A, u) is a pseudoface, but there might be pseudofaces that do not correspond to any face. We deÔ¨Åne a centroid set for (I0 , I1 ) as C(I0 , I1 ) := arg min {Ay ‚àí b | y ‚àà L(I0 , I1 ))} . y  
   
  Proposition 1. For (I0 , I1 ) ‚àà 3N with I1 ‚äÜ I‚àó , C(I0 , I1 ) is an aÔ¨Éne subspace of RN , and there exists w ‚àà RM such that Ay = w for every y ‚àà C(I0 , I1 ). The centroid mapping Œ® : B(u) ‚Üí RN is a mapping that satisÔ¨Åes Œ® (Œ® (x)) = Œ® (x) and  
   
  Œ® (x) ‚àà C(I0 (x), I1 (x)) , ‚àÄx ‚àà B(u)  
   
  We say that x ‚àà B(u) is a stable point if Œ® (x) = x. A simple, ‚Äòoblivious‚Äô centroid mapping arises by taking the minimum-norm point of the centroid set: Œ® (x) := arg min{y | y ‚àà C(I0 (x), I1 (x))} .  
   
  (1)  
   
  However, this mapping has some undesirable properties. For example, we may have an iterate x that is already in C(I0 (x), I1 (x)), but Œ® (x) = x. Instead, we aim for centroid mappings that move the current point ‚Äòas little as possible‚Äô. The centroid mapping Œ® is called cycle-free, if the vector Œ® (x) ‚àí x is cycle-free w.r.t. A for every x ‚àà B(u). √óN be a positive diagonal matrix. Lemma 3. For every x ‚àà B(u), let D(x) ‚àà RN >0 Then, the following Œ® (x) deÔ¨Ånes a cycle-free centroid mapping:  
   
  Œ® (x) := arg min{D(x)(y ‚àí x) | y ‚àà C(I0 (x), I1 (x))} .  
   
  (2)  
   
  We emphasize that D(x) in the statement is a function of x and can be any positive diagonal matrix. Note also that the diagonal entries for indices in I0 (x) ‚à™ I1 (x) do not matter. In our experiments, deÔ¨Åning D(x) with diagonal entries 1/x(i)+1/(u(i)‚àíx(i)) for i ‚àà J(x) performs particularly well. Intuitively, this choice aims to move less the coordinates close to the boundary. The next proposition follows from Lagrangian duality. We note that Œ® (x) as in (1) or (2) can be computed by solving a system of linear equations. Proposition 2. For a partition N = I0 ‚à™ I1 ‚à™ J, the centroid set can be written as C(I0 , I1 ) = y ‚àà L(I0 , I1 ) | (AJ ) (Ay ‚àí b) = 0 .  
   
  An Update-and-Stabilize Framework for the Minimum-Norm-Point Problem  
   
  3  
   
  149  
   
  The Update-and-Stabilize Framework  
   
  Now we describe a general algorithmic framework MNPZ(A, b, u) for solving (P), shown in Algorithm 1. Similarly to Wolfe‚Äôs MNP algorithm, the algorithm comprises major and minor cycles. We maintain a point x ‚àà B(u), and x is stable at the end of every major cycle. Each major cycle starts by calling the subroutine Update(x); the only general requirement on this subroutine is: (U1) for y = Update(x), y = x if and only if x is optimal to (P), and Ay ‚àí b < Ax ‚àí b otherwise, and (U2) if y = x, then for any Œª ‚àà [0, 1), z = Œªy + (1 ‚àí Œª)x satisÔ¨Åes Ay ‚àí b < Az ‚àí b. Property (U1) can be obtained from any Ô¨Årst order algorithm; we introduce some important examples below. Property (U2) might be violated if using a Ô¨Åxed step-length, which is a common choice. In order to guarantee (U2), we can post-process the Ô¨Årst order update that returns y  by choosing y as the optimal point on the line segment [x, y  ]. The algorithm terminates once x = Update(x). In the minor cycles, as long as w := Œ® (x) = x, i.e., x is not stable, we set x := w if w ‚àà B(u); otherwise, we set the next x as the intersection of the line segment [x, w] and the boundary of B(u). The requirement (U1) is already suÔ¨Écient to show Ô¨Ånite termination.  
   
  Algorithm 1: MNPZ(A, b, u)  
   
  1 2 3 4 5 6 7 8 9 10 11 12  
   
  Input : A ‚àà RM √óN , b ‚àà RM , u ‚àà (R ‚à™ {‚àû})N Output: An optimal solution x to (P) x ‚Üêinitial point from B(u) ; repeat x ‚Üê Update(x) ; w ‚Üê Œ® (x) ; while Œ® (x) = x do Œ±‚àó ‚Üê arg max{Œ± ‚àà [0, 1] | x + Œ±(w ‚àí x) ‚àà B(u)} ; x ‚Üê x + Œ±‚àó (w ‚àí x) ; w ‚Üê Œ® (x) ;  
   
  // Major cycle // Minor cycle  
   
  x‚Üêw; until x = Update(x) return x  
   
  Theorem 1. Consider any Update(x) subroutine that satisÔ¨Åes (U1) and any centroid mapping Œ® . The algorithm MNPZ(A, b, u) Ô¨Ånds an optimal solution to (P) within 3n major cycles. Every major cycle contains at most n minor cycles.  
   
  150  
   
  S. Fujishige et al.  
   
  We can implement the Update(x) subroutine satisfying (U1) and (U2) using various Ô¨Årst order methods for constrained optimization. Recall the notation g x for the gradient g x ; we use g = g x when x is clear from the context. The following property of stable points can be compared to the optimality conditions: Lemma 4. If x(= Œ® (x)) is a stable point, then g(j) = 0 for all j ‚àà J(x). We now describe three classical options. We stress that the choice of the centroid mapping Œ® can be chosen independently of the update step. The Frank‚ÄìWolfe Update. The Frank‚ÄìWolfe or conditional gradient method is applicable only in the case when u(i) is Ô¨Ånite for every i ‚àà N . In every update step, we start by computing y¬Ø as the minimizer of the linear objective g, y over B(u), that is, y¬Ø ‚àà arg min{ g, y | y ‚àà B(u)}. We set Update(x) := x if g, y¬Ø = g, x , or y = Update(x) is selected so that y minimizes 12 Ay ‚àí b2 on the line segment [x, y¬Ø]. Clearly, y¬Ø(i) = 0 if g(i) > 0, and y¬Ø(i) = u(i) if g(i) < 0. But, y¬Ø(i) can be chosen arbitrarily if g(i) = 0. In this case, we keep y¬Ø(i) = x(i); this will be signiÔ¨Åcant to guarantee stability of solutions in the analysis. The Projected Gradient Update. The projected gradient update moves in the opposite gradient direction to y¬Ø := x ‚àí Œªg for some step-length Œª > 0, and obtains the output y = Update(x) as the projection y of y¬Ø to the box B(u). This projection simply changes every negative coordinate to 0 and every y¬Ø(i) > u(i) to y(i) = u(i). To ensure (U2), we can perform an additional step that replaces y by the point y  ‚àà [x, y] that minimizes 12 Ay  ‚àí b2 . Consider now an uncapacitated instance (i.e., u(i) = ‚àû for all i ‚àà N ), and let x be a stable point. Recall I1 (x) = ‚àÖ in the uncapacitated setting. Lemma 4 allows us to write the projected gradient update in the following simple form that also enables to use optimal line search. DeÔ¨Åne z x (i) := max{‚àíg x (i), 0}, and use z = z x when clear from the context. Note that x is optimal to (P) if and only if z = 0. We use the optimal line search  
   
  y := arg min 12 Ay ‚àí b2 | y = x + Œªz, Œª ‚â• 0 . y  
   
  If z = 0, this can be written explicitly as y := x +  
   
  z2 z. Az2  
   
  To verify this formula, note that z2 = ‚àí g, z , since either z(i) = 0 or z(i) = ‚àíg(i).  
   
  An Update-and-Stabilize Framework for the Minimum-Norm-Point Problem  
   
  151  
   
  The Wolfe Update. Our third update resembles Wolfe‚Äôs algorithm. In the uncapacitated case, it corresponds to the Wolfe‚ÄìWilhelmsen algorithm. Given a stable point x ‚àà B(u), we select a coordinate j ‚àà N where either j ‚àà I0 (x) and g(j) < 0 or j ‚àà I1 (x) and g(j) > 0, and set y such that y(i) = x(i) if i = j, and y(j) is chosen in [0, u(j)] so that 12 Ay ‚àí b2 is minimized. Analogously to Wolfe‚Äôs algorithm, we can maintain basic solutions throughout. Namely, if AJ is linearly independent for J = J(x), then one can show that  AJ is also linearly independent for J  = J(y) = J ‚à™ {j}, where y = Update(x ). Assume we start with x = 0, i.e., J(x) = I1 (x) = ‚àÖ, I0 (x) = N . Then, AJ(x) remains linearly independent throughout. Hence, every stable solution x is a basic solution to (P). Note that whenever AJ(x) is linearly independent, C(I0 (x), I1 (x)) contains a single point, hence, Œ® (x) is uniquely deÔ¨Åned. Consider now the uncapacitated setting. For z = z x , let us return y = x if z = 0. Otherwise, let j ‚àà arg maxk z(k); note that j ‚àà I0 (x). Let x(i) if i ‚àà N \ {j}, y(i) := z(i) if i = j. Ai 2 It is easy to verify that the Frank‚ÄìWolfe, projected gradient, and Wolfe update rules all satisfy (U1) and (U2). For projected gradient, for the updates in the uncapacitated form as described above, (U2) is guaranteed. For the general form with upper bounds, we can perform a post-processing as noted above to ensure (U2). We say that Update(x ) is a cycle-free update rule, if for every x ‚àà B(u) and y = Update(x ), x ‚àí y is cycle-free w.r.t. A. One can show that the Frank‚ÄìWolfe, projected gradient, and Wolfe updates are all cycle-free.  
   
  4  
   
  Analysis  
   
  Theorem 2. Consider an uncapacitated instance of (P), and assume we use a cycle-free centroid mapping. Algorithm 1 terminates with an optimal solution in O(n3 Œ∫2 (XA )A2 log(n + Œ∫(XA ))) major cycles using projected gradient updates, and in O(n4 Œ∫2 (XA )A2 log(n + Œ∫(XA ))) major cycles using Wolfe updates. In both cases, the total number of minor cycles is O(n4 Œ∫2 (XA )A2 log(n+Œ∫(XA ))). Proximity Bounds. We show that if using a cycle-free update rule and a cycle-free centroid mapping, the movement of the iterates in Algorithm 1 can be bounded by the change in the objective value. First, a nice property of the centroid set is that the movement of Ax directly relates to the decrease in the objective value. Namely, Lemma 5. For x ‚àà B(u), let y ‚àà C(I0 (x), I1 (x)). Then, Ax ‚àí Ay2 = Ax ‚àí b2 ‚àíAy‚àíb2 . Consequently, if Œ® is a cycle-free centroid mapping and y = Œ® (x), then  
   
  x ‚àí y2 ‚â§ n2 Œ∫2 (XA ) Ax ‚àí b2 ‚àí Ay ‚àí b2 . Next, let us consider the movement of x during a call to Update(x ).  
   
  152  
   
  S. Fujishige et al.  
   
  Lemma 6. Let x ‚àà B(u) and y = Update(x ). Then, Ax ‚àí Ay2 ‚â§ Ax ‚àí b2 ‚àí Ay ‚àí b2 . If using a cycle-free update rule, we also have  
   
  x ‚àí y2 ‚â§ n2 Œ∫2 (XA ) Ax ‚àí b2 ‚àí Ay ‚àí b2 . Lemma 7. Let x ‚àà B(u), and let x be an iterate obtained by consecutive t major or minor updates of Algorithm 1 using a cycle-free update rule and a cycle-free centroid mapping, starting from x. Then,  ‚àö x ‚àí x  ‚â§ nŒ∫(XA ) 2t ¬∑ 21 Ax ‚àí b2 ‚àí 12 Ax ‚àí b2 . Geometric Convergence of the Projected Gradient and Wolfe Updates. Recall that Œ∑(x) denotes the optimality gap at x. Theorem 3. Consider an uncapacitated instance of (P), and let x ‚â• 0 be a stable point. Then for y = Update(x) using the projected gradient update, we 2 2 2 Œ∫ (XA )A ) Œ∑(x). Using the Wolfe updates, we have have Œ∑(y)  
   
  ‚â§ 1 ‚àí 31/(2n 2 2 Œ∑(y) ‚â§ 1 ‚àí 1/(2n Œ∫ (XA )A ) Œ∑(x). The theorem follows easily from the next two lemmas. First, we formulate the update progress using optimal line search, and next, we use Lemma 2 to bound z. Lemma 8. For a stable point x ‚â• 0, the projected gradient update satisÔ¨Åes Ax ‚àí b2 ‚àí Ay ‚àí b2 ‚â• z2 /A2 , and the Wolfe update satisÔ¨Åes Ax ‚àí b2 ‚àí Ay ‚àí b2 = z(j)2 /Aj 2 . x Lemma  9. For‚àöa stable point x ‚â• 0 and the update direction z = z , we have z ‚â• Œ∑(x)/( 2nŒ∫(XA )).  
   
  Proof. Let x‚àó ‚â• 0 be an optimal solution to (P) as in Lemma 2, and b‚àó = Ax‚àó . Using convexity of f (x) := 12 Ax‚àíb2 , we have p‚àó = f (x‚àó ) ‚â• f (x)+ g, x‚àó ‚àíx ‚â• f (x) ‚àí z, x‚àó ‚àí x , where the second inequality follows by noting that for each i ‚àà N , either z(i) = ‚àíg(i), or z(i) = 0 and g(i)(x‚àó (i) ‚àí x(i)) ‚â• 0. From the Cauchy-Schwarz inequality and Lemma 2, we get p‚àó ‚â• f (x) ‚àí z ¬∑ x‚àó ‚àí x ‚â• f (x) ‚àí nŒ∫(XA )Ax ‚àí b‚àó  ¬∑ z, that is, z ‚â• Œ∑(x)/nŒ∫(XA )Ax ‚àí b‚àó . The proof is complete by showing 2Œ∑(x) ‚â• Ax ‚àí b‚àó 2 . Recalling that Œ∑(x) = 12 Ax ‚àí b2 ‚àí 12 Ax‚àó ‚àí b2 and that b‚àó = Ax‚àó , this is equivalent to Ax ‚àí Ax‚àó , Ax‚àó ‚àí b ‚â• 0. This can be further written as x ‚àí ‚àó x‚àó , g x ‚â• 0, which is implied by the Ô¨Årst order optimality condition at x‚àó . This proves 2Œ∑(x) ‚â• Ax ‚àí b‚àó 2 , and hence the lemma follows. Overall Convergence Bounds. We now prove Theorem 2. Using Lemma 7 and Theorem 3, we can derive the following stronger proximity bound: Lemma 10. Consider an uncapacitated instance of (P). Let x ‚â• 0 be an iterate of Algorithm 1 using projected gradient updates, and let x ‚â• 0 be any  later iterate. Then, for a value Œò := O(n2.5 Œ∫2 (XA )A), we have x ‚àí x  ‚â§ Œò Œ∑(x).  
   
  An Update-and-Stabilize Framework for the Minimum-Norm-Point Problem  
   
  153  
   
  We need one more auxiliary lemma. Lemma 11. Consider an uncapacitated instance of (P), and let x ‚â• 0 be a stable point. Let x ÀÜ ‚â• 0 such that for each i ‚àà N , either x ÀÜ(i) = x(i), or x ÀÜ(i) = x ‚àí Ax2 . 0 < x(i). Then, AÀÜ x ‚àí b2 = Ax ‚àí b2 + AÀÜ For the threshold Œò as in Lemma 10 and for any x ‚â• 0, let us deÔ¨Åne    J  (x) := i | x(i) > Œò Œ∑(x) . The following is immediate from Lemma 10. Lemma 12. Consider an uncapacitated instance of (P). Let x ‚â• 0 be an iterate of Algorithm 1 using projected gradient updates, and x ‚â• 0 be any later iterate. Then, J  (x) ‚äÜ J(x ). Proof (Proof of Theorem 2). At any point of the algorithm, let J  denote the Consider a stable iterate x union of the sets J  (x) for all iterations thus far.  at the beginning of any major cycle, and let Œµ := Œ∑(x)/4nŒòA. Theorem 3 (XA )A2 log(n + for projected gradient updates guarantees that within O(n2 Œ∫2  Œ∫(XA ))) major cycles we arrive at an iterate x such that Œ∑(x ) < Œµ. The bound is O(n3 Œ∫2 (XA )A2 log(n + Œ∫(XA ))) for Wolfe updates. We note that log(n + Œ∫(XA ) + A) = O(log(n + Œ∫(XA ))) according to Remark 1. We show that (3) J  (x ) ‚à© I0 (x) = ‚àÖ. From here, we can conclude that J  was extended between iterates x and x . This may happen at most n times, and so we get the claimed bounds on the number of major cycles. The bound on the minor cycles for projected gradient updates follows since every major cycle contains at most n minor cycles. For Wolfe updates, it follows since every major cycle adds on one component to J(x) whereas every minor cycle removes at least one. Hence, the total number of minor cycles is at most m plus the total number of major cycles. For a contradiction, assume that (3) does not hold. Thus, for every i ‚àà I0 (x), ÀÜ ‚àà RN as x ÀÜ(i) := 0 if i ‚àà I0 (x), and we have x (i) ‚â§ ŒòŒµ. Let us deÔ¨Åne x  By the above assumption, ÀÜ x ‚àí x ‚àû ‚â§ ŒòŒµ, and x ÀÜ(i) := x (i) if i ‚àà J(x). ‚àö  x ‚àí b2 = therefore AÀÜ x ‚àí Ax  ‚â§ nŒòAŒµ. From Lemma 11, we can bound AÀÜ  2  2 ‚àó 2 2 2 x ‚àí Ax  ‚â§ 2p + (nŒò A + 2)Œµ . Recall that since x is a Ax ‚àí b + AÀÜ ÀÜ is a feasible stable solution, Ax‚àíb = min {Ay ‚àí b : y ‚àà L(I0 (x), ‚àÖ)}. Since x solution to this program, it follows that AÀÜ x ‚àí b2 ‚â• Ax ‚àí b2 . We get that x ‚àí b2 ‚àí 2p‚àó ‚â§ (nŒò2 A2 + 2)Œµ2 , in contradiction 2Œ∑(x) = Ax ‚àí b2 ‚àí 2p‚àó ‚â§ AÀÜ with the choice of Œµ.  
   
  5  
   
  Computational Experiments  
   
  We give preliminary computational experiments of diÔ¨Äerent versions of our algorithm, and compare them to standard gradient methods. The experiments were  
   
  154  
   
  S. Fujishige et al.  
   
  programmed and executed by MATLAB version R2021b on a personal computer having 11th Gen Intel(R) Core(TM) i7-11370H @ 3.30GHz and 16GB of memory. We present results on randomly generated uncapacitated instances. The full version contains more experiments, also on capacitated instances, and also including the Frank‚ÄìWolfe method. The entries of the m √ó n matrix A and the m dimensional vector b were chosen independently uniformly at random from the interval [‚àí0.5, 0.5]. Thus, the underlying LP Ax = b, x ‚â• 0 may or may not be feasible. For the case m = 1000, n = 1050, this leads to infeasible instance with high probability. In this case, we also generated feasible instances by sampling coeÔ¨Écients w(i) ‚àà [0, 1] uniformly at random, and setting b = Aw. We test each combination of two update methods: Projected Gradient (PG) and Wolfe (W); and two centroid mappings, the ‚Äòoblivious‚Äô mapping (1) and the ‚Äòlocal norm‚Äô mapping (2) with diagonal entries 1/x(i) + 1/(u(i) ‚àí x(i)). Recall that for Wolfe updates, there is a unique centroid mapping. We benchmark against standard constrained Ô¨Årst order methods: the projected gradient (PG), and the projected fast (accelerated) gradient method (PFG). In contrast to our algorithm, these do not Ô¨Ånitely terminate. We stopped these algorithms once they found a near-optimal solution within a certain accuracy threshold. We stopped each algorithm when the computation time reached 180 s. For each (m, n), we test all the algorithms 10 times and the results shown below are the 10-run averaged Ô¨Ågures. Table 1 shows the overall computational times; values in brackets show the number of trials whose computation time exceeded 180 s. For the ‚Äònear-square‚Äô case m = 1000, n = 1050, status ‚ÄòI‚Äô denotes infeasible and ‚ÄòF‚Äô feasible instances. Table 2 shows the number of major cycles and the total number of minor cycles. In our framework, projected gradient updates perform signiÔ¨Åcantly better than Wolfe updates, except for infeasible ‚Äònear-square‚Äô instances. For Wolfe updates, the number of major and minor cycles is similar; projected gradient performs much fewer major cycles. Among the two centroid mappings, the ‚Äòlocalnorm‚Äô update (2) performs signiÔ¨Åcantly better than the ‚Äòoblivious‚Äô update (1). There is a marked diÔ¨Äerence between infeasible and feasible ‚Äònear-square‚Äô instances. Our algorithms perform well on feasible instances. For infeasible instances, the running time is much longer, with an excessive number of minor cycles. As one may expect, projected fast gradient is signiÔ¨Åcantly better than project gradient. For ‚Äòrectangular‚Äô (n ‚â• 2m) instances, our method with projected gradient updates together with the centroid mapping (2), outperforms fast gradient by a factor 10 or more. This is despite the fact that centroid mappings are computationally more expensive than Ô¨Årst order methods. For feasible near-square instances, the performance in these two cases is similar. However, for infeasible near-square instances, our algorithms are outperformed by projected gradient and projected fast gradient methods.  
   
  An Update-and-Stabilize Framework for the Minimum-Norm-Point Problem  
   
  155  
   
  Table 1. Computation time (in sec) for random uncapacitated instances m n Status  
   
  100 200 200 400  
   
  PG+(1) 0.37 3.19 PG+(2) 0.04 0.31 0.20 1.90 W PG PFG  
   
  300 600  
   
  500 1000  
   
  500 3000  
   
  11.07 0.81 6.69  
   
  79.17 1.76 33.75  
   
  142.67 (4) 58.57 5.68 2.37 57.72 0.86 47.83 22.80 149.73  
   
  3.49 87.07 (3) 118.91 (5) 112.85 (5) 5.31 0.17 4.13 17.48 47.33 (1) 6.17  
   
  1000 1000 1050 1050 I F  
   
  6.26 5.15  
   
  180.00 (10) 92.59  
   
  Table 2. # of major cycles (Ô¨Årst number) and total # of minor cycles (second number) for random uncapacitated instances m n Status  
   
  100 200  
   
  200 400  
   
  300 600  
   
  500 1000  
   
  500 3000  
   
  1000 1000 1050 1050 I F  
   
  PG+(1) 6.0 9.5 11.6 11.9 1.0 4.1 1.0 199.7 462.6 759.6 1505.2 1513.9 599.7 31.6 PG+(2) 2.2 23.5 W  
   
  2.8 30.9  
   
  2.6 36.9  
   
  2.2 29.8  
   
  121 265.4 401.1 653.5 144.6 333.4 506.1 810.2  
   
  1.0 1.0  
   
  4.1 1.0 570.2 3.1  
   
  501.4 508.2  
   
  526.7 1091.6 530.8 1182.2  
   
  Acknowledgments. The third author would like to thank Richard Cole, Daniel Dadush, Christoph Hertrich, Bento Natura, and Yixin Tao for discussions on Ô¨Årst order methods and circuit imbalances.  
   
  References 1. Bach, F.: Learning with submodular functions: a convex optimization perspective. Found. Trends Mach. Learn. 6(2‚Äì3), 145‚Äì373 (2013) 2. Chakrabarty, D., Jain, P., Kothari, P.: Provable submodular minimization using Wolfe‚Äôs algorithm. In: Advances in Neural Information Processing Systems, vol. 27 (2014) 3. Dadush, D., Huiberts, S., Natura, B., V√©gh, L.A.: A scaling-invariant algorithm for linear programming whose running time depends only on the constraint matrix. In: Proceedings of the 52nd Annual ACM Symposium on Theory of Computing (STOC), pp. 761‚Äì774 (2020) 4. Dadush, D., Natura, B., V√©gh, L.A.: Revisiting Tardos‚Äôs framework for linear programming: faster exact solutions using approximate solvers. In: Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science (FOCS), pp. 931‚Äì942 (2020)  
   
  156  
   
  S. Fujishige et al.  
   
  5. De Loera, J.A., Haddock, J., Rademacher, L.: The minimum Euclidean-norm point in a convex polytope: Wolfe‚Äôs combinatorial algorithm is exponential. SIAM J. Comput. 49(1), 138‚Äì169 (2020) 6. Ekbatani, F., Natura, B., V√©gh, A.L.: Circuit imbalance measures and linear programming. In: Surveys in Combinatorics 2022. London Mathematical Society Lecture Note Series, pp. 64‚Äì114. Cambridge University Press, Cambridge (2022) 7. Ene, A., Vladu, A.: Improved convergence for 1 and ‚àû regression via iteratively reweighted least squares. In: International Conference on Machine Learning, pp. 1794‚Äì1801. PMLR (2019) 8. Fujishige, S.: Lexicographically optimal base of a polymatroid with respect to a weight vector. Math. Oper. Res. 5(2), 186‚Äì196 (1980) 9. Fujishige, S.: A capacity-rounding algorithm for the minimum-cost circulation problem: a dual framework of the Tardos algorithm 35(3), 298‚Äì308 (1986) 10. Fujishige, S., Hayashi, T., Yamashita, K., Zimmermann, U.: Zonotopes and the LP-Newton method. Optim. Eng. 10(2), 193‚Äì205 (2009) 11. Fujishige, S., Isotani, S.: A submodular function minimization algorithm based on the minimum-norm base. Pac. J. Optim. 7(1), 3‚Äì17 (2011) 12. Fulkerson, D.: Networks, frames, blocking systems. Math. Decis. Sci. Part I, Lect. Appl. Math. 2, 303‚Äì334 (1968) 13. HoÔ¨Äman, A.J.: On approximate solutions of systems of linear inequalities. J. Res. Natl. Bur. Stand. 49(4), 263‚Äì265 (1952) 14. Lacoste-Julien, S., Jaggi, M.: On the global linear convergence of Frank-Wolfe optimization variants. In: Advances in Neural Information Processing Systems, vol. 28 (2015) 15. Lawson, C.L.: Contribution to the theory of linear least maximum approximation. Ph.D. thesis (1961) 16. Necoara, I., Nesterov, Y., Glineur, F.: Linear convergence of Ô¨Årst order methods for non-strongly convex optimization. Math. Program. 175(1), 69‚Äì107 (2019) 17. Orlin, J.B.: A faster strongly polynomial minimum cost Ô¨Çow algorithm. Oper. Res. 41(2), 338‚Äì350 (1993) 18. Osborne, M.R.: Finite Algorithms in Optimization and Data Analysis. Wiley, Hoboken (1985) 19. Pe√±a, J., Vera, J.C., Zuluaga, L.F.: New characterizations of HoÔ¨Äman constants for systems of linear constraints. Math. Program. 1‚Äì31 (2020) 20. Rockafellar, R.T.: The elementary vectors of a subspace of RN . In: Combinatorial Mathematics and Its Applications: Proceedings North Carolina Conference, Chapel Hill, 1967, pp. 104‚Äì127. The University of North Carolina Press (1969) 21. Tardos, √â.: A strongly polynomial minimum cost circulation algorithm. Combinatorica 5(3), 247‚Äì255 (1985) 22. Vavasis, S.A., Ye, Y.: A primal-dual interior point method whose running time depends only on the constraint matrix 74(1), 79‚Äì120 (1996) 23. Wilhelmsen, D.R.: A nearest point algorithm for convex polyhedral cones and applications to positive linear approximation. Math. Comput. 30(133), 48‚Äì57 (1976) 24. Wolfe, P.: Finding the nearest point in a polytope. Math. Program. 11(1), 128‚Äì149 (1976)  
   
  Stabilization of Capacitated Matching Games Matthew Gerstbrein1 , Laura Sanit√†2 , and Lucy Verberk3(B) 1  
   
  3  
   
  University of Waterloo, Waterloo, Canada [email protected]  2 Bocconi University, Milan, Italy [email protected]  Eindhoven University of Technology, Eindhoven, The Netherlands [email protected]   
   
  Abstract. An edge-weighted, vertex-capacitated graph G is called stable if the value of a maximum-weight capacity-matching equals the value of a maximum-weight fractional capacity-matching. Stable graphs play a key role in characterizing the existence of stable solutions for popular combinatorial games that involve the structure of matchings in graphs, such as network bargaining games and cooperative matching games. The vertex-stabilizer problem asks to compute a minimum number of players to block (i.e., vertices of G to remove) in order to ensure stability for such games. The problem has been shown to be solvable in polynomialtime, for unit-capacity graphs. This stays true also if we impose the restriction that the set of players to block must not intersect with a given speciÔ¨Åed maximum matching of G. In this work, we investigate these algorithmic problems in the more general setting of arbitrary capacities. We show that the vertex-stabilizer problem with the additional restriction of avoiding a given maximum matching remains polynomial-time solvable. DiÔ¨Äerently, without this restriction, the vertex-stabilizer problem becomes NP-hard and even hard to approximate, in contrast to the unit-capacity case. Finally, in unit-capacity graphs there is an equivalence between the stability of a graph, existence of a stable solution for network bargaining games, and existence of a stable solution for cooperative matching games. We show that this equivalence does not extend to the capacitated case. Keywords: Matching  
   
  1  
   
  ¬∑ Game theory ¬∑ Network bargaining  
   
  Introduction  
   
  Network Bargaining Games (NBG) and Cooperative Matching Games (CMG) are popular combinatorial games involving the structure of matchings in graphs. CMG were introduced in the seminal paper of Shapley and Shubik 50 years ago [18], and have been widely studied since then. NBG are relatively more recent, and were deÔ¨Åned by Kleinberg and Tardos [14] as a generalization of Nash‚Äôs 2-player bargaining solution [17]. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 157‚Äì171, 2023. https://doi.org/10.1007/978-3-031-32726-1_12  
   
  158  
   
  M. Gerstbrein et al.  
   
  Instances of these games are described by a graph G = (V, E) with weights w ‚àà RE ‚â•0 , where the vertices and the edges model the players and their potential interactions, respectively. The value of a maximum-weight matching, denoted as ŒΩ(G), is the total value that players can collectively accumulate. The goal, roughly speaking, is to assign values to players in such a way that players have no incentive to deviate from the current allocation. Formally, in an instance of a NBG, players want to enter in a deal with one of their neighbours, and agree on how to split the value of the deal given by the weight of the corresponding edge. Hence, an outcome is naturally associated with a matching M of G representing the deals, and allocation vector y ‚àà RV‚â•0 with yu + yv = wuv if uv ‚àà M , and yv = 0 if v is not matched. An outcome (M, y) is stable if each player‚Äôs allocation yu is at least as large as their outside option, formally deÔ¨Åned as maxv:uv‚ààE\M {wuv ‚àí yv }. In an instance of a CMG, one wants to Ô¨Ånd an allocation of total value ŒΩ(G), given by a vector y ‚àà RV‚â•0 in which no subset of players can gain more by forming a coalition. This condition is enforced by the constraint v‚ààS yv ‚â• ŒΩ(G[S]) for all S ‚äÜ V , where G[S] indicates the subgraph of G induced by the vertices in S. Such allocation is called stable, and the set of stable allocations constitutes the core of the game. Despite having been deÔ¨Åned in diÔ¨Äerent contexts, there is a tight link between stable solutions of these types of games. In particular, if each game is played on the same graph G, then it has been shown that either a stable solution exists for both games, or for neither game. This follows as both games admit the same polyhedral characterization of instances with stable solutions [7,14]. SpeciÔ¨Åcally, a stable solution exists if and only if ŒΩ(G) equals the value of the standard linear programming (LP) relaxation of the maximum matching problem deÔ¨Åned as     xuv ‚â§ 1 ‚àÄv ‚àà V, x ‚â• 0 . (1) ŒΩf (G) := max w x : u:uv‚ààE  
   
  A graph G for which ŒΩ(G) = ŒΩf (G) is called stable. As a result of this characterization, it is easy to see that there are graphs which do not admit stable solutions (to either type of game), such as odd cycles. Given that not all graphs are stable, naturally arises the stabilization problem of how to minimally modify a graph to turn it into a stable one. Stabilization problems attracted a lot of attention in the literature in the past years (see e.g. [1,3‚Äì6,13,15,16]). In this context, very natural operations to stabilize graphs are edge- and vertex-removal operations. Those have an interesting interpretation: they correspond to blocking interactions and players, respectively, in order to ensure a stable outcome. While removing a minimum number of edges to stabilize a graph is NP-hard already for unit weight graphs [4], and even hard-to-approximate with a constant factor [11,15], stabilizing the graph via vertex-removal operations turned out to be solvable in polynomial-time. SpeciÔ¨Åcally, [1,13] showed that computing a minimum-cardinality set of players to block in order to stabilize an unweighted graph (called the vertex-stabilizer problem) can be done in polynomial time. Furthermore, [1] showed that computing a minimum set of players to block in order to make a given maximum matching realizable as a stable outcome  
   
  Stabilization of Capacitated Matching Games  
   
  159  
   
  (called the M -vertex-stabilizer problem) is also eÔ¨Éciently solvable. The authors of [15] showed that both results generalize to weighted graphs. This paper focuses on Capacitated NBG, introduced by Bateni et al [2] as a generalization of NBG, to capture the more realistic scenario where players are allowed to enter in more than one deal. This generalization can be modeled by allowing for vertex capacities c ‚àà ZV‚â•0 . The notion of a matching is therefore generalized to a c-matching, where each vertex v ‚àà V is matched with at most cv vertices. In this case, the value of a maximum-weight c-matching of a graph G is denoted as ŒΩ c (G), and the standard LP relaxation is given by    c  xuv ‚â§ cv ‚àÄv ‚àà V, 0 ‚â§ x ‚â§ 1 . (2) ŒΩf (G) := max w x : u:uv‚ààE  
   
  An outcome to the NBG is associated with a c-matching M and a vector a ‚àà R2E ‚â•0 that satisÔ¨Åes auv + avu = wuv if uv ‚àà M , and auv = avu = 0 otherwise. The concepts of outside option and stable outcome can be deÔ¨Åned similarly as in the unit-capacity case, see [2]. The authors of [2] proved that the LP characterization of stable solutions generalize, i.e., there exist a stable outcome for the capacitated NBG on G if and only if ŒΩ c (G) = ŒΩfc (G) (i.e., G is stable). Farczadi et al [9] show that some other important properties of NBG extend to this capacitated generalization, such as the possibility to eÔ¨Éciently compute a so-called balanced solution (we refer to [9] for details). The goal of this paper is to investigate whether the other two signiÔ¨Åcant features of NBG mentioned before generalize to the capacitated setting. Namely: (i) Can one still eÔ¨Éciently stabilize instances via vertex-removal operations? (ii) Does the equivalence between existence of stable allocations for capacitated CMG and existence of stable solutions for capacitated NBG still hold? Our Results. In this paper we provide an answer to the above questions. We investigate the M -vertex-stabilizer problem and the vertex-stabilizer problem in the capacitated setting in Sects. 3 and 4, respectively. While for unit-capacity graphs both problems are eÔ¨Éciently solvable, we show that adding capacities makes the complexity status of the vertex-stabilizer problem diverge. In particular, we prove that the vertex-stabilizer problem is NP-complete, and no n1‚àíŒµ -approximation is possible, for any Œµ > 0, unless P=NP. Note that a trivial n-approximation algorithm can be easily developed. In contrast, we show that the M -vertex-stabilizer problem is still polynomialtime solvable in the capacitated setting. Our results here extend those of [15] for unit-capacity graphs, and builds upon an auxiliary construction of [9]. Finally, in Sect. 5 we show that the equivalence between stability of a graph, existence of a stable allocation for CMG and existence of a stable outcome for NBG does not extend in the capacitated setting. In particular, we provide an unstable graph which does attain a stable allocation for the capacitated CMG.1 1  
   
  It is stated in [8] (Theorem 2.3.9) that a stable allocation for capacitated CMG exists iÔ¨Ä G is stable, but our example shows this statement is not correct.  
   
  160  
   
  2  
   
  M. Gerstbrein et al.  
   
  Preliminaries and Notation  
   
  Problem DeÔ¨Ånition. A set S ‚äÜ V is called a vertex-stabilizer if G \ S is stable, where G \ S is the subgraph induced by the vertices V \ S. We say that a vertex-stabilizer S preserves a matching M of G if M is a matching in G \ S. We now formally deÔ¨Åne the stabilization problems considered in this paper. Vertex-stabilizer Problem: given G = (V, E) with edge weights w ‚àà RE ‚â•0 and vertex capacities c ‚àà ZV‚â•0 , Ô¨Ånd a vertex-stabilizer of minimum cardinality. M -vertex-stabilizer Problem: given G = (V, E) with edge weights w ‚àà RE ‚â•0 , V vertex capacities c ‚àà Z‚â•0 , and a maximum-weight c-matching M , Ô¨Ånd a vertexstabilizer of minimum cardinality among the ones preserving M . An instance of the vertex-stabilizer problem will be denoted as (G, w, c). An instance of the M -vertex-stabilizer problem will be denoted as [(G, w, c), M ]. We say that an instance is stable if G is stable. Without loss of generality, we can assume that cv is bounded by the degree of v ‚àà V . Notation. For a vertex v, we let Œ¥(v) be the set of edges of G incident into it, we let N (v) be the set of its neighbours, and N + (v) = N (v) ‚à™ {v}. For F ‚äÜ E, we denote by dF v the degree of v in G with respect to the edges in F . We deÔ¨Åne w(F ) := e‚ààF we . Given a c-matching M , we say that v ‚àà V is exposed if = 0, and covered if dM for feasible solutions x dM v v > 0. We also use these terms  of (2), called fractional c-matchings, e.g., v is exposed if e‚ààŒ¥(v) xe = 0. We let n := |V |, and  denote the symmetric diÔ¨Äerence operator. We denote a (uv-)walk W by listing its edges and endpoints sequentially, i.e., by W = (u; e1 , . . . , ek ; v). We deÔ¨Åne its inverse as W ‚àí1 = (v; ek , . . . , e1 ; u). Note that a path is a walk in which edges do not repeat, and internal vertices do not repeat. A cycle is a path which starts and ends at the same vertex. If we refer to the edge set of a walk W , we just write W . Note that this can be a multi-set. Duality and Augmenting Structures. The dual of (2) is given by   œÑfc (G) := min c y + 1 z : yu + yv + zuv ‚â• wuv ‚àÄuv ‚àà E, y ‚â• 0, z ‚â• 0 .  
   
  (3)  
   
  A solution (y, z) feasible for (3) is called a fractional vertex cover. By LP theory, we have ŒΩ c (G) ‚â§ ŒΩfc (G) = œÑfc (G). Definition 1. We say that a walk W is M -alternating (w.r.t. a matching M ) if it alternates edges in M and edges not in M . We say W is M -augmenting if it is M -alternating and w(W \ M ) > w(W ‚à© M ). An M -alternating uv-walk W M M ‚â§ cu and dW ‚â§ cv . is proper if dW u v Definition 2. Given an M -alternating walk W = (u; e1 , . . . , ek ; v) and an Œµ > 0, the Œµ-augmentation of W is the vector xM/W (Œµ) ‚àà RE given by  1 ‚àí Œ∫(e)Œµ if e ‚àà M, M/W xe (Œµ) = (4) Œ∫(e)Œµ if e ‚àà / M,  
   
  Stabilization of Capacitated Matching Games  
   
  161  
   
  where Œ∫(e) = |{i ‚àà [k] : ei = e}|. We say that W is feasible if there exists an Œµ > 0 such that the corresponding Œµ-augmentation of W is a fractional c-matching. Remark 1. A feasible M -alternating walk with distinct endpoints is proper. Definition 3. An odd cycle C = (v; e1 , . . . , ek ; v) is called an M -blossom if it is M -alternating such that either e1 and ek are both in M , or are both not in M . The vertex v is called the base of the blossom. Definition 4. An M -Ô¨Çower C ‚à™ P consists of an M -blossom C with base u and an M -alternating path P = (u; e1 , . . . , ek ; v) such that (P, C, P ‚àí1 ) is M alternating and feasible. The vertex v is called the root of the Ô¨Çower. The Ô¨Çower is M -augmenting if w(C \ M ) + 2w(P \ M ) > w(C ‚à© M ) + 2w(P ‚à© M ).  
   
  (5)  
   
  Definition 5. An M -bi-cycle C ‚à™ P ‚à™ D consists of two M -blossoms C and D with bases u and v, respectively, and an M -alternating path P = (u; e1 , . . . , ek ; v) such that (P, D, P ‚àí1 , C) is M -alternating. The bi-cycle is M -augmenting if w(C \ M ) + 2w(P \ M ) + w(D \ M ) > w(C ‚à© M ) + 2w(P ‚à© M ) + w(D ‚à© M ). (6) Note that, in the last two deÔ¨Ånitions, it may happen that P has no edges. Auxiliary Construction. We will use a construction given in [9], to transform an M -vertex-stabilizer instance [(G, w, c), M ] into another one ([(G , w , 1), M  ]) deÔ¨Åned on an auxiliary graph with unit capacities. Construction: [(G, w, c), M ] ‚Üí [(G , w , 1), M  ] 1. For each v ‚àà V , create the set Cv = {v1 , . . . , vcv } of cv copies of v, add Cv to V (G ), and initialize J(v) = {1, . . . , cv }. 2. For each uv ‚àà M , add a single edge ui vj to both E(G ) and M  with edgeweight wuv , where i ‚àà J(u) and j ‚àà J(v) are chosen arbitrarily. Remove i and j from J(u) and J(v), respectively. 3. For each edge uv ‚àà E \ M , add an edge ui vj to E(G ) with edge-weight wuv , for all ui ‚àà Cu and vj ‚àà Cv . See Fig. 1 for an example. In this Ô¨Ågure it is easy to see that the matching M  in G is not maximum, even though M is maximum in G.2 Remark 2. If [(G, w, c), M ] has auxiliary [(G , w , 1), M  ], and X ‚äÜ V is any set of vertices which avoids M , then (G \ X) = G \ X  , where X  = ‚à™v‚ààX Cv . We deÔ¨Åne a map Œ∑ to go back from the auxiliary graph G to the original graph G. SpeciÔ¨Åcally, if ui ‚àà V (G ) ‚à© Cu for some u ‚àà V , then Œ∑(ui ) := u, and if ui vj ‚àà E(G ) such that ui ‚àà Cu , vj ‚àà Cv for some u, v ‚àà V , then Œ∑(ui vj ) := uv. This extends in the obvious way to paths, cycles, walks, and so on. We will need the following theorem. 2  
   
  It was stated in [9, corollary 1] that M is maximum if and only if M  is maximum, but this example shows this to be false.  
   
  162  
   
  M. Gerstbrein et al.  
   
  y t  
   
  u  
   
  v  
   
  x  
   
  t1  
   
  a  
   
  b  
   
  c  
   
  (a) Original graph.  
   
  u1 b1  
   
  z a1  
   
  v1  
   
  b2  
   
  v2  
   
  x1  
   
  x2  
   
  y1  
   
  z1  
   
  c1  
   
  (b) Auxiliary graph.  
   
  Fig. 1. Example of the auxiliary construction on an instance [(G, w, c), M ]. Capacities are all 1 except for cv = cx = cb = 2. Weights are all 1 except for wbc = 0.5. The matching is displayed as bold edges.  
   
  Theorem 1. [(G, w, c), M ] is not stable if and only if the graph G in the auxiliary instance [(G , w , 1), M  ] contains at least one of the following: (i) an M  -augmenting Ô¨Çower; (ii) an M  -augmenting bi-cycle; (iii) a proper M  augmenting path; (iv) an M  -augmenting cycle. Proof. It was proven in [9, Theorem 2] that [(G, w, c), M ] is not stable if and only if [(G , w , 1), M  ] is not stable. We distinguish two scenarios for when the latter condition occurs. If M  is maximum-weight, then G contains an M  -augmenting Ô¨Çower or bi-cycle, see [15, Theorem 1]. If M  is not maximum-weight, G must contain a proper M  -augmenting path or cycle, by standard matching theory. We will refer to an augmenting structure of type (i) ‚àí (iv) in Theorem 1 as a basic augmenting structure. The next lemma follows from [15]. Lemma 1. Let [(G , w , 1), M  ] be an unstable instance of NBG. (a) For any M  -exposed vertex u, one can compute a feasible M  -augmenting walk starting at u of length at most 3 |V (G )|, or determine that none exists, in polynomial time. (b) A feasible M  -augmenting uv-walk contains a feasible M  -augmenting uvpath (proper if u = v), an M  -augmenting cycle, an M  -augmenting Ô¨Çower rooted at u or v, or an M  -augmenting bi-cycle. Furthermore, this augmenting structure can be computed in polynomial time. Proof. (a) When given a graph G , a matching M  , a vertex u, and an integer k, algorithm 3 in [15] computes a feasible M  -augmenting uv-walk of length at most k, or determines none exist, for all v ‚àà V (G ). Correctness is shown in Lemma 7 and 8 in [15]. The algorithm is polynomial time in k, |V (G )|, and |E(G )|. We use this algorithm and select an arbitrary v for which a uv-walk is returned, or determine that no such walk starting at u exists. Since we set k = 3 |V (G )|, this procedure terminates in polynomial time. (b) Lemma 9 in [15] states that a feasible M  -augmenting uv-walk contains a feasible M  -augmenting uv-path, an M  -augmenting cycle, an M  -augmenting  
   
  Stabilization of Capacitated Matching Games  
   
  163  
   
  Ô¨Çower rooted at u or v, or an M  -augmenting bi-cycle. By remark 1 the path is proper if u = v. Lemma 9 in [15] is proven in a constructive way, hence it also gives a way to compute the augmenting structure in polynomial time. The following easy lemma will be useful. Lemma 2. Given [(G, w, c), M ] and auxiliary [(G , w , 1), M  ], let P be a feasible M  -augmenting walk. Then, Œ∑(P ) is a feasible M -augmenting walk. Proof. Let e1 = uv and e2 = vw be two consecutive edges on P . Then Œ∑(e1 ) and Œ∑(e2 ) are the corresponding edges on Œ∑(P ), and they are both incident with Œ∑(v). Hence, Œ∑(P ) is a walk. For any edge e on P , we have e ‚àà M  if and only if Œ∑(e) ‚àà M . In addition, we = wŒ∑(e) . So, Œ∑(P ) is an M -augmenting walk. Suppose P = (u; e1 , . . . , ek ; v). Feasibility of P means that either e1 ‚àà M  , or u is M  exposed. Likewise for ek and v. It follows that either Œ∑(e1 ) ‚àà M , or Œ∑(u) is M -unsaturated. Likewise for Œ∑(ek ) and Œ∑(v). This means Œ∑(P ) is feasible. The next theorem is standard. Theorem 2. [(G, w, c), M ] is stable if and only if G does not contain a feasible M -augmenting walk. Proof. (‚áí) Assume there exists a feasible M -augmenting walk W . Since W is augmenting, w(W \ M ) > w(W ‚à© M ), and since W is feasible, xM/W (Œµ) is a fractional c-matching for some Ô¨Åxed Œµ > 0. Together they imply ŒΩfc (G) ‚â• w xM/W (Œµ) = w(M ) ‚àí Œµw(W ‚à© M ) + Œµw(W \ M ) > w(M ),  
   
  (7)  
   
  i.e., the instance [(G, w, c), M ] is not stable. (‚áê) Assume the instance is not stable. Then by Theorem 1, the graph G from the auxiliary [(G , w , 1), M  ] contains a basic augmenting structure, which clearly is a feasible M  -augmenting walk P . Then Œ∑(P ) is a feasible M augmenting walk, by Lemma 2.  
   
  3  
   
  M -vertex-stabilizer  
   
  The goal of this section is to prove the following theorem. Theorem 3. The M -vertex-stabilizer problem on weighted, capacitated graphs can be solved in polynomial time. Overview of the Strategy. A natural strategy would be to Ô¨Årst apply the auxiliary construction described in Sect. 2 to reduce to unit-capacity instances, and then apply the algorithm proposed in [15] which solves the problem exactly. However, there is a critical issue with this strategy. Namely, the auxiliary construction applied to unstable instances does not always preserve maximality of the corresponding matchings, as shown in Fig. 1. In that example, the matching M  is not maximum in G . The algorithm of [15], if applied to an instance where  
   
  164  
   
  M. Gerstbrein et al.  
   
  the given matching is not maximum, is not guaranteed to Ô¨Ånd an optimal solution, but only a 2-approximate one (see Theorem 12 in [15]). In addition, since the auxiliary construction splits a vertex into multiple ones, we may even get infeasible solutions. As a concrete example of this, the algorithm of [15] applied to the instance of Fig. 1b will include b2 in its proposed solution. Mapping this solution to our capacitated instance would imply to remove b, which is clearly not allowed as b is M -covered. To overtake this issue, we do not apply the algorithm of [15] as a black-box, but use parts of it (highlighted in Lemma 1) in a careful way. In particular, we use it to compute a sequence of feasible augmenting walks in G . We actually show that the walks in G which might create the issue described before when mapped backed to G, are the walks in which at least one edge of G is traversed more than once in opposite directions, and that have two distinct endpoints. When this happens, we prove that we can modify the walk and get one where the endpoints coincide, which will still be feasible and augmenting. In this latter case, we can then either correctly identify a vertex to remove (the unique endpoint), or determine that the instance cannot be stabilized. A More Detailed Description. We start by deÔ¨Åning the operation of traceback, which we will use to modify the feasible augmenting walks, when needed. Definition 6. Given [(G, w, c), M ] and an M -alternating walk P = (u; e1 , . . . , ek ; v) which repeats an edge in opposite directions, let t be the least index such that et = es for some s < t, and es and et are traversed in opposite directions by P . Then the u-traceback and v-traceback of P are deÔ¨Åned as the walks tb(P, u) = (e1 , . . . , et , es‚àí1 , es‚àí2 , . . . , e1 ) and tb(P, v) = (ek , ek‚àí1 . . . , es , et+1 , et+2 , . . . , ek ). The next lemma explains how to use the traceback operation. Due to space constraint, the proof is deferred to the full version of this extended abstract [10]. Lemma 3. Given [(G, w, c), M ] and auxiliary [(G , w , 1), M  ], let P  = (ui ; e1 , . . . , ek ; vj ) be a proper M  -augmenting path such that both ui and vj are M  exposed and Œ∑(ui ) = Œ∑(vj ). Then tb(Œ∑(P  ), Œ∑(ui )) and tb(Œ∑(P  ), Œ∑(vj )) are welldeÔ¨Åned, feasible M -alternating walks, and at least one of them is M -augmenting. Proof (Proof of Theorem 3). Let [(G, w, c), M ] be the input for the M -vertexstabilizer problem, with auxiliary [(G , w , 1), M  ]. Algorithm 1 iteratively considers an M  -exposed vertex ui , and computes a feasible M  -augmenting walk U starting at ui , if one exists. Lemma 2 implies that Œ∑(U ) is a feasible M augmenting walk in G. Theorem 2 implies that we need to remove at least one vertex of the walk Œ∑(U ) to stabilize the graph. Note that every vertex a = ui , vj of U is M  -covered, and hence, Œ∑(a) is M -covered. Therefore, the only vertices we can potentially remove are Œ∑(ui ) or Œ∑(vj ). Hence, if both Œ∑(ui ) and Œ∑(vj ) are M -covered, the graph cannot be stabilized and Algorithm 1 checks this in line 9. If only one among Œ∑(ui ) and Œ∑(vj ) is M -covered, then necessarily we have to remove the M -exposed vertex among the two. Algorithm 1 checks this in  
   
  Stabilization of Capacitated Matching Games  
   
  165  
   
  Algorithm 1: Ô¨Ånding an M -vertex-stabilizer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  
   
  32 33 34 35  
   
  input: [(G, w, c), M ] compute the auxiliary [(G , w , 1), M  ] initialize S ‚Üê ‚àÖ, L ‚Üê M  -exposed vertices while L = ‚àÖ do select ui ‚àà L and compute a feasible M  -augmenting walk starting at ui using lemma 1(a) if no such walk exists then L ‚Üê L \ {ui } else consider the computed feasible M  -augmenting ui vj -walk if both Œ∑(ui ) and Œ∑(vj ) are M -covered then return infeasible else if Œ∑(ui ) is M -covered and Œ∑(vj ) is not then S ‚Üê S ‚à™ Œ∑(vj ), G ‚Üê G \ Œ∑(vj ), G ‚Üê G \ CŒ∑(vj ) , L ‚Üê L \ CŒ∑(vj ) else if Œ∑(vj ) is M -covered and Œ∑(ui ) is not then S ‚Üê S ‚à™ Œ∑(ui ), G ‚Üê G \ Œ∑(ui ), G ‚Üê G \ CŒ∑(ui ) , L ‚Üê L \ CŒ∑(ui ) else if Œ∑(ui ) = Œ∑(vj ) then S ‚Üê S ‚à™ Œ∑(ui ), G ‚Üê G \ Œ∑(ui ), G ‚Üê G \ CŒ∑(ui ) , L ‚Üê L \ CŒ∑(ui ) else Ô¨Ånd a basic M  -augmenting structure W contained in the ui vj -walk using lemma 1(b) if W is an M  -augmenting cycle or bi-cycle then return infeasible if W is an M  -augmenting flower rooted at ui then S ‚Üê S ‚à™ Œ∑(ui ), G ‚Üê G \ Œ∑(ui ), G ‚Üê G \ CŒ∑(ui ) , L ‚Üê L \ CŒ∑(ui ) if W is an M  -augmenting flower rooted at vj then S ‚Üê S ‚à™ Œ∑(vj ), G ‚Üê G \ Œ∑(vj ), G ‚Üê G \ CŒ∑(vj ) , L ‚Üê L \ CŒ∑(vj ) if W is a proper M  -augmenting ui vj -path then compute tb(Œ∑(W ), Œ∑(ui )) and tb(Œ∑(W ), Œ∑(vj )) if tb(Œ∑(W ), Œ∑(ui )) is M -augmenting then S ‚Üê S ‚à™ Œ∑(ui ), G ‚Üê G \ Œ∑(ui ), G ‚Üê G \ CŒ∑(ui ) , L ‚Üê L \ CŒ∑(ui ) if tb(Œ∑(W ), Œ∑(vj )) is M -augmenting then S ‚Üê S ‚à™ Œ∑(vj ), G ‚Üê G \ Œ∑(vj ), G ‚Üê G \ CŒ∑(vj ) , L ‚Üê L \ CŒ∑(vj ) if w(M ) < ŒΩfc (G) then return infeasible else return S  
   
  166  
   
  M. Gerstbrein et al.  
   
  line 11 and 13. Note that, by remark 2, instead of computing a new auxiliary for the modiÔ¨Åed G, we can just remove CŒ∑(ui ) (resp. CŒ∑(vj ) ) from G . Similarly, if Œ∑(ui ) = Œ∑(vj ) and Œ∑(ui ) is M -exposed, we necessarily have to remove Œ∑(ui ). Algorithm 1 checks this in line 16. If instead Œ∑(ui ) = Œ∑(vj ), and both are M  -exposed, we apply Lemma 1(b) to Ô¨Ånd a basic augmenting structure W contained in U . Once again, we know by Lemma 2 and Theorem 2 that we need to remove a vertex in Œ∑(W ). In case W is a cycle or bi-cycle, all vertices of Œ∑(W ) are M -covered so the graph cannot be stabilized and Algorithm 1 checks this in line 20. In case W is a M  -augmenting Ô¨Çower with base ui or vj , Algorithm 1 accordingly removes Œ∑(ui ) or Œ∑(vj ) as all other vertices in Œ∑(W ) are M -covered, in line 23 and 25. Finally, if W is a proper (because Œ∑(ui ) = Œ∑(vj )) M  -augmenting path, by Lemma 3 we know that we can Ô¨Ånd a feasible M -augmenting walk, where the only M -exposed vertex is either Œ∑(ui ) or Œ∑(vj ). Once again, this implies that this vertex must be removed. Algorithm 1 does so in lines 29 and 31. From the discussion so far, it follows that when we exit the while loop each vertex in S is a necessary vertex to be removed from G, in order to stabilize the instance. We now argue that either removing all vertices in S is also suÔ¨Écient, or G cannot be stabilized. Suppose that G \ S is not stable. Theorem 1 implies that (G \ S) contains a basic augmenting structure Q. Note that Q cannot be an M  -augmenting Ô¨Çower with exposed root, or a proper M  -augmenting path with at least one exposed endpoint. To see this, observe that a Ô¨Çower and path are feasible M  -augmenting walks of length at most 3 |V (G )| and |V (G )|, respectively. Hence, they would have been found by Algorithm 1 in line 4, contradicting that Q exists in (G\S) . It follows that Q is a basic augmenting structure where all vertices are M  -covered. By Lemma 2 Œ∑(Q) is a feasible M -augmenting walk where all vertices are M -covered. This implies that G cannot be stabilized. Furthermore, using the Œµ-augmentation of Œ∑(Q) we can obtain a fractional cmatching whose value is strictly greater than w(M ). Hence, w(M ) < ŒΩfc (G \ S). Algorithm 1 correctly determines this in line 32. This proves correctness of our algorithm. Finally, we argue about the running time of the algorithm. Note that each operation that the algorithm performs can be done in polynomial time. Furthermore, after each iteration of the while loop, we either determine that the instance cannot be stabilized, or remove a vertex from G. Therefore, the while loop can be executed at most n times. The result follows. We close this section with a remark. The authors in [15] have also considered the following problem: given a weighted graph G and a (non necessarily maximum-weight) matching M , Ô¨Ånd a minimum-cardinality S ‚äÜ V such that G \ S is stable, and M is a maximum-weight matching in G \ S. This is a generalization of our deÔ¨Ånition of the M -vertex-stabilizer problem, which essentially allows M to be not maximum-weight.3 The authors show that this problem is  
   
  3  
   
  In fact, this is the way the M -vertex-stabilizer problem is deÔ¨Åned in [15]. We instead use the original deÔ¨Ånition in [1, 6] which assumes M to be maximum.  
   
  Stabilization of Capacitated Matching Games  
   
  167  
   
  NP-hard, but admits a 2-approximation algorithm (we mentioned this in the strategy overview), which is best possible assuming Unique Game Conjecture. With a minor modiÔ¨Åcation of Algorithm 1, we can generalize this result to the capacitated setting. We state here the result, and refer to the full version of this extended abstract paper for details [10]. Theorem 4. Given a weighted, capacitated graph G = (V, E) and a c-matching M , the problem of computing a minimum-cardinality S ‚äÜ V such that G \ S is stable, and M is a maximum-weight c-matching in G \ S, admits an eÔ¨Écient 2-approximation algorithm.  
   
  4  
   
  Vertex-Stabilizer  
   
  The goal of this section is to prove the following theorem. Theorem 5. The vertex-stabilizer problem on capacitated graphs is NPcomplete, even if all edges have unit-weight. Furthermore, no eÔ¨Écient n1‚àíŒµ approximation exists for any Œµ > 0, unless P = NP. Note that, given an unstable instance (G, w, c), removing all vertices (but two) trivially yields a stable graph. This gives a (trivial) n-approximation algorithm for the vertex-stabilizer problem. The theorem above essentially implies that one cannot hope for a much better approximation. To prove it, we will use: Minimum Independent Dominating Set (MIDS) Problem. Given a graph G = (V, E), compute a minimum-cardinality subset S ‚äÜ V that is independent (for all uv ‚àà E at most one of u and v is in S) and dominating (for all v ‚àà V at least one u ‚àà N + (v) is in S). There is no eÔ¨Écient n1‚àíŒµ -approximation for any Œµ > 0 for the MIDS problem, unless P = NP. [12, corollary 3] Proof (Proof of Theorem 5). The decision variant of the problem asks to Ô¨Ånd a vertex-stabilizer of size at most k. This problem is in NP, since if a vertex set S is given, it can be veriÔ¨Åed in polynomial time if |S| ‚â§ k and if ŒΩ c (G \ S) = ŒΩfc (G \ S). We prove the NP-hardness and the inapproximability result by giving an approximation-preserving reduction from the MIDS problem. Let G = (V, E) be an instance of the MIDS problem. For v ‚àà V , we deÔ¨Åne the gadget Œìv by V (Œìv ) = N + (v) ‚à™ {v1 , v2 , v3 , v4 } ,   E(Œìv ) = uv1 : u ‚àà N + (v) ‚à™ {v1 v2 , v2 v3 , v3 v4 , v2 v4 } . i For e = uv ‚àà E and i ‚àà {1, . . . , n}, we deÔ¨Åne the gadget Œìuv by   i V (Œìuv ) = u, v, ei1 , ei2 , ei3 , ei4 , ei5 ,   i E(Œìuv ) = uei1 , vei1 , ei1 ei2 , ei1 ei3 , ei3 ei4 , ei4 ei5 , ei3 ei5 .  
   
  (8) (9)  
   
  (10) (11)  
   
  168  
   
  M. Gerstbrein et al. v3  
   
  v4  
   
  ei4  
   
  v2  
   
  ei2  
   
  v1 v  
   
  ¬∑¬∑¬∑  
   
  ei5 ei3 ei1  
   
  N (v)  
   
  (a) Gadget Œìv .  
   
  u  
   
  v  
   
  i (b) Gadget Œìuv .  
   
  Fig. 2. Examples of gadgets.  
   
  See Fig. 2 for an example of these gadgets. Now let G be deÔ¨Åned as the union i , such that vertices from V overlap. We set the capacity as of all Œìv and all Œìuv E(G ) follows: cv = dv for all v ‚àà V , cv1 = dE v + 1 for all v ‚àà V , cei1 = cei3 = 2 i i i for e1 , e3 ‚àà V (Œìuv ) for all e = uv ‚àà E and i ‚àà {1, . . . , n}, and cv = 1 for all remaining v ‚àà V (G ). All edges are set to have unit-weight. The key point is: Claim. G has an independent dominating set of size at most k if and only if (G , 1, c) has a vertex-stabilizer of size at most k. Proof. (‚áí) Let S be an independent dominating set of G of size k. The vertices in S naturally correspond with vertices in G . We show that S is a vertex-stabilizer of (G , 1, c). We deÔ¨Åne a c-matching M and fractional vertex cover (y, z) on G \ S as follows. First, set yv = 0 for all v ‚àà V \ S. Next, for all v ‚àà V , consider Œìv . Add {uv1 : u ‚àà N + (v) \ S} ‚à™ {v1 v2 , v3 v4 } to M . Note that at least one vertex from N + (v) is in S, since S is dominating. Set yv1 = 0, yv2 = 1, yv3 = yv4 = 0.5, ze = 1 for all e ‚àà {uv1 : u ‚àà N + (v) \ S} and ze = 0 for the remaining edges in the gadget. i . Since S is Finally, for all e = uv ‚àà E and i ‚àà {1, . . . , n}, consider Œìuv independent, at most one of u and v is in S. If neither are in S, add both uei1 and vei1 to M . If one of them is in S, without loss of generality let it be u, then add vei1 and ei1 ei2 to M . Furthermore, add ei3 ei4 and ei3 ei5 to M . Set yei1 = 1, yei2 = 0, yei3 = yei4 = yei5 = 0.5, and zf = 0 for all edges f in the gadget. Let x be the indicator vector of M . One can verify that x and (y, z) satisfy the complementary slackness conditions for ŒΩfc (G \ S) and œÑfc (G \ S). Since x is integral, this implies that G \ S is stable. (‚áê) Let S be a vertex-stabilizer of (G , 1, c) of size k. We show that: (i) S contains at least one vertex of each gadget Œìv ; (ii) without loss of generality, one can assume that at most one of u and v is in S for each edge uv ‚àà E. (i) Suppose for the sake of contradiction that there is some v ‚àà V such that S contains no vertices of Œìv . Since G \ S is stable, there is a maximum-cardinality  
   
  Stabilization of Capacitated Matching Games  
   
  fractional c-matching x‚àó , that is ‚éß ‚àó x ‚é™ ‚é™ ‚é™ e ‚é® 1 xe = ‚é™ 0 ‚é™ ‚é™ ‚é© 0.5  
   
  169  
   
  integral. DeÔ¨Åne for each e ‚àà E(G \ S) if if if if  
   
  e ‚àà E(G \ S) \ E[Œìv ], e ‚àà {uv1 : u ‚àà N + (v)} , e = v1 v2 , e ‚àà {v2 v3 , v3 v4 , v2 v4 } .  
   
  (12)  
   
  Note that x is a fractional in G \ S, since x‚àó is. However,  c-matching    ‚àó ‚àó ‚àó e‚ààE[Œìv ] xe = dv + 2.5 > e‚ààE[Œìu ] xe , since x is integral. Hence, 1 x > 1 x , ‚àó contradicting the optimality of x . (ii) Suppose there is some e = uv ‚àà E such that S contains both u and v. All i are then components in G \ S. If u and v are the only vertices in S gadgets Œìuv i , then a maximum-cardinality fractional c-matching in from some component Œìuv this components is given by xei1 ei2 = xei1 ei3 = 1 and xei3 ei4 = xei4 ei5 = xei3 ei5 = 0.5. Which means this component is not stable, and thus G \ S is not stable, a i that is contradiction. Hence, S must contain at least one vertex of each Œìuv neither u nor v. Consequently, k = |S| ‚â• n + 2. Since G has only n vertices, it obviously has an independent dominating set of size at most n, and hence of size at most k. Such a set can for example be obtained by a greedy approach. Hence, for the remainder of the proof we can assume that at most one of u and v is in S for each uv ‚àà E. We now create a set S  ‚äÜ V from S, that is an independent dominating set of G of size at most k, as follows. Iterate over v ‚àà V . Let Sv = S ‚à© V (Œìv ). Note that Sv = ‚àÖ by (i). DeÔ¨Åne  (Sv ‚à™ S  ) ‚à© N + (v) if this is nonempty,  Sv = (13) v otherwise. Set S  = S  ‚à™ Sv , and repeat for the next vertex. Clearly, all Sv ‚Äôs are nonempty, which means that S  contains at least one vertex from N + (v) for all v ‚àà V , which means S  is dominating. Suppose for the sake of contradiction that S  contains both u and v for some edge uv ‚àà E. We know S did not contain both of them, by (ii). If S contained exactly one of them, without loss of generality let it be u. Then, when v is considered by the iterative process, (Sv ‚à™ S  ) ‚à© N + (v) contains u, but not v. In particular, this means that we did not add v to Sv and consequently also not to S  , a contradiction. If S contained neither of them, then because we do the process iteratively, one of them will be added Ô¨Årst to S  . Without loss of generality let it be u. Then again, when v is considered by the iterative process, (Sv ‚à™ S  ) ‚à© N + (v) contains u but not v, so we reach a contradiction in the same way. In conclusion, S  is independent. For all v ‚àà V , before we added Sv to S  , we had |Sv \ S  | ‚â§ |Sv |. Conse quently, |S  | ‚â§ ‚à™v‚ààV |Sv | ‚â§ |S| = k. By this claim, any minimum-cardinality vertex-stabilizer of (G , 1, c) is of the same size as any minimum independent dominating set of G. Further, any  
   
  170  
   
  M. Gerstbrein et al.  
   
  eÔ¨Écient Œ±-approximation algorithm for the vertex-stabilizer problem translates into an eÔ¨Écient Œ±-approximation algorithm for the MIDS problem. Hence, the result follows from the inapproximability of the MIDS problem.  
   
  5  
   
  Capacitated Cooperative Matching Games  
   
  Cooperative matching games in unit-capacity graphs, deÔ¨Åned in the introduction, extend quite easily to capacitated graphs, by replacing each ŒΩ with ŒΩ c . In unitcapacity graphs G the following statements are equivalent [7,14]: (i) G is stable, (ii) there exists an allocation in the core of the CMG on G, (iii) there exists a stable outcome for the NBG on G. We here note that the equivalence does not extend to capacitated graphs. In particular, as mentioned in the introduction, we still have (i) ‚áê‚áí (iii) proven in [2, corollary 3.3]. The implication (i) =‚áí (ii) still holds, and follows  
   
  from [2, lemma 3.4]4 . However, the graph G given in Fig. 3 shows that (ii) =‚áí (i) (and hence (ii) =‚áí  
   
  (iii)). Assuming all the edges of G in Fig. 3 have unit weight, it is quite easy to see that ŒΩ c (G) = 3 and ŒΩfc (G) = 3.5, thus G is not stable. One can check that y = (1, 1, 1, 0) is in the core. 2  
   
  2  
   
  1  
   
  1  
   
  2  
   
  1  
   
  1  
   
  0  
   
  Fig. 3. On the left: the graph G where the values close to the vertices indicate the capacities. Bold edges indicate a maximum c-matching. On the right: the graph G where the values close to the vertices indicate the allocation y. A maximum fractional c-matching is given by xe = 12 for dashed edges, xe = 1 otherwise. Acknowledgements. The second and third authors are supported by the NWO VIDI grant VI.Vidi.193.087. The second author thanks the 2021 HausdorÔ¨Ä Research Institute for Mathematics Program Discrete Optimization, during which part of this work was developed.  
   
  References 1. Ahmadian, S., Hosseinzadeh, H., Sanit√†, L.: Stabilizing network bargaining games by blocking players. Math. Program. 172, 249‚Äì275 (2018) 2. Bateni, M., Hajiaghayi, M., Immorlica, N., Mahini, H.: The cooperative game theory foundations of network bargaining games (2010) 4  
   
  [2] assumes that the graph is bipartite, but bipartiteness is not needed in their proof.  
   
  Stabilization of Capacitated Matching Games  
   
  171  
   
  3. Bir√≥, P., Kern, W., Paulusma, D.: On solution concepts for matching games. In: Kratochv√≠l, J., Li, A., Fiala, J., Kolman, P. (eds.) Theory Appl. Models Comput., pp. 117‚Äì127. Springer, Berlin Heidelberg, Berlin, Heidelberg (2010) 4. Bock, A., Chandrasekaran, K., K√∂nemann, J., Peis, B., Sanit√†, L.: Finding small stabilizers for unstable graphs. Math. Program. 154, 173‚Äì196 (2015) 5. Chandrasekaran, K.: Graph stabilization: a survey. In: Fukunaga, T., Kawarabayashi, K. (eds.) Combinatorial Optimization and Graph Algorithms, pp. 21‚Äì41. Springer, Singapore (2017). https://doi.org/10.1007/978-981-10-6147-9_2 6. Chandrasekaran, K., Gottschalk, C., K√∂nemann, J., Peis, B., Schmand, D., Wierz, A.: Additive stabilizers for unstable graphs. Discret. Optim. 31, 56‚Äì78 (2019) 7. Deng, X., Ibaraki, T., Nagamochi, H.: Algorithmic aspects of the core of combinatorial optimization games. Math. Oper. Res. 24(3), 751‚Äì766 (1999) 8. Farczadi, L.: Matchings and games on networks, Ph. D. thesis, University of Waterloo (2015) 9. Farczadi, L., Georgiou, K., K√∂nemann, J.: Network bargaining with general capacities. arXiv preprint arXiv:1306.4302 (2013) 10. Gerstbrein, M., Sanit√†, L., Verberk, L.: Stabilization of capacitated matching games. arXiv preprint (2022) 11. Gottschalk, C.: Personal communication (2018) 12. Halld√≥rsson, M.M.: Approximating the minimum maximal independence number. Inf. Process. Lett. 46(4), 169‚Äì172 (1993) 13. Ito, T., Kakimura, N., Kamiyama, N., Kobayashi, Y., Okamoto, Y.: EÔ¨Écient stabilization of cooperative matching games. Theoret. Comput. Sci. 677, 69‚Äì82 (2017) 14. Kleinberg, J.M., Tardos, √â.: Balanced outcomes in social exchange networks. In: Proceedings of the 40th STOC, pp. 295‚Äì304 (2008) 15. Koh, Z.K., Sanit√†, L.: Stabilizing weighted graphs. Math. Oper. Res. 45(4), 1318‚Äì 1341 (2020) 16. K√∂nemann, J., Larson, K., Steiner, D.: Network bargaining: using approximate blocking sets to stabilize unstable instances. In: Serna, M. (ed.) SAGT 2012. LNCS, pp. 216‚Äì226. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3642-33996-7_19 17. Nash, J.F.: The bargaining problem. Econometrica 18, 155‚Äì162 (1950) 18. Shapley, L., Shubik, M.: The assignment game i: The core. Internat. J. Game Theory 1(1), 111‚Äì130 (1971)  
   
  Designing Optimization Problems with Diverse Solutions Oussama Hanguir1(B) , Will Ma2 , and Christopher Thomas Ryan3 1  
   
  2  
   
  Industrial Engineering and Operations Research, Columbia University, New York, NY 10027, USA [email protected]  Graduate School of Business, Columbia University, New York, NY 10027, USA [email protected]  3 UBC Sauder School of Business, University of British Columbia, Vancouver, BC V6T 1Z2, Canada [email protected]  Abstract. We consider the problem of designing a linear program that has diverse solutions as the right-hand side varies. This problem arises in video game settings where designers aim to have players use diÔ¨Äerent ‚Äúweapons‚Äù or ‚Äútactics‚Äù as they progress. We model this design question as a choice over the constraint matrix A and cost vector c to maximize the number of possible supports of unique optimal solutions (what we call ‚Äúloadouts‚Äù) of Linear Programs max{c x | Ax ‚â§ b, x ‚â• 0} with nonnegative data considered over all resource vectors b. We provide an upper bound on the optimal number of loadouts and provide a family of constructions that have an asymptotically optimal number of loadouts. The upper bound is based on a connection between our problem and the study of triangulations of point sets arising from polyhedral combinatorics, and speciÔ¨Åcally the combinatorics of the cyclic polytope. Our asymptotically optimal construction also draws inspiration from the properties of the cyclic polytope. Keywords: linear programming  
   
  1  
   
  ¬∑ triangulations ¬∑ diversity  
   
  Introduction  
   
  In this paper, we formulate the problem of designing linear programs that allow for diversity in their optimal solutions. This setting is motivated by video games, in particular, the design of competitive games where players optimize their strategies to improve their in-game status. For such games, a desideratum for game designers is for optimizing players to play diÔ¨Äerent strategies at diÔ¨Äerent stages of the game. Informally, we interpret the player‚Äôs problem as solving a Linear Program of the form max{c x | Ax ‚â§ b, x ‚â• 0}. Players at diÔ¨Äerent stages of the game have diÔ¨Äerent resource vectors b. The columns of A correspond to the tools that the player can use in the game. We call a subset of these tools (represented by subsets of the columns of A) a loadout (which literally means the equipment carried into battle by a soldier), if they correspond to the support c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 172‚Äì186, 2023. https://doi.org/10.1007/978-3-031-32726-1_13  
   
  Designing Optimization Problems with Diverse Solutions  
   
  173  
   
  of an optimal solution x‚àó to the linear program max{c x | Ax ‚â§ b, x ‚â• 0} for some resource vector b (In fact, we require x‚àó to be the unique optimal solution of this linear program, for reasons that will become clear later). The support of a vector corresponds to a selection of the available tools, forming a strategy for how the player approaches the game given available resources. We assume that the game designer is able to choose A and c. We refer to this choice as the design of the game. We measure the diversity of a design as the number of possible loadouts that arise as the resource vector b changes. For a Ô¨Åxed design (A, c) and resource vector b, players solve the linear program (1) LP (A, c, b) : max{c x | Ax ‚â§ b, x ‚â• 0}, where c, A, and b all have nonnegative data. If x is the optimal solution of LP (A, c, b), we deÔ¨Åne the support of x as supp(x)  {i ‚àà {1, . . . , n} | xi > 0}. If x is the unique optimal solution of LP (A, c, b), then we call supp(x) an optimal loadout (or simply, a loadout) of design (A, c). For Ô¨Åxed n and m, the loadout maximization problem is to choose c and A that maximize the total number of loadouts of the design (A, c). That is, the goal is to design beneÔ¨Åts for each tool (the vector c) and limitations on investing in tools (the matrix A) so that the linear programs LP (A, c, b) have as many possible supports of unique optimal solutions as possible, as b varies in Rm . Our Contributions: Our Ô¨Årst contribution consists in introducing the loadout maximization problem and establishing a link between the loadout maximization problem and the theory of polyhedral subdivisions and triangulations. In particular, for a Ô¨Åxed design (A, c), the theory of triangulations oÔ¨Äers a nice decomposition of the cone generated by the columns of the constraint matrix A. This decomposition depends on the objective vector c. We show that for a Ô¨Åxed design (A, c), the loadouts can be seen as elements of this decomposition. This allows us to use a set of powerful tools from the theory of triangulations to prove structural results on the loadouts of a design. Our second contribution is to show a non-trivial upper bound on the number of loadouts of any design. The upper bound involves an interesting connection to the faces of the so-called cyclic polytope, a compelling object central to the theory of polyhedral combinatorics. We also show that this upper bound holds when the constraints of the linear program are equality constraints. The third contribution of this paper is to present a construction of a design (A, c) with a number of loadouts that asymptotically matches the above upper bound. Furthermore, for cases with few constraints, we present optimal constructions that exactly match the upper bound. Due to space constraints, we defer the proofs to the full version1 . Related Work. Our work is closely related to parametric linear programming, which is the study of how optimal properties depend on parameterizations of the data. The study of parametric linear programming dates back to the work of [12,14], [19], and [18] in the 1950s and 1960s. In parametric programming, 1  
   
  Full version: https://arxiv.org/abs/2106.11538.  
   
  174  
   
  O. Hanguir et al.  
   
  the objective is to understand the dependence of optimal solutions on one or more parameters; that is, on the entries of A, b, and c. Our work is novel in the sense that the objective is to understand the structure of the supports of optimal solutions by Ô¨Åxing A and c and having b vary in Rm ‚â•0 . To the best of our knowledge, this question has not previously been studied in the literature. There have been several studies on the interface of optimization and video games. [8,15,17]. Guo et al. [7] study the impact of selling virtual currency on players‚Äô gameplay behavior, game provider‚Äôs strategies, and social welfare. Another signiÔ¨Åcant research direction concentrates on studying ‚Äúloot boxes‚Äù in video games. Chen et al. [2] study the design and pricing of loot boxes, while Ryan et al. [13] study the pricing and deployment of enhancements that increase the player‚Äôs chance of completing the game. Chen et al. [3] and Huang et al. [9] study the problem of in-game matchmaking to maximize a player‚Äôs engagement in a video game.  
   
  2  
   
  Statement of Main Results  
   
  In this section, we state our main results. To make these statements precise, we require some preliminary deÔ¨Ånitions. Let [k] denote the set {1, . . . , k} for any positive integer k. Using this notation, we can deÔ¨Åne the support of x ‚àà Rn‚â•0 as supp(x) = {j ‚àà [n] | xj > 0}. For a matrix A ‚àà Rm√ón ‚â•0 , the (i, j)th entry is denoted aij for i ‚àà [n] and j ‚àà [m], the jth column is denoted Aj for j ‚àà [n], and the ith row is denoted ai (where ai is a column vector) for i ‚àà [m]. For a m  column vector m y ‚àà R , y Aj denote the scalar product of y and column Aj , i.e.,  y Aj = i=1 yi ai,j . Recall the deÔ¨Ånition of the linear program LP (A, b, c) in (1). As mentioned in the introduction, we are interested in the unique optimal solutions of the design (A, c). For simplicity, we simply call these the loadouts of design (A, c); that is, L ‚äÜ [n] is a loadout of design (A, c) if there exists a nonnegative resource ‚àó vector b ‚àà Rm ‚â•0 such that LP (A, c, b) has a unique optimal solution x with supp(x‚àó ) = L. We say that loadout L is supported by resource vector b. If |L| = k then we say L is a k-loadout. Given a design (A, c) and an integer k ‚àà [m], let Lk (A, c) denote the set of all k-loadouts of design (A, c). The set of all loadouts of any size is L(A, c)  ‚à™nk=1 Lk (A, c). Using this notation, we can restate the loadout optimization problem. Given dimensions n and m and integer k ‚â§ n, the k-loadout optimization problem is max{|Lk (A, c)| | A ‚àà Rm√ón , c ‚àà Rn , A and c are nonnegative}.  
   
  (Lk )  
   
  We can assume without loss of generality that the linear programs LP (A, c, b) are bounded and thus possess an optimal solution because otherwise there is no optimal solution and, therefore, no loadout. Given that a loadout corresponds to the support of a unique solution of a linear program, any optimal solution with support size greater than m cannot be unique. Therefore, the number of k-loadouts when k > m is always equal to zero. This leads us to consider the optimization problems (Lk ) only for k ‚àà  
   
  Designing Optimization Problems with Diverse Solutions  
   
  175  
   
  {1, . . . , min(m, n)}. For convenience, we will avoid the trivial case of k = 1 where the optimal number of loadouts is min(m, n). A Ô¨Ånal case we eliminate immediately is when min(m, n) = n, i.e. m ‚â• n. In this case, a trivial design is optimal. By setting A = In to be the identity matrix of  size n, and c = (1, . . . , 1), we ensure that for k ‚àà [1, n], every one of the nk subsets is a loadout. In summary, we proceed without loss under the assumption that n > m ‚â• k ‚â• 2. 2.1  
   
  The Cyclic Polytope  
   
  All of our bounds are intimately related to the number of faces on the cyclic polytope, which is formally deÔ¨Åned in Sect. 3. A remarkable aspect of the cyclic polytope is that for n > m ‚â• 2, the cyclic polytope C(n, m) simultaneously maximizes the number of k-dimensional faces for all k = 0, . . . , m ‚àí 1 among mdimensional polytopes over n vertices, a property known as McMullen‚Äôs Upper Bound Theorem [11]. The number of k-dimensional faces on C(n, m) is given by the formula   m/2    n‚àím+‚àí1 fk (C(n, m)) = m‚àík‚àí1  =0    m   n‚àí‚àí1 + . m‚àík‚àí1 m‚àí =m/2+1  
   
  When k = m ‚àí 1, through the ‚Äúhockey stick‚Äù identity on Pascal‚Äôs triangle, this simpliÔ¨Åes to     n ‚àí m/2 n ‚àí m/2 ‚àí 1 + . fm‚àí1 (C(n, m)) = m/2  
   
  m/2 ‚àí 1 2.2  
   
  Results and Techniques  
   
  Theorem 1. Fix positive integers n, m, k with n > m ‚â• k ‚â• 2. Then the number of k-loadouts for any design (A, c) with A ‚àà Rm√ón and c ‚àà Rn satisfies   m k |L (A, c)| ‚â§ fk‚àí1 (C(n + 1, m)) ‚àí . (2) k‚àí1 We note that the  trivial upper bound on the number of k-loadouts in a design with n tools is nk . When m < n, the RHS of (2) will always be smaller than this trivial upper bound, which shows that having a limited number of resource types in the game does indeed prevent all subsets of tools from being viable. Theorem 2. Fix positive integers n, m, k with n > m ‚â• k ‚â• 2. Then there exists an explicit design (A, c) with A ‚àà Rm√ón and c ‚àà Rn‚â•0 that satisfy ‚â•0 ‚éß fk‚àí1 (C(n, m)) if k < m/2 ‚é™ ‚é™ ‚é™ ‚é®f (C(n, m))/2 if k ‚â• m/2 and m is odd, k‚àí1 |Lk (A, c)| ‚â• ‚é™ or k = m/2 and m is even ‚é™ ‚é™ ‚é© fk‚àí1 (C(n, m))/4 if k > m/2 and m is even.  
   
  176  
   
  O. Hanguir et al.  
   
  The constructions from Theorem 2 are always within a 1/4-factor of being optimal asymptotically as n ‚Üí ‚àû because it is known that fk‚àí1 (C(n, m)) = 1. n‚Üí‚àû fk‚àí1 (C(n + 1, m)) lim  
   
  Theorem 3. For n > m = 3, there exists an explicit design (A, c) with A ‚àà Rm√ón and c ‚àà Rn‚â•0 that satisfy |L3 (A, c)| ‚â• 2n ‚àí 5 and |L2 (A, c)| ‚â• 3n ‚àí 6. ‚â•0 Theorem 4. For n > m = 2, there exists an explicit design (A, c) with A ‚àà Rm√ón and c ‚àà Rn that satisfy |L2 (A, c)| ‚â• n ‚àí 1. The constructions from Theorem 3 and Theorem 4 are exactly tight; it can be checked that they match the upper bound expression from Theorem 1 when evaluated at m = 3 and m = 2. Example of Construction from Theorem 2 and Intuition. Table 1 shows an example of the asymptotically optimal construction for m = 4 and n = 6. The fact that the cost vector is (1, 1, . . . , 1) is simply a normalization and can be assumed without loss. Our construction provides a pattern that game designers can follow to diversify loadouts on a set of tools 1, . . . , n, by having two types of constraints. The Ô¨Årst type of constraints (rows 1 and 3) accords more importance to tools with big indices (because these tools have lower costs to rows 1 and 3) while the second type of constraints (rows 2 and 4) give more advantage to tools with a small index (because these tools have lower costs to rows 2 and 4). This ‚Äútension‚Äù between the two types of constraints ensures that a given combination of tools cannot be optimal for too many resource vectors. This captures the rough intuition that a game with an overpowered tool (meaning one that is more useful than the others but also not signiÔ¨Åcantly ‚Äúcumbersome‚Äù to limit its use) leads to uniform strategies among players. In other words, for diversity, all tools should have strengths and weaknesses. Table 1. Example of our construction with m = 4, n = 6, and M = 64 + 1. c  
   
  1  
   
  A 1 M ‚àí 12 13 M ‚àí 14  
   
  1  
   
  1  
   
  1  
   
  1  
   
  1  
   
  2 M ‚àí 22 23 M ‚àí 24  
   
  3 M ‚àí 32 33 M ‚àí 34  
   
  4 M ‚àí 42 43 M ‚àí 44  
   
  5 M ‚àí 52 53 M ‚àí 54  
   
  6 M ‚àí 62 63 M ‚àí 64  
   
  We end this section with a high-level overview of our approach for establishing our upper and lower bounds. All the undeÔ¨Åned terminology used here will be deÔ¨Åned in more detail in later sections. We prove our upper bound Theorem 1 using a sequence of transformations. We Ô¨Årst introduce the intermediate concept of an equality loadout problem that replaces the inequality constraint Ax ‚â§ b with an equality Ax = b. We show that for a Ô¨Åxed design (A, c) and for every dimension k, the number of k-loadouts  
   
  Designing Optimization Problems with Diverse Solutions  
   
  177  
   
  is less than the number of k-equality loadouts (Lemma 1). This allows us to focus on proving an upper bound on the number of equality loadouts. Here, we can exploit the dual structure of the equality LP and prove that equality loadouts belong to a cell complex Œîc (A) that is characterized by A and c. Importantly, we show that loadouts correspond to simplicial cells in this cell complex (Lemma 2). In turn, this allows us to, without loss of generality, assume that Œîc (A) is a triangulation (as opposed to an arbitrary subdivision), of a cone in the positive orthant of Rm (Lemma 3). We show that triangulations of cones in the positive orthant of Rm correspond to triangulations of points in the lower dimension Rm‚àí1 (Lemma 4). Finally, we show that the simplices in this triangulation can be embedded into faces of a simplicial polytope in Rm . Therefore, any upper bound on the number of faces of polytopes in Rm implies an upper bound on the number of loadouts. This allows us to invoke the ‚Äúmaximality‚Äù of the cyclic polytope with respect to its number of faces mentioned in Sect. 2.1. Therefore, the number faces of the cyclic polytope of dimension m bounds the number of faces in a polytope of dimension m, and implies a bound on the number of equality loadouts. We also carefully count the number of extraneous faces added through our transformations, by invoking a bound on the minimal number of faces a polytope can have, which allows us to derive tight bounds for small values of m (Lemma 5). To prove our complementing lower bound Theorem 2, we Ô¨Årst explicitly provide our design (A, c) in Sect. 5.1, which is also inspired by the cyclic polytope. Compared to the cyclic polytope, every even row of the matrix A has been ‚ÄúÔ¨Çipped‚Äù, as we show in the proof, which we now outline. First, we focus on the dual program of LP (A, c, b) and present a suÔ¨Écient condition (DeÔ¨Ånition 4) for loadouts in terms of dual variables (Lemma 7). We show that by taking hyperplanes corresponding to the facets of the cyclic polytope in dimension m, one can attempt to construct dual variables that satisfy the suÔ¨Écient condition (Lemma 8). Our aforementioned ‚ÄúÔ¨Çipping‚Äù of the even rows in A is crucial to this construction of the dual variables. We show that as long as the facet of the cyclic polytope is of the ‚Äúodd‚Äù parity, the constructed dual variables will indeed be suÔ¨Écient (Lemma 9), and hence such a facet and all of the faces contained within it correspond to loadouts. To be more precise, we require odd parity when m is even, and even parity when m is odd. What we mean by the parity of a facet will be made clear later. Therefore, to count the number of k-loadouts, we need to count the number of (k ‚àí 1)-dimensional faces on a cyclic polytope in dimension m that are contained within at least one odd facet. To the best of our knowledge, this is an unsolved problem in the literature. Nonetheless, using Gale‚Äôs evenness criterion we can map this to a purely combinatorial problem on binary strings. Through some combinatorial bijections, we show that at worst a factor of 4 is lost when one adds the requirement that the (k ‚àí 1)-dimensional face must be contained within at least one odd facet, with the factor improving to 2 if m is odd, and improving to 1 if k is small. These arguments form the cases in Theorem 2. We should note that generally, a cyclic polytope does not have an equal number of odd and even facets. Therefore, one should not expect this factor to always be 2.  
   
  178  
   
  3  
   
  O. Hanguir et al.  
   
  Preliminaries  
   
  We present terminology we use in the proofs of both Theorems 1 and 2. Additional terminology needed in the proof of only one of these results is found in the relevant sections. A d-simplex is a d-dimensional polytope that is the convex hull of d + 1 aÔ¨Énely independent points. For instance, a 0-simplex is a point, a 1-simplex is a line segment and 2-simplex is a triangle. For a matrix A = (A1 , . . . , An ) of rank m, let cone(A) = cone({A1 , . . . , An }) represent the closed convex polyhedral cone {Ax | x ‚àà Rn‚â•0 }. We use the notation cone(C) to denote the cone generated by the columns indexed by C ‚äÜ [n]. If C ‚äÜ [n] is a subset of indices, the relative interior of C is the relatively open (i.e., open in its aÔ¨Éne hull) convex set    
   
  ŒªAj | Œªj > 0 for all j ‚àà C, and Œªj = 1 . relintA (C)  j‚ààC  
   
  j‚ààC  
   
  A subset F of polytope P is a face if there exists Œ± ‚àà Rn and Œ≤ ‚àà R such that Œ± x + Œ≤ ‚â§ 0 for all x ‚àà P and F = {x ‚àà P | Œ± x + Œ≤ = 0}. If dim(F ) = k then F is called a k-dimensional face or k-face. The faces of dimensions 0, 1, and dim(P ) ‚àí 1 are called vertices, edges, and facets, respectively. Furthermore, we say that F is face of C, where F, C ‚äÜ [n], when cone(F ) is a face of cone(C). We deÔ¨Åne a polyhedral subdivision of cone(A) as follows. Definition 1. Let A = (A1 , . . . , An ) be a matrix of rank m. A collection S of subsets of [n] is a polyhedral subdivision of cone(A) if it satisfies the following conditions: ‚Äì (CP): If C ‚àà S and F is aface of C, then F ‚àà S . (Closure Property) cone(C). (Union Property) ‚Äì (UP): cone({1, . . . , n}) ‚äÇ C‚ààS  
   
  ‚Äì (IP): If C, C ‚àà S with C = C , then relintA (C) ‚à© relintA (C ) = ‚àÖ. (Intersection Property) If the set of indices {j1 , . . . , jk } belongs to a subdivision of cone(A), then it is called a cell of the subdivision, and if the cone is of dimension k, it is called a k-cell. We note that a polyhedral cone subdivision is completely speciÔ¨Åed by listing its maximal cells. Next, we deÔ¨Åne a special subdivision of cone(A) as a function of the cost vector c. The cells of this subdivision map to the loadouts of the design (A, c). and c ‚àà Rn‚â•0 , we deÔ¨Åne the polyhedral subdivision Œîc (A) of For A ‚àà Rm√ón ‚â•0 cone(A) as the family of subsets of {1, . . . , n} such that C ‚àà Œîc (A) if and only if there exists a column vector y ‚àà Rm such that y  Aj = cj if j ‚àà C and y  Aj > cj if j ‚àà {1, . . . , n}\C. In such a case, we say C is a cell of Œîc (A) and that Œîc (A) is a cell complex. A cell C ‚àà Œîc (A) is simplicial if the column vectors (Aj )j‚ààC are linearly independent. If all the cells of Œîc (A) are simplicial, then we say Œîc (A) is a triangulation. The maximum size of a simplicial cell is m. The next results shows that Œîc (A) is indeed a polyhedral subdivision of cone(A).  
   
  Designing Optimization Problems with Diverse Solutions  
   
  179  
   
  Proposition 1. Œîc (A) is a polyherdal subdivision of cone(A). Intuitively, we can think of the subdivision Œîc (A) as follows: take the cost vector c, and use it to lift the columns of A to Rn+1 then look at the projection of the upper faces (those faces you would see if you ‚Äúlook from above‚Äù). This is illustrated in Example 1. Example 1. Consider the following matrix and cost vectors   1/4 1/2 3/4 A= , c1 = (2, 2.125 + , 2.25) and c2 = (2, 2.125 ‚àí , 2.25), 1 1 1 where  > 0 is a small constant. The corresponding subdivisions of cone(A) are  

  Œîc1 (A) = {1, 2}, {2, 3}, {1}, {2}, {3}, ‚àÖ and Œîc2 (A) = {1, 2, 3}, {1}, {3}, ‚àÖ . For example, to see that {1, 2} is a cell of Œîc1 (A), we consider y = (0.5 + 4, 1.875 ‚àí ). One can verify that y  A1 = c1 and y  A2 = c2 , while y  A3 > c3 . We observe that for the cost vector c1 , the cell {1, 2} is simplicial, while for c2 , the cell {1, 2, 3} is not simplicial. In our deÔ¨Ånition of simplicial cell, we mentioned that if all the cells in the subdivision Œîc (A) are simplicial, then Œîc (A) is called a triangulation. More generally, a triangulation of cones is a cone subdivision where all the cells are simplicial (the columns of every cell are linearly independent). We will also deÔ¨Åne the notion of triangulations of point conÔ¨Ågurations, that is sets of points whose convex hull is subdivided into simplices. The formal deÔ¨Ånition mirrors that of polyhedral subdivisions and can be found in the full version of the paper. Definition 2 (Cyclic Polytope). The cyclic polytope C(n, d) is defined as the convex hull of n distinct vertices on the moment curve t ‚Üí (t, t2 , . . . , td ). The precise choice of which n points on this curve are selected is irrelevant for the combinatorial structure of this polytope. Definition 3 (f -vector). The f -vector of a d-dimensional polytope P is given by (f0 (P ), . . . , fd‚àí1 (P )), where fi (P ) is the number of i-dimensional faces in the d-dimensional polytope for all i = 0, . . . , d ‚àí 1. For instance, a 3dimensional cube has eight vertices, twelve edges, and six facets, so its f -vector is (f0 (P ), f1 (P ), f2 (P )) = (8, 12, 6).  
   
  4  
   
  Upper Bound (Proof of Theorem 1)  
   
  Throughout this section we Ô¨Åx positive integers n > m ‚â• 2 and A ‚àà Rm√ón ‚â•0 , c ‚àà Rn‚â•0 . We start by formally introducing the equality loadout problem. We consider the parametric family of linear programming problems with equality constraints LP= (A, c, b) :  
   
  max{c x | Ax = b, x ‚â• 0},  
   
  180  
   
  O. Hanguir et al.  
   
  By analogy to the deÔ¨Ånition of loadouts in Sect. 2, an equality loadout is deÔ¨Åned as a subset of indices L ‚äÜ {1, . . . , n} such that there exists a resource vector b for which LP= (A, c, b) has a unique optimal solution x‚àó such that supp(x‚àó ) = L. If |L| = k then we say that L is a k-equality loadout. Given A and c and an integer k ‚àà [m], let Lk= (A, c) denote the family of all equality loadouts L of dimension k. Finally, L= (A, c) denotes the family of equality loadouts of all dimensions given k A and c. Namely, L= (A, c)  ‚à™m k=1 L= (A, c). The following proposition bounds the number of loadouts by the number of equality loadouts, for Ô¨Åxed A and c. n k k Lemma 1. For every A ‚àà Rm√ón ‚â•0 , c ‚àà R‚â•0 and k ‚àà [m], L (A, c) ‚äÜ L= (A, c).  
   
  In the rest of this section, assume without loss of generality that A is a fullrow rank matrix. We present, for all k ‚àà [m], an upper bound for the number |Lk= (A, c)| of equality loadouts of size k with respect to the design (A, c). Some of the results of this section are known in the literature (an excellent reference is the textbook [4]), but we present them using our notation and adapted to the loadout terminology. We provide proofs for clarity and of our a desire to be as self-contained as possible. The proofs are also suggestive of some aspects of our later constructions in Sect. 5. From Equality Loadouts to Triangulations. The following result links the optimal solutions of LP= (A, c, b) to the cells of subdivision Œîc (A). Proposition 2. ( [16], Lemma 1.4) The optimal solutions x to LP= (A, c, b) are the solutions to the problem Find x ‚àà Rn s.t. Ax = b, x ‚â• 0 and supp(x) lies in a cell of Œîc (A).  
   
  (3)  
   
  Lemma 2. A subset L ‚äÜ [n] is a loadout of (A, c) if and only if it is a simplicial cell in the subdivision Œîc (A). The lemma above implies that we can focus on the simplicial cells of the subdivision Œîc (A). We next show that we can consider without loss of generality choices of c where all the cells of Œîc (A) are simplicial. The idea is that if Œîc (A) has some non-simplicial cells, then we can ‚Äúperturb‚Äù the cost vector c to some c and transform at least one non-simplicial cell into one or more simplicial cells. This perturbation conserves all the simplicial cells of Œîc (A) and thus the number of equality loadouts for the design (A, c ) cannot be less than the number of equality loadouts for the design (A, c). Without loss of optimality, we can ignore cost vectors c that give rise to non-simplicial cells. We Ô¨Årst deÔ¨Åne the notion of reÔ¨Ånement that formalizes the ‚Äúperturbation‚Äù of c. Given two cell complexes C1 and C2 , we say that C1 reÔ¨Ånes C2 if every cell of C1 is contained in a cell of C2 . [4, Lemma 2.3.15] shows that if c = c +  ¬∑ e is perturbation of c with  > 0 suÔ¨Éciently small and e = (1, . . . , 1), then the new subdivision Œîc (A) reÔ¨Ånes Œîc (A). Since Œîc (A) reÔ¨Ånes Œîc (A), then Œîc (A) will have more cells. However, it is not clear if Œîc (A) will have more simplicial cells than Œîc (A). We show in the following lemma that this is the case. Lemma 3. A refinement of Œîc adds to the number of simplicial cells in Œîc .  
   
  Designing Optimization Problems with Diverse Solutions  
   
  181  
   
  In [4, Corollary 2.3.18], it is shown Œîc (A) can be reÔ¨Åned to a triangulation within a Ô¨Ånite number of reÔ¨Ånements (suÔ¨Éces for c to be generic). Therefore, the lemma above implies that in order to maximize the number of loadouts for any dimension k ‚â§ m, we can restrict attention to designs (A, c) such that Œîc (A) is a triangulation without loss of generality. has all nonnegative entries, We observe that since the matrix A ‚àà Rm√ón ‚â•0 cone(A) is contained entirely in the positive orthant and therefore cannot contain a line. Cones that do not contain lines are called pointed. The following lemma shows that triangulations of pointed cones in dimension m are equivalent to triangulations of a non-restricted set of points (columns) in dimension m ‚àí 1. This implies that equality loadouts can be seen as cells of a triangulation of a point conÔ¨Åguration. Lemma 4 ([1], Theorem 3.2). Every triangulation T of a pointed cone of dimension m can be considered as a triangulation T of a point configuration of dimension m ‚àí 1 such that for 1 ‚â§ k ‚â§ m, the k-simplices of T map to (k ‚àí 1)-simplices of T . Lemma 4 implies that equality loadouts of dimension k correspond to (k ‚àí1)simplices in a triangulation of a point conÔ¨Åguration in dimension m ‚àí 1. From Cells of a Triangulation to Faces of a Polytope ( [4], Corollary 2.6.5). We now show that any n-point triangulation in Rm‚àí1 can be embedded onto the boundary of an (n+1)-vertex polytope in Rm , in a way such that (k‚àí1)simplices in the triangulation correspond to (k ‚àí 1)-faces on the polytope. We then apply the cyclic polytope upper bound on the number of (k ‚àí 1)-faces on any (n+1)-vertex polytope in Rm to establish our result. To get a tighter bound, we carefully subtract the ‚Äúextraneous‚Äù faces added from the embedding that did not correspond to (k ‚àí1)-simplices in the original triangulation. We lower bound the number of such extraneous faces using the lower bound theorem of [10]. Let T denote the original n-point triangulation in Rm‚àí1 . We will use conv T to refer to the polytope obtained by taking the convex hull of all the faces in T . Let gk‚àí1 (T ) denote the number of (k ‚àí 1)-simplices in the triangulation T . We embed conv T into a polytope P in Rm as follows. Let z 1 , . . . , z n ‚àà Rm‚àí1 denote the vertices in triangulation T . We now deÔ¨Åne the following lifted points in Rm . i , 0). For all i = 1, . . . , n, For all i = 1, . . . , n, let z i denote the point (z1i , . . . , zm‚àí1 i i i let z¬Ø denote the point (z1 , . . . , zm‚àí1 , ), for some Ô¨Åxed  > 0. Let  > 0, and replace each point z i that is in the interior of conv({z 1 , . . . , z n }) by the ‚Äúlifted‚Äù i , ). The points on the boundary of conv({z 1 , . . . , z n }) point z¬Øi = (z1i , . . . , zm‚àí1 are not lifted. Let S be the set of the n points in Rm after lifting. Let Sm be the unit sphere of Rm with center at the origin, and S be the projection of S onto Sm , where every point is projected along the line connecting the point to the center of the sphere. The set S has the property that all the points that are on the ‚Äúequator‚Äù hyperplane zm = 0 are exactly the projections of the points of S on the boundary of conv(S) (the points that were not lifted). The other points of S are in the ‚Äúnorthern hemisphere‚Äù (the half space xm > 0). The Ô¨Ånal step is to adjoin the boundary points to the ‚Äúsouth pole‚Äù, (0, . . . , 0, ‚àí1) ‚àà Rm . Let P be the resulting polytope, i.e., P = conv(S ).  
   
  182  
   
  O. Hanguir et al.  
   
  The next lemma shows that for 2 ‚â§ k ‚â§ m, the (k ‚àí 1)-dimensional faces of P are either (k ‚àí 1)-simplices of T , or (k ‚àí 2)-faces of T that were adjoined to the south pole. Lemma 5. For 2 ‚â§ k ‚â§ m, we have fk‚àí1 (P ) = gk‚àí1 (T ) + fk‚àí2 (T ). The previous lemma implies that gk‚àí1 (T ) = fk‚àí1 (P ) ‚àí fk‚àí2 (T ). Since P has n + 1 points, we know from the upper bound theorem that fk‚àí1 (P ) ‚â§ fk‚àí1 (C(n + 1, m)). Therefore, gk‚àí1 (T ) ‚â§ fk‚àí1 (C(n + 1, m)) ‚àí fk‚àí2 (T ), and all we need is a lower bound on fk‚àí2 (T ). The following lemma uses the lower bound theorem (Theorem 1.1, [10]) to establish a lower bound on fk‚àí2 (T ). The lower bound theorem presents a lower bound on the number of faces in every dimension among all polytopes of dimension d over p points, for d ‚â• 2 and p ‚â• 2. m Lemma 6. For 2 ‚â§ k ‚â§ m, we have gk‚àí1 (T ) ‚â§ fk‚àí1 (C(n + 1, m)) ‚àí k‚àí1 . See the full version of the paper to see how these lemmas come together to prove Theorem 1.  
   
  5  
   
  General Lower Bound (Proof of Theorem 2)  
   
  Throughout this section, we Ô¨Åx positive integers n > m ‚â• 4, and explicitly present designs (A, c) that have the number of k-loadouts promised in Theorem 2 for all k ‚â§ m. For m = 2 and m = 3, the exactly optimal designs are presented in the full version. All of the designs constructed in this paper will satisfy the property that A has linearly independent rows, hence we assume in the rest of this section that A is a full row rank matrix. 5.1  
   
  Construction Based on Moment Curve  
   
  Let t1 , . . . , tn be arbitrary real numbers satisfying 0 < t1 < t2 < . . . < tn . Let M be an arbitrary constant satisfying M ‚â• tm . We deÔ¨Åne the design (A, c) so (t1 ), . . . vm (tn )]. where that c = (1, . . . , 1) ‚àà Rn and A = [vm (t) = t ‚Üí vm  
   
   t, M ‚àí t2 , t3 , M ‚àí t4 , . . . ,  
   
  (‚àí1)m + 1 M ‚àí (‚àí1)m tm 2  
   
   ‚àà Rm .  
   
  Note that the Ô¨Ånal row equals M ‚àí tm if m is even, or tm if m is odd. For any such values t1 , . . . , tn and M , we will get a design that satisÔ¨Åes our Theorem 2. We set all the entries of the cost vector c to 1 to simplify computations. It is not a requirement and the construction would still hold by setting cj to be any positive number and scaling the column Aj by a factor of cj . We will also later show that any of these constructions satisfy our assumption of A having full row rank.  
   
  Designing Optimization Problems with Diverse Solutions  
   
  183  
   
  Motivation Behind the Construction. Let P be the convex hull of (t1 ), . . . vm (tn )}. Let t ‚Üí vm (t) = (t, t2 , t3 , . . . , tm )T ‚àà Rm denote the m{vm dimensional original moment curve the deÔ¨Ånes the cyclic polytope. is motivated by role the cyclic polytope plays in The choice of the curve vm our corresponding upper bound Theorem 1. In fact, Theorem 1 shows that the number of k-dimensional loadouts is less than the number of (k ‚àí 1)-dimensional faces of the cyclic polytope C(n + 1, m) (for 2 ‚â§ k ‚â§ m). An ideal lower bound proof would connect the number of loadouts to the number of faces of the cyclic polytope. However, simply setting the columns of the constraint matrix A to be points on the moment curve of the cyclic polytope does not guarantee the that describes a existence of loadouts. We therefore, introduce the curve vm ‚Äúrotated‚Äù cyclic polytope and show that it is rotated to ensure that the supporting normals of ‚Äúhalf‚Äù of the facets are nonnegative. We use these rotated facets to construct a number of loadouts that asymptotically matches the upper bound. The rotation is performed by multiplying the even coordinates of the moments curve by ‚àí1, and we use a suÔ¨Éciently big constant M to ensure the positivity of the new constraint matrix. 5.2  
   
  Dual Certificate for Loadouts  
   
  Using LP duality, we derive a suÔ¨Écient condition for subsets of [n] to be loadouts. Definition 4. A set C ‚äÜ [n] is an inequality cell of the design (A, c) if there exists a variable y ‚àà Rm such that yi > 0,  
   
  ‚àÄ i ‚àà [m];  
   
    
   
  ‚àÄ j ‚àà C;  
   
    
   
  ‚àÄ j ‚àà C.  
   
  y Aj = cj , y Aj > cj ,  
   
  Here, y can be interpreted as a dual variable. However, in contrast to the deÔ¨Ånition of a cell that features in Proposition 2, here we require y > 0. This is because non-negativity is needed for y to be feasible in the dual when the LP has an inequality constraint Ax ‚â§ b instead of an equality constraint as considered in Proposition 2. Lemma 7. Suppose C ‚äÜ [n] is an inequality cell with |C| = m. Then every non-empty subset of C is a loadout. To establish Lemma 7, we show that for every subset L ‚äÜ C, y will verify the complementary slackness constraints with a primal variable x that has support equal to L. This establishes the optimality of x, and to show its uniqueness, we use the assumption that A has a full row rank equal to m. Note that this lemma only works in one direction. If L is a loadout, it is not clear that we can Ô¨Ånd a corresponding dual certiÔ¨Åcate that satisÔ¨Åes DeÔ¨Ånition 4. However, for our construction, we only need the direction proved in the lemma. In order to prove Theorem 2, we consider our design from Sect. 5.1, and show that there are many inequality cells of cardinality m. To do so, we take an  
   
  184  
   
  O. Hanguir et al.  
   
  arbitrary C ‚äÜ [n] with |C| = m and consider the hyperplane that goes through (tj ) | j ‚àà C}. We show in Lemma 8 that the coeÔ¨Écients of the the m points {vm equation for this hyperplane have the same sign. We then use these coeÔ¨Écients to construct a candidate dual vector y. The last step (Lemma 9) is to show that when the hyperplane satisÔ¨Åes a gap parity combinatorial condition, this dual vector will indeed satisfy DeÔ¨Ånition 4, certifying that C is an inequality cell. Lemma 8. Let C = {j1 , . . . , jm } ‚äÜ [n] be a subset of m indices with j1 < ¬∑ ¬∑ ¬∑ < jm . The equation   1 ... 1 1 det =0 (4) vm (tj1 ) . . . vm (tjm ) y defines a hyperplane in variable y ‚àà Rm that passes through the points (tj1 ), . . . , vm (tjm ). Furthermore if equation (4) is written in the form Œ±1 y1 + vm . . . Œ±m ym ‚àí Œ≤ = 0, then we have Œ±1 = 0, . . . Œ±m = 0, Œ≤ = 0, and m  
   
  sign(Œ±1 ) = . . . = sign(Œ±m ) = sign(Œ≤) = (‚àí1) 2 +m+1 , where sign(Œ±j ) is equal to 1 if Œ±j > 0 and equal to ‚àí1 otherwise. We now consider a subset C = {j1 , . . . , jm } ‚äÜ [n] with j1 < ¬∑ ¬∑ ¬∑ < jm , such that the corresponding hyperplane has equation Œ±1 y1 + . . . Œ±m ym ‚àí Œ≤ = 0, as deÔ¨Åned above. The previous lemma shows that the dual variable y = Œ±/Œ≤ satisÔ¨Åes yi > 0 for all i ‚àà [m]. We now proceed towards a gap parity condition on the subset C under which setting y = Œ±/Œ≤ also satisÔ¨Åes DeÔ¨Ånition 4. Definition 5. (Gaps). For a set C ‚äÇ [n], a gap of C refers to an index i ‚àà [n] \ C. A gap i of C is an even gap if the number of elements in C larger than i is even, and i is an odd gap otherwise. Definition 6. (Facets and Gap Parity). A subset C ‚äÜ [n] is called a facet if |C| = m and either: (i) all of its gaps are even; or (ii) all of its gaps are odd. If all of its gaps are even, then we call C an even facet and define g(C) = 2. On the other hand, if all of its gaps are odd, then we call C an odd facet and define g(C) = 1. We let g(C) ‚àà {1, 2} denote the gap parity of a facet C, with g(C) being undefined if C is not a facet. Lemma 9. Every facet C with g(C) ‚â° m (mod 2) is an inequality cell. The proofs of Lemmas 8 and 9 require some technical developments on the sub-determinants of A. The outline of the proof of Lemma 9 is as follows. To show that C = {j1 , . . . , jm } is an inequality cell, we consider the dual certiÔ¨Åcate y = Œ± Œ≤ where Œ±1 y1 + . . . Œ±m ym ‚àí Œ≤ = 0 is the equation of C. By Lemma 8, Œ≤ and Œ± have the same signs, and that Œ≤ = 0 and Œ±i = 0 for i ‚àà [m]. Therefore, yi > 0, ‚àÄi ‚àà [m]. For j ‚àà C, y  vm (tj ) =  
   
  (tj ) Œ≤ Œ±  vm = = 1 = cj . Œ≤ Œ≤  
   
  The last step is to show y  vm (tj ) > cj for j ‚àà C.  
   
  Designing Optimization Problems with Diverse Solutions  
   
  5.3  
   
  185  
   
  Counting the Number of k-Loadouts  
   
  The preceding duality certiÔ¨Åcates combine to provide a purely combinatorial lower bound on the number of k-loadouts in our construction. Indeed, Lemma 7 shows a subset L ‚äÜ [n] with |L| = k is a k-loadout as long as L is contained within some inequality cell C. In turn, Lemma 9 shows that C is an inequality cell as long as it is a facet with gap parity opposite to m. The last step is to count the number of k-subsets that are contained within at least one facet with gap parity opposite to m, for all k = 1, . . . , m. The challenge is not to over-count these subsets because such a subset can be contained in diÔ¨Äerent facets. We map the counting of these subsets to a purely combinatorial problem on binary strings. Through some combinatorial bijections, we show that at worst a factor of 4 is lost when one adds the requirement that the (k ‚àí 1)dimensional face must be contained within at least one odd facet, with the factor improving to 2 if m is odd, and improving to 1 if k is small. The full proofs are deferred to the full version.  
   
  6  
   
  Conclusion  
   
  We study the novel problem of diversity maximization. This problem can be motivated by the video game design context where designing for diversity is one of its core design philosophies. We model this diversity optimization problem as a parametric linear programming problem where we are interested in the diversity of supports of optimal solutions. Using this model, we establish upper bounds and construct designs that match this upper bound asymptotically. To our knowledge, this is the Ô¨Årst paper to systematically study the question of ‚Äúdiversity maximization‚Äù as we have deÔ¨Åned it here. The goal here is ‚Äúdiverse-in diverse-out‚Äù, if two players have right-hand resource vectors, they will optimally play diÔ¨Äerent strategies. We believe there could be other applications for ‚Äúdiverse-in diverse-out‚Äù optimization problems. Consider, for example, a diet problem where a variety of ingredients are used in the making of meals, depending on diÔ¨Äerent availability in resources. We leave this exploration for future work. There are also natural extensions to our model and analysis that could be pursued. For instance, we have studied the linear programming version of the problem. An obvious next step is the integer linear setting, which also arises naturally in the design of games. Just as in our analysis of the linear program, a deep understanding of the parametric nature of the integer optimization problems is necessary to proceed in the integer setting. [16] introduce a theory of reduced Gr¬® obner bases of toric ideals that play a role analogous to triangulations of cones. Another compelling extension would involve mixed -integer decision sets. This will require a deep appreciation of parametric mixed-integer linear programming, a topic that remains of keen interest in the integer programming community (see, for instance, [5,6]). Finally, another direction is to consider multiple objectives for the player. In our setting, we have assumed a single meaningful objective for the player, such as maximizing the damage of a loadout of weapons.  
   
  186  
   
  O. Hanguir et al.  
   
  References 1. Beck, M., Robins, S.: Computing the Continuous Discretely: Integer-Point Enumeration in Polyhedra. Springer, NY (2007). https://doi.org/10.1007/978-0-38746112-0 2. Chen, N., Elmachtoub, A.N., Hamilton, M.L., Lei, X.: Loot box pricing and design. Management Science (forthcoming) (2020) 3. Chen, Z., et al.: EOMM: An engagement optimized matchmaking framework. In: Proceedings of the 26th International Conference on World Wide Web, pp. 1143‚Äì 1150 (2017) 4. De Loera, J.A., Rambau, J., Santos, F.: Triangulations: Structures for Algorithms and Applications. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-64212971-1 5. Eisenbrand, F., Shmonin, G.: Parametric integer programming in Ô¨Åxed dimension. Math. Oper. Res. 33(4), 839‚Äì850 (2008) 6. Gribanov, D., Malyshev, D., Pardalos, P.: Parametric integer programming in the average case: sparsity, proximity, and FPT-algorithms. arXiv preprint arXiv:2002.01307 (2020) 7. Guo, H., Hao, L., Mukhopadhyay, T., Sun, D.: Selling virtual currency in digital games: implications for gameplay and social welfare. Inf. Syst. Res. 30(2), 430‚Äì446 (2019) 8. Guo, H., Zhao, X., Hao, L., Liu, D.: Economic analysis of reward advertising. Prod. Oper. Manag. 28(10), 2413‚Äì2430 (2019) 9. Huang, Y., Jasin, S., Manchanda, P.: ‚ÄúLevel up‚Äù: leveraging skill and engagement to maximize player game-play in online video games. Information Systems Research 30(3), 927‚Äì947 (2019) 10. Kalai, G.: Rigidity and the lower bound theorem 1. Invent. Math. 88(1), 125‚Äì151 (1987) 11. McMullen, P.: The maximum numbers of faces of a convex polytope. Mathematika 17(2), 179‚Äì184 (1970) 12. Mills, H.: Marginal values of matrix games and linear programs. In: Kuhn, H.W., Tucker, A.W. (eds.) Linear Inequalities and Related Systems, pp. 183‚Äì194. Princeton University Press (1956) 13. Ryan, C.T., Sheng, L., Zhao, X.: Selling enhanced attempts. Available at SSRN 3751523 (2020) 14. Saaty, T., Gass, S.: Parametric objective function (part 1). J. Oper. Res. Soc. Am. 2(3), 316‚Äì319 (1954) 15. Sheng, L., Ryan, C.T., Nagarajan, M., Cheng, Y., Tong, C.: Incentivized actions in freemium games. Manufacturing & Service Operations Management (2020) 16. Sturmfels, B., Thomas, R.R.: Variation of cost functions in integer programming. Math. Program. 77(2), 357‚Äì387 (1997) 17. Turner, J., Scheller-Wolf, A., Tayur, S.: Scheduling of dynamic in-game advertising. Oper. Res. 59(1), 1‚Äì16 (2011) 18. Walkup, D., Wets, R.: Lifting projections of convex polyhedra. Pac. J. Math. 28(2), 465‚Äì475 (1969) 19. Williams, A.: Marginal values in linear programming. J. Soc. Ind. Appl. Math. 11(1), 82‚Äì94 (1963)  
   
  ReLU Neural Networks of Polynomial Size for Exact Maximum Flow Computation Christoph Hertrich1(B) 1  
   
  and Leon Sering2  
   
  London School of Economics and Political Science, London, UK [email protected]  2 ETH Zurich, Zurich, Switzerland [email protected]   
   
  Abstract. This paper studies the expressive power of artiÔ¨Åcial neural networks with rectiÔ¨Åed linear units. In order to study them as a model of real-valued computation, we introduce the concept of Max-AÔ¨Éne Arithmetic Programs and show equivalence between them and neural networks concerning natural complexity measures. We then use this result to show that two fundamental combinatorial optimization problems can be solved with polynomial-size neural networks. First, we show that for any undirected graph with n nodes, there is a neural network (with Ô¨Åxed weights and biases) of size O(n3 ) that takes the edge weights as input and computes the value of a minimum spanning tree of the graph. Second, we show that for any directed graph with n nodes and m arcs, there is a neural network of size O(m2 n2 ) that takes the arc capacities as input and computes a maximum Ô¨Çow. Our results imply that these two problems can be solved with strongly polynomial time algorithms that solely uses aÔ¨Éne transformations and maxima computations, but no comparison-based branchings. Keywords: Neural Network Expressivity ¬∑ Strongly Polynomial Algorithms ¬∑ Minimum Spanning Tree Problem ¬∑ Maximum Flow Problem  
   
  1  
   
  Introduction  
   
  ArtiÔ¨Åcial neural networks (NNs) achieved breakthrough results in various application domains like computer vision, natural language processing, autonomous driving, and many more [40]. Also in the Ô¨Åeld of combinatorial optimization (CO), promising approaches to utilize NNs for problem solving or improving classical solution methods have been introduced [7]. However, the theoretical understanding of NNs still lags far behind these empirical successes. All neural networks considered in this paper are feedforward neural networks with rectiÔ¨Åed linear unit (ReLU) activations, one of the most popular models in practice [19]. These NNs are directed, acyclic, computational graphs in which The full version is available on arXiv: https://arxiv.org/abs/2102.06635. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 187‚Äì202, 2023. https://doi.org/10.1007/978-3-031-32726-1_14  
   
  188  
   
  C. Hertrich and L. Sering  
   
  Fig. 1. A small NN with two input neurons x1 and x2 , a single ReLU neuron labelled with the shape of the ReLU function, and one output neuron y. It computes the function x ‚Üí y = x2 ‚àí max { 0, x2 ‚àí x1 } = ‚àí max { ‚àíx2 , ‚àíx1 } = min { x1 , x2 } .  
   
  Fig. 2. This example shows that the outcome of one iteration of the Edmonds-Karp algorithm for computing a maximum Ô¨Çow depends discontinuously on the arc capacities. Here, a small adjustment of the capacity of arc st leads to a drastic change of the Ô¨Çow after the Ô¨Årst iteration.  
   
  each edge is equipped with a Ô¨Åxed weight and each node with a Ô¨Åxed bias. Each node (neuron) computes an aÔ¨Éne transformation of the outputs of its predecessors and applies the ReLU activation function x ‚Üí max{0, x} on top. The full NN then computes a function mapping real-valued inputs to real-valued outputs. A simple example is given in Fig. 1. The neurons are commonly organized in layers. The depth, width, and size of an NN are deÔ¨Åned as the number of layers, the maximum number of neurons per layer, and the total number of neurons, respectively. An important theoretical question about these NNs is concerned with their expressivity: which functions can be represented by an NN of a certain depth, width, or size? Neural network expressivity has been thoroughly investigated from an approximation point of view. For example, so-called universal approximation theorems [3,11,31] show that every continuous function on a bounded domain can be arbitrarily well approximated with only a single nonlinear layer. However, for a full theoretical understanding of this fundamental machine learning model it is necessary to understand what functions can be exactly expressed with different NN architectures. For instance, insights about exact representability have boosted our understanding of the computational complexity of the task to train an NN with respect to both, algorithms [4,36] and hardness results [9,18,20].  
   
  Poly-Size ReLU Neural Networks for Maximum Flow Computation  
   
  189  
   
  It is known that a function can be expressed with a ReLU NN if and only if it is continuous and piecewise linear (CPWL) [4]. However, many surprisingly basic questions remain open. For example, it is not known whether two layers of ReLU units (with any width) are suÔ¨Écient to compute the function f : R4 ‚Üí R, x ‚Üí max{0, x1 , x2 , x3 , x4 } [24,28]. In this paper we explore another fundamental question within the research stream of exact representability: what are families of CPWL functions that can be represented with ReLU NNs of polynomial size? In other words, using NNs as a model of computation operating on real numbers (in contrast to Turing machines or Boolean circuits, which operate on binary encodings), which problems do have polynomial complexity in this model? Our motivation to study this model stems from a variety of diÔ¨Äerent perspectives, including strongly polynomial time algorithms, arithmetic circuit complexity, parallel computation, and learning theory. We believe that classical combinatorial optimization problems are a natural example to study this model of computation because their algorithmic properties are well understood in each of these areas. Clearly, if there are polynomial-size NNs to solve a certain problem, and assuming that the weights of these NNs are computable in polynomial time1 , then there exists a strongly polynomial time algorithm for that problem, simply by executing the NN. However, the converse might be false. This is due to the fact that ReLU NNs only allow a very limited set of possible operations, namely aÔ¨Éne combinations and maxima computations. In particular, every function computed by such NNs is continuous, making it impossible to realize instructions like a simple if -branching based on a comparison of real numbers. In fact, there are related models of computation for which the use of branchings is exponentially powerful [32]. For some CO problems, classical algorithms do not involve comparison-based branchings and, thus, can easily be implemented as an NN. This is, for example, true for many dynamic programs. In these cases, the existence of eÔ¨Écient NNs follows immediately. We refer to Hertrich and Skutella [29] for some examples of this kind. In particular, polynomial-size NNs to compute the length of a shortest path in a network from given arc lengths are possible. For other problems, like the Minimum Spanning Tree Problem or the Maximum Flow Problem, all classical algorithms use comparison-based branchings. For example, many maximum Ô¨Çow algorithms use them to decide whether an arc is part of the residual network. More speciÔ¨Åcally, in the Edmonds-Karp algorithm a slight perturbation (from 0 to Œµ) in the capacities can lead to diÔ¨Äerent augmenting path and therefore to a completely diÔ¨Äerent intermediate Ô¨Çow; see Fig. 2. Such a discontinuous behavior can never be represented by a ReLU NN.  
   
  1  
   
  In circuit complexity language, one would say ‚Äúif there is a uniform neural network family to solve a certain problem‚Äù.  
   
  190  
   
  1.1  
   
  C. Hertrich and L. Sering  
   
  Our Main Results  
   
  In order to make it easier to think about NNs in an algorithmic way, we introduce the pseudo-code language Max-AÔ¨Éne Arithmetic Programs (MAAPs). We show that MAAPs and NNs are equivalent (up to constant factors) concerning three basic complexity measures corresponding to depth, width, and overall size of NNs. Hence, MAAPs serve as a convenient tool for constructing NNs with bounded size and could be useful for further research about NN expressivity beyond the scope of this paper. We use this result to prove our two main theorems. The Ô¨Årst one shows that computing the value of a minimum spanning tree has polynomial complexity on NNs. The proof is based on a result from subtraction-free circuit complexity [17]. Theorem 1. For a Ô¨Åxed graph with n vertices, there exists an NN of depth O(n log n), width O(n2 ), and size O(n3 ) that correctly maps a vector of edge weights to the value of a minimum spanning tree. The second result shows that computing a maximum Ô¨Çow has polynomial complexity on NNs. Since all classical algorithms involve conditional branchings based on the comparison of real numbers, the proof involves the development of a new strongly polynomial maximum Ô¨Çow algorithm which avoids such branchings. While, in terms of standard running times, the algorithm is deÔ¨Ånitely not competitive with algorithms that exploit comparison-based branchings, it is of independent interest with respect to the structural understanding of Ô¨Çow problems. Theorem 2. Let G = (V, E) be a Ô¨Åxed directed graph with s, t ‚àà V , |V | = n, and |E| = m. There exists an NN of depth and size O(m2 n2 ) and width O(1) that correctly maps a vector of arc capacities to a vector of Ô¨Çow values in a maximum s-t-Ô¨Çow. Let us point out that in case of minimum spanning trees, the NN computes only the objective value, while for maximum Ô¨Çows, the NN computes the actual solution. There is a structural reason for this diÔ¨Äerence: Due to their continuous nature, ReLU NNs cannot compute a discrete solution vector, like an indicator vector of the optimal spanning tree, because inÔ¨Ånitesimal changes of the edge weights would lead to jumps in the output. For the Maximum Flow Problem, however, the optimal Ô¨Çow itself does indeed have a continuous dependence on the arc capacities. 1.2  
   
  Discussion of the Results  
   
  Before presenting our result in more detail, we discuss the signiÔ¨Åcance and limitations of our results from various perspectives. Due to space constraints, we refer to the full version for a more detailed discussion. Learning Theory. A standard approach to create a machine learning model usually contains the following two steps. The Ô¨Årst step is to Ô¨Åx a particular  
   
  Poly-Size ReLU Neural Networks for Maximum Flow Computation  
   
  191  
   
  hypothesis class. When using NNs, this means to Ô¨Åx an architecture, that is, the underlying graph of the NN. Then, each possible choice of weights and biases of all aÔ¨Éne transformations in the network constitutes one hypothesis in the class. The second step is to run an optimization routine to Ô¨Ånd a hypothesis in the class that Ô¨Åts given training data as accurately as possible. A core theme in learning theory is to analyse how the choice of the hypothesis class inÔ¨Çuences diÔ¨Äerent kind of errors made by the machine learning model. While there exist many attempts to mathematically explain the mysterious success of modern NNs [8], there is still a long way ahead of us. Understanding what CPWL functions are actually contained in the hypothesis classes deÔ¨Åned by NNs of a certain size (in particular, polynomial size) is a key insight in this direction. We see our combinatorial, exact perspective as a counterbalance and complement to the usual approximate point of view. Strongly Polynomial Time Algorithms. As pointed out above, polynomialsize NNs correspond to a subclass of strongly polynomial time algorithms with a very limited set of operations allowed. Given that this subclass stems from one of the most basic machine learning models, our grand vision, to which we contribute with our results, is to understand for diÔ¨Äerent CO problems whether they admit strongly polynomial time algorithms of this type. Algorithms of this type have not been known before for the two problems considered in this paper. It remains an open question whether such algorithms, and hence, polynomial-size NNs, exist to solve other CO problems for which strongly polynomial time algorithms are known. Can they, for instance, compute the weight of a minimum weight perfect matching in (bipartite) graphs? Can they compute the cost of a minimum cost Ô¨Çow from either node demands or arc costs, while the other of the two quantities is considered to be Ô¨Åxed? A major open question is also to prove lower bounds on NN sizes. Can we Ô¨Ånd a family of CPWL functions (corresponding to a CO problem or not) that can be evaluated in strongly polynomial time, but not computed by polynomialsize NNs? While proving lower bounds in complexity theory always seems to be a challenging task, we believe that not all hope is lost. For example, in the area of extended formulations, it has been shown that there exist problems (in particular, minimum weight perfect matching) which can be solved in strongly polynomial time, but every linear programming formulation to this problem must have exponential size [52]. Possibly, one can show in the same spirit that also polynomial-size NN representations are not achievable. Boolean Circuits. Even though NNs are naturally a model of real computation, it is worth to have a look at their computational power with respect to Boolean inputs. Interestingly, this makes understanding the computational power of NNs much easier. It is easy to see that ReLU NNs can directly simulate AND-, OR-, and NOT-gates, and thus every Boolean circuit [44]. Hence, in Boolean arithmetics, every problem in P can be solved with polynomial-size NNs. However, requiring the networks to solve a problem for all possible realvalued inputs seems to be much stronger. Consequently, the class of functions representable with polynomial-size NNs is much less understood than in Boolean  
   
  192  
   
  C. Hertrich and L. Sering  
   
  arithmetics. Our results suggest that rethinking and forbidding basic algorithmic paradigms (like comparison-based branchings) can help towards improving this understanding. Arithmetic Circuits. As a circuit model with real-valued computation, ReLU networks are naturally closely related to arithmetic circuits. Just like NNs, arithmetic circuits are computational graphs in which each node computes some arithmetic expression (traditionally addition or multiplication) from the outputs of all its predecessors. Arithmetic circuits are well-studied objects in complexity theory [56]. Closer to ReLU NNs, there is a special kind of arithmetic circuits called tropical circuits [33]. In contrast to ordinary arithmetic circuits, they contain maximum (or minimum) gates instead of sum gates and sum gates instead of product gates. Thus, they are arithmetic circuits in the max-plus algebra. A tropical circuit can be simulated by an NN of roughly the same size since NNs can compute maxima and sums. However, neural networks are strictly more powerful than tropical circuits for two reasons: they can realize subtractions (that is, tropical division) by using negative weights and scalar multiplication (tropical exponentiation) with any real number. Thus, lower bounds on the size of tropical circuits do not apply to NNs. A particular example with an exponential gap between NNs and tropical circuits is the computation of the value of a minimum spanning tree. By Jukna and Seiwert [34], no polynomial-size tropical circuit can do this. However, Theorem 1 shows that NNs of cubic size (in the number of nodes of the input graph) are suÔ¨Écient for this task. Parallel Computation. Neural networks are naturally a model of parallel computation by performing all operations within one layer at the same time. Without going into detail here, the depth of an NN is related to the running time of a parallel algorithm, its width is related to the required number of processing units, and its size to the total amount of work conducted by the algorithm. One takeaway from this perspective is that, although the result by Arora et al. [4] guarantees that logarithmic depth should be suÔ¨Écient to compute a maximum Ô¨Çow, this would probably require superpolynomial width and size. The reason is that the Maximum Flow Problem is P-complete [22,23], meaning that it probably cannot be eÔ¨Éciently parallelized. 1.3  
   
  Further Related Work  
   
  Using NNs to solve optimization problems started with so-called HopÔ¨Åeld networks in the 1980s [30,35,57], which has also been specialized to the Maximum Flow Problem [2,14,45]. However, the NNs used in these works are conceptually very diÔ¨Äerent from modern feedforward NNs that are considered in this paper. In recent years interactions between NNs and CO have regained a lot of attention in the literature [7], for example, for boosting MIP solvers [42] and solving speciÔ¨Åc CO problems [6,16,37,38,47,60]. These approaches usually are of heuristic nature without quality or running time guarantees. Concerning the expressivity of ReLU neural networks, various tradeoÔ¨Äs between depth and width of NNs [4,15,25,27,41,46,51,53,58,59,62] and  
   
  Poly-Size ReLU Neural Networks for Maximum Flow Computation  
   
  193  
   
  approaches to count and bound the number of linear regions of a ReLU NN [26,43,50,51,54] have been found. NNs have been studied from a circuit complexity point of view before [5,49,55]. However, these works focus on Boolean circuit complexity of NNs with sigmoid or threshold activation functions. We are not aware of previous work investigating the computational power of ReLU NNs as arithmetic circuits operating on the real numbers. For an introduction to classical minimum spanning tree and maximum Ô¨Çow algorithms, we refer to textbooks [1,39,61]. The asymptotically fastest known combinatorial maximum Ô¨Çow algorithm due to Orlin [48] runs in O(nm) time for n nodes and m arcs. Recently, almost linear, weakly polynomial algorithms based on interior point methods have been developed [10]. However, polynomialsize NNs necessarily correspond to strongly polynomial algorithms.  
   
  2  
   
  Algorithms and Proof Overview  
   
  In this section we provide an intuitive overview of how we prove our results. The detailed proofs are deferred to the full version due to space constraints. Max-AÔ¨Éne Arithmetic Programs. For the purpose of algorithmic investigations of ReLU NNs, we introduce the pseudo-code language Max-AÔ¨Éne Arithmetic Programs (MAAPs). A MAAP operates on real-valued variables. The only operations allowed in a MAAP are computing maxima and aÔ¨Éne transformations of variables as well as parallel and sequential for loops with a Ô¨Åxed 2 number of iterations. In particular, no if branchings are allowed. With a MAAP A, we associate three complexity measures d(A), w(A), and s(A), which can easily be calculated from a MAAP‚Äôs description. The intuition behind these measures is that they correspond (up to constant factors) to the depth, width, and size of an NN computing the same function as the MAAP does. We formalize this intuition by proving the following proposition, which is similar to the transformation of circuits into straight-line programs in Boolean or arithmetic circuit complexity. Proposition 3. For a function f : Rn ‚Üí Rm the following is true. (i) If f can be computed by a MAAP A, then it can also be computed by an NN with depth d(A) + 1, width w(A), and size s(A). (ii) If f can be computed by an NN with depth d + 1, width w, and size s, then it can also be computed by a MAAP A with d(A) = d, w(A) = 2w, and s(A) = 4s. The proof of the proposition works by providing explicit constructions to convert a MAAP into an NN (part (i)), and vice versa (part (ii)) while taking care that the diÔ¨Äerent complexity measures translate respectively. The takeaway from this exercise is that for proving that NNs of a certain size can compute certain functions, it is suÔ¨Écient to develop an algorithm in the 2  
   
  In this context, Ô¨Åxed means that the number of iterations cannot depend on the speciÔ¨Åc instance. It can still depend on the size of the instance (e.g., the size of the graph in case of the two CO problems considered in this paper).  
   
  194  
   
  C. Hertrich and L. Sering  
   
  Algorithm 1: MSTn : Compute the value of a minimum spanning tree for the complete graph on n ‚â• 3 vertices. Input: Edge weights (xij )1‚â§i 0. However, we cannot recover the shortest s-t path with capacity ak,s . Therefore, in general, Ô¨Çow will not be sent along a single path and the value of the Ô¨Çow output by FindAugmentingFlowk might be strictly less than ak,s . After computing the ai,v values, FindAugmentingFlowk greedily pushes Ô¨Çow from s towards t, using a lexicographic selection rule to pick the next arc to push Ô¨Çow on (line 12 to 22). On the high level, this is similar to the preÔ¨Çow-push algorithm, but using the ai,v values that encode the shortest path distance information implicitly. This may leave some nodes with excess Ô¨Çow; a Ô¨Ånal cleanup phase (line 23 to 29) is needed to send the remaining Ô¨Çow back to the source s. An example for the FindAugmentingFlowk -subroutine is given in Fig. 3. We emphasize again that, although the description of the subroutine in the example in Fig. 3 seems to rely heavily on the distance of a node to t, this information is calculated and used only in an implicit way via the precomputed ai,v values. This way, we are able to implement the subroutine without the usage of comparisonbased branchings. The proof of correctness for our algorithm consists of two main steps. The Ô¨Årst step is the analysis of the subroutine. This involves carefully showing that the returned Ô¨Çow indeed satisÔ¨Åes Ô¨Çow conservation, is feasible with respect to the residual capacities, uses only arcs that lie on a s-t-path of length exactly k in the  
   
  Poly-Size ReLU Neural Networks for Maximum Flow Computation  
   
  Algorithm 3: FindAugmentingFlowk for a Ô¨Åxed graph G = (V, E) and a Ô¨Åxed length k. Input: Residual capacities (ce )e‚ààE . 1 2 3 4 5 6  
   
  7 8 9 10 11  
   
  12 13 14 15 16 17 18 19 20 21 22  
   
  23 24 25 26 27 28 29 30 31 32  
   
  // Initializing:  do parallel for each vw ‚àà E zvw ‚Üê 0 // flow in residual network zwv ‚Üê 0 for each (i, v) ‚àà [k] √ó (V \ { t }) do parallel Yvi ‚Üê 0 // excessive flow at v in iteration i (from k to 1) ai,v ‚Üê 0 // initialize fattest path values // Determining the fattest path values: for each v ‚àà Nt‚àí do parallel a1,v ‚Üê cvt for i = 2, 3, . . . , k do for each v ‚àà V \ { t } do parallel ai,v ‚Üê maxw‚ààNv+ \{ t } min { ai‚àí1,w , cvw } // Pushing flow of value ak,s from s to t: Ysk ‚Üê ak,s // excessive flow at s for i = k, k ‚àí 1, . . . , 2 do for v ‚àà V \ { t } in index order do for w ‚àà Nv+ \ { t } in index order do // Push flow out of v and into w: f ‚Üê min { Yvi , cvw , ai‚àí1,w ‚àí Ywi‚àí1 } // value we can push over vw such that this flow can still arrive at t zvw ‚Üê zvw + f Yvi ‚Üê Yvi ‚àí f Ywi‚àí1 ‚Üê Ywi‚àí1 + f for each v ‚àà Nt‚àí do parallel // Push flow out of v and into t: zvt ‚Üê Yv1 Yv1 ‚Üê 0 // Clean-up by bounding: for i = 2, 3, . . . , k ‚àí 1 do for w ‚àà V \ { t } in reverse index order do for v ‚àà Nw‚àí \ { t } in reverse index order do b ‚Üê min { Ywi , zvw } // value we can push backwards along vw zvw ‚Üê zvw ‚àí b Ywi ‚Üê Ywi ‚àí b Yvi+1 ‚Üê Yvi+1 + b  do parallel for each uv ‚àà E yvw ‚Üê zvw ‚àí zwv return (ye )e‚ààE  
   
  197  
   
  198  
   
  C. Hertrich and L. Sering  
   
  Fig. 3. Example of the FindAugmentingFlowk subroutine for k = 4. The edge labels in the top Ô¨Ågure are the residual capacity bounds in the current iteration. The Ô¨Årst step is to compute the fattest path values ai,v , which are depicted as node labels in the top Ô¨Ågure. The values Yvi always denote the excessive Ô¨Çow of a vertex v with distance i from the sink. All values that are not displayed are zero. At s, we initialize Ys4 = a4,s = 6. Then, excessive Ô¨Çow is pushed greedily towards the sink, as shown in the four Ô¨Ågures in the middle. While doing so, we ensure that at each vertex the arriving Ô¨Çow does not exceed its value ai,v . For this reason, Ô¨Çow can get stuck, as it happens at v4 in this example. Therefore, in a Ô¨Ånal cleanup phase, depicted in the two bottom Ô¨Ågures, we push Ô¨Çow back to the source s. Observe that the result is an s-t-Ô¨Çow that is feasible with respect to the residual capacities, uses only paths of length k = 4, and saturates the arc v6 t.  
   
  Poly-Size ReLU Neural Networks for Maximum Flow Computation  
   
  199  
   
  residual network, and most importantly, if such a path exists, it saturates at least one arc. This last property can be shown using the lexicographic selection rule to pick the next arc to push Ô¨Çow on. Note that, in general, the subroutine neither returns a single path (as in the Edmonds-Karp algorithm [13]), nor a blocking Ô¨Çow (as in the Dinic algorithm [12]). The second main step is to show that, nevertheless, the properties of the subroutine are suÔ¨Écient to ensure that the distance from s to t in the residual network increases at least every m iterations, such that we terminate with a maximum Ô¨Çow after nm iterations. With the correctness of the whole MAAP at hand, Theorem 2 follows by simply counting the complexity measures d(A), w(A), and s(A), and applying Proposition 3. Acknowledgements. A large portion of this work was completed while both authors were aÔ¨Éliated with TU Berlin. We thank Max Klimm, Jennifer Manke, Arturo Merino, Martin Skutella, and L√°szl√≥ V√©gh for many inspiring and fruitful discussions and valuable comments. Christoph Hertrich acknowledges funding by DFG-GRK 2434 Facets of Complexity and by the European Research Council (ERC) under the European Union‚Äôs Horizon 2020 research and innovation programme (grant agreement ScaleOpt-757481). Leon Sering acknowledges funding by DFG Excellence Cluster MATH+ (EXC-2046/1, project ID: 390685689).  
   
  References 1. Ahuja, R.K., Magnanti, T.L., Orlin, J.B.: Network Flows: Theory, Algorithms, and Applications. Prentice Hall, Upper Saddle River, New Jersey, USA (1993) 2. Ali, M.M., Kamoun, F.: A neural network approach to the maximum Ô¨Çow problem. In: IEEE Global Telecommunications Conference GLOBECOM‚Äô91: Countdown to the New Millennium. Conference Record, pp. 130‚Äì134 (1991) 3. Anthony, M., Bartlett, P.L.: Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge (1999) 4. Arora, R., Basu, A., Mianjy, P., Mukherjee, A.: Understanding deep neural networks with rectiÔ¨Åed linear units. In: International Conference on Learning Representations (2018) 5. Beiu, V., Taylor, J.G.: On the circuit complexity of sigmoid feedforward neural networks. Neural Netw. 9(7), 1155‚Äì1171 (1996) 6. Bello, I., Pham, H., Le, Q.V., Norouzi, M., Bengio, S.: Neural combinatorial optimization with reinforcement learning. arXiv:1611.09940 (2016) 7. Bengio, Y., Lodi, A., Prouvost, A.: Machine learning for combinatorial optimization: a methodological tour d‚Äôhorizon. arXiv:1811.06128 (2018) 8. Berner, J., Grohs, P., Kutyniok, G., Petersen, P.: The modern mathematics of deep learning. arXiv:2105.04026 (2021) 9. Bertschinger, D., Hertrich, C., Jungeblut, P., Miltzow, T., Weber, S.: Training fully connected neural networks is ‚àÉR-complete. arXiv:2204.01368 (2022) 10. Chen, L., Kyng, R., Liu, Y.P., Peng, R., Gutenberg, M.P., Sachdeva, S.: Maximum Ô¨Çow and minimum-cost Ô¨Çow in almost-linear time. arXiv:2203.00671 (2022) 11. Cybenko, G.: Approximation by superpositions of a sigmoidal function. Math. Control Signals Syst. 2(4), 303‚Äì314 (1989)  
   
  200  
   
  C. Hertrich and L. Sering  
   
  12. Dinic, E.A.: Algorithm for solution of a problem of maximum Ô¨Çow in a network with power estimation. Soviet Math. Doklady 11, 1277‚Äì1280 (1970) 13. Edmonds, J., Karp, R.M.: Theoretical improvements in algorithmic eÔ¨Éciency for network Ô¨Çow problems. J. ACM 19(2), 248‚Äì264 (1972) 14. EÔ¨Äati, S., Ranjbar, M.: Neural network models for solving the maximum Ô¨Çow problem. Appl. Appl. Math. 3(3), 149‚Äì162 (2008) 15. Eldan, R., Shamir, O.: The power of depth for feedforward neural networks. In: Conference on Learning Theory, pp. 907‚Äì940 (2016) 16. Emami, P., Ranka, S.: Learning permutations with Sinkhorn policy gradient. arXiv:1805.07010 (2018) 17. Fomin, S., Grigoriev, D., Koshevoy, G.: Subtraction-free complexity, cluster transformations, and spanning trees. Found. Comput. Math. 16(1), 1‚Äì31 (2016) 18. Froese, V., Hertrich, C., Niedermeier, R.: The computational complexity of ReLU network training parameterized by data dimensionality. arXiv:2105.08675 (2021) 19. Glorot, X., Bordes, A., Bengio, Y.: Deep sparse rectiÔ¨Åer neural networks. In: 14th International Conference on ArtiÔ¨Åcial Intelligence and Statistics, pp. 315‚Äì323 (2011) 20. Goel, S., Klivans, A.R., Manurangsi, P., Reichman, D.: Tight hardness results for training depth-2 ReLU networks. In: 12th Innovations in Theoretical Computer Science Conference (ITCS ‚Äô21) (2021) 21. Goldberg, A.V., Tarjan, R.E.: A new approach to the maximum-Ô¨Çow problem. J. ACM (JACM) 35(4), 921‚Äì940 (1988) 22. Goldschlager, L.M., Shaw, R.A., Staples, J.: The maximum Ô¨Çow problem is log space complete for P. Theoretical Comput. Sci. 21(1), 105‚Äì111 (1982) 23. Greenlaw, R., Hoover, H.J., Ruzzo, W.L.: Limits to parallel computation: Pcompleteness theory. Oxford University Press, Oxford (1995) 24. Haase, C.A., Hertrich, C., Loho, G.: Lower bounds on the depth of integral ReLU neural networks via lattice polytopes. In: International Conference on Learning Representations (ICLR) (2023) 25. Hanin, B.: Universal function approximation by deep neural nets with bounded width and ReLU activations. Mathematics 7(10), 992 (2019) 26. Hanin, B., Rolnick, D.: Complexity of linear regions in deep networks. In: International Conference on Machine Learning (2019) 27. Hanin, B., Sellke, M.: Approximating continuous functions by ReLU nets of minimal width. arXiv:1710.11278 (2017) 28. Hertrich, C., Basu, A., Di Summa, M., Skutella, M.: Towards lower bounds on the depth of ReLU neural networks. Adv. Neural. Inf. Process. Syst. 34, 3336‚Äì3348 (2021) 29. Hertrich, C., Skutella, M.: Provably good solutions to the knapsack problem via neural networks of bounded size. In: AAAI Conference on ArtiÔ¨Åcial Intelligence (2021) 30. HopÔ¨Åeld, J.J., Tank, D.W.: Neural computation of decisions in optimization problems. Biol. Cybernet. 52(3), 141‚Äì152 (1985) 31. Hornik, K.: Approximation capabilities of multilayer feedforward networks. Neural Netw. 4(2), 251‚Äì257 (1991) 32. Jerrum, M., Snir, M.: Some exact complexity results for straight-line computations over semirings. J. ACM (JACM) 29(3), 874‚Äì897 (1982) 33. Jukna, S.: Lower bounds for tropical circuits and dynamic programs. Theory Comput. Syst. 57(1), 160‚Äì194 (2015) 34. Jukna, S., Seiwert, H.: Greedy can beat pure dynamic programming. Inf. Process. Lett. 142, 90‚Äì95 (2019)  
   
  Poly-Size ReLU Neural Networks for Maximum Flow Computation  
   
  201  
   
  35. Kennedy, M.P., Chua, L.O.: Neural networks for nonlinear programming. IEEE Trans. Circuits Syst. 35(5), 554‚Äì562 (1988) 36. Khalife, S., Basu, A.: Neural networks with linear threshold activations: structure and algorithms. In: International Conference on Integer Programming and Combinatorial Optimization, pp. 347‚Äì360. Springer, Cham (2022). https://doi.org/10. 1007/978-3-031-06901-7_26 37. Khalil, E., Dai, H., Zhang, Y., Dilkina, B., Song, L.: Learning combinatorial optimization algorithms over graphs. In: Advances in Neural Information Processing Systems 30 (2017) 38. Kool, W., van Hoof, H., Welling, M.: Attention, learn to solve routing problems! In: International Conference on Learning Representations (2019) 39. Korte, B., Vygen, J.: Combinatorial Optimization: Theory and Algorithms, 4th edn. Springer, Heidelberg (2008). https://doi.org/10.1007/3-540-29297-7 40. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521, 436‚Äì444 (2015) 41. Liang, S., Srikant, R.: Why deep neural networks for function approximation? In: International Conference on Learning Representations (2017) 42. Lodi, A., Zarpellon, G.: On learning and branching: a survey. TOP 25(2), 207‚Äì236 (2017). https://doi.org/10.1007/s11750-017-0451-6 43. Montufar, G.F., Pascanu, R., Cho, K., Bengio, Y.: On the number of linear regions of deep neural networks. In: Advances in Neural Information Processing Systems, vol. 27 (2014) 44. Mukherjee, A., Basu, A.: Lower bounds over Boolean inputs for deep neural networks with ReLU gates. arXiv:1711.03073 (2017) 45. Nazemi, A., Omidi, F.: A capable neural network model for solving the maximum Ô¨Çow problem. J. Comput. Appl. Math. 236(14), 3498‚Äì3513 (2012) 46. Nguyen, Q., Mukkamala, M.C., Hein, M.: Neural networks should be wide enough to learn disconnected decision regions. In: International Conference on Machine Learning (2018) 47. Nowak, A., Villar, S., Bandeira, A.S., Bruna, J.: Revised Note on Learning Algorithms for Quadratic Assignment with Graph Neural Networks. arXiv:1706.07450 (2017) 48. Orlin, J.B.: Max Ô¨Çows in O(nm) time, or better. In: Proceedings of the Forty-Fifth Annual ACM Symposium on Theory of Computing (STOC ‚Äô13), pp. 765‚Äì774. Association for Computing Machinery (2013) 49. Parberry, I., Garey, M.R., Meyer, A.: Circuit Complexity and Neural Networks. MIT Press, Cambridge (1994) 50. Pascanu, R., Montufar, G., Bengio, Y.: On the number of inference regions of deep feed forward networks with piece-wise linear activations. In: International Conference on Learning Representations (2014) 51. Raghu, M., Poole, B., Kleinberg, J., Ganguli, S., Dickstein, J.S.: On the expressive power of deep neural networks. In: International Conference on Machine Learning (2017) 52. Rothvo√ü, T.: The matching polytope has exponential extension complexity. J. ACM (JACM) 64(6), 1‚Äì19 (2017) 53. Safran, I., Shamir, O.: Depth-width tradeoÔ¨Äs in approximating natural functions with neural networks. In: International Conference on Machine Learning (2017) 54. Serra, T., Tjandraatmadja, C., Ramalingam, S.: Bounding and counting linear regions of deep neural networks. In: International Conference on Machine Learning (2018) 55. Shawe-Taylor, J.S., Anthony, M.H., Kern, W.: Classes of feedforward neural networks and their circuit complexity. Neural Netw. 5(6), 971‚Äì977 (1992)  
   
  202  
   
  C. Hertrich and L. Sering  
   
  56. Shpilka, A., YehudayoÔ¨Ä, A.: Arithmetic circuits: a survey of recent results and open questions. Now Publishers Inc. (2010) 57. Smith, K.A.: Neural networks for combinatorial optimization: a review of more than a decade of research. INFORMS J. Comput. 11(1), 15‚Äì34 (1999) 58. Telgarsky, M.: Representation beneÔ¨Åts of deep feedforward networks. arXiv:1509.08101 (2015) 59. Telgarsky, M.: BeneÔ¨Åts of depth in neural networks. In: Conference on Learning Theory, pp. 1517‚Äì1539 (2016) 60. Vinyals, O., Fortunato, M., Jaitly, N.: Pointer networks. In: Advances in Neural Information Processing Systems, vol. 28 (2015) 61. Williamson, D.P.: Network Flow Algorithms. Cambridge University Press, Cambridge (2019) 62. Yarotsky, D.: Error bounds for approximations with deep ReLU networks. Neural Netw. 94, 103‚Äì114 (2017)  
   
  On the Correlation Gap of Matroids Edin Husiƒá1 , Zhuan Khye Koh2(B) , Georg Loho3 , and L√°szl√≥ A. V√©gh4 1  
   
  IDSIA, USI-SUPSI, Lugano, Switzerland [email protected]  2 Centrum Wiskunde & Informatica, Amsterdam, The Netherlands [email protected]  3 University of Twente, Enschede, The Netherlands [email protected]  4 London School of Economics and Political Science, London, UK [email protected]  Abstract. A set function can be extended to the unit cube in various ways; the correlation gap measures the ratio between two natural extensions. This quantity has been identiÔ¨Åed as the performance guarantee in a range of approximation algorithms and mechanism design settings. It is known that the correlation gap of a monotone submodular function is at least 1 ‚àí 1/e, and this is tight for simple matroid rank functions. We initiate a Ô¨Åne-grained study of the correlation gap of matroid rank functions. In particular, we present an improved lower bound on the correlation gap as parametrized by the rank and girth of the matroid. We also show that for any matroid, the correlation gap of its weighted rank function is minimized under uniform weights. Such improved lower bounds have direct applications for submodular maximization under matroid constraints, mechanism design, and contention resolution schemes.  
   
  1  
   
  Introduction  
   
  A continuous function h : [0, 1]E ‚Üí R+ is an extension of a set function f : 2E ‚Üí R+ if for every x ‚àà [0, 1]E , h(x) =EŒª [f (S)] where Œª is a probability distribution over 2E with marginals x, i.e., S:i‚ààS ŒªS = xi for all i ‚àà E. Note that this in particular implies f (S) = h(œáS ) for every S ‚äÜ E, where œáS denotes the 0-1 indicator vector of S. Two natural extensions are the following. The Ô¨Årst one corresponds to sam pling each i ‚àà E independently with probability xi , i.e., ŒªS = i‚ààS xi i‚ààS / (1 ‚àí xi ). Thus,    F (x) := f (S) xi (1 ‚àí xi ) . (1) S‚äÜE  
   
  i‚ààS  
   
  i‚ààS /  
   
  This is an extended abstract. The full version of the paper with all proofs is available on arXiv:2209.09896. This project has received funding from the European Research Council (ERC) under the European Union‚Äôs Horizon 2020 research and innovation programme (grant agreement no. 757481‚ÄìScaleOpt). Z. K. Koh‚ÄîThis work was done while the author was at the London School of Economics. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 203‚Äì216, 2023. https://doi.org/10.1007/978-3-031-32726-1_15  
   
  204  
   
  E. Husiƒá et al.  
   
  This is known as the multilinear extension in the context of submodular optimization, see [8]. The second extension corresponds to the probability distribution with maximum expectation: ‚é´ ‚éß ‚é¨ ‚é®   ŒªS f (S) : ŒªS = xi ‚àÄi ‚àà E, ŒªS = 1, Œª ‚â• 0 . (2) fÀÜ(x) := max ‚é≠ Œª ‚é© S‚äÜE  
   
  S‚äÜE:i‚ààS  
   
  S‚äÜE  
   
  Equivalently, fÀÜ(x) is the upper part of the convex hull of the graph of f ; we call it the concave extension following terminology of discrete convex analysis [20]. Agrawal, Ding, Saberi and Ye [2] introduced the correlation gap as the worst case ratio F (x) CG(f ) := min . (3) x‚àà[0,1]E fÀÜ(x) It bounds the maximum loss incurred in the expected value of f by ignoring correlations. This quantity plays a fundamental role in stochastic optimization [2, 22], mechanism design [7,18,28], prophet inequalities [11,15,24], and a variety of submodular optimization problems [3,12]. The focus of this paper is on weighted matroid rank functions. For a matroid M = (E, I) and a weight vector w ‚àà RE + , the corresponding weighted matroid rank function is given by rw (S) := max {w(T ) : T ‚äÜ S, T ‚àà I} . It is monotone nondecreasing and submodular. Recall that a function f : 2E ‚Üí R is monotone if f (X) ‚â§ f (Y ) for all X ‚äÜ Y ‚äÜ E, and submodular if f (X) + f (Y ) ‚â• f (X ‚à© Y ) + f (X ‚à™ Y ) for all X, Y ‚äÜ E. The correlation gap of a weighted matroid rank function has been identiÔ¨Åed as the performance guarantee in a range of approximation algorithms and mechanism design settings: Monotone Submodular Maximization. Calinescu et al. [8] considered the problem m of maximizing a sum of weighted matroid rank functions i=1 fi subject to a matroid constraint. Using an LP relaxation and pipage rounding [1], they gave a (1 ‚àí 1/e)-approximation algorithm. This was extended by Shioura [26] to the problem of maximizing a sum of monotone M  -concave functions [19]. In [9], a (1 ‚àí 1/e)-approximation algorithm was obtained for maximizing an arbitrary monotone submodular function subject to a matroid constraint. A fundamental special case of this model is the maximum coverage problem. Given m subsets Ei ‚äÜ E, the corresponding coverage function is deÔ¨Åned as this is a special case of maximizing f (S) = |{i ‚àà [m] : Ei ‚à© S = ‚àÖ}|. Note that  m a sum of matroid rank functions: f (S) = i=1 ri (S) where ri (S) is the rank function of a rank-1 uniform matroid with support Ei . Even for maximization under a cardinality constraint, there is no better than (1 ‚àí 1/e)-approximation for this problem unless P = N P (see Feige [16]). Recently, tight approximations have been established for the special case when the function values fi (S) are determined by the cardinality of the set S.  
   
  On the Correlation Gap of Matroids  
   
  205  
   
  Barman et al. [5] studied the maximum concave coverage problem: given a monotone concave function œï :  Z+ ‚Üí R+ and weights w ‚àà Rm + , the submodular funcm tion is deÔ¨Åned as f (S) = i=1 wi œï(|S ‚à© Ei |).1 The maximum coverage problem corresponds to œï(x) = min{1,  x}; on the other extreme, for œï(x) = x we get the trivial problem f (S) = j‚ààS |{i ‚àà [m] : j ‚àà Ei }|. In [5], they present a tight approximation guarantee for maximizing such an objective subject to a matroid constraint, parametrized by the Poisson curvature of the function œï. This generalizes previous work by Barman et al. [6] which considered œï(x) = min{, x} (for  > 1), motivated by the list decoding problem in coding theory. It also generalizes the work by Dudycz et al. [14] which considered geometrically dominated concave functions œï, motivated by approval voting rules such as Thiele rules, proportional approval voting, and p-geometric rules. In both cases, the obtained approximation guarantees improve over the 1 ‚àí 1/e factor. In the full version, we make the observation that the algorithm of Calinescu et al. [8] and Shioura [26] actually has an approximation ratio of mini‚àà[m] CG(fi ). We also prove that the Poisson curvature of œï is equal to the correlation gap of the functions œï(|S ‚à© Ei |). Hence, the approximation guarantees in [5,6,14] are in fact correlation gap bounds, and they can be obtained via a single uniÔ¨Åed algorithm, i.e., the one by Calinescu et al. [8] and Shioura [26]. In particular, the result of Barman et al. [6] which concerned œï(x) = min{, x} (for  > 1) boils down to the analysis of uniform matroid correlation gaps. Sequential Posted-Price Mechanisms. Following Yan [28], consider a seller with a set of identical services (or goods), and a set E of unit-demand agents. Each agent i ‚àà E has a private valuation vi for winning the service, and 0 otherwise, where vi is drawn independently from a known distribution Fi with positive smooth density function over [0, L] for some large L. The seller can only service certain subsets of the agents simultaneously; this is captured by a matroid M = (E, I) where I represents the feasible subsets. Mechanisms like Myerson‚Äôs mechanism [21] or the VCG mechanism [13,17, 27] have optimal revenue or welfare guarantees, but suÔ¨Äer from complicated formats [4] or high computational overhead [23]. Hence, simple mechanisms are often favoured in practice, such as sequential posted-price mechanisms (SPM), in which the seller makes take-it-or-leave-it price oÔ¨Äers to agents one by one. Yan [28] showed that the greedy SPM of Chawla et al. [10] achieves an approximation CG(rw ), where rw is the weighted rank function of M with ratio of inf w‚ààRE + weights w. Contention Resolution Schemes. Chekuri et al. [12] introduced contention resolution (CR) schemes as a tool for maximizing a (not necessarily monotone) submodular function f subject to downward-closed constraints, such as matroid constraints, knapsack constraints, and their intersections. Let M = (E, I) be a matroid imposing one of these constraints. Given a fractional solution x with 1  
   
  We note that such functions are exactly the one-dimensional monotone M  -concave functions fi : Z+ ‚Üí R+ .  
   
  206  
   
  E. Husiƒá et al.  
   
  multilinear extension value F (x), their CR scheme randomly rounds x to an CG(rw )F (x). Here, integral solution œáS where S ‚àà I such that E[œáS ] ‚â• inf w‚ààRE + rw is again the weighted rank function of M with weights w. Motivated by the signiÔ¨Åcance of the correlation gap in algorithmic applications, we study the correlation gap of weighted matroid rank functions. It is well-known that CG(f ) ‚â• 1 ‚àí 1/e for every monotone submodular function f [8]. Moreover, the extreme case 1 ‚àí 1/e is already achieved by the rank function of a rank-1 uniform matroid as |E| ‚Üí ‚àû. More generally, the rank function of a rank- uniform matroid has correlation gap 1 ‚àí e‚àí  /! ‚â• 1 ‚àí 1/e [6,28]. Other than for uniform matroids, we are not aware of any previous work that gave better than 1 ‚àí 1/e bounds on the correlation gap of speciÔ¨Åc matroids. First, we show that among all weighted rank functions of a matroid, the smallest correlation gap is realized by its (unweighted) rank function. Theorem 1. For any matroid M = (E, I) with rank function r = r1 , inf CG(rw ) = CG(r).  
   
  w‚ààRE +  
   
  For the purpose of lower bounding CG(rw ), Theorem 1 allows us to ignore the weights w and just focus on the matroid M. As an application, to bound the approximation ratio of sequential posted-price mechanisms as in [28], it suÔ¨Éces to focus on the underlying matroid. We remark that M can be assumed to be connected, that is, it cannot be written as a direct sum of at least two nonempty m matroids. Otherwise, r = i=1 ri for matroid rank functions ri with disjoint supports, and so CG(r) = mini‚àà[m] CG(ri ). For example, the correlation gap of a partition matroid is equal to the smallest correlation gap of its parts (uniform matroids). Our goal is to identify the parameters of a matroid which govern its correlation gap. A natural candidate is the rank r(E). However, as pointed out by Yan [28], there exist matroids with arbitrarily high rank whose correlation gap is still 1 ‚àí 1/e, e.g., partition matroids with rank-1 parts. The 1 ‚àí e‚àí  /! bound for uniform matroids [6,28] is suggestive of girth as another potential candidate. Recall that the girth of a matroid is the smallest size of a dependent set. On its own, a large girth does not guarantee improved correlation gap bounds: in the full version, we show that for any Œ≥ ‚àà N, there exist matroids with girth Œ≥ whose correlation gaps are arbitrarily close to 1 ‚àí 1/e. It turns out that the correlation gap heavily depends on the relative values of the rank and girth of the matroid. Our second result is an improved lower bound on the correlation gap as a function of these two parameters. Theorem 2. Let M = (E, I) be a loopless matroid with rank function r, rank r(E) = œÅ, and girth Œ≥. Then, Œ≥‚àí2  
   
     i œÅ œÅ 1 1 e‚àíœÅ  (Œ≥ ‚àí 1 ‚àí i) (e ‚àí 1)i ‚àí ‚â•1‚àí . CG(r) ‚â• 1 ‚àí + i e œÅ i! e i=0 Furthermore, the last inequality is strict whenever Œ≥ > 2.  
   
  On the Correlation Gap of Matroids  
   
  207  
   
  Figure 1 illustrates the behaviour of the expression in Theorem 2. For any Ô¨Åxed girth Œ≥, it is monotone decreasing in œÅ. On the other hand, for any Ô¨Åxed rank œÅ, it is monotone increasing in Œ≥. In the full version, we also give complementing albeit non-tight upper bounds that behave similarly with respect to these parameters. When œÅ = Œ≥ ‚àí 1, our lower bound simpliÔ¨Åes to 1 ‚àí e‚àíœÅ œÅœÅ /œÅ!, i.e., the correlation gap of a rank-œÅ uniform matroid (proven in the full version).  
   
  Fig. 1. Our correlation gap bound as a function of the rank œÅ and girth Œ≥ separately.  
   
  The rank and girth have meaningful interpretations in the aforementioned applications. For instance, the problem of maximizing a sum of weighted consider m matroid rank functions i=1 fi under a matroid constraint (E, J ). For every i ‚àà [m], let Mi be the matroid of fi . In game-theoretic contexts, each fi usually represents the utility function of agent i. Thus, our goal is to select a bundle of items S ‚àà J which maximizes the total welfare. If Mi has girth Œ≥ and rank œÅ, this means that agent i is interested in Œ≥ ‚àí1 ‚â§ k ‚â§ œÅ items with positive weights. The special case œÅ = Œ≥ ‚àí 1 (uniform matroids) has already found applications in list decoding [6] and approval voting [14]. On the other hand, for sequential posted-price mechanisms, if the underlying matroid M has girth Œ≥ and rank œÅ, this means that the seller can service Œ≥ ‚àí 1 ‚â§ k ‚â§ œÅ agents simultaneously. To the best of our knowledge, our results give the Ô¨Årst improvement over the (1 ‚àí 1/e) bound on the correlation gap of general matroids. We hope that our paper will motivate further studies into more reÔ¨Åned correlation gap bounds, exploring the dependence on further matroid parameters, as well as obtaining tight bounds for special matroid classes. 1.1  
   
  Our Techniques  
   
  We now give a high-level overview of the proofs of Theorem 1 and Theorem 2. Weighted Rank Functions. The Ô¨Årst step in proving both theorems is to deduce structural properties of the points which realize the correlation gap. In Theorem 4, we show that such a point x can be found in the independent set polytope  
   
  208  
   
  E. Husiƒá et al.  
   
  P. This implies that rÀÜw (x) = w x for any weights w ‚àà RE + . Moreover, we deduce that x(E) is integral. To prove Theorem 1, we Ô¨Åx a matroid M and derive a contradiction for a nonuniform weighting. More precisely, we consider a weighting w ‚àà RE + and a point rw (x‚àó ) < CG(r). By the above, we x‚àó ‚àà [0, 1]E which give a smaller ratio Rw (x‚àó )/ÀÜ rw (x‚àó ) = Rw (x‚àó )/w x‚àó . We pick w such that can use the simpler form Rw (x‚àó )/ÀÜ it has the smallest number of diÔ¨Äerent values. If the number of distinct values is at least 2, then we derive a contradiction by showing that a better solution can be obtained by increasing the weights in a carefully chosen value class until they coincide with the next smallest value. The greedy maximization property of matroids is essential for this argument. Uniform Matroids. Before outlining our proof of Theorem 2, let us revisit the correlation gap of uniform matroids. Let M = (E, I) be a uniform matroid on n elements with rank œÅ = r(E). If œÅ = 1, then it is easy to verify that the symmetric point x = (1/n) ¬∑ 1 realizes the correlation gap 1 ‚àí 1/e. Since x lies in the independent set polytope, we have rÀÜ(x) = 1 x = 1. If one samples each i ‚àà E with probability 1/n, the probability of selecting at least one element is R(x) = 1 ‚àí (1 ‚àí 1/n)n . Thus, CG(r) = 1 ‚àí (1 ‚àí 1/n)n , which converges to 1 ‚àí 1/e as n ‚Üí ‚àû. More generally, for œÅ ‚â• 1, Yan [28] showed that the symmetric point x = (œÅ/n) ¬∑ 1 similarly realizes the correlation gap 1 ‚àí e‚àíœÅ œÅœÅ /œÅ!. Poisson Clock Analysis. To obtain the (1 ‚àí 1/e) lower bound on the correlation gap of a monotone submodular function, Calinescu et al. [8] introduced an elegant probabilistic analysis. Instead of sampling each i ‚àà E with probability xi , they consider n independent Poisson clocks of rate xi that are active during the time interval [0, 1]. Every clock may send at most one signal from a Poisson process. Let Q(t) be the set of elements whose signal was sent between time 0 and t; the output is Q(1). It is easy to see that E[f (Q(1))] ‚â§ F (x). In [8], they show that the derivative of E[f (Q(t))] can be lower bounded as f ‚àó (x) ‚àí E[f (Q(t))] for every t ‚àà [0, 1], where   ‚àó fS (i)xi (4) f (x) := min f (S) + S‚äÜE  
   
  i‚ààE  
   
  is an extension of f such that f ‚àó ‚â• fÀÜ. The bound E[f (Q(1))] ‚â• (1 ‚àí 1/e)f ‚àó (x) is obtained by solving a diÔ¨Äerential inequality. Thus, F (x) ‚â• E[f (Q(1))] ‚â• (1 ‚àí 1/e)f ‚àó (x) ‚â• (1 ‚àí 1/e)fÀÜ(x) follows. A Two Stage Approach. If f is a matroid rank function, then f ‚àó = fÀÜ (see Theorem 3). Still, the factor (1 ‚àí 1/e) in the analysis of [8] cannot be improved: for an integer x ‚àà P, we lose a factor (1‚àí1/e) due to E[f (Q(1))] = (1‚àí1/e)F (x), even though the extensions coincide: F (x) = fÀÜ(x). Our analysis in Sect. 4 proceeds in two stages. Let M = (E, I) be a matroid with rank œÅ and girth Œ≥. The basic idea is that up to sets of size Œ≥ ‚àí 1, our  
   
  On the Correlation Gap of Matroids  
   
  209  
   
  matroid ‚Äòlooks like‚Äô a uniform matroid. Since the correlation gap of uniform matroids is well-understood, we Ô¨Årst extract a uniform matroid of rank Œ≥ ‚àí 1 from our matroid, and then analyze the contribution from the remaining part separately. More precisely, we decompose the rank function as r = g + h, where g(S) = min{|S|, } is the rank function of a uniform matroid of rank  = Œ≥ ‚àí 1. Note that the residual function h := f ‚àí g is not submodular in general, as h(S) = 0 for all |S| ‚â§ . We will lower bound the multilinear extensions G(x) and H(x) separately. As g is the rank function of a uniform matroid, similarly as above we can derive a tight lower bound on G in terms of its rank  = Œ≥ ‚àí 1. Bounding H(x) is based on a Poisson clock analysis as in [8], but is signiÔ¨Åcantly more involved. Due to the monotonicity of h, directly applying the result in [8] would yield E[h(Q(1)] ‚â• (1 ‚àí 1/e)h‚àó (x). However, h‚àó (x) = 0 whenever M is loopless ( ‚â• 1), as h(‚àÖ) = 0 and h({i}) = 0 for all i ‚àà E. So, the argument of [8] directly only leads to the trivial E[h(Q(1))] ‚â• 0. Nevertheless, one can still show that, conditioned on the event |Q(t)| ‚â• , the derivative of E[H(Q(t))] is at least r‚àó (x) ‚àí  ‚àí E[H(Q(t))]. Let T ‚â• 0 be the earliest time such that |Q(T )| ‚â• , which we call the activation time of Q. Then, solving a diÔ¨Äerential inequality produces E[h(Q(1))|T = t] ‚â• (1 ‚àí e‚àí(1‚àít) )(r‚àó (x) ‚àí ) for all t ‚â§ 1. To lower bound E[h(Q(1))], it is left to take the expectation over all possible 1 ¬Ø activation times T ‚àà [0, 1]. Let h(x) = (r‚àó (x) ‚àí ) 0 Pr[T = t](1 ‚àí e‚àí(1‚àít) )dt ¬Ø be the resulting expression. We prove that h(x) is concave in each direction ei ‚àí ej for i, j ‚àà E. This allows us to round x to an integer x ‚àà [0, 1]E such ¬Ø  ) ‚â§ h(x); ¬Ø recall that x(E) ‚àà Z by Theorem 4. that x (E) = x(E) and h(x ¬Ø After substantial simpliÔ¨Åcation of h(x ), we arrive at the formula in Theorem 2, except that œÅ is replaced by x(E). So, the rounding procedure eÔ¨Äectively shifts the dependency of the lower bound from the value of x to the value of x(E). Since x(E) ‚â§ œÅ by Theorem 4, the Ô¨Ånal step is to prove that the formula in Theorem 2 is monotone decreasing in œÅ.  
   
  2  
   
  Preliminaries  
   
  We denote Z+ and R+ as the integers and nonnegative reals  set of nonnegative n! if n ‚â• k, and 0 otherwise. For a set S respectively. For n, k ‚àà Z+ , nk = k!(n‚àík)! and i ‚àà S, j ‚àà / S, we use the shorthand S ‚àí i = S \ {i} and S + j = S ‚à™ {j}. For a function f : 2E ‚Üí R, a set S ‚äÜ E and an element i ‚àà E, let fS (i) denote the marginal gain of adding  i to S, i.e., fS (i) := f (S + i) ‚àí f (S). For x ‚àà RE and S ‚äÜ E, we write x(S) = i‚ààS xi . Matroids. Let M = (E, I) be a matroid with rank function r : 2E ‚Üí Z+ . Its independent set polytope P(r) is the convex hull of incidence vectors of indepen dent sets in I. Equivalently, P(r) = x ‚àà RE + : x(S) ‚â§ r(S) ‚àÄS ‚äÜ E , as shown by Edmonds [25, Theorem 40.2]. We need another classical result by Edmonds [25, Theorem 40.3] on intersecting the independent set polytope with a box. Theorem 3. For a matroid rank function r : 2E ‚Üí Z+ and x ‚àà RE +, max{y(E) : y ‚àà P(r), y ‚â§ x} = min{r(T ) + x(E \ T ) : T ‚äÜ E}.  
   
  210  
   
  E. Husiƒá et al.  
   
  Probability Distributions. Let Bin(n, p) denote the binomial distribution with n trials and success probability p. Let Poi(Œª) denote the Poisson distribution with rate Œª. Recall that Pr(Poi(Œª) = k) = e‚àíŒª Œªk /k! for any k ‚àà Z+ . Definition 1. Given random variables X and Y , we say that X is at least Y in the concave order if for every concave function œï : R ‚Üí R, we have E[œï(X)] ‚â• E[œï(Y )] whenever the expectations exist. It is denoted as X ‚â•cv Y . Lemma 1 ([6]). For any n ‚àà N and p ‚àà [0, 1], we have Bin(n, p) ‚â•cv Poi(np). Properties of the Multilinear Extension. For a set function f : 2E ‚Üí R, let F : [0, 1]E ‚Üí R denote its multilinear extension. We will use the following well-known properties of F , see e.g. [9]. Proposition 1. If f is monotone, then F (x) ‚â• F (y) for all x ‚â• y. Proposition 2. If f is submodular, then for any x ‚àà [0, 1]E and i, j ‚àà E, the function œÜ(t) := F (x + t(ei ‚àí ej )) is convex.  
   
  3  
   
  Locating the Correlation Gap  
   
  In this section, given a weighted matroid rank function rw , we locate a point x‚àó ‚àà [0, 1]E on which the correlation gap CG(rw ) is realized, and derive some structural properties. Using this, we prove Theorem 1, i.e., the smallest correlation gap over all possible weightings is attained by uniform weights. We start with a more convenient characterization of the concave extension of rw . Lemma 2. Let M = (E, I) be a matroid with rank function r and weights E w ‚àà RE ÀÜw (x) = max{w y : y ‚àà P(r), y ‚â§ x}. + . For any x ‚àà [0, 1] , we have r Next, we show that x‚àó can be chosen to lie in the independent set polytope P(r); and that supp(x‚àó ) is a tight set w.r.t. x‚àó , meaning x‚àó (E) = r(supp(x‚àó )). Theorem 4. Let M = (E, I) be a matroid with rank function r. For any weights ‚àó ‚àó w ‚àà RE rw (x‚àó ) + \ {0}, there exists a point x ‚àà P(r) such that CG(rw ) = Rw (x )/ÀÜ ‚àó ‚àó and x (E) = r(supp(x )). Proof (of Theorem 1). For the purpose of contradiction, suppose that there exist ‚àó E such that Rw (x‚àó )/ÀÜ rw (x‚àó ) < CG(r). weights w ‚àà RE + and a point x ‚àà [0, 1] ‚àó According to Theorem 4, we may assume that x ‚àà P(r). Thus, rÀÜw (x‚àó ) = w x‚àó by Lemma 2. Let w1 > w2 > ¬∑ ¬∑ ¬∑ > wk ‚â• 0 denote the distinct values of w. For each i ‚àà [k], let Ei ‚äÜ E denote the set of elements with weight wi . Clearly, k ‚â• 2, as rw (x‚àó ) = w1 R(x‚àó )/(w1 x‚àó (E)) = R(x‚àó )/x‚àó (E) ‚â• CG(r). Let otherwise Rw (x‚àó )/ÀÜ us pick a counterexample with k minimal. First, we claim that wk > 0. Indeed, if the smallest weight is wk = 0, then Rw (x‚àó ) and rÀÜw (x‚àó ) remain unchanged after setting we ‚Üê w1 and x‚àóe ‚Üê 0 for all e ‚àà Ek ; this contradicts the minimal choice of k.  
   
  On the Correlation Gap of Matroids  
   
  211  
   
  Let X be the random variable for the set obtained by sampling every element e ‚àà E independently with probability x‚àóe . Let IX ‚äÜ X denote a maximum weight independent subset of X. Recall the well-known property of matroids that a maximum weight independent set can be selected greedily in decreasing order of the weights we . We Ô¨Åx an arbitrary tie-breaking rule inside each set Ei . The correlation gap of rw is given by  k  i Rw (x‚àó ) S‚äÜE Pr(X = S)rw (S) i=1 w e‚ààEi Pr(e ‚àà IX ) = . = k ‚àó  ‚àó i ‚àó rÀÜw (x ) w x i=1 w x (Ei )   
   
  Consider the set J := arg min  
   
  e‚ààEi  
   
  Pr(e ‚àà IX ) . i)  
   
  x‚àó (E  
   
  i‚àà[k]  
   
  We claim that J \ {1} = ‚àÖ. Suppose that J = {1} for a contradiction. DeÔ¨Åne the point x ‚àà P(r) as xe := x‚àóe if e ‚àà E1 , and xe := 0 otherwise. Then, we get a contradiction from w1 R(x ) CG(r) ‚â§ = rÀÜ(x )  
   
    
   
  e‚ààE1 Pr(e ‚àà w1 x‚àó (E1 )  
   
  IX )  
   
  k  
  0. Now, pick any index j ‚àà J \ {1}. Since wj > 0, we have  k  i wj e‚ààEj Pr(e ‚àà IX ) e‚ààEi Pr(e ‚àà IX ) i=1 w ‚â§ . k i ‚àó wj x‚àó (Ej ) i=1 w x (Ei ) So, we can increase wj to wj‚àí1 without increasing the correlation gap. That is, ¬Øe := wj‚àí1 if e ‚àà Ej and w ¬Øe := we otherwise, we get deÔ¨Åning w ¬Ø ‚àà RE + as w    i j‚àí1 Rw (x‚àó ) i=j w e‚ààEi Pr(e ‚àà IX ) + w e‚ààEj Pr(e ‚àà IX )  ‚â• ‚àó i ‚àó j‚àí1 ‚àó rÀÜw (x ) x (Ej ) i=j w x (Ei ) + w  Pr(X = S)r (S) Rw¬Ø (x) w ¬Ø S‚äÜE . = ‚â• min w ¬Ø  x‚àó ÀÜw¬Ø (x) x‚àà[0,1]E r The equality holds because for every S ‚äÜ E, IS remains a max-weight independent set with the new weights w. ¬Ø This contradicts the minimal choice of k.    
   
  4  
   
  Lower Bounding the Correlation Gap  
   
  This section is dedicated to the proof of Theorem 2. Let M = (E, I) be a matroid with rank function r, rank œÅ = r(E) and girth Œ≥ > 1. By Theorem 4, there exists a point x‚àó ‚àà P(r) such that CG(r) = R(x‚àó )/r(x‚àó ) and x‚àó (E) = r(supp(x‚àó )). For the sake of brevity, we denote  = Œ≥ ‚àí 1 and Œª = x‚àó (E) ‚àà Z+ . Note that if  
   
  212  
   
  E. Husiƒá et al.  
   
  Œª < , then supp(x‚àó ) is independent. As x‚àó (E) = r(supp(x‚àó )) = |supp(x‚àó )|, we have x‚àói = 1 for all i ‚àà supp(x‚àó ). Since x‚àó is integral, the correlation gap is 1 because R(x‚àó ) = rÀÜ(x‚àó ). Henceforth, we will assume that Œª ‚â• . From Lemma 2, we already know that rÀÜ(x‚àó ) = 1 x‚àó = Œª. So, it remains to analyze R(x‚àó ). Let g be the rank function of a rank- uniform matroid on ground set E, and deÔ¨Åne the function h := r ‚àíg ‚â• 0. By linearity of expectation, R(x‚àó ) = G(x‚àó ) + H(x‚àó ). We lower bound G(x‚àó ) and H(x‚àó ) separately. 4.1  
   
  Lower Bounding G(x‚àó )  
   
  As g is the rank function of a uniform matroid, the arguments of Yan [28] and Barman et al. [6] apply. In particular, since G is a symmetric polynomial, and convex along ei ‚àí ej for all i, j ‚àà E by Proposition 2, we have   
   
       Œª Œª ‚àó ¬∑ 1 = E min Bin n, G(x ) ‚â• G , ‚â• E [min {Poi(Œª), }] . (5) n n The last inequality follows from Lemma 1. The latter expectation is equal to  j‚àí1 k ‚àíŒª   ‚àí1     Œª e Œªk e‚àíŒª =‚àí . (6) Pr(Poi(Œª) ‚â• j) = ( ‚àí k) 1‚àí k! k! j=1 j=1 k=0  
   
  4.2  
   
  k=0  
   
  Lower Bounding H(x‚àó )  
   
  Our analysis of H(x‚àó ) uses the Poisson clock setup of Calinescu et al. [8], which incrementally builds a set Q(1) as follows. Each element i ‚àà E is assigned a Poisson clock of rate x‚àói . We start all the clocks simultaneously at time t = 0, and begin with the initial set Q(0) = ‚àÖ. For t ‚àà [0, 1], if the clock on an element i rings at time t, then we add i to our current set Q(t). We stop at time t = 1. ‚àó Clearly, Pr(i ‚àà Q(1)) = 1 ‚àí e‚àíxi ‚â§ x‚àói for all i ‚àà E. Since h is monotone, ‚àó Proposition 1 yields H(x‚àó ) ‚â• H(1 ‚àí e‚àíx ) = E[h(Q(1))], where equality is due to independence of the Poisson clocks. So, it suÔ¨Éces to lower bound E[h(Q(1))]. Let t ‚àà [0, 1) and consider an inÔ¨Ånitesimally small interval [t, t + dt]. For each i ‚àà E, the probability of adding i during this interval is Pr(Poi(x‚àói dt) ‚â• 1) = x‚àói dt + O(dt2 ). Note that the probability of adding two or more elements is also O(dt2 ). Since dt is very small, we can eÔ¨Äectively neglect all O(dt2 ) terms. Definition 2. We say that Q is activated at time T if |Q(t)| <  for all t < T and |Q(t)| ‚â•  for all t ‚â• T . We call T the activation time of Q. Let S ‚äÜ E where |S| ‚â•  and let t ‚â• t ‚â• 0. Conditioning on the events Q(t) = S and T = t , the expected increase of h(Q(t)) (up to O(dt2 ) terms) is  rS (i)x‚àói dt ‚â• (Œª ‚àí  ‚àí h(S))dt, E[h(Q(t + dt)) ‚àí h(Q(t))|Q(t) = S ‚àß T = t ] = i‚ààE  
   
  On the Correlation Gap of Matroids  
   
  213  
   
  where the inequality is due to   h(S) + rS (i)x‚àói = r(S) ‚àí  + rS (i)x‚àói ‚â• r‚àó (x‚àó ) ‚àí  = rÀÜ(x‚àó ) ‚àí  = Œª ‚àí . i‚ààE  
   
  i‚ààE  
   
  The inequality follows from the deÔ¨Ånition of r‚àó in (4), the second equality is by Theorem 3, while the third equality is due to Lemma 2 because x‚àó ‚àà P(r). Dividing by dt and taking expectation over S, we obtain for all t ‚â• t ‚â• 0, 1 E[h(Q(t + dt)) ‚àí h(Q(t))|T = t ] ‚â• Œª ‚àí  ‚àí E[h(Q(t))|T = t ]. dt  
   
  (7)  
   
  Let œÜ(t) := E[h(Q(t))|T = t ]. Then, (7) can be written as dœÜ dt ‚â• Œª ‚àí  ‚àí œÜ(t). t dœÜ To solve this diÔ¨Äerential inequality, let œà(t) := et œÜ(t) and consider dœà dt = e ( dt + t   œÜ(t)) ‚â• e (Œª ‚àí ). Since œà(t ) = œÜ(t ) = 0, we get  t  t  dœà ds ‚â• es (Œª ‚àí )ds = (et ‚àí et )(Œª ‚àí ) œà(t) = ds   t t   
   
  for all t ‚â• t . It follows that E[h(Q(t))|T = t ] = œÜ(t) = e‚àít œà(t) ‚â• (1 ‚àí et ‚àít )(Œª ‚àí ) for all t ‚â• t . In particular, at time t = 1, we have E[h(Q(1))|T = t ] ‚â•  (1 ‚àí et ‚àí1 )(Œª ‚àí ) for all t ‚â§ 1. By the law of total expectation,  1 E[h(Q(1))] ‚â• (Œª ‚àí ) Pr(T = t)(1 ‚àí et‚àí1 )dt. (8) 0  
   
  Now, the cumulative distribution function of T is given by    ‚àó ‚àó Pr(T ‚â§ t) = 1 ‚àí (1 ‚àí e‚àíxi t ) e‚àíxi t i‚ààS /  
   
  S‚äÜE: i‚ààS |S| . Then, using  (10) evaluates Œª‚àíi‚àí2 1 = Œª‚àí ‚àíi‚àí1 , we can simplify (10) as  
   
  Œª‚àíi‚àí1 1 Œª‚àíi‚àí1 ‚àíi‚àí1  
   
        ‚àí1 ! Œª Œª‚àíi‚àí2 1 e‚àí1 ‚àí e‚àí(Œª‚àíi) (Œª ‚àí ) 1 ‚àí (‚àí1)‚àíi + i ‚àíi‚àí1 e i=0      ‚àí1  Œª Œª‚àíi‚àí2 i 1 =Œª 1‚àí (‚àí1)‚àíi‚àí1 e . ‚àí  + e‚àíŒª i ‚àíi‚àí1 e i=0  
   
  (11)  
   
  The sum in (11) can be viewed as a univariate polynomial of degree  ‚àí 1 in Œ± ‚àà R for Œ± = e. Taking its Taylor expansion at Œ± = 1, we can rewrite (11) as   ‚àí1    Œª 1 Œª 1‚àí ( ‚àí i)(e ‚àí 1)i . ‚àí  + e‚àíŒª i e i=0 4.3  
   
  (12)  
   
  Putting Everything Together  
   
  We are Ô¨Ånally ready to lower bound the correlation gap of the matroid rank function r. Recall that we assumed Œª >  in the previous subsection. Combining the lower bounds (6) and (12) gives us CG(r) =  
   
     ‚àí1 Œª G(x‚àó ) + H(x‚àó ) 1 e‚àíŒª  Œªi i + = 1 ‚àí ( ‚àí i) (e ‚àí 1) ‚àí . (13) i 1 x‚àó e Œª i=0 i!  
   
  On the other hand, if Œª = , then h = 0. By (6), we obtain CG(r) =  
   
   ‚àí1   ‚àí1 e‚àí G(x‚àó ) G(x‚àó ) k k e‚àí ‚â• 1 ‚àí =1‚àí , = 1 ‚àí  ‚àó 1 x   k! ( ‚àí 1)!  
   
  (14)  
   
  k=0  
   
  which agrees with (13) when Œª =  (proven in full version). To Ô¨Ånish the proof of Theorem 2, it is left to show that (13) is a decreasing function of Œª because Œª ‚â§ œÅ. We also need to prove that the Ô¨Ånal expression is strictly greater than 1 ‚àí 1/e whenever  ‚â• 2. These are done in the full version.  
   
  On the Correlation Gap of Matroids  
   
  215  
   
  References 1. Ageev, A.A., Sviridenko, M.: Pipage rounding: a new method of constructing algorithms with proven performance guarantee. J. Comb. Optim. 8(3), 307‚Äì328 (2004) 2. Agrawal, S., Ding, Y., Saberi, A., Ye, Y.: Price of correlations in stochastic optimization. Oper. Res. 60(1), 150‚Äì162 (2012) 3. Asadpour, A., Niazadeh, R., Saberi, A., Shameli, A.: Sequential submodular maximization and applications to ranking an assortment of products. In: EC 2022: The 23rd ACM Conference on Economics and Computation, p. 817 (2022) 4. Ausubel, L.M., Milgrom, P.: The lovely but lonely Vickrey auction. In: Cramton, P., Shoham, Y., Steinberg, R. (eds.) Combinatorial Auctions, chap. 1. MIT Press (2006) 5. Barman, S., Fawzi, O., Ferm√©, P.: Tight approximation guarantees for concave coverage problems. In: 38th International Symposium on Theoretical Aspects of Computer Science (STACS). LIPIcs, vol. 187, pp. 1‚Äì17 (2021) 6. Barman, S., Fawzi, O., Ghoshal, S., G√ºrpinar, E.: Tight approximation bounds for maximum multi-coverage. Math. Program. 192(1), 443‚Äì476 (2022) 7. Bhalgat, A., Chakraborty, T., Khanna, S.: Mechanism design for a risk averse seller. In: Goldberg, P.W. (ed.) WINE 2012. LNCS, vol. 7695, pp. 198‚Äì211. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-35311-6_15 8. Calinescu, G., Chekuri, C., P√°l, M., Vondr√°k, J.: Maximizing a submodular set function subject to a matroid constraint (Extended Abstract). In: Fischetti, M., Williamson, D.P. (eds.) IPCO 2007. LNCS, vol. 4513, pp. 182‚Äì196. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-72792-7_15 9. CƒÉlinescu, G., Chekuri, C., P√°l, M., Vondr√°k, J.: Maximizing a monotone submodular function subject to a matroid constraint. SIAM J. Comput. 40(6), 1740‚Äì1766 (2011) 10. Chawla, S., Hartline, J.D., Malec, D.L., Sivan, B.: Multi-parameter mechanism design and sequential posted pricing. In: Schulman, L.J. (ed.) Proceedings of the 42nd ACM Symposium on Theory of Computing, STOC 2010, Cambridge, Massachusetts, USA, 5‚Äì8 June 2010, pp. 311‚Äì320. ACM (2010) 11. Chekuri, C., Livanos, V.: On submodular prophet inequalities and correlation gap. In: 14th International Symposium on Algorithmic Game Theory, SAGT. Lecture Notes in Computer Science, vol. 12885, p. 410 (2021) 12. Chekuri, C., Vondr√°k, J., Zenklusen, R.: Submodular function maximization via the multilinear relaxation and contention resolution schemes. SIAM J. Comput. 43(6), 1831‚Äì1879 (2014) 13. Clarke, E.H.: Multipart pricing of public goods. Public choice, pp. 17‚Äì33 (1971) 14. Dudycz, S., Manurangsi, P., Marcinkowski, J., Sornat, K.: Tight approximation for proportional approval voting. In: Bessiere, C. (ed.) Proceedings of the TwentyNinth International Joint Conference on ArtiÔ¨Åcial Intelligence, IJCAI 2020, pp. 276‚Äì282. ijcai.org (2020) 15. Dughmi, S.: Matroid secretary is equivalent to contention resolution. In: 13th Innovations in Theoretical Computer Science Conference, ITCS. LIPIcs, vol. 215, pp. 1‚Äì23 (2022) 16. Feige, U.: A threshold of ln n for approximating set cover. J. ACM (JACM) 45(4), 634‚Äì652 (1998) 17. Groves, T.: Incentives in teams. Econometrica: J. Econometric Soc. 41, 617‚Äì631 (1973) 18. Hartline, J.D.: Mechanism design and approximation (2013)  
   
  216  
   
  E. Husiƒá et al.  
   
  19. Leme, R.P.: Gross substitutability: an algorithmic survey. Games Econom. Behav. 106, 294‚Äì316 (2017) 20. Murota, K.: On basic operations related to network induction of discrete convex functions. Optim. Methods Softw. 36(2‚Äì3), 519‚Äì559 (2021) 21. Myerson, R.B.: Optimal auction design. Math. Oper. Res. 6(1), 58‚Äì73 (1981) 22. Nikolova, E.: Approximation algorithms for reliable stochastic combinatorial optimization. In: Serna, M., Shaltiel, R., Jansen, K., Rolim, J. (eds.) APPROX/RANDOM -2010. LNCS, vol. 6302, pp. 338‚Äì351. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-15369-3_26 23. Nisan, N., Ronen, A.: Computationally feasible VCG mechanisms. J. Arti. Intell. Res. 29, 19‚Äì47 (2007) 24. Rubinstein, A., Singla, S.: Combinatorial prophet inequalities. In: Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 1671‚Äì1687. SIAM (2017) 25. Schrijver, A.: Combinatorial optimization: polyhedra and eÔ¨Éciency, vol. 24. Springer (2003) 26. Shioura, A.: On the Pipage rounding algorithm for submodular function maximization - a view from discrete convex analysis. Discret. Math. Algorithms Appl. 1(1), 1‚Äì24 (2009) 27. Vickrey, W.: Counterspeculation, auctions, and competitive sealed tenders. J. Financ. 16(1), 8‚Äì37 (1961) 28. Yan, Q.: Mechanism design via correlation gap. In: Proceedings of the twentysecond annual ACM-SIAM symposium on Discrete Algorithms, pp. 710‚Äì719. SIAM (2011)  
   
  A 4/3-Approximation Algorithm for Half-Integral Cycle Cut Instances of the TSP Billy Jin1(B) , Nathan Klein2 , and David P. Williamson1 1  
   
  Cornell University, Ithaca, USA {bzj3,davidpwilliamson}@cornell.edu 2 University of Washington, Seattle, USA [email protected]   
   
  Abstract. A long-standing conjecture for the traveling salesman problem (TSP) states that the integrality gap of the standard linear programming relaxation of the TSP (sometimes called the Subtour LP or the Held-Karp bound) is at most 4/3 for symmetric instances of the TSP obeying the triangle inequality. In this paper we consider the halfintegral case, in which a feasible solution to the LP has solution values in {0, 1/2, 1}. Karlin, Klein, and Oveis Gharan [9], in a breakthrough result, were able to show that in the half-integral case, the integrality gap is at most 1.49993; Gupta et al. [6] showed a slight improvement of this result to 1.4983. Both of these papers consider a hierarchy of critical tight sets in the support graph of the LP solution, in which some of the sets correspond to cycle cuts and the others to degree cuts. Here we show that if all the sets in the hierarchy correspond to cycle cuts, then we can Ô¨Ånd a distribution of tours whose expected cost is at most 4/3 times the value of the half-integral LP solution; sampling from the distribution gives us a randomized 4/3-approximation algorithm. We note that known bad cases for the integrality gap have a gap of 4/3 and have a half-integral LP solution in which all the critical tight sets in the hierarchy are cycle cuts; thus our result is tight.  
   
  1  
   
  Introduction  
   
  In the traveling salesman problem (TSP), we are given a set of n cities and the costs cij of traveling from city i to city j for all i, j, and the goal of the problem is to Ô¨Ånd the least expensive tour that visits each city exactly once and returns to its starting point. An instance of the TSP is called symmetric if cij = cji for all i, j. Costs obey the triangle inequality (or are metric) if cij ‚â§ cik + ckj for all i, j, k. For ease of exposition, we consider the problem input as a complete graph G = (V, E) for the set of cities V , with ce = cij for edge e = (i, j). All instances we consider will be symmetric and obey the triangle inequality. In a breakthrough result, Karlin, Klein, and Oveis Gharan [8] gave the Ô¨Årst approximation algorithm with performance ratio better than 3/2, although the c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 217‚Äì230, 2023. https://doi.org/10.1007/978-3-031-32726-1_16  
   
  218  
   
  B. Jin et al.  
   
  amount by which the bound was improved is quite small (approximately 10‚àí36 ). The algorithm follows the ChristoÔ¨Ådes-Serdyukov template by selecting a random spanning tree from the max-entropy distribution, then using a T -join on the odd degree vertices of the tree to create a connected Eulerian subgraph. One special case of the TSP is known as the half-integral case. To understand the half-integral case, we need to introduce a well-known LP relaxation of the TSP, sometimes called the Subtour LP or the Held-Karp bound [4,7], which is as follows:  ce xe min e‚ààE  
   
  s.t.  
   
  x(Œ¥(v)) = 2,  
   
  ‚àÄ v ‚àà V,  
   
  x(Œ¥(S)) ‚â• 2, 0 ‚â§ xe ‚â§ 1,  
   
  ‚àÄ S ‚äÇ V, S = ‚àÖ, ‚àÄe ‚àà E,  
   
  where Œ¥(S) is the set of all  edges with exactly one endpoint in S and we use the shorthand that x(F ) = e‚ààF xe . A half-integral solution to the Subtour LP is one such that xe ‚àà {0, 1/2, 1} for all e ‚àà E, and a half-integer instance of the TSP is one whose LP solution is half-integral.  
   
  Fig. 1. Illustration of a known worst-case example for the integrality gap for the symmetric TSP with triangle inequality. The Ô¨Ågure on the left gives an (unweighted) graph, and costs cij are the shortest path lengths in the graph. The Ô¨Ågure in the center gives the LP solution, in which the dotted edges have value 1/2, and the solid edges have value 1. The Ô¨Ågure on the right gives the optimal tour. The ratio of the cost of the optimal tour to the value of the LP solution tends to 4/3 as k increases.  
   
  The integrality gap of an LP relaxation is the worst-case ratio of an optimal integer solution to the linear program to the optimal linear programming  
   
  Half-Integral Cycle Cut Instances of the TSP  
   
  219  
   
  solution. Wolsey [13] showed that the analysis of the ChristoÔ¨Ådes-Seryukov algorithm could be used to show that the integrality gap of the Subtour LP is at most 3/2. It is known that the integrality gap of the Subtour LP is at least 4/3, due to a set of half-integral graph TSP instances shown in Fig. 1, and another set of half-integral weighted instances due to Boyd and Seb≈ë [2] known as k-donuts. Schalekamp, Williamson, and van Zuylen [11] have conjectured that half-integral instances are the worst-case instances for the integrality gap. It has long been conjectured that the integrality gap is exactly 4/3, but until the work of Karlin et al. there had been no progress on the conjecture for several decades. In the case of half-integral instances, some results are known. M√∂mke and Svensson [10] have shown a 4/3-approximation algorithm for half-integral graph TSP (in which cost cij is the number of edges in the shortest i-j path in an input graph), also yielding an integrality gap of 4/3 for such instances; because of the worst-case examples of Fig. 1, their result is tight. Boyd and Carr [1] give a 4/3approximation algorithm (and an integrality gap of 4/3) for a subclass of halfinteger solutions they call triangle points (in which the half-integer edges form disjoint triangles); the examples of Fig. 1 show that their result is tight also. Boyd and Seb≈ë [2] give an upper bound of 10/7 for a subclass of half-integral solutions they call square points (in which the half-integer edges form disjoint 4-cycles). In a paper released just prior to their general improvement, Karlin, Klein, and Oveis Gharan [9] (KKO) gave a 1.49993-approximation algorithm in the halfintegral case; in particular, they show that given a half-integral solution, they can produce a tour of cost at most 1.49993 times the value of the corresponding objective function. Gupta, Lee, Li, Mucha, Newman, and Sarkar [6] improve this factor to 1.4983. With the improvements on the 3/2 bound remaining very incremental for weighted instances of the TSP, even in the half-integral case, we turn the question around and look for a large class of weighted half-integral instances for which we can prove that the 4/3 conjecture is correct, preferably one containing the known worst-case instances. To deÔ¨Åne our instances, we turn to some terminology of KKO. The KKO result uses induction on a hierarchy of critical tight sets of the half-integral LP solution x. A set S ‚äÇ V is tight if the corresponding LP constraint is met with equality; that is, x(Œ¥(S)) = 2. A set S is critical if it does not cross any other tight set; that is, for any other tight set T , either S ‚à© T = ‚àÖ or S ‚äÜ T or T ‚äÜ S. The critical tight sets then give rise to a natural tree-like hierarchy based on subset inclusion. KKO follow a ChristoÔ¨Ådes-Serdyukov style algorithm that performs induction on the hierarchy. In their analysis, they diÔ¨Äerentiate between cycle cuts (in which the child nodes of a parent are linked by pairs of edges in a chain) and degree cuts (in which the child nodes of a parent form a 4-regular graph; more detail is given in subsequent sections). In this paper, we will consider half-integral instances in which there are only cycle cuts, which we will refer to as half-integral cycle cut instances. Our contribution is to give a randomized 43 -approximation algorithm for these instances. More precisely, we give a distribution over connected Eulerian subgraphs such  
   
  220  
   
  B. Jin et al.  
   
  that each edge e is used with expectation at most 43 xe , which implies the result (note that edges are sometimes doubled in the Eulerian graph). Our main theorem is as follows: Theorem 1. There is a randomized 4/3-approximation algorithm for halfintegral cycle cut instances  of the TSP that produces an Eulerian tour with expected cost at most 43 e‚ààE ce xe . It is not hard to show that both the bad examples in Fig. 1 and the k-donut instances of Boyd and Seb≈ë [2] are cycle cut instances (Boyd and Carr‚Äôs result for triangle points works for the examples of Fig. 1, but not for k-donuts). Thus our bound of 4/3 is tight and cannot be improved. Our approach to the problem is novel and does not use the same ChristoÔ¨ÅdesSerdyukov framework as employed by KKO and others. Instead, we perform a top-down induction on the hierarchy of critical tight sets. For each set in the hierarchy, we deÔ¨Åne a set of ‚Äúpatterns‚Äù of edges incident on it such that the set has even degree. For each pattern, we give a distribution of edges connecting the chain of child nodes in the cycle cut, which induces a distribution of patterns on each child. Crucially, we then show that there is a feasible region R of distributions over patterns, such that if the distribution of patterns on the parent node belongs to R, then the induced distribution on patterns on each child node also belongs to R. Our abstract is structured as follows. We give some needed preliminary deÔ¨Ånitions in Sect. 2. We then sketch our main result in Sect. 3, and conclude in Sect. 4. Due to space constraints, some proofs are omitted or sketched. The full paper can be accessed at https://arxiv.org/abs/ 2211.04639.  
   
  2  
   
  Preliminaries  
   
  Given a half-integral LP solution x, we construct a 4-regular 4-edge-connected multigraph G = (V, E) by including a single copy of every edge e for which xe = 12 and two copies of every edge e for which xe = 1. We state the following for general k-edge-connected multigraphs. In our setting, k = 4. Definition 1. For a k-edge-connected multigraph G = (V, E), we say: ‚Äì Any set S ‚äÜ V such that |Œ¥(S)| = k (i.e., its boundary is a minimum cut) is a tight set. ‚Äì A set S ‚äÜ V is proper if 2 ‚â§ |S| ‚â§ n ‚àí 2 and a singleton if |S| = 1. ‚Äì Two sets S, S  ‚äÜ V cross if all of S  S  , S   S, S ‚à© S  , and V  (S ‚à™ S  ) = ‚àÖ are non-empty. The following are two standard facts about minimum cuts; for proofs see [5]. Lemma 1. If two tight sets S and S  cross, then each of S  S  , S   S, S ‚à© S  and S ‚à™ S  are tight. Moreover, there are no edges from S  S  to S   S, and there are no edges from S ‚à© S  to S ‚à™ S  .  
   
  Half-Integral Cycle Cut Instances of the TSP  
   
  221  
   
  Lemma 2. Let G = (V, E) be a k-regular k-edge-connected graph. Suppose either |V | = 3 or G has at least one proper min cut, and every proper min cut is crossed by some other proper min cut. Then, k is even and G forms a cycle, with k/2 parallel edges between each adjacent pair of vertices. We now deÔ¨Åne our class of instances. Definition 2 (Cycle cut instance). We say a graph G is a cycle cut instance if every non-singleton tight set S can be written as the union of two tight sets A, B = S. As mentioned in the introduction this condition captures the two known integrality gap examples of the subtour LP. We now show an equivalent deÔ¨Ånition of cycle cut instances after giving some deÔ¨Ånitions. First, Ô¨Åx an arbitrary root vertex r ‚àà V , and for all cuts we consider we will take the side which does not contain r. Definition 3 (Critical cuts). A critical cut is any tight set S ‚äÜ V {r} which does not cross any other tight set. Definition 4 (Hierarchy of critical cuts, H). Let H ‚äÜ 2V r be the set of all critical cuts. The hierarchy naturally gives rise to a parent-child relationship between sets as follows: Definition 5 (Child, parent, E ‚Üí (S)). Let S ‚àà H such that |S| ‚â• 2. Call the maximal sets C ‚àà H for which C ‚äÇ S the children of S, and call S their parent. Finally, deÔ¨Åne E ‚Üí (S) to be the set of edges with endpoints in two diÔ¨Äerent children of S. Definition 6 (Cycle cut, degree cut). Let S ‚àà H with |S| ‚â• 2. Then we call S a cycle cut if when G  S and all of the children of S are contracted, the resulting graph forms a cycle of length at least three with two parallel edges between each adjacent node. Otherwise, we call it a degree cut. While this deÔ¨Ånition of a cycle cut may sound specialized, due to Lemma 2, cycle cuts arise very naturally from collections of crossing min cuts. Lemma 3. If G is a cycle cut instance, then for any choice of r, H is composed only of cycle cuts (and singletons). One can also show that if for some choice of r, H is composed only of cycle cuts, then G is a cycle cut instance. Thus, in the remainder of the paper, we assume H is a collection of cycle cuts. Given S ‚àà H, let a0 = G  S and let a1 , . . . , ak be its children in H (which are either vertices or cycle cuts). By Lemma 2 a0 , . . . , ak can be arranged into a cycle such that two edges go between each adjacent vertex. WLOG let a1 , . . . , ak be in counterclockwise order starting from a0 . We call a1 the leftmost child of S and ak the rightmost child.  
   
  222  
   
  B. Jin et al. GS  
   
  S  
   
  Fig. 2. S is an example of a cycle cut with three children. In blue are contracted critical tight sets. In gray is the rest of the graph with S contracted. As in Lemma 2, we can see that when G  S is contracted into a single vertex, the resulting graph is a cycle with 2 edges between each adjacent vertex. In our recursive proof of our main theorem in Sect. 3, we are given a distribution of Eulerian tours over G/S, so in particular on the red edges here, and will then extend it to G with the blue critical sets contracted by picking a distribution over the black edges. (Color Ô¨Ågure online)  
   
  Definition 7 (External and internal cycles cuts). Let S ‚àà H such that S = V  {r} be a cut with parent S  . We call S external if in the ordering a0 , . . . , ak of S  (as given above), S = a1 or S = ak . Otherwise, call S internal. For example, if the blue nodes in Fig. 2 are contracted cycle cuts, the left and right nodes are external, while the middle one is internal. Note that for an cycle cut S with parent S  , if S is external then |Œ¥(S) ‚à© Œ¥(S  )| = 2, and if S is internal then |Œ¥(S) ‚à© Œ¥(S  )| = 0. Using the following simple fact, we will now describe our convention for drawing and describing cycle cuts: Lemma 4. Let A, B, C ‚àà H be three distinct critical cuts such that A  B and B ‚à© C = ‚àÖ or B ‚äÜ C. Then |Œ¥(A) ‚à© Œ¥(C)| ‚â§ 1. Definition 8 (Œ¥ L (S), Œ¥ R (S)). Let S ‚àà H be a cycle cut. We will deÔ¨Åne a partition of Œ¥(S) into two sets Œ¥ L (S), Œ¥ R (S) each consisting of two edges. If S = V  {r}, then it has a parent S  . S  has children a1 , . . . , ak such that S = ai for i = 0. Let Œ¥ L (S) = Œ¥(S) ‚à© Œ¥(ai‚àí1 ) and Œ¥ R (S) = Œ¥(S) ‚à© Œ¥(ai+1 (mod k+1) ). In other words, we partition the edges of S into the two edges going to the left neighbor of S in the cycle deÔ¨Åned by S  ‚Äôs children and the two edges going to the right neighbor. Otherwise S = V  {r}. Then if a1 , . . . , ak are the children of S, let Œ¥ L (S) consist of an arbitrary edge from Œ¥(a1 ) ‚à© Œ¥(S) and an arbitrary edge from Œ¥(ak ) ‚à© Œ¥(S). Let Œ¥ R (S) = Œ¥(S)  Œ¥ L (S). By Lemma 4 and the deÔ¨Ånition of Œ¥ L (S), Œ¥ R (S) for S = V  {r}, if S  is an external child of a cycle cut S, then |Œ¥ L (S) ‚à© Œ¥(S  )| = |Œ¥ R (S) ‚à© Œ¥(S  )| = 1. This allows us to adopt the following convention for drawing cycle cuts which we will call the caterpillar drawing of S: for an example, see Fig. 3. Formally,  
   
  Half-Integral Cycle Cut Instances of the TSP  
   
  223  
   
  let S ‚àà H be a cycle cut with children a1 , . . . , ak ‚àà H. Arrange a1 , . . . , ak in a horizontal line. First, expand a1 vertically into its children (if it is not a singleton) such that the unique edge in Œ¥ L (S) ‚à© Œ¥(a1 ) is pointing up (if it is a singleton, simply draw this edge pointing up. Then, expand a2 , . . . , ak one by one into their respective children (if they exist), placing the children vertically in increasing or decreasing order of their index so that the edges from ai to ai+1 do not cross. If ak is a singleton, arbitrarily choose which edge to draw pointing up. Otherwise, let a be the topmost child of ak . Draw the unique edge in Œ¥(S) ‚à© Œ¥(a ) pointing up. There are two types of cycle cuts: Definition 9 (Straight and twisted cycle cuts). Let S ‚àà H be a cycle cut. If Œ¥ L (S) has both edges pointing up in the caterpillar drawing of S, then call it a straight cycle cut. Otherwise, call it a twisted cycle cut. See Fig. 3 for examples.  
   
  S  
   
  S  
   
  Fig. 3. Caterpillar drawings of two diÔ¨Äerent cycle cuts S. The red edges are in the Œ¥ L (S) partition, and the blue edges are in the Œ¥ R (S) partition. The left drawing is a straight cycle cut, and the right is a twisted cycle cut as per DeÔ¨Ånition 9 (Color Ô¨Ågure online).  
   
  In the next section, we abbreviate the caterpillar drawing by contracting the non-singleton children of S (see Fig. 4). We do so partially for cleaner pictures but also to emphasize that all the relevant information used by our construction in the following section is contained in the abbreviated pictures.  
   
  S  
   
  S  
   
  Fig. 4. On the left is a shorthand caterpillar drawing for the straight cycle cut on the left in Fig. 3 obtained by contracting its children. Similarly for the right. We will use this style of picture in future sections.  
   
  224  
   
  3  
   
  B. Jin et al.  
   
  Proof of Theorem 1  
   
  We now present a summary of the proof of our main result, a 43 -approximation for half-integral cycle-cut instances of the TSP. To prove Theorem 1, we construct a distribution of Eulerian tours such that every edge is used at most 23 of the time. Since xe = 12 for every edge in the graph, this immediately implies that when we sample a tour from this distribution, its expected cost is at most 43 times the value of the LP. We work on the cycle cut hierarchy from the top down, and inductively specify the distribution of edges that enter every cut. Figure 4 depicts our convention for visualizing a cycle cut as described in Sect. 2. We say that a cycle cut is even if it contains an even number of children, and odd otherwise. Fig. 6 illustrates the patterns we use, where ‚Äúpattern‚Äù refers to a multiset of edges that enter a cycle cut. For each pattern entering a parent cycle cut, we give (randomized) rules which describe how to connect up its children ‚Äì this induces a distribution of patterns entering each child. We represent this process using a Markov chain with 4 states, illustrated in Fig. 6. The Ô¨Ågure shows the mapping from patterns to states; the transitions will come from the rules for connecting up the children, which we describe later. In the Ô¨Ågure, each state contains two pictures, which represent the parity of the edges in the patterns that are mapped to the state. SpeciÔ¨Åcally, a present edge is used exactly once, whereas an edge that is not present may be either unused or doubled. For example, Fig. 7 illustrates all possible patterns that are captured by the top picture of state 1. Finally, we maintain the invariant that if a cycle cut is in a given state, then each of the two pictures are equally likely. (When we later give the rules for connecting up the children, we will ensure this invariant is preserved.) Thus, when we say a cycle cut is in a given state with probability p, this means the parity of the pattern entering it follows the top picture in the state with probability p2 , and the bottom picture with probability p2 . We will use the phrase ‚Äúthe distribution of patterns entering a cycle cut C is (p1 , p2 , p3 , p4 )‚Äù to mean that for all i ‚àà {1, 2, 3, 4}, C is in state i with probability pi . To prove our main result, we will give a feasible region R of distributions over the states of the Markov chain, such that: 1) If the distribution of patterns entering a cycle cut C belongs to R, there is a way to connect up the children of C such that the distribution on each child also belongs to R, and 2) for each p ‚àà R, the corresponding rule for connecting the children of C uses each edge in E ‚Üí (C) at most 23 = 43 xe of the time in expectation. The feasible region is given in DeÔ¨Ånition 10. As long as R is nonempty, 1) and 2) are suÔ¨Écient to give the result since we can induce any distribution on the cycle cut V  {r}. Definition 10 (The Feasible Region). Let   2 1 R = (p1 , p2 , p3 , p4 ) ‚àà R4+ : p1 + p2 + p3 + p4 = 1, p1 + p2 = , p2 + p4 ‚â• . 3 3 See Fig. 5 for an visualization of R in a 2-dimensional space.  
   
  Half-Integral Cycle Cut Instances of the TSP  
   
  1 3  
   
  (0, 13 )  
   
  225  
   
  ( 23 , 13 )  
   
  q  
   
  Z  
   
  1 6  
   
  p 1 6  
   
  1 3  
   
  1 2  
   
  2 3  
   
  Fig. 5. The feasible region of distributions is R = {(p, where Z is the polytope above. S1  
   
  S2  
   
  S3  
   
  S4  
   
  2 3  
   
  ‚àí p,  
   
  1 3  
   
  ‚àí q, q) : (p, q) ‚àà Z},  
   
  Fig. 6. The patterns and how they map to states of a Markov chain. The states are unchanged regardless of the number of children: they are deÔ¨Åned only with respect to which of the edges are in. Note that we ignore doubled edges.  
   
  To describe the transitions of the Markov chain, we give (randomized) rules that dictate, for a cycle cut C and a pattern entering it, how to connect up its children. These rules depend on whether C is even or odd. The Ô¨Ånal form of the Markov chains is illustrated in Fig. 8.1 The meaning of taking one transition is as follows. Suppose the distribution of patterns entering C is (p1 , p2 , p3 , p4 ), 1  
   
  In the Ô¨Ågure, if there is a variable on an arc, it means that any transition probability in the range of that variable is possible. For example, in Peven , we can transition from S2 to S1 with probability z for any z ‚àà [0, 1]; the transition from S2 to S3 then happens with probability 1 ‚àí z.  
   
  226  
   
  B. Jin et al.  
   
  Fig. 7. In our illustrations of the patterns entering a given cycle cut, any edge that is not present may either be unused or doubled. Therefore, all four of the given edge conÔ¨Ågurations are represented by the upper left most state, S1 .  
   
  and suppose (q1 , q2 , q3 , q4 ) is the resulting distribution after one transition of a Markov chain. What this means is that for each child of C, the distribution of patterns entering it will be either (q1 , q2 , q3 , q4 ) or (q2 , q1 , q3 , q4 ) depending on if the child is straight or twisted, respectively (see DeÔ¨Ånition 9 and Fig. 3). In particular, it can be shown that if (q1 , q2 , q3 , q4 ) is the distribution induced on a child which is a straight cycle cut, then (q2 , q1 , q3 , q4 ) would be the distribution induced on a child which is a twisted cycle cut. Thus, it is suÔ¨Écient to check that: i) the distributions induced on straight children lie in the feasible region and ii) if (q1 , q2 , q3 , q4 ) is a distribution induced on straight children, then (q2 , q1 , q3 , q4 ) is also in the feasible region. This corresponds to the set of distributions induced on the children being symmetric under this transformation.2  
   
  1/2  
   
  z  
   
  S1  
   
  1/2  
   
  S3  
   
  S2  
   
  1‚àíz 1/2  
   
  1/2 Pev en  
   
  z ‚àà [0, 1], w ‚àà [0, 1]  
   
  x  
   
  w  
   
  S4  
   
  1‚àíx  
   
  1‚àíw  
   
  z  
   
  S1  
   
  S3  
   
  S2  
   
  1‚àíz y  
   
  1‚àíy Po dd  
   
  w  
   
  S4  
   
  1‚àíw  
   
  x ‚àà [ 13 , 1], y ‚àà [ 13 , 1], z ‚àà [0, 23 ], w ‚àà [0, 23 ]  
   
  Fig. 8. The variables on the arcs indicate that one can feasibly transition according to any probability in the range.  
   
  2  
   
  Note that the feasible region is not symmetric under this transformation. The distribution induced on the children is thus a symmetric subset of the feasible region.  
   
  Half-Integral Cycle Cut Instances of the TSP  
   
  227  
   
  Proposition 1. For any cycle cut C ‚àà H and any distribution of patterns entering C, there is a way to connect its children so that the induced distribution on each child is given by 1) applying the corresponding Markov chain in Fig. 8, and then 2) swapping the Ô¨Årst two coordinates if the child is twisted. Proof (Sketch). The proof involves going through the 8 cases one by one (depending on the parity of the cut, and which of the 4 states it is in), and showing that in each case, there is a (randomized) rule for connecting the children that achieve the transitions in Fig. 8. To illustrate the main idea, we show the rule in the case that C is even and in state 4. In this case, the rule for connecting the children of C is illustrated in Fig. 9. Let w ‚àà [0, 1]. With probability w, we make all children transition to state 2. To do this, Ô¨Årst suppose C has all 4 single edges entering it (the top picture in the left box). In this case, we consider the pairs of edges in E ‚Üí (C) from left to right, and alternate 1) doubling one of the two edges with equal probability (shown by the dotted black edges), and 2) using both edges (shown by the solid black edges). Because C is even, the rightmost pair of edges ends up falling in case 1) of the alternating rule, and so all children transition to state 2. The case where all the edges entering C are used an even number of times (the bottom picture in the left box) is quite similar, except we begin the alternating rule by using both edges. On the other hand, with probability 1 ‚àí w, we transition back to state 4. This is accomplished by using each pair of edges in the top case of state 4, and by doubling one edge from each pair uniformly at random in the bottom case of state 4. The net transition probabilities are then (0, w, 0, 1 ‚àí w), where w can be any number from 0 to 1.  

  w  
   
  + (1 ‚àí w)  
   
  Fig. 9. Transition for state 4 in the even case.  
   
  We ensure that in all cases, each edge in E ‚Üí (C) is used 12 , 12 , 1, 1 times in expectation if the pattern entering C belongs to state 1, 2, 3, 4, respectively. Therefore, if p = (p1 , p2 , p3 , p4 ) are the probabilities that we are in states 1, 2, 3, 4 respectively, then each edge in E ‚Üí (C) is used exactly 21 p1 + 12 p2 + p3 + p4 = 1 ‚àí 12 (p1 + p2 ) of the time in expectation. Thus to get a 43 -approximation, it is necessary that p1 + p2 ‚â• 23 . Note that if p ‚àà R, then p1 + p2 = 23 , so that each edge is used exactly 23 of the time.  
   
  228  
   
  B. Jin et al.  
   
  To complete the proof, we only need show that if the distribution of patterns entering a cycle cut C belongs to R, then the induced distributions on the children also belong to R. Thus R is suÔ¨Écient, in sense that if the distribution entering a cycle cut belongs to R, then it is possible to get a 43 -approximation all the way down the hierarchy using the Markov chains in Fig. 8. Moreover, we are able to show that R is necessary; if the distribution entering a cycle cut does not belong to R, then it is impossible to obtain a 43 -approximation using our Markov chains. In this sense, R is the largest feasible region using our technique. Theorem 2. 1. (R is suÔ¨Écient) If the distribution of patterns entering a cycle cut belongs to R, then there are feasible Markov chains (among the ones shown in Fig. 8) such that the induced distribution entering each child also belongs to R. 2. (R is necessary) Suppose the distribution of patterns entering a cycle cut does not belong to R. Then it is not possible to obtain a 43 -approximation using the Markov chains in Fig. 8. Proof (Sketch). For 1), we show that for any p ‚àà R and for C even or odd, there are feasible values for the transition probabilities of the corresponding Markov chain such that the resulting distribution q ‚àà R (and also q with its Ô¨Årst two coordinates swapped is in R.) The values of the transition probabilities are derived as a function of p. For 2), we consider an arbitrary distribution p (not necessarily in R), and let q(1) and q(2) be the distributions obtained by applying Peven once and twice, respectively. We then argue that p must belong to R in order for q(1) and q(2) to each have their Ô¨Årst two coordinates sum to at  
   
  least 23 . Example. To give the reader some more intuition, we give a speciÔ¨Åc example of how to maintain distributions in R on all the cuts in the hierarchy by choosing appropriate transition probabilities on the Markov chains in Fig. 8. Let p = ( 49 , 29 , 29 , 19 ) and q = ( 29 , 49 , 29 , 19 ) (i.e. q is p with the Ô¨Årst two coordinates swapped). It is easy to check that p, q ‚àà R. We now show for any half-integral cycle cut instance, it is possible to make it so that the distribution entering any cycle cut is either p or q. To see this, let C be a cycle cut and suppose C is odd. Set the transition probabilities in Podd to be x = y = z = w = 23 . For these probabilities, it is easy to check that Podd p = Podd q = p.3 On the other hand, if C is even, setting z = w = 1 in Peven gives Peven p = p, and setting z = 34 , w = 1 gives Peven q = p. Thus, as long as the distribution entering C is p or q, we can make the distribution on each child of C be either p or q. Together with Proposition 1, this already proves a 43 -approximation for halfintegral cycle cut instances. The additional contribution of Theorem 2 is an exact characterization of the region of distributions that give a 43 -approximation using our techniques. 3  
   
  In fact, it can be checked that for these probabilities, Podd maps every distribution (whose Ô¨Årst two coordinates sum to 23 ), to p.  
   
  Half-Integral Cycle Cut Instances of the TSP  
   
  4  
   
  229  
   
  Conclusion and Open Questions  
   
  Our result leads to several interesting open questions. One such open question is whether our result extends to the case of cycle cuts for non-half-integral solutions. We believe this to be possible through a more reÔ¨Åned understanding of the patterns that result from considering non-half-integral solutions. Clearly a better understanding of what happens in the case of degree cuts is needed to make substantial progress on the overall half-integral case. We think it is possible to improve incrementally on the 1.4983-approximation of Gupta et al. [6] by using a combination of ideas from this paper with a few other small improvements. Recall that in a degree cut, each vertex has degree four, there are no parallel edges, and every proper cut has at least six edges crossing it. Ideally one would be able to show that any distribution on a parent cut lying in the feasible region of Fig. 5 could be used to induce a distribution on patterns of the children of the degree cut in a subregion of the feasible region with each edge used at most 2/3 of the time; such a result would lead immediately to a 4/3 integrality gap for half-integral instances. Acknowledgment. The Ô¨Årst and third authors would like to thank Anke van Zuylen for early discussions on this problem. The Ô¨Årst and third authors were supported in part by NSF grant CCF-2007009. The Ô¨Årst author was also supported by NSERC fellowship PGSD3-532673-2019. The second author was supported in part by NSF grants DGE1762114, CCF-1813135, and CCF-1552097. We would like to thank Martin Drees for his helpful suggestions that allowed us to simplify the proof of the main result.  
   
  References 1. Boyd, S., Carr, R.: Finding low cost TSP and 2-matching solutions using certain half-integer subtour vertices. Discret. Optim. 8, 525‚Äì539 (2011) 2. Boyd, S., Seb≈ë, A.: The salesman‚Äôs improved tours for fundamental classes. Math. Program. 186, 289‚Äì307 (2021) 3. ChristoÔ¨Ådes, N.: Worst case analysis of a new heuristic for the traveling salesman problem. Report 388, Graduate School of Industrial Administration, CarnegieMellon University, Pittsburgh, PA (1976) 4. Dantzig, G., Fulkerson, R., Johnson, S.: Solution of a large-scale traveling-salesman problem. J. Oper. Res. Soc. Am. 2(4), 393‚Äì410 (1954). ISSN 00963984. URL https://www.jstor.org/stable/166695 5. Fleiner, T., Frank, A.: A quick proof for the cactus representation of mincuts. Technical Report QP-2009-03, Egerv√°ry Research Group, Budapest (2009). https:// www.cs.elte.hu/egres 6. Gupta, A., Lee, E., Li, J., Mucha, M., Newman, H., Sarkar, S.: Matroid-based TSP rounding for half-integral solutions. In: Aardal, K., Sanit√†, L. (eds.) Integer Programming and Combinatorial Optimization. LNCS, vol. 13265, pp. 305‚Äì 318 (2022). https://doi.org/10.1007/978-3-031-06901-7_23,See also https://arxiv. org/pdf/2111.09290.pdf 7. Held, M., Karp, R.M.: The traveling-salesman problem and minimum spanning trees. Oper. Res. 18, 1138‚Äì1162 (1971)  
   
  230  
   
  B. Jin et al.  
   
  8. Karlin, A.R., Klein, N., Gharan, S.O.: A (slightly) improved approximation algorithm for metric tsp. In: STOC. ACM (2021) 9. Karlin, A.R., Klein, N., Gharan, S.O.: An improved approximation algorithm for TSP in the half integral case. In: Makarychev, K., Makarychev, Y., Tulsiani, M., Kamath, G., Chuzhoy, J. (eds.) STOC, pp. 28‚Äì39. ACM (2020) 10. M√∂mke, T., Svensson, O.: Removing and adding edges for the traveling salesman problem. J. ACM, 63 (2016). Article 2 11. Schalekamp, F., Williamson, D.P., van Zuylen, A.: 2-matchings, the traveling salesman problem, and the subtour LP: a proof of the Boyd-Carr conjecture. Math. Oper. Res. 39(2), 403‚Äì417 (2014) 12. Serdyukov, A.: On some extremal walks in graphs. Upravlyaemye Sistemy 17, 76‚Äì 79 (1978) 13. Wolsey, L.A.: Heuristic analysis, linear programming and branch and bound. Math. Program. Study 13, 121‚Äì134 (1980)  
   
  The Polyhedral Geometry of Truthful Auctions Michael Joswig1,2 , Max Klimm1(B) , and Sylvain Spitz1  
   
  2  
   
  1 Technische Universit√§t Berlin, 10623 Berlin, Germany {joswig,spitz}@math.tu-berlin.de, [email protected]  Max-Planck Institute for Mathematics in the Sciences, 04103 Leipzig, Germany  
   
  Abstract. The diÔ¨Äerence set of an outcome in an auction is the set of types that the auction mechanism maps to the outcome. We give a complete characterization of the geometry of the diÔ¨Äerence sets that can appear for a dominant strategy incentive compatible multi-unit auction showing that they correspond to regular subdivisions of the unit cube. This observation is then used to construct mechanisms that are robust in the sense that the set of items allocated to a player does change only slightly when the player‚Äôs reported type is changed slightly.  
   
  1  
   
  Introduction  
   
  Mechanism design is concerned with the implementation of favorable social outcomes in environments where information is distributed and only released strategically. SpeciÔ¨Åcally, this article is concerned with multi-dimensional mechanism design problems where a set of m items is to be allocated to a set of n players. The attitude of each player for receiving a subset of the items is determined by the so-called type of the player and is their private information and not available to the mechanism. In this setting, a mechanism elicits the types from the players, and‚Äîbased on the reported types‚Äîdecides on an allocation of the items to the players, and on a price vector that speciÔ¨Åes the amount of money that the different players have to pay to the mechanism. In order to incentivize the players to truthfully report their types to the mechanism, one is interested in mechanisms that have the property that no matter what the other players report to the mechanism, no player can beneÔ¨Åt from misreporting their type; mechanisms that enjoy this property are called dominant strategy incentive compatible, short DSIC. In this paper, we investigate the geometric properties of DSIC mechanisms. Because DSIC mechanisms require truthful reporting of the type no matter of the types declared by the other players, they can be characterized by the one-player mechanisms that arise when the declared valuations of the other players are Ô¨Åxed. As an example for a mechanism, consider the basic case of a combinatorial auction (see De Vries and Vohra [41] for a survey) where two items are sold to two players with additive valuations. In that case, each player i has a twoparameter type Œ∏i = (Œ∏i,1 , Œ∏i,2 ) where the scalar Œ∏i,j is the monetary equivalent c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 231‚Äì245, 2023. https://doi.org/10.1007/978-3-031-32726-1_17  
   
  232  
   
  M. Joswig et al.  
   
   Œ∏1,2  
   
   Œ∏1,2  
   
  Q{2}  
   
  Q{1,2}  
   
  Q‚àÖ  
   
  Q{1}  
   
   Œ∏1,2  
   
  Q{2}  
   
  Q{1,2}  
   
  4/3  
   
  Q{2}  
   
  Q{1,2}  
   
  Q‚àÖ  
   
  Q{1}  
   
  1  
   
  1 (a)  
   
  2/3  
   
  Q‚àÖ  
   
  Q{1}  
   
   Œ∏1,1  
   
  4/3 (b)  
   
   Œ∏1,1  
   
  2/3  
   
   Œ∏1,1  
   
  (c)  
   
  Fig. 1. DiÔ¨Äerence sets of several mechanisms; cf. [39, Fig. 1].  
   
  that player i attaches to receiving item j. For illustration, assume that player 2 reported Œ∏2 = (1, 1) and consider the corresponding one-player mechanism for player 1. If each item j is sold independently to the bidder i with the highest  (breaking ties in favor of player 1), we obtain that player 1 reported type Œ∏i,j  ‚â• 1. Geometrically, this one-player mechanism receives item j if and only if Œ∏1,j can be represented by its diÔ¨Äerence sets QS , S ‚àà 2{1,2} where QS is equal to the closure of the set of types reported by player 1 so that they get allocated the set of items S. The diÔ¨Äerence sets were introduced by Vohra [40, p. 41] and reveal valuable information about the properties of the mechanism; see Fig. 1. Figure 1a shows the diÔ¨Äerence sets of the mechanism selling each item to the highest bidder; Figs. 1b and c show the diÔ¨Äerence sets of other DSIC mechanisms (not speciÔ¨Åed here). Under reasonable assumptions, the diÔ¨Äerence sets form a polyhedral decomposition of the type space. In this paper, we are interested in characterizing their polyhedral geometry. This is a continuation of work of Vidali [39] who showed that the two combinatorial types shown in Fig. 1b and c are the only cases that can appear for a DSIC mechanism for two items (where the combinatorial type in Fig. 1a is a common degenerate case of both). She then also suggested a similar characterization of the combinatorial types that can appear for three items and asked how these Ô¨Åndings can be generalized to more items. Characterizing the combinatorial types of mechanisms is interesting for a variety of reasons. First, such geometric arguments are often used in order to characterize the set of allocation functions that are implementable by a DSIC mechanism. For instance, the diÔ¨Äerence sets whose closures have nonempty intersection correspond exactly to two-cycles in an auxiliary network used by Rochet [35] in order to characterize the allocation functions that are implementable by DSIC mechanisms. Second, the combinatorial types can be used to study the sensitivity of mechanisms to deviations in the reported types. As an example consider the mechanism in Fig. 1a. For any  > 0, reporting the type (1 ‚àí , 1 ‚àí ) yields no item for player 1 while the report of the type (1 + , 1 + ) grants them both items. Put diÔ¨Äerently, a small change in the reported type may  
   
  The Polyhedral Geometry of Truthful Auctions  
   
  233  
   
  change the outcome from no items being allocated to player 1 to all items being allocated to the same player. This is in contrast with the mechanism shown in Fig. 1c where a small change in the reported type may change the cardinality of the set of allocated items only by 1.1 Third, the combinatorial types of the mechanism are relevant for the eÔ¨Éciency of the mechanism; see, e.g., [13,39]. Our Results. We give a complete characterization of the combinatorial types of all DSIC combinatorial auctions with m items for any value of m. This answers an open question of Vidali [39] about how to generalize her results for m = 2 and m = 3 to larger values of m. We employ methods from polyhedral geometry [16] and tropical combinatorics [26]. Our results rest on the observation that a multiplayer mechanism is DSIC if and only if its single-player components are DSIC; see [36]. We show that for m items, the combinatorial types of those single-player components are in bijection to equivalence classes of regular subdivisions of the m-dimensional unit cube. We identify the relevant symmetries for exchangeable items and conclude that there are exactly 23 nondegenerate combinatorial types for m = 3 and 3,706,261 such types for m = 4 (Theorem 9). We then use this characterization to study the optimal sensitivity of mechanisms to slight changes in the reported types. SpeciÔ¨Åcally, we show that for any number of items m, there is a one-player combinatorial auction so that the cardinality of the set of items received by the player changes by at most 1 when the reported type is slightly perturbed (Proposition 11). We also give bounds on a similar measure involving the Hamming distance of the set of received items (Proposition 12). In the full version of this paper, we further show how to apply the same methodology in order to classify the combinatorial types of aÔ¨Éne maximizers with n players. Further Related Work. Rochet‚Äôs Theorem [35] states that an allocation function is implementable by a DSIC mechanism if and only if the allocation networks of the corresponding one-player mechanisms have no Ô¨Ånite cycles of negative lengths. There is a substantial stream of literature exhibiting conditions where it is enough to require conditions on shorter cycles [4,5,8,9,11,18,27]; for instance, it suÔ¨Éces to require the nonnegativity for cycles of length 2 when the preferences are single-peaked [30] or when the type-space is convex [36]. Roberts [34] showed that when the type space of all players is RŒ© , then only aÔ¨Éne maximizers are implementable by a DSIC mechanism. Gui et al. [23] and Vohra [40] studied the diÔ¨Äerence sets QA of a mechanism and showed that under reasonable assumptions their closures are polyhedra. Vidali [39] studied the geometry of the polyhedra for the case of two and three items. In recent years, tropical geometric methods proved to be useful for algorithmic game theory and lead to new results in mechanism design [7,28,37], mean payoÔ¨Ä games [1,2], linear optimization [3] and beyond. Beyond the scope of combinatorial auctions, our results are also applicable to the mechanism design problem of scheduling on unrelated machines [12,14,17,21,31]. 1  
   
  This higher stability of the mechanism in Fig. 1c comes at the expense of a smaller social welfare (i.e., sum of player valuations of the received items) which is maximized for the mechanism in Fig. 1a. Maximizing social welfare, however, is mathematically well-understood since it is achieved by the class of VCG-mechanisms [15, 22, 38].  
   
  234  
   
  2  
   
  M. Joswig et al.  
   
  Preliminaries  
   
  In this section, we give a brief overview of basic concepts from mechanism design theory, polyhedral geometry and tropical combinatorics used in this paper. For a more comprehensive treatment we refer to [16,26,29,32]. Mechanism Design. A multi-dimensional mechanism design problem consists of a Ô¨Ånite set [m] := {1, . . . , m} of items and a Ô¨Ånite set [n] := {1, . . . , n} of players. Every player i has a type Œ∏i = (Œ∏i,1 , . . . , Œ∏i,m ) ‚àà Rm , where the value Œ∏i,j , for j ‚àà [m], is the monetary value player i attaches to receiving item j. A vector Œ∏ = (Œ∏1 , . . . , Œ∏n ) with Œ∏i ‚àà Rm for all i ‚àà [n] is called a type vector and Rn√óm is the space of all type vectors. The type Œ∏i is the private information of player i and unknown to all other players j = i and the mechanism designer. Let      n√óm  ai,j = 1 for all j ‚àà [m] Œ© = A ‚àà {0, 1}   i‚àà[n]  
   
  be the set of allocations of the m items to the n players. The i-th row Ai of an allocation matrix A ‚àà Œ© corresponds to the allocation for the i-th player. A (direct revelation) auction mechanism is a tuple M = (f, p) consisting of an allocation function f : Rn√óm ‚Üí Œ© and a payment function p : Rn√óm ‚Üí Rn . The mechanism Ô¨Årst elicits a claimed type vector Œ∏ = (Œ∏1 , . . . , Œ∏n ) ‚àà Rn√óm where Œ∏i ‚àà Rm is the type reported by player i. It then chooses an alternative f (Œ∏ ) and payments p(Œ∏ ) = (p1 (Œ∏ ), . . . , pn (Œ∏ )) ‚àà Rn where pi (Œ∏ ) is the payment from player i to the mechanism. We assume that the players‚Äô utilities are quasi-linear and the valuations are additive, i.e., the utility of player i with type Œ∏i when the type vector reported to the mechanism is Œ∏ is ui (Œ∏ | Œ∏i ) = fi (Œ∏ )¬∑Œ∏i ‚àípi (Œ∏ ), where fi (Œ∏ ) is the i-th row of f (Œ∏ ) = A. An auction mechanism is called dominant strategy incentive compatible (DSIC) or truthful if ui (Œ∏ | Œ∏i ) ‚â• ui ((Œ∏i , Œ∏‚àíi ) | Œ∏i ), for all i ‚àà [n], Œ∏ ‚àà Rn√óm , and Œ∏i ‚àà Rm . Here and throughout, (Œ∏i , Œ∏‚àíi ) denotes the type vector where player i reports Œ∏i and every other player j reports Œ∏j as in Œ∏. An allocation function is truthfully implementable, or just truthful, (in weakly dominant strategies) if there is an incentive compatible direct revelation mechanism M = (f, p). For an allocation A ‚àà Œ©, let RA = {Œ∏ ‚àà Rn√óm | f (Œ∏) = A} be the preimage of A under f , and let QA = cl(RA ) be the topological closure of RA . We call QA the diÔ¨Äerence set of A. Polyhedral Geometry and Tropical Combinatorics. We consider the maxtropical semiring (T, ‚äï, ) with T := R ‚à™ {‚àí‚àû}, a ‚äï b := max{a, b} and a  b := a + b. Picking coeÔ¨Écients Œªu ‚àà T for u ‚àà Zm such that only Ô¨Ånitely many are distinct from ‚àí‚àû deÔ¨Ånes an m-variate tropical (Laurent) polynomial, p, whose evaluation at x ‚àà Rm reads  p(x) = Œªu  xu = max {Œªu + x ¬∑ u | u ‚àà Zm } . (1) u‚ààZm  
   
  The Polyhedral Geometry of Truthful Auctions  
   
  235  
   
  y (0, 2)  
   
  (2, 1)  
   
  (0, 2)  
   
  (1, 1)  
   
  q  
   
  (0, 1)  
   
  (0, 1) (2, 0) (0, 0)  
   
  (a)  
   
  (2, 1)  
   
  x  
   
  (0, 0)  
   
  (1, 0)  
   
  (1, 1)  
   
  (1, 0)  
   
  (2, 0)  
   
  (b)  
   
  Fig. 2. (a) Tropical hypersurface H(p) and (b) dual regular subdivision of supp(p), where p(x, y) = max{0, x + 1, y + 1, 2x, x + y, 2y ‚àí 1, 2x + y ‚àí 2}. In (a) regions are marked by their support vectors; in (b) the same labels mark vertices of the subdivision. Conversely, e.g., the blue quadrangular cell on the right is dual to the vertex q on the left. (Color Ô¨Ågure online)  
   
  The support of p is the set supp(p) = {u ‚àà Zm | Œªu = ‚àí‚àû}. The tropical hypersurface H(p) is the set of points x ‚àà Rm such that the maximum in (1) is attained at least twice. The tropical hypersurface partitions the set Rm \ H(p) into sets in which the maximum of (1) is attained exactly once, for some Ô¨Åxed u ‚àà Zm ; see Fig. 2a. Taking the closure of such a part, we get a region of H(p), which is a (possibly unbounded) polyhedron. The Newton polytope N (p) = conv(supp(p)) of a tropical polynomial p is the convex hull of its support. A Ô¨Ånite set C of polyhedra in Rm is a polyhedral complex, if it is closed with respect to taking faces and if for any two P, Q ‚àà C the intersection P ‚à©Q is a face of both P and Q. The polyhedra in C are the cells of C. If every polyhedron in C is bounded, it is a polytopal complex. Further, given a Ô¨Ånite set of points U ‚äÇ Rm , a polytopal complex C in Rm is a polytopal subdivision of U if the vertices of all polytopes in C are points in U and if the union of all polytopes in C is the convex hull of the points in U . If the cells of a polytopal subdivision are all simplices, it is called a triangulation. A subdivision of U is called regular, if it can be obtained via a lifting function  Œª : U ‚Üí R on U . Formally, let P (U, Œª) := conv (u, Œª(u)) ‚àà Rm+1  u ‚àà U be the lifted polytope. Its upper faces have an outer normal vector with positive last coordinate. Projecting these upper faces by omitting the last coordinate yields a polytopal subdivision of U that is called the regular subdivision of U induced by Œª. The following proposition explains the duality between a tropical hypersurface and the regular subdivision of its support. Here we identify a polytopal subdivision with its Ô¨Ånite set of cells, partially ordered by inclusion. A proof can be found in [26, Theorem 1.13.]; see Fig. 2 for an example.  
   
  236  
   
  M. Joswig et al.  
   
  Proposition 1. Let p = max {Œªu + x ¬∑ u | u ‚àà Zm } be a tropical Laurent polynomial. Then there is an inclusion reversing bijection between the regular subdivision of the support of p with respect to Œª(u) = Œªu and the polyhedral complex induced by the regions of H(p).  
   
  3  
   
  Characterization of One-Player Mechanisms  
   
  In many applications, the intersections of diÔ¨Äerence sets QA are special, as the mechanism is essentially indiÔ¨Äerent between the outcomes and uses a tie-breaking rule or random selection to determine the outcome. Observing which diÔ¨Äerence sets intersect and which do not gives rise to a combinatorial pattern which we attribute to the allocation function. We want to study these patterns in order to classify which of them can be attributed to truthfulness. We express such a pattern as an abstract simplicial complex over the allocation space and call it the indiÔ¨Äerence complex of the allocation function. Formally, an abstract simplicial complex over some Ô¨Ånite set E is a nonempty set family S of subsets of E, such that for any set S ‚àà S and any subset T ‚äÜ S, we also have T ‚àà S. The maximal elements (by inclusion) of an abstract simplicial complex are called facets. DeÔ¨Ånition 2 (IndiÔ¨Äerence Complex). The indiÔ¨Äerence complex I(f ) of an allocation function f is the abstract simplicial complex deÔ¨Åned as    
   
  QA = ‚àÖ . I(f ) = O ‚äÜ Œ©  A‚ààO  
   
  Note that the indiÔ¨Äerence complex I(f ) is precisely the nerve complex of the family of diÔ¨Äerence sets of f ; see [10, ¬ß10]. Recall that an allocation function f is implementable if there is a DSIC mechanism M = (f, p). Likewise, we call an indiÔ¨Äerence complex I implementable, if there is an implementable allocation function f such that I(f ) = I. We deÔ¨Åne the local allocation function of player i for a given type vector Œ∏‚àíi to be fi,Œ∏‚àíi (Œ∏i ) = Ai , where Ai is the i-th row of A = f (Œ∏i , Œ∏‚àíi ). Further, let m m us Ô¨Åx a payment vector p ‚àà R2 , which we index by  a ‚àà {0, 1} .  allocations Then, for any type Œ∏i ‚àà Rm , we let up (Œ∏i ) = max Œ∏i ¬∑ a ‚àí pa  a ‚àà {0, 1}m . The resulting function up : Rm ‚Üí R is a max-tropical polynomial of degree m. We refer to the allocation in {0, 1}m which maximizes up (Œ∏i ) as arg max up (Œ∏i ). We restate [32, Proposition 9.27], which says that in a truthful setting, the local allocation functions are deÔ¨Åned by such tropical polynomials, where the vector p depends only on the types of the other players. Proposition 3. The allocation function f is truthful, if and only if for all players i ‚àà [n] and all type vectors Œ∏ ‚àà Rn√óm , there exists a payment vector m pi (Œ∏‚àíi ) ‚àà R2 , such that fi,Œ∏‚àíi (Œ∏i ) ‚àà arg max upi (Œ∏‚àíi ) (Œ∏i ).  
   
  The Polyhedral Geometry of Truthful Auctions  
   
  237  
   
  As an important consequence of Proposition 3, the allocation function f is truthful if and only if all of its local functions fi,Œ∏‚àíi are truthful. Therefore, for the remainder of this section, we Ô¨Åx a player i and the type vector Œ∏‚àíi of the other players and consider the corresponding one-player mechanism for player i. Note that in this setting, not all items need to be allocated, since the non-allocated items will be distributed among the remaining players. That is, from now on we abuse the notation slightly by considering single-player allocation functions f : Rm ‚Üí Œ© = {0, 1}m . Next, we want to discuss the relationship between the indiÔ¨Äerence complex and the allocation network, which is a tool often used to analyze the truthfulness of allocation functions. The latter is the weighted complete directed graph Gf with a node for each allocation a ‚àà Œ© and where the arc lengths are given as (a, a ) = inf Œ∏‚ààRa {Œ∏ ¬∑ a ‚àí Œ∏ ¬∑ a}. The value of (a, a ) is the minimal loss of the player‚Äôs valuation that would occur if the mechanism always chooses allocation a instead of a . Recall that Ra = {Œ∏ ‚àà Rm | f (Œ∏) = a}. We can link the indiÔ¨Äerence complex and the allocation network through the following proposition. This generalizes [36, Proposition 5] and also occurs in [39, Lemma 3], without a proof. Here we restate the result in our notation, adding a short proof for the sake of completeness. Proposition 4. Let (f, p) be a DSIC mechanism with quasi-linear utilities for one player. Let C = (a(1) , . . . , a(k) = a(1) ) be a cycle in the allocation network Gf , such that for each j ‚àà [k ‚àí 1], we get Qa(j) ‚à© Qa(j+1) = ‚àÖ. Then the length of the cycle C is 0. Proof. Let C = (a(1) , . . . , a(k) ) be a cycle as in the statement of the proposition. Using [36, Proposition 5], we obtain that for any Œ∏ ‚àà Qa(j) ‚à©Qa(j+1) , the equation Œ∏ ¬∑ a(j) ‚àí Œ∏ ¬∑ a(j+1) = (a(j) , a(j+1) ) is satisÔ¨Åed. Since the mechanism (f, p) is truthful and Œ∏ ‚àà Qa(j) ‚à© Qa(j+1) , we obtain Œ∏ ¬∑ a(j) ‚àí pa(j) = Œ∏ ¬∑ a(j+1) ‚àí pa(j+1) . Therefore pa(j) ‚àí pa(j+1) = (a(j) , a(j+1) ). Adding up all the lengths of the arcs of C we get 0, which Ô¨Ånishes the proof.   A consequence of Proposition 4 is that all cycles in Gf with the property that all of its edges connect two common nodes of some facet O ‚äÜ Œ© of the indiÔ¨Äerence complex I(f ), have length 0. Especially, each oriented cycle in the one-skeleton of I(f ) is also a zero-cycle in Gf . For the remainder of this section, our goal is to classify truthful allocation functions for the given type of allocation mechanisms. Recall that Proposition 3 shows that the diÔ¨Äerence sets of a truthful one-player allocation mechanism are exactly the regions of the tropical utility function of the player. We use this observation to give an alternative proof for the following theorem. It states that there is a bijection between implementable one-player indiÔ¨Äerence complexes for m items and the regular subdivisions of the m-dimensional cube. A similar result has been shown by Frongillo and Kash [19], who employ power diagrams. The latter are equivalent to regular subdivisions as shown by Aurenhammer [6].2 2  
   
  Characterizations of implementability in terms of geometric subdivisions of the type space have been used before, e.g., in [24, Proposition 2].  
   
  238  
   
  M. Joswig et al.  
   
  Theorem 5. An indiÔ¨Äerence complex I for m items and one player is implementable if and only if there is a regular subdivision S of the m-dimensional unit cube, such that the facets of I are precisely the vertex sets of the maximal cells of S. Proof. Let I be an indiÔ¨Äerence complex. It is implementable if and only if there exists a truthful allocation function f with I(f ) = I. By Proposition 3, this is m equivalent to the fact that there is a payment vector p ‚àà R2 , such that the diÔ¨Äerence sets Qa are exactly the regions of the tropical hypersurface H(up ). As the Newton polytope of up is the unit cube [0, 1]m , Proposition 1 provides a duality between the diÔ¨Äerence sets Qa and the regular subdivision of [0, 1]m with respect to the lifting Œª(a) = ‚àípa . Hence, a maximal cell in the regular subdivision with vertices (a(1) , . . . , a(k) ) corresponds to a maximal set of allocations such that a‚àà{a(1) ,...,a(k) } Qa = ‚àÖ. The latter is a facet of I(f ). Conversely, if we start with a regular subdivision S of the unit cube induced by some lifting Œª, setting the prices pa = ‚àíŒª(a) and deÔ¨Åning f (Œ∏) ‚àà arg max up (Œ∏) results in a DSIC mechanism such that the indiÔ¨Äerence complex I(f ) corresponds to S in the way described in the statement of the theorem.   Note that the proof is constructive. Further, Theorem 5 says that the simplicial complex I(f ) is precisely the crosscut complex of the poset of cells of the regular subdivision S [10, ¬ß10]. If S is a triangulation then its crosscut complex is S itself, seen as an abstract simplicial complex. The main consequence of Theorem 5 is that for truthful one-player mechanisms with additive and quasi-linear utilities, the partitioning of the type space into diÔ¨Äerence sets is characterized by the duality to the regular subdivision of the cube, which is captured by the indiÔ¨Äerence complex. Example 6. Consider the one-player case with m = 3 items and where the player has a type Œ∏ ‚àà R3 . Let f be the local allocation function which we deÔ¨Åne via f (Œ∏) ‚àà arg max{Œ∏ ¬∑ a ‚àí pa | a ‚àà {0, 1}3 }, with p000 = 0, p100 = p010 = p001 = 3/7, p110 = p101 = p011 = 8/7, and p111 = 10/7. Figure 3 shows the subset of the type space for Œ∏ ‚àà [0, 1]3 . The Ô¨Åve facets of I(f ) are {0, 1, 2, 4}, {2, 3, 4, 7}, {1, 4, 5, 7}, {1, 2, 3, 7}, {3, 5, 6, 7}; here we use the binary encoding 4a1 + 2a2 + a3 for the vertex a ‚àà {0, 1}3 . Those cells form a regular triangulation of [0, 1]3 , which is type F in Fig. 4. We deÔ¨Åne two allocation functions f, g : Rm ‚Üí Œ© as combinatorially equivalent if their indiÔ¨Äerence complexes agree; i.e., I(f ) = I(g). As before we are primarily concerned with the case where Œ© = {0, 1}m and the allocation functions are truthful allocations of m items. In this way we can relate the allocation space with regular subdivisions of the cube [0, 1]m . DeÔ¨Ånition 7. A truthful allocation function on m items is nondegenerate if the associated regular subdivision of the m-cube is a triangulation. Triangulations of m-cubes are described in [16, ¬ß6.3]. The Ô¨Årst two columns of Table 1 summarize the known values of the number of all (regular) triangulations of the m-cube. In particular the second column shows the number of  
   
  The Polyhedral Geometry of Truthful Auctions  
   
  239  
   
  Q1111  
   
  Q000 00 0  
   
  Fig. 3. Subdivision of the subset of the type space, where Œ∏ ‚àà [0, 1]3 , induced by the allocation function described in Example 6. The region Q000 corresponds to the corner in the lower back of the cube and Q111 corresponds to the upper front corner. This and other pictures were obtained via polymake [20]. Table 1. Triangulations of m-cubes. Orbit sizes refer to regular triangulations m All 2 3 4  
   
  Regular  
   
  Sym(m)-orbits Œìm -orbits  
   
  2 2 2 74 74 23 92,487,256 87,959,448 3,706,261  
   
  1 6 235,277  
   
  combinatorial types of nondegenerate truthful allocations. The number of all, not necessarily regular, triangulations of the 4-cube was found by Pournin [33]. The corresponding numbers of triangulations for m ‚â• 5 are unknown. Remark 8. Any regular subdivision may be reÔ¨Åned to a regular triangulation, on the same set of vertices; see [16, Lemma 2.3.15]. Our next goal is to explain the third and fourth columns of Table 1. To this end we need to discuss the symmetries of the cube, which are known. That will be the key to understanding (truthful) allocations of exchangeable items. The automorphism group, Œìm , of the m-cube [0, 1]m comprises those bijections on the vertex set which map faces to faces. The group Œìm is known to be a semidirect product of the symmetric group Sym(m) with Zm 2 ; its order is m! ¬∑ 2m . Here the j-th component of Zm 2 Ô¨Çips the j-th coordinate, and this is a reÔ¨Çection at the aÔ¨Éne hyperplane xj = 12 ; that map does not have any Ô¨Åxed points among the vertices of [0, 1]m . The subgroup Zm 2 of all coordinate Ô¨Çips acts transitively on the 2m vertices. The symmetric group Sym(m) naturally acts on the coordinate directions; this is precisely the stabilizer of the origin in Œìm . Since the cells in each triangulation of [0, 1]m are convex hulls of a subset of the vertices, the group Œìm also acts on the set of all triangulations of [0, 1]m . Moreover, since Œìm acts via aÔ¨Éne maps, it sends regular triangulations  
   
   to regular triangulations. The stabilizer Sym(m) acts transitively on the m k vertices of [0, 1]m with exactly k ones. In this way, a Sym(m)-orbit of regular triangulations corresponds to a set of nondegenerate truthful allocation functions for which the  
   
  240  
   
  M. Joswig et al.  
   
  Fig. 4. The combinatorial types of truthful allocation functions corresponding to the six Œì3 -orbits of the 3-cube, together with the corresponding triangulations (exploded) and their tight spans; cf. [16, Fig. 6.35]. The tight span of a regular triangulation S is the subcomplex of bounded cells of the tropical hypersurface dual to S (seen as an ordinary polyhedral complex); see [26, ¬ß10.7]. The numbers below the types show how many regular triangulations or Sym(3)-orbits are of the given type, respectively.  
   
  indiÔ¨Äerence complexes agree, up to permuting the items. We call such allocation functions combinatorially equivalent for exchangeable items. The Sym(m)-orbits of regular triangulations have been computed with mptopcom [25]; see the third column of Table 1. By Theorem 5 those triangulations bijectively correspond to the implementable indiÔ¨Äerence complexes. So that computation furnishes a proof of the following result. Theorem 9. There are 23 combinatorial types of nondegenerate truthful allocation functions for one player and m = 3 exchangeable items. Further, the corresponding count for m = 4 yields 3,706,261. It makes sense to focus on the combinatorics of triangulations, without paying attention to their interpretations for auctions. This amounts to studying the orbits of the full group Œìm acting on the set of (regular) triangulations; see the fourth column of Table 1. The six Œì3 -orbits of triangulations of the 3-cube are depicted in Fig. 4. This number expands to 23 if we consider the possible choices of locating the origin. We illustrate the idea for the subdivision of the type space in Fig. 3. Its Œì3 -orbit splits into two Sym(3)-orbits: one from putting the origin in one of the four cubes or in one of the four noncubical cells.  
   
  The Polyhedral Geometry of Truthful Auctions  
   
  241  
   
  Remark 10. Vidali considered a more restrictive notion of nondegeneracy of allocation functions [39, DeÔ¨Ånition 8], and in [39, Theorem 1] she arrived at a classiÔ¨Åcation of Ô¨Åve types for three items. In our terminology, the number of types is equal to six, which is the count of Œì3 -orbits of regular triangulations of [0, 1]3 reported in Table. 1. The missing type in the classiÔ¨Åcation of Vidali is type F (as in Fig. 4), arising from Example 6. Further details will be given in the full version of this paper.  
   
  4  
   
  Sensitivity of Mechanisms  
   
  In this section, we study by how much the allocations for a Ô¨Åxed player may change under a slight modiÔ¨Åcation of the reported type. These changes are measured in the following two ways. For two localallocations a, b ‚àà {0, 1}m , let the cardinality distance be dc (a, b) := |a|1 ‚àí |b|1 , and let the Hamming distance be dh (a, b) := |a ‚àí b|1 , where | ¬∑ |1 is the 1-norm. Note that the cardinality distance is a pseudometric. Let f be an one-player allocation function, we deÔ¨Åne the cardinality sensitivity of f as    Œºc (f ) = max dc (a, b)  a, b ‚àà F for some F ‚àà I(f ) . The Hamming sensitivity Œºh (f ) arises in the same way, with dh instead of dc . Intuitively, the cardinality sensitivity Œºc (f ) is the maximal amount such that any slight change in the type of the player does not cause her allocated bundle to change its cardinality by more than Œºc (f ). Let Œ¶m be the set of truthful allocation functions for one player and m items. We are now interested in computing the values Mc (m) := minf ‚ààŒ¶m Œºc (f ) and Mh (m) := minf ‚ààŒ¶m Œºh (f ). Our strategy to compute these values is as follows. From Theorem 5 we know that the indiÔ¨Äerence complexes of allocation functions f ‚àà Œ¶m are in bijection with the regular subdivisions of [0, 1]m . So we need to identify those subdivisions, for which the maximal distance between any two vertices of one of its cells is minimized. In this way, we can compute Mc (m) exactly, and we give bounds for Mh (m). Proposition 11. The minimal cardinality sensitivity of DSIC one-player mechanisms is Mc (m) = 1. Proof. We Ô¨Årst slice the unit cube into the polytopes    m   m  Pk = x ‚àà [0, 1]  k ‚àí 1 ‚â§ xi ‚â§ k , k = 1, . . . , m .  i=1  
   
  The polytopes P1 , . . . , Pm form the maximal cells of a polytopal subdivision S m 2 of [0, 1]m . That subdivision is regular with height function Œª(x) = ‚àí ( i=1 xi ) . This proves the claim, as for each Pk , the diÔ¨Äerence in the coordinate sums of two of its vertices diÔ¨Äer by at most one.    
   
  242  
   
  M. Joswig et al.  
   
  Note that the height function we used in the proof of the last proposition m 2 leads to the mechanism which is deÔ¨Åned by the prices p(a) = ( i=1 ai ) for the allocations a ‚àà {0, 1}m . Proposition 12. The minimal Hamming sensitivity for DSIC one-player mechanisms on m ‚â• 3 items is bounded by 2 ‚â§ Mh (m) ‚â§ m ‚àí 1. Proof. For the lower bound let us consider a triangle with the vertices a, b, c ‚àà {0, 1}m . If we assume dh (a, b) = dh (a, c) = 1 then the vertices a and b (resp. a and c) diÔ¨Äer by a coordinate Ô¨Çip. Therefore, the vertices b and c diÔ¨Äer by either two coordinate Ô¨Çips or none. As b = c, the former is the case and dh (b, c) = 2. As the maximal cells of a subdivision of [0, 1]m for m ‚â• 2 contain at least three vertices, this proves the lower bound. For the upper bound, we show that there is a subdivision, S, of [0, 1]m such that no cell of S contains two antipodal vertices, i.e., two vertices such that their sum equals the all ones vector. We Ô¨Årst consider the case where m is odd. For a vertex x ‚àà {0, 1}m , let Œî(x) be the cornered simplex with apex x. That is, its vertices comprise x and all its neighbors in the vertex-edge graph of the unit cube; cf. [16, Fig. 6.3.1]. Let Sm be the subdivision of [0, 1]m with the following maximal cells: the big cell is the convex hull of all vertices with an even number m of ones,  and the small cells are the cornered simplices Œî(x), where x ‚àà {0, 1} with xi odd. The subdivision S3 is the triangulation of type F in Fig. 4; for m ‚â• 5 the big cell is not a simplex, and so Sm is not a triangulation in general. At any rate, the subdivision Sm is always regular: it is induced by the height function which sends a vertex x to 0, if it has an even number of ones and to ‚àí1, if that number is odd. For m ‚â• 3 odd, no antipodal pair of vertices is contained in any cell of Sm , which proves the claim for the uneven case. If the dimension m is even, we consider the m-dimensional unit cube as a prism over [0, 1]m‚àí1 . Then m ‚àí 1 is odd, and we can employ the subdivision Sm‚àí1 of [0, 1]m‚àí1 that we discussed before. We obtain a subdivision, Sm , of [0, 1]m whose maximal cells are prisms over the maximal cells of Sm‚àí1 . The subdivision Sm is again regular: this can be seen from assigning the vertices x √ó {0} and x √ó {1} the same height as the vertex x in Sm‚àí1 . Now let P be a maximal cell in Sm‚àí1 , such that Q = P √ó [0, 1] is a maximal cell of Sm . If Q contained an antipodal pair of vertices, then by removing the last coordinate, we would get an antipodal pair in P , which is absurd. This completes the proof.    
   
  5  
   
  Conclusion  
   
  We studied DSIC allocation mechanisms where a set of m items is allocated to n players. These mechanisms can be described by the corresponding one-player mechanisms when the types declared by the other players are Ô¨Åxed. For a single player, the allocations correspond to vectors {0, 1}m , and the combinatorial types of the allocation mechanisms correspond to regular subdivisions of the m-dimensional unit cube. We then used this insight to design mechanisms that  
   
  The Polyhedral Geometry of Truthful Auctions  
   
  243  
   
  are robust in the sense that small changes in the declared type do not lead to a major change in the set of allocated items. In the full version of this paper, we will show how this method can be applied in order to describe aÔ¨Éne maximizers with n players. For multiple copies of items, the deterministic allocations to a single player correspond to a subset of the lattice Nm , and it seems plausible that DSIC mechanisms for such scenarios can also be described by regular subdivisions. Question 13. How does our approach generalize to allocation mechanisms in a setting with multiple copies of items? Acknowledgments. We thank Benny Moldovanu for pointing out the work of Frongillo and Kash [19]. Further, we are indebted to three anonymous reviewers for their comments and corrections. This work was supported by Deutsche Forschungsgemeinschaft under Germany‚Äôs Excellence Strategy, Berlin Mathematics Research Center (Grant EXC-2046/1, project-ID 390685689). M. Joswig has further been supported by ‚ÄúSymbolic Tools in Mathematics and their Application‚Äù (TRR 195, project-ID 286237555).  
   
  References 1. Akian, M., Gaubert, S., Guterman, A.: Tropical polyhedra are equivalent to mean payoÔ¨Ä games. Internat. J. Algebra Comput. 22(1), 1250001 (2012). https://doi. org/10.1142/S0218196711006674 2. Allamigeon, X., Benchimol, P., Gaubert, S., Joswig, M.: Combinatorial simplex algorithms can solve mean payoÔ¨Ä games. SIAM J. Opt. 24(4), 2096‚Äì2117 (2014). https://doi.org/10.1137/140953800 3. Allamigeon, X., Benchimol, P., Gaubert, S., Joswig, M.: What tropical geometry tells us about the complexity of linear programming. SIAM Rev. 63(1), 123‚Äì164 (2021). https://doi.org/10.1137/20M1380211 4. Archera, A., Kleinberg, R.: Truthful germs are contagious: a local-to-global characterization of truthfulness. Games Econ. Behav. 86, 340‚Äì366 (2014). https://doi. org/10.1016/j.geb.2014.01.004 5. Ashlagi, I., Braverman, M., Hassidim, A., Monderer, D.: Monotonicity and implementability. Econometrica 78(5), 1749‚Äì1772 (2010). https://doi.org/10.3982/ ECTA8882 6. Aurenhammer, F.: Power diagrams: Properties, algorithms and applications. SIAM J. Comput. 16(1), 78‚Äì96 (1987). https://doi.org/10.1137/0216006 7. Baldwin, E., Klemperer, P.: Understanding preferences: ‚Äúdemand types", and the existence of equilibrium with indivisibilities. Econometrica 87(3), 867‚Äì932 (2019). https://doi.org/10.3982/ECTA13693 8. Berger, A., M√ºller, R., Naeemi, S.H.: Characterizing implementable allocation rules in multi-dimensional environments. Soc. Choice Welfare 48(2), 367‚Äì383 (2016). https://doi.org/10.1007/s00355-016-1008-6 9. Bikhchandani, S., Chatterji, S., Lavi, R., Mu‚Äôalem, A., Nisan, N., Sen, A.: Weak monotonicity characterizes deterministic dominant-strategy implementation. Econometrica 74(4), 1109‚Äì1132 (2006). https://doi.org/10.1111/j.1468-0262.2006. 00695.x  
   
  244  
   
  M. Joswig et al.  
   
  10. Bj√∂rner, A.: Topological methods. In: Handbook of Combinatorics, vol. 1, 2, pp. 1819‚Äì1872. Elsevier, Amsterdam (1995) 11. Carbajala, J.C., M√ºller, R.: Implementability under monotonic transformations in diÔ¨Äerences. J. Econ. Theory 160, 114‚Äì131 (2015). https://doi.org/10.1016/j.jet. 2015.09.001 12. Christodoulou, G., Koutsoupias, E., Kov√°cs, A.: On the Nisan-Ronen conjecture. In: 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pp. 839‚Äì850 (2022). https://doi.org/10.1109/FOCS52979.2021.00086 13. Christodoulou, G., Koutsoupias, E., Vidali, A.: A characterization of 2-player mechanisms for scheduling. In: Proceedings of the 16th Annual European Symposium on Algorithms, (ESA), pp. 297‚Äì307 (2008). https://doi.org/10.1007/9783-540-87744-8_25 14. Christodoulou, G., Koutsoupias, E., Vidali, A.: A lower bound for scheduling mechanisms. Algorithmica 55(4), 729‚Äì740 (2009). https://doi.org/10.1007/s00453-0089165-3 15. Clarke, E.H.: Multipart pricing of public goods. Public Choice 11, 17‚Äì33 (1971). https://doi.org/10.1007/bf01726210 16. De Loera, J.A., Rambau, J., Santos, F.: Triangulations: Structures for algorithms and applications, Algorithms and Computation in Mathematics, vol. 25. Springer, Berlin (2010). https://doi.org/10.1007/978-3-642-12971-1 17. Dobzinski, S., Shaulker, A.: Improved lower bound for truthful scheduling, abs/2007.04362 (2020) 18. Edelman, P.H., Weymark, J.A.: Dominant strategy implementability and zero length cycles. Econ. Theor. 72(4), 1091‚Äì1120 (2020). https://doi.org/10.1007/ s00199-020-01324-7 19. Frongillo, R.M., Kash, I.A.: General truthfulness characterizations via convex analysis. Games Econ. Behav. 130, 636‚Äì662 (2021). https://doi.org/10.1016/j.geb. 2021.09.010 20. Gawrilow, E., Joswig, M.: a framework for analyzing convex polytopes. In: Polytopes‚Äìcombinatorics and computation (Oberwolfach, 1997), DMV Sem., vol. 29, pp. 43‚Äì73. Birkh√§user, Basel (2000). https://doi.org/10.1007/978-3-0348-84389_2 21. Giannakopoulos, Y., Hammerl, A., Po√ßas, D.: A New Lower Bound for Deterministic Truthful Scheduling. Algorithmica 83(9), 2895‚Äì2913 (2021). https://doi.org/ 10.1007/s00453-021-00847-2 22. Groves, T.: Incentives in teams. Econometrica 41, 617‚Äì631 (1973). https://doi. org/10.2307/1914085 23. Gui, H., M√ºller, R., Vohra, R.V.: Dominant strategy mechanisms with multidimensional types. In: Lehmann, D., M√ºller, R., Sandholm, T. (eds.) Computing and Markets. Dagstuhl Seminar Proceedings, vol. 5011, pp. 1‚Äì23 (2005). https:// doi.org/10.4230/DagSemProc.05011.8 24. Jehiel, P., Moldovanu, B., Stacchetti, E.: Multidimensional mechanism design for auctions with externalities. J. Econ. Theory 85(2), 258‚Äì293 (1999). https://doi. org/10.1006/jeth.1998.2501 25. Jordan, C., Joswig, M., Kastner, L.: Parallel enumeration of triangulations. Electron. J. Combin. 25(3), Paper 3.6, 27 (2018). https://doi.org/10.37236/7318 26. Joswig, M.: Essentials of tropical combinatorics. Graduate Studies in Mathematics, American Mathematical Society, Providence, RI (2022) 27. Kushnir, A.I., Lokutsievskiy, L.V.: When is a monotone function cyclically monotone? Theor. Econ. 16, 853‚Äì879 (2021). https://doi.org/10.3982/TE4305  
   
  The Polyhedral Geometry of Truthful Auctions  
   
  245  
   
  28. Lin, B., Tran, N.M.: Two-player incentive compatible outcome functions are aÔ¨Éne maximizers. Linear Algebra Its Appl. 578, 133‚Äì152 (2019). https://doi.org/10. 1016/j.laa.2019.04.027 29. Maclagan, D., Sturmfels, B.: Introduction to tropical geometry, Graduate Studies in Mathematics, vol. 161. American Mathematical Society, Providence, RI (2015) 30. Mishra, D., Pramanik, A., Roy, S.: Multidimensional mechanism design in single peaked type spaces. J. Econ. Theory 153, 103‚Äì116 (2014). https://doi.org/10. 1016/j.jet.2014.06.002 31. Nisan, N., Ronen, A.: Algorithmic mechanism design. Games Econ. Behav. 35(1), 166‚Äì196 (2001). https://doi.org/10.1006/game.1999.0790 32. Nisan, N., Roughgarden, T., Tardos, E., Vazirani, V.V.: Algorithmic game theory. Cambridge University Press, USA (2007). https://doi.org/10.1017/ CBO9780511800481 33. Pournin, L.: The Ô¨Çip-graph of the 4-dimensional cube is connected. Discrete Comput. Geometry 49(3), 511‚Äì530 (2013). https://doi.org/10.1007/s00454-013-9488y 34. Roberts, K.: The characterization of implementable choice rules. Aggregation and Revelation of Preferences, pp. 321‚Äì349 (1979) 35. Rochet, J.C.: A necessary and suÔ¨Écient condition for rationalizability in a quasilinear context. J. Math. Econ. 16(2), 191‚Äì200 (1987). https://doi.org/10.1016/ 0304-4068(87)90007-3 36. Saks, M.E., Yu, L.: Weak monotonicity suÔ¨Éces for truthfulness on convex domains. In: Riedl, J., Kearns, M.J., Reiter, M.K. (eds.) Proceedings of the 6th ACM Conference on Electronic Commerce (EC), pp. 286‚Äì293 (2005). https://doi.org/10.1145/ 1064009.1064040 37. Tran, N.M., Yu, J.: Product-mix auctions and tropical geometry. Math. Oper. Res. 44(4), 1396‚Äì1411 (2019). https://doi.org/10.1287/moor.2018.0975 38. Vickrey, W.: Counterspeculation, auctions, and competitive sealed tenders. J. Finance 16, 8‚Äì37 (1961). https://doi.org/10.1111/j.1540-6261.1961.tb02789.x 39. Vidali, A.: The geometry of truthfulness. In: Leonardi, S. (ed.) Proceedings of the 5th International Workshop on Internet and Network Economics (WINE), pp. 340‚Äì350 (2009). https://doi.org/10.1007/978-3-642-10841-9_31 40. Vohra, R.V.: Mechanism design. A Linear Programming Approach, Econometric Society Monographs, vol. 47. Cambridge University Press, Cambridge (2011). https://doi.org/10.1017/CBO9780511835216 41. de Vries, S., Vohra, R.V.: Combinatorial auctions: a survey. INFORMS J. Comput. 15, 284‚Äì309 (2003). https://doi.org/10.1287/ijoc.15.3.284.16077  
   
  Competitive Kill-and-Restart and Preemptive Strategies for Non-clairvoyant Scheduling Sven J√§ger1 , Guillaume Sagnol2(B) , Daniel Schmidt genannt Waldschmidt2 , and Philipp Warode3 1  
   
  RPTU Kaiserslautern-Landau, Paul-Ehrlich-Stra√üe 14, 67663 Kaiserslautern, Germany [email protected]  2 TU Berlin, Stra√üe des 17. Juni 136, 10623 Berlin, Germany {sagnol,dschmidt}@math.tu-berlin.de 3 HU Berlin, Unter den Linden 6, 10099 Berlin, Germany [email protected]   
   
  Abstract. We study kill-and-restart and preemptive strategies for the fundamental scheduling problem of minimizing the sum of weighted completion times on a single machine in the non-clairvoyant setting. First, we show a lower bound of 3 for any deterministic non-clairvoyant kill-andrestart strategy. Then, we give for any b > 1 a tight analysis for the natural b-scaling kill-and-restart strategy as well as for a randomized variant ‚àö of it. In particular, we show a competitive ratio of (1 + 3 3) ‚âà 6.197 for the deterministic and of ‚âà 3.032 for the randomized strategy by making use of the largest eigenvalue of a Toeplitz matrix. In addition, we show that the preemptive Weighted Shortest Elapsed Time First (WSETF) rule is 2-competitive when jobs are released online, matching the lower bound for the unit weight case with trivial release dates for any nonclairvoyant algorithm. Furthermore, we prove performance guarantees smaller than 10 for adaptions of the b-scaling strategy to online release dates and unweighted jobs on identical parallel machines.  
   
  1  
   
  Introduction  
   
  Minimizing the total weighted completion time on a single processor is one of the most fundamental problems in the Ô¨Åeld of machine scheduling. The input consists of n jobs with processing times p1 , . . . , pn and weights w1 , . . . , wn , and the task is  to sequence them in such a way that the sum of weighted complen wj Cj . tion times j=1 wj Cj is minimized. We denote this problem as 1 || Full version preprint: http://arxiv.org/abs/2211.02044. The research of the second, third and fourth authors was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany‚Äôs Excellence Strategy ‚Äî The Berlin Mathematics Research Center MATH+ (EXC-2046/1, project ID: 390685689). c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 246‚Äì260, 2023. https://doi.org/10.1007/978-3-031-32726-1_18  
   
  Competitive Kill-and-Restart and Preemptive Strategies  
   
  247  
   
  Smith [29] showed in the 50‚Äôs that the optimal schedule is obtained by the Weighted Shortest Processing Time Ô¨Årst (WSPT) rule, i.e., jobs are sequenced in non-decreasing order of the ratio of their processing time and their weight. Reality does not always provide all information beforehand. Around 30 years ago, the non-clairvoyant model, in which the processing time of any job becomes known only upon its completion, was introduced for several scheduling problems [10,25,27]. It is easy to see that no non-preemptive non-clairvoyant algo rithm can be constant-competitive for 1 || Cj . In their seminal work, Motwani et al. [25] proved for this problem that allowing preemption breaks the nonconstant barrier. SpeciÔ¨Åcally, they showed that the round-robin algorithm is 2-competitive, matching a lower bound for all non-clairvoyant algorithms. This opened up a new research direction, leading to constant-competitive preemptive non-clairvoyant algorithms in more general settings, like weighted jobs [21], multiple machines [6,15,16], precedence constraints [11], and non-trivial release dates. When jobs are released over time, they are assumed to be unknown before their arrivals (online scheduling). No lower bound better than 2 is known for this case, whereas the best known upper bound before this work was 3, see e.g. [23]. But there is a downside of the preemptive paradigm as it uses an unlimited number of interruptions at no cost and has a huge memory requirement to maintain the ability to resume all interrupted jobs. Therefore, we continue by studying the natural class of kill-and-restart strategies that‚Äîinspired by computer processes‚Äîcan abort the execution of a job (kill), but when processed again later, the job has to be re-executed from the beginning (restart). It can be considered as an intermediate category of algorithms between preemptive and non-preemptive ones, as on the one hand jobs may be interrupted, and on the other hand all completed jobs have been processed as a whole. Hence, by removing all aborted executions one obtains a non-preemptive schedule. This class of algorithms has already been investigated since the 90‚Äôs [27], but to the best of our knowledge, the competitive ratio of non-clairvoyant kill-and-restart strategies for the total completion time objective has never been studied. Our Contribution. We start by strengthening the preemptive lower bound of 2 for the kill-and-restart model.  Theorem 1. For 1 || Cj , no deterministic non-clairvoyant kill-and-restart 2 on instances with strategy can achieve a competitive ratio smaller than 3 ‚àí n+1 n ‚â• 3 jobs, even if every job j has processing time pj ‚â• 1. The main part of this work is devoted to the natural b-scaling strategy Db that repeatedly probes each unÔ¨Ånished job for the time of some integer power of b > 1 multiplied by its weight. While the fact that D2 is 8-competitive can easily be concluded from the 2-competitiveness of the weighted round-robin algorithm [21], a tight analysis requires more involved techniques. 3/2    wj Cj . Moreover, Theorem 2. For b > 1, Db is 1+ 2b b‚àí1 -competitive for 1 || for all b > 1 this bound is tight, even for unit weight instances. In particular, for ‚àö b = 3 the competitive ratio is 1 + 3 3 ‚âà 6.196.  
   
  248  
   
  S. J√§ger et al.  
   
  Our main technique is to reduce the problem of Ô¨Ånding the exact competitive ratio to the computation of the largest eigenvalue of a tridiagonal Toeplitz matrix. Subsequently, we obtain a signiÔ¨Åcantly better exact competitive ratio for a randomized version Rb of the b-scaling strategy. ‚àö  Theorem 3. For every b > 1, Rb is ‚àöb+2b‚àí1 -competitive for 1 || wj Cj . b log(b) Moreover, for all b > 1 this bound is tight, even for unit weight instances. In particular, for b ‚âà 8.16 the competitive ratio is ‚âà 3.032. The analysis mimics that of the deterministic strategy, but it is necessary to group jobs with similar Smith ratios. This approach leads to the computation of the largest eigenvalues of a sequence of banded symmetric Toeplitz matrices, and the result is obtained by taking its limit. We then study more general scheduling environments. For the  online problem wj Cj , we close in which jobs are released over time, denoted by 1 | rj , pmtn | the gap for the best competitive ratio of preemptive algorithms by analyzing the Weighted Shortest Elapsed Time First rule (WSETF). This algorithm runs at every point in time the job(s) with minimum ratio of elapsed processing time over weight.  wj Cj . Theorem 4. WSETF is 2-competitive for 1 | rj , pmtn | Theorem 4 generalizes the known 2-competitiveness for trivial release dates shown by Kim and Chwa [21]. It also matches the performance guarantee of the best known stochastic online scheduling policy, called F-GIPP [24], for the stochastic variant of the problem, where the probability distributions of the processing times are given at the release dates and the expected objective value is to be minimized. Our improvement upon the analysis of this policy, applied to a single machine, is threefold: First, our strategy does not require any information about the distributions of the processing times, second, we compare to the clairvoyant optimum, while F-GIPP is compared to the optimal non-anticipatory policy, and third, WSETF is more intuitive and easier to implement in applications than the F-GIPP policy. Using Theorem 4, we then give an upper bound on the competitive ratio of a generalized version of Db for jobs arriving online over time.  2b4 Theorem 5. Db is 2b2 ‚àí3b+1 -competitive for 1 | rj | wj Cj . In particular, for ‚àö 9+ 17 , 8  
   
  ‚àö 107+51 17 32  
   
  ‚âà 9.915.  Finally, we also analyze the unweighted problem P || Cj on multiple identical parallel machines.  2 ‚àíb Theorem 6. Db is 3bb‚àí1 -competitive for P || Cj . In particular, for b = ‚àö ‚àö 3+ 6 3 , its performance guarantee is 5 + 2 6 ‚âà 9.899.  
   
  b=  
   
  its performance guarantee is  
   
  The proofs of these results are sketched in Sect. 3 to 6 below; full proofs are provided in the preprint [17].  
   
  Competitive Kill-and-Restart and Preemptive Strategies  
   
  249  
   
  Related Work. The clairvoyant oÔ¨Ñine variants of all scheduling problems considered in this paper are well understood; either there is a polynomialtime algorithm [8,29], or the problem is strongly NP-hard [22,26] and there is a polynomial-time approximation scheme [1]. In the clairvoyant online model, where the processing times become known at the jobs‚Äô release dates, there is a 1.566-competitive deterministic algorithm [28] and a deterministic lower bound of 1.073 [9], when preemption is allowed. For non-preemptive online scheduling the best possible deterministic competitive ratio is exactly 2. [2,14]. In the non-clairvoyant setting no (randomized) non-preemptive algorithm is constant-competitive, so that allowing preemption is crucial. Motwani et al. [25] showed that the simple (non-clairvoyant) round-robin procedure has a competitive ratio of 2 for minimizing the total completion time on identical machines. A weighted variant was presented for a single machine by Kim and Chwa [21] and for identical machines by Beaumont et al. [6]. In the context of non-clairvoyant online scheduling one distinguishes between total (weighted) completion time and total (weighted) Ô¨Çow time. For weighted Ô¨Çow time constant competitiveness is unattainable [25]. Besides work on non-constant competitiveness [7], the problem has been primarily studied in the resource augmentation model [18]. Kim and Chwa and Bansal and Dhamdhere [4] independently showed that WSETF is (1 + Œµ)-speed (1 + 1/Œµ)-competitive for weighted Ô¨Çow time on a single machine, entailing a 4-competitive algorithm without speed augmentation for weighted completion time [5]. The proof technique used is, however, not suitable for obtaining better bounds for the weighted completion time objective. For unrelated machines there is a (1 + Œµ)-speed O(1/Œµ2 )-competitive algorithm [16]. Shmoys et al. [27] introduced the kill-and-restart model in the context of makespan minimization. We are not aware of any work for the total completion time objective in the non-clairvoyant model. However, for the clairvoyant online model, lower bounds on the competitive ratio of kill-and-restart strategies have been obtained [9,31], and van Stee and La Poutr√© [30] developed a 3/2-competitive strategy for a single machine, beating the aforementioned best competitive ratio of 2 for non-preemptive algorithms. Motwani et al. also considered preemptive scheduling with a limited number of allowed preemptions, for which they devised algorithms similar to the kill-and-restart strategies presented in this paper. The kill-and-restart model also shares many similarities with optimal search problems, in particular the w-lanes cow-path problem. For w = 2, deterministic and randomized strategies achieving the best possible competitive ratio are studied in [3,20], respectively. This work has been extended by Kao et al. [19] to the general case w ‚àà N.  
   
  2  
   
  Preliminaries  
   
  We consider the machine scheduling problem of minimizing the weighted sum of completion times on a single machine ( 1 || wj Cj ). Formally, an instance I = (p, w) consists of a vector of processing times p = (pj )nj=1 and a vector  
   
  250  
   
  S. J√§ger et al.  
   
  of weights w = (wj )nj=1 . Sequencing jobs in WSPT order, i.e., ordered nondecreasingly by their Smith ratios pj /wj , yields an optimal schedule, denoted by OPT(I). We also denote its objective value by OPT(I). The focus of our work lies on the analysis of non-clairvoyant strategies. We call a strategy non-clairvoyant if it does not use information on the processing time pj of a job j before j has been completed. A deterministic strategy D is said to be c-competitive if, for all instances I = (p, w), D(I) ‚â§ c ¬∑ OPT(I), where D(I) denotes the cost of the strategy D for instance I. The competitive ratio of D is deÔ¨Åned as the inÔ¨Åmum over all c such that D is c-competitive. Similarly, a randomized strategy R, is said to be c-competitive if, for all instances I = (p, w), E[R(I)] ‚â§ c ¬∑ OPT(I), where E[¬∑] denotes the expected value. The following proposition suggests to consider strategies beyond non-preemptive ones. Proposition 1. No randomized non-preemptive non-clairvoyant strategy has a  constant competitive ratio for 1 || Cj . Proof sketch. Consider n ‚àí 1 unit jobs and one job of length n2 , randomize uniformly over all permutations, and use Yao‚Äôs principle [32].   Due to this negative result, we study non-clairvoyant kill-and-restart strategies for 1 || wj Cj that may abort the processing of a job, but when it is processed again later, it has to be executed from the beginning. Such a strategy performs probings (t, j, œÑ ), i.e., it processes at time t job j for a time of min{œÑ, pj }. More formally, for a given state consisting of the current time, the set of unÔ¨Ånished jobs and lower bounds on the processing times learned from past probings, a kill-and-restart strategy decides on a family of probings (ti , ji , œÑi )i‚ààI , such that the intervals (ti , ti + œÑi ), i ‚àà I, are disjoint. Whenever a job is completed, i.e., a job is processed completely within one probing, the strategy decides on new probings. We require that probings be chosen independently of the actual processing times of unÔ¨Ånished jobs, ensuring that kill-and-restart strategies are non-clairvoyant. A formal deÔ¨Ånition is given in the full version. Observe that such strategies may not be implementable, e.g., on a Turing machine, as the above deÔ¨Ånition allows for an inÔ¨Ånite number of probings in a bounded time range. This subtlety is in fact inherent to all scheduling problems with unknown processing times or search problems with unknown distances. It is not hard to see that no deterministic kill-and-start strategy can be constantcompetitive without inÔ¨Ånitesimal probing, as there is no lower bound on the processing times at time 0. On the other hand, inÔ¨Ånitesimal probing can be avoided if we know a lower bound on the pj ‚Äôs, thus turning the strategies analyzed in this paper into implementable algorithms. We denote by YjD (I, t) the total time for which the machine has been busy processing job j until time t in the schedule constructed by the strategy D on the instance I.  
   
  3  
   
  Lower Bound  
   
  In contrast to the lower bound of 2 for preemptive algorithms by Motwani et al. [25], we prove a higher lower bound for kill-and-restart strategies.  
   
  Competitive Kill-and-Restart and Preemptive Strategies  
   
  251  
   
   Theorem 1. For 1 || Cj , no deterministic non-clairvoyant kill-and-restart 2 on instances with strategy can achieve a competitive ratio smaller than 3 ‚àí n+1 n ‚â• 3 jobs, even if every job j has processing time pj ‚â• 1. 2  
   
  +n) 2 Proof. Let Œµ ‚àà ( n+1 , 1] and deÔ¨Åne T := (2‚àíŒµ)(n Œµ(n+1)‚àí2 . Consider an arbitrary deterministic kill-and-restart strategy  D with the initially chosen family of probings (ti , ji , œÑi )i‚ààI . Let Yj (Œ∏) := i‚ààI:ti 1 is simple and quite natural: it proceeds by rounds q ‚àà Z. In round q every non-completed job j is probed (once) for wj bq , in the order of job indices. To execute Db , we can store for each job its rank at time t, i.e., the largest q such that it was probed for wj bq‚àí1 until t. At any end of a probing, Db probes the job j with minimum rank q and minimum index for time wj bq . We also introduce a randomized variant Rb of the strategy Db . Randomization occurs in two places: First the jobs are reordered according to a permutation Œ£ drawn uniformly at random from Sn at the beginning. Second, we replace the probing time wj bq of the qth round by wj bq+Œû with a random uniform oÔ¨Äset Œû ‚àº U([0, 1]). In general, Db starts with inÔ¨Ånitesimally small probings at time 0 in rounds q ‚Üí ‚àí‚àû. As discussed earlier, this is not implementable. However, if a lower bound of wj bqmin on every processing time pj is known, the algorithm can start with round q = qmin . 4.1  
   
  The Deterministic b-scaling Strategy  
   
   wj Cj . For an We compute tight bounds for the competitive ratio of Db for 1 || instance I := (p, w), we denote by sj := pj /wj the Smith ratio of job j ‚àà [n] and  
   
  252  
   
  S. J√§ger et al.  
   
  by Djk := YjDb (I, CkDb (I)) the amount of time spent probing job j before the completion of job k. For all j, k ‚àà [n] we deÔ¨Åne the weighted mutual delay Œîjk by Œîjk := wk Djk + wj Dkj if j = k and Œîjj := wj Djj . Thus, it holds Db (I) =  
   
  n  j=1  
   
  wj CjDb (I)  
   
  =  
   
  n  j=1  
   
  wj  
   
  n   
   
  Dkj =  
   
  k=1  
   
    
   
  Œîjk .  
   
  1‚â§j‚â§k‚â§n  
   
  We Ô¨Årst provide an overestimator of Œîjk that is piecewise linear in sj and sk . Lemma 1. Define the function F : {(s, s ) ‚àà R2>0 : s ‚â§ s } ‚Üí R by  log s+1 b 2b + s if logb (s) = logb (s ) b‚àí1 F (s, s ) := 2 logb s +1 b ¬∑ ( b‚àí1 + 1) + s otherwise. Then for all j, k ‚àà [n] such that sj ‚â§ sk , it holds Œîjk ‚â§ wj wk F (sj , sk ). Proof sketch. Let qj := logb (sj )‚àí1, so that bqj < sj ‚â§ bqj +1 . By distinguishing between the case where jobs j, k complete in the same round, i.e., logb (sj ) = logb (sk ), and the case where k completes in a later round, we obtain an upper bound of the form Œîjk ‚â§ wj wk FÀú (sj , sk ), where FÀú is deÔ¨Åned as F except that all occurrences of Ô¨Çoor operations logb (s), s ‚àà {sj , sk } are replaced by logb (s) ‚àí 1. The result follows by observing that FÀú is non-decreasing with respect to both its arguments and taking its upper semi-continuous envelope.   Summing the bounds of the previous lemma yields    wj wk F min(sj , sk )), max(sj , sk ) =: U (p, w). Db (p, w) ‚â§  
   
  (1)  
   
  1‚â§j‚â§k‚â§n  
   
  We next prove a lemma showing that for bounding the ratio U/OPT we can restrict to instances in which all Smith ratios are integer powers of b. Lemma 2. For any instance (p, w), there exists another instance (p , w) with pj = wj bqj for some qj ‚àà Z, for all j ‚àà [n], such that U (p, w) U (p , w) ‚â§ . OPT(p, w) OPT(p , w) Proof sketch. Let Q := {logb (sj ) : j ‚àà [n]} \ Z. We construct the vector p by sequentially rounding the Smith ratios of a subset of jobs Jq = {j ‚àà [n] : sj = bq } with q := min Q to a larger or a smaller value, in such a way that the ratio U/OPT does not decrease, and we repeat this until Q = ‚àÖ. SpeciÔ¨Åcally, for Œ¥ ‚àà R we deÔ¨Åne the modiÔ¨Åed processing times pj (Œ¥) = pj + wj Œ¥1Jq (j). Both Œ¥ ‚Üí U (p(Œ¥), w) and Œ¥ ‚Üí OPT(p(Œ¥), w) are linear in a neighborhood of 0 in which the order of the Smith ratios does not change and no job changes the round U (p(Œ¥),w ) where it completes. Thus, Œ¥ ‚Üí OPT(p(Œ¥),w ) is a monotone rational function in this neighborhood, so Œ¥ can be increased or decreased to a value such that |Q| is decremented by 1, without decreasing our bound on the competitive ratio.    
   
  Competitive Kill-and-Restart and Preemptive Strategies  
   
  253  
   
  The next lemma gives a handy upper bound for the competitive ratio of Db relying on the ratio of two quadratic forms.   Lemma 3. For every L ‚àà N, let AL := 12 bmin(i,j)‚àí1 ¬∑ 1{i=j} 1‚â§i,j‚â§L and  1 min(i,j)‚àí1  BL := 2 b be symmetric matrices. For any instances (p, w) 1‚â§i,j‚â§L  
   
  there exists an integer L and a vector x ‚àà RL ‚â•0 such that  
   
  2b x AL x Db (p, w) ‚â§ +1+b¬∑ . OPT(p, w) b‚àí1 x BL x Proof sketch. Let i0 ‚àà Z and L ‚àà N be such that the Smith ratio of each job is of the form bi0 ‚àí1+ for some ‚àà [L] in the instance (p , w) from Lemma 2. Let J i0 ‚àí1+ be the subset of jobs with Smith ratio , for ‚àà [L], and deÔ¨Åne the  equal to b  L vectors x, y ‚àà R such that x = j‚ààJ wj and y = j‚ààJ wj2 , respectively. We 2b +1)¬∑OPT(p , w)+ show that OPT(p , w) ‚â• bi0 x BL x and that U (p , w) = ( b‚àí1 i0 +1 b x AL x. Then, the result follows from (1) and Lemma 2.   In order to determine an upper bound for the competitive ratio of Db , we need to bound the ratio of the two quadratic forms in the bound of Lemma 3. To this end, we bound the maximum eigenvalue of the matrix ZL := YL‚àí AL YL‚àí1 , where YL YL = BL is the Cholesky decomposition of the matrix BL . An explicit computation of the matrix ZL reveals that it is tridiagonal. In particular, the principal submatrix of ZL obtained by deleting the Ô¨Årst row and Ô¨Årst column is a tridiagonal Toeplitz matrix, which we refer to as TL‚àí1 . 3/2    wj Cj . Moreover, Theorem 2. For b > 1, Db is 1+ 2b b‚àí1 -competitive for 1 || for all b > 1 this bound is tight, even for unit weight instances. In particular, for ‚àö b = 3 the competitive ratio is 1 + 3 3 ‚âà 6.196.   
   
  Lx Proof sketch. Due to Lemma 3, it only remains to bound œÅL := supx‚ààRL xx  A . B Lx As described above, we can express œÅL as the largest eigenvalue of the matrix ZL , whose principal submatrix TL‚àí1 ‚àöis a tridiagonal Toeplitz matrix. This has ‚àí2/(b‚àí1) on the main diagonal and b/(b‚àí1) on both adjacent diagonals. We show value as the largest eigenthat the largest eigenvalue of ZL converges to the same ‚àö ‚àö ‚àí2 œÄ L‚Üí‚àû ‚àí2 + 2b‚àí1b ¬∑ cos L ‚àí‚àí‚àí‚àí‚Üí b‚àí1 + 2b‚àí1b . value of TL‚àí1 , which has the closed form b‚àí1 3/2   Therefore, we obtain by Lemma 3 that Db is 1 + 2b b‚àí1 -competitive. This ratio is minimized for b = 3, yielding the desired bound. For the tightness part, we deÔ¨Åne the vector xL ‚àà RL ‚â•0 by  
   
  ‚àí1  
   
  xL, = 2(Lb  
   
  and show that  
   
  ‚àí1/2  
   
  (b ‚àí 1))  
   
  x L A L xL x L B LxL  
   
   ¬∑ max 0,  
   
  ‚àö  
   
  converges to  
   
  b ¬∑ sin  
   
   ( ‚àí 1)œÄ  
   
  ‚àö 2( b‚àí1) b‚àí1  
   
  L  
   
  ‚àí sin  
   
   œÄ  
   
  L  
   
  , ‚àÄ ‚àà [L],  
   
  as L ‚Üí ‚àû. The above formula  
   
  was obtained by transforming the eigenvector belonging to Œªmax (TL‚àí1 ). Then, for t > 0 and Œµ > 0 we construct an instance with nL, = t ¬∑ xL,  jobs of unit  
   
  254  
   
  S. J√§ger et al.  
   
  weight and processing times equal to b +Œµ, for = 1, . . . , L, ordered in such a way that in each round, the jobs that Ô¨Ånish are executed after all failed probings. A careful analysis of the weighted mutual delays Œîjk shows that Db /OPT = 2b b‚àí1  
   
  +b¬∑  
   
   n L A L n L +a L n L  n L B L n L +b L n L  
   
  + oŒµ‚Üí0 (1) for some vectors aL , bL ‚àà RL . Finally, the  
   
  result follows by letting Œµ ‚Üí 0 and t ‚Üí ‚àû and using nL = t(xL + ot‚Üí‚àû (1)).   4.2  
   
  The Randomized b-scaling Strategy  
   
  We now consider the randomized variant Rb of the strategy Db , in which jobs are ordered according to a random permutation Œ£ and probed for wj bq+Œû in round q ‚àà Z, with Œû ‚àº U([0, 1]). As in the analysis of the deterministic strategy, we start with a lemma giving an overestimator of Œîjk for jobs j and k such that sj ‚â§ sk . This time, our overestimator is not piecewise linear in sj and sk anymore, but depends on a concave function applied to the ratio sskj ‚â• 1. The next lemma follows from standard calculations involving integrals of the form Œ≤ Œæ Œ≤ ‚àíbŒ± b dŒæ = b log b and case distinctions on the rounds in which j and k complete. Œ± 2 Lemma 4. Let f (Œ±) := 1+Œ± 2 + log b + j = k such that sj ‚â§ sk it holds  
   
  Œ±‚àí1 2 log b  
   
  ¬∑ (1 ‚àí log(Œ±)) for Œ± ‚àà [1, b]. For all  
   
     s  
   
  1 k E[Œîjj ] = wj2 sj ¬∑ 1+ ‚â§ wj2 sj ¬∑f (1) and E[Œîjk ] = wj wk sj ¬∑f min b, . log b sj The diÔ¨Éculty of proving the main result of this subsection resides in the fact that we cannot reduce to a worst-case instance in which all Smith ratios are integer powers of b. Instead, we push the technique used for Theorem 2 to the limit, by partitioning the set of jobs according to the interval of the form [bi/K , b(i+1)/K ) containing their Smith ratio, and letting K ‚Üí ‚àû. This leads to the analysis of a Toeplitz matrix which is not tridiagonal anymore but has a bandwitdth of 2K ‚àí 1. While the maximum eigenvalue of this matrix does not have a closed-form expression for K > 1, its limit for L ‚Üí ‚àû can be computed using the Fourier series associated with this matrix. ‚àö  -competitive for 1 || wj Cj . Theorem 4. For every b > 1, Rb is ‚àöb+2b‚àí1 b log(b) Moreover, for all b > 1 this bound is tight, even for unit weight instances. In particular, for b ‚âà 8.16 the competitive ratio is ‚âà 3.032. Proof sketch. For K ‚àà N, let Œ≤ = b1/K . We group the set of all jobs into sets Jk = {j ‚àà [n] : Œ≤ k ‚â§ sj < Œ≤ k+1 } for all k ‚àà Z. Using Lemma 4 and calculations similar to those used in the proof of Theorem 2, we show that   Rb (p, w) ‚â§ Œ≤ f (Œ≤) + Œªmax (Z) , OPT(p, w) K‚àí1 i+1 where Z := ) ‚àí f (Œ≤ i ))Zi and Zi is a sparse symmetric matrix i=1 (f (Œ≤ having non-zero entries only on its ¬±ith and ¬±(i ‚àí 1)th superdiagonals for all  
   
  Competitive Kill-and-Restart and Preemptive Strategies  
   
  255  
   
  i ‚àà [K ‚àí 1]. The principal submatrix of Z obtained by removing its Ô¨Årst row and column is a Toeplitz matrix T of size L √ó L. Then,‚àö we show by using a when both L Schur complement that the above bound is smaller than ‚àöb+2b‚àí1 b log(b) and K grow to ‚àû, with L/K ‚Üí ‚àû. To construct a matching lower bound on the competitive ratio, we have to use an approximate eigenvector zÀÜ ‚àà RL of T œÄ ‚àí1/2 sin L+1 because no closed form is available if K > 1. We set z := ( L+1 2 ) L‚Üí‚àû  
   
  ÀÜ z ÀÜ 2 ‚àí‚àí‚àí‚àí‚Üí Œªmax (T ) by using the Fourier series for ‚àà [L] and show that zÀÜ T z/ associated with T . The rest of the proof mimics the steps used in in Theorem 2,   where b is replaced by Œ≤ = b1/K and we let K ‚Üí ‚àû.  
   
  5  
   
  Weighted Shortest Elapsed Time First  
   
  In this we consider the online time model, where each job j arrives at its release date rj and is not known before that time. Thus, an instance for our problem is now given by a triple I = (p, w, r) of processing times, weights, and release dates of all jobs. Intuitively, the classical Weighted Shortest Elapsed Time First (WSETF) rule is the limit for Œµ ‚Üí 0 of the algorithm that divides the time into time slices of length Œµ and in each time slice processes a job with minimum ratio of elapsed processing time over weight. To formalize this limit process we allow fractional schedulesS that, at every point in time t, assign each job j a n rate yjS (t) ‚àà [0, 1] so that j=1 yjS (t) ‚â§ 1 for all t ‚àà R‚â•0 and yjS (t) = 0 if t < rj or t t > CjS (I), where CjS (I) is the smallest t such that YjS (I, t) := 0 yjS (s) ds ‚â• pj . At any time t let J(t) be the set of all released and unÔ¨Ånished jobs, and let A(t) be the set of all jobs from J(t) that currently have minimum ratio of elapsed time over weight.  Then WSETF sets the rate for all jobs j ‚àà A(t) to yjWSETF (t) := wj / k‚ààA(t) wk and to 0 for all other jobs. In other words, WSETF always distributes the available processor rate among the jobs in J(t) so as to maximize minj‚ààJ(t) YjWSETF (I, t)/wj . The following gives the tight competitive ratio of WSETF for non-clairvoyant online scheduling on a single machine.  Theorem 5. WSETF is 2-competitive for 1 | rj , pmtn | wj Cj . Proof sketch. To bound the optimal objective value from below, we consider the ‚àû mean busy times MjS (I) := 0 t ¬∑ yjS (t) dt of all jobs j in the optimal schedule. It is well known [12,13] that the the sum of weighted mean busy times is minimized by the Preemptive WSPT (PWSPT) rule, which always processes an available job with smallest index nratio pj /wj ). Therefore, it n (i.e. with smallest Smith suÔ¨Éces to show that j=1 wj ¬∑ CjWSETF (I) ‚â§ 2 ¬∑ j=1 wj ¬∑ MjPWSPT (I). For instances I0 with trivial release dates, the weighted delay of each job in the WSETF schedule compared to the optimal WSPT schedule is exactly its processing time multiplied with the total weight of jobs with larger indices, or  
   
  256  
   
  S. J√§ger et al.  
   
  in other words, its weighted completion time is n   
   
  wj ¬∑ CjWSETF (I0 ) = wj ¬∑ CjWSPT (I0 ) +  
   
  wk ¬∑ pj  
   
  (2)  
   
  k=j+1  
   
    
   
  = wj ¬∑ MjWSPT (I0 ) +  
   
  n  wj + wk ¬∑ pj . 2 k=j+1  
   
    (‚àó)  
   
  In order to extend this observation to instances I with release dates, we deÔ¨Åne for each job j an auxiliary instance I(j) with trivial release dates and relate the completion times of j in the WSETF and PWSPT schedules for I to the completion times in the corresponding schedules for I(j). We then bound the diÔ¨Äerence wj (CjWSETF (I) ‚àí MjPWSPT (I)) by an expression generalizing (‚àó). To this end, we apply (Eq. 2) to the instance I(j) and use the fact that each deviation of the PWSPT schedule from the WSPT schedule for the instance without release dates increases the total weighted mean busy time. Finally, we show that the sum of the obtained bounds over all jobs is equal to the sum of weighted mean busy times in the PWSPT schedule.    
   
  6  
   
  Upper Bounds for More General Settings  
   
  In this section, we give  upper bounds on  the competitive ratio of the b-scaling wj Cj and P || Cj . Let I = (p, w, r, m) denote an strategy for 1 | rj | instance on m identical parallel machines in which each job j has processing time pj , weight wj and release date rj . The overall idea is to compare the schedule of Db to schedules of WSETF and round-robin (RR) for the release date and the parallel machine case, respectively, since, by Theorem 4 and [25], both strategies are 2-competitive. To this end, we need to consider modiÔ¨Åed instances with increased processing times and release dates. The following straightforward lemma bounds the increase of the optimal costs under these modiÔ¨Åcations. Lemma 5. Let I = (p, w, r, m) and I  = (p , w, r  , m) be two instances with p ‚â§ Œ±p and r  ‚â§ Œ±r. Then, we have OPT(I  ) ‚â§ Œ± ¬∑ OPT(I).  wj Cj we extend Db as follows: At the end of a probing, probe For 1 | rj | the job with minimum rank and index that is released and not completed.  2b4 -competitive for 1 | rj | wj Cj . In particular, for Theorem 5. Db is 2b2 ‚àí3b+1 ‚àö 9+ 17 , 8  
   
  ‚àö  
   
  17 its performance guarantee is 107+51 ‚âà 9.915. 32 We prove a slightly stronger result by bounding the ratio of Db (I) to the cost of an optimal preemptive schedule for I.  
   
  b=  
   
  Proof sketch. Let I = (p, w, r, 1) be an arbitrary instance. As a Ô¨Årst step we construct an auxiliary instance I  = (p , w, r  , 1) as follows: We deÔ¨Åne processing times pj ‚â§ pj ‚â§ bpj such that pj /wj = bqj with qj = logb (pj /wj ), i.e., all  
   
  Competitive Kill-and-Restart and Preemptive Strategies  
   
  257  
   
  Smith ratios in the instance I  are integer powers of b. Further, we deÔ¨Åne new release dates rj ‚â• rj by either setting rj to the end of the probing that runs at rj in the schedule of I  , whenever such a probing exists, or rj = rj otherwise. By construction, we have rj ‚â§  
   
  b3 2b‚àí1 rj  
   
  and pj ‚â§ bpj ‚â§   
   
  b3 2b‚àí1 pj  
   
  for any job j.  
   
  b3 2b‚àí1 OPT(I).  
   
  Moreover, we obtain Therefore, by Lemma 5, we have OPT(I ) ‚â§ Db (I) ‚â§ Db (I  ), as the sequence of probings in both schedules is the same and the processing times in I  are longer. Next, we consider another instance I  = (p , w, r  , 1) with processing times qj qj +1 b bi = bb‚àí1 = b‚àí1 pj . We show inductively that, pj := YjDb (I  , CjDb (I  )) = i=‚àí‚àû by construction, at any completion time of a job j in the schedule of WSETF for I  , all already released, not completed jobs with minimum rank qj were probed by Db for an amount of wj bqj . Therefore, by deÔ¨Ånition of I  and I  we have CjDb (I  ) ‚â§ CjWSETF (I  ) and hence, Db (I  ) ‚â§ WSETF(I  ). Altogether, we obtain Db (I) ‚â§ Db (I  ) ‚â§ WSETF(I  ) ‚â§ 2OPT(I  ) ‚â§  
   
  2b 2b4 OPT(I  ) ‚â§ 2 OPT(I), b‚àí1 2b ‚àí 3b + 1  
   
  applying Lemma 5 a second time.    For P || Cj we extend Db as follows: probe each job for bq in a list scheduling manner. If at most m jobs remain, process each job on a distinct machine until completion, otherwise increase q by 1 and repeat. ‚àö  2 ‚àíb Theorem 6. Db is 3bb‚àí1 -competitive for P || Cj . In particular, for b = 3+3 6 , ‚àö its performance guarantee is 5 + 2 6 ‚âà 9.899. Proof sketch. Let I = (p, 1, 0, m) be an instance and deÔ¨Åne a new instance I  = (p , 1, 0, m) with processing times pj = bqj where qj = logb pj . Note that pj ‚â§ bpj and Db (I) ‚â§ Db (I  ). For the schedule of Db on I  , let Ti (q) denote the point in time, when the last probing of length bq on machine i ends. Next, we deÔ¨Åne another instance I  = (p , 1, 0, m), where the processing times pj are deÔ¨Åned to be the exactly the elapsed time of j in the schedule of Db for I  at its completion time. We consider the schedule of RR on I  and denote by T  (q) the point in time where the elapsed time of all non-completed jobs is m q+1 1  exactly bb‚àí1 . By induction, we show that T  (q) = m i=1 Ti (q). This identity  Db  RR  allows us to relate Cj (I ) and Cj (I ). In particular, we obtain j CjDb (I  ) ‚â§  RR   j Cj (I ) + pj . The 2-competitiveness of RR and Lemma 5 yield Db (I) ‚â§ Db (I  ) ‚â§ RR(I  ) +  
   
    
   
  pj ‚â§ 2OPT(I  ) + OPT(I  )  
   
  j  
   
   2b  2b + 1 OPT(I  ) ‚â§ + 1 ¬∑ b ¬∑ OPT(I). ‚â§ b‚àí1 b‚àí1    
   
  258  
   
  7  
   
  S. J√§ger et al.  
   
  Conclusion  
   
  We studied kill-and-restart as well as preemptive strategies for the problem of minimizing the sum of weighted completion times and gave a tight analysis of the  deterministic and randomized version of the natural b-scaling strategy for  wj Cj . 1 || wj Cj as well as of WSETF for 1 | rj , pmtn | We hope that this work might lay a basis for obtaining tight bounds on the performance of the b-scaling strategy for more general settings such as nontrivial release dates and parallel machines. Moreover, we think that the class of kill-and-restart strategies combines the best of two worlds. On the one hand, they allow for interruptions leading to small competitive ratios in contrast to non-preemptive algorithms, on the other hand, they reÔ¨Çect the non-preemptive property of only completing a job if it has been processed as a whole. Acknowledgements. We thank Sungjin Im for helpful comments on an earlier version of this manuscript.  
   
  References 1. Afrati, F., et al.: Approximation schemes for minimizing average weighted completion time with release dates. In: 40th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pp. 32‚Äì43. IEEE (1999). https://doi.org/10.1109/ SFFCS.1999.814574 2. Anderson, E.J., Potts, C.N.: Online scheduling of a single machine to minimize total weighted completion time. Math. Oper. Res. 29(3), 686‚Äì697 (2004). https:// doi.org/10.1287/moor.1040.0092 3. Baeza-Yates, R.A., Culberson, J.C., Rawlins, G.J.E.: Searching in the plane. Inf. Comput. 106(2), 234‚Äì252 (1993). https://doi.org/10.1006/inco.1993.1054 4. Bansal, N., Dhamdhere, K.: Minimizing weighted Ô¨Çow time. ACM Trans. Algorithms 3(4), 39:1‚Äì39:14 (2007). https://doi.org/10.1145/1290672.1290676 5. Bansal, N., Pruhs, K.: Server scheduling in the weighted p -norm. In: FarachColton, Mart√≠n (ed.) LATIN 2004. LNCS, vol. 2976, pp. 434‚Äì443. Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-24698-5_47 6. Beaumont, O., Bonichon, N., Eyraud-Dubois, L., Marchal, L.: Minimizing weighted mean completion time for malleable tasks scheduling. In: 26th International Symposium on Parallel and Distributed Processing (IPDPS), pp. 273‚Äì284. IEEE (2012). https://doi.org/10.1109/ipdps.2012.34 7. Becchetti, L., Leonardi, S.: Nonclairvoyant scheduling to minimize the total Ô¨Çow time on single and parallel machines. J. ACM 51(4), 517‚Äì539 (2004). https://doi. org/10.1145/1008731.1008732 8. Conway, R.W., Maxwell, W.L., Miller, L.W.: Theory of Scheduling. AddisonWesley Publishing Company, Boston (1967) 9. Epstein, L., van Stee, R.: Lower bounds for on-line single-machine scheduling. Theor. Comput. Sci. 299(1), 439‚Äì450 (2003). https://doi.org/10.1016/S03043975(02)00488-7 10. Feldmann, A., Sgall, J., Teng, S.H.: Dynamic scheduling on parallel machines. Theor. Comput. Sci. 130(1), 49‚Äì72 (1994). https://doi.org/10.1016/03043975(94)90152-X  
   
  Competitive Kill-and-Restart and Preemptive Strategies  
   
  259  
   
  11. Garg, N., Gupta, A., Kumar, A., Singla, S.: Non-clairvoyant precedence constrained scheduling. In: Baier, C., Chatzigiannakis, I., Flocchini, P., Leonardi, S. (eds.) 46th International Colloquium on Automata, Languages, and Programming (ICALP). LIPIcs, vol. 132, pp. 63:1‚Äì63:14 (2019). https://doi.org/10.4230/LIPIcs.ICALP. 2019.63 12. Goemans, M.X.: A supermodular relaxation for scheduling with release dates. In: Cunningham, W.H., McCormick, S.T., Queyranne, M. (eds.) IPCO 1996. LNCS, vol. 1084, pp. 288‚Äì300. Springer, Heidelberg (1996). https://doi.org/10.1007/3540-61310-2_22 13. Goemans, M.X.: Improved approximation algorthims for scheduling with release dates. In: Proceedings of the Eighth Annual ACM-SIAM Symposium Discrete Algorithms (SODA), pp. 591‚Äì598. SIAM (1997) 14. Hoogeveen, J.A., Vestjens, A.P.A.: Optimal on-line algorithms for single-machine scheduling. In: Cunningham, W.H., McCormick, S.T., Queyranne, M. (eds.) IPCO 1996. LNCS, vol. 1084, pp. 404‚Äì414. Springer, Heidelberg (1996). https://doi.org/ 10.1007/3-540-61310-2_30 15. Im, S., Kulkarni, J., Munagala, K.: Competitive algorithms from competitive equilibria: non-clairvoyant scheduling under polyhedral constraints. J. ACM 65(1), 1‚Äì33 (2017). https://doi.org/10.1145/3136754 16. Im, S., Kulkarni, J., Munagala, K., Pruhs, K.: SelÔ¨ÅshMigrate: A scalable algorithm for non-clairvoyantly scheduling heterogeneous processors. In: 55th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pp. 531‚Äì540 (2014). https://doi.org/10.1109/FOCS.2014.63 17. J√§ger, S., Sagnol, G., Schmidt genannt Waldschmidt, D., Warode, P.: Competitive kill-and-restart and preemptive strategies for non-clairvoyant scheduling (2022). https://doi.org/10.48550/ARXIV.2211.02044 18. Kalyanasundaram, B., Pruhs, K.: Speed is as powerful as clairvoyance. J. ACM 47(4), 617‚Äì643 (2000). https://doi.org/10.1145/347476.347479 19. Kao, M.Y., Ma, Y., Sipser, M., Yin, Y.: Optimal constructions of hybrid algorithms. J. Alg. 29(1), 142‚Äì164 (1998). https://doi.org/10.1006/jagm.1998.0959 20. Kao, M.Y., Reif, J.H., Tate, S.R.: Searching in an unknown environment: an optimal randomized algorithm for the cow-path problem. Inf. Comput. 131(1), 63‚Äì79 (1996). https://doi.org/10.1006/inco.1996.0092 21. Kim, J., Chwa, K.: Non-clairvoyant scheduling for weighted Ô¨Çow time. Inf. Process. Lett. 87(1), 31‚Äì37 (2003). https://doi.org/10.1016/S0020-0190(03)00231-X 22. Labetoulle, J., Lawler, E.L., Lenstra, J.K., Rinnooy Kan, A.H.G.: Preemptive scheduling of uniform machines subject to release dates. In: Pulleyblank, W.R. (ed.) Progress in Combinatorial Optimization, pp. 245‚Äì261. Academic Press (1984). https://doi.org/10.1016/B978-0-12-566780-7.50020-9 23. Lindermayr, A., Megow, N.: Permutation predictions for non-clairvoyant scheduling. In: Proceedings of the 34th Symposium on Parallelism in Algorithms and Architectures (SPAA), pp. 357‚Äì368 (2022). https://doi.org/10.1145/3490148. 3538579 24. Megow, N., Vredeveld, T.: A tight 2-approximation for preemptive stochastic scheduling. Math. Oper. Res. 39(4), 1297‚Äì1310 (2014). https://doi.org/10.1287/ moor.2014.0653 25. Motwani, R., Phillips, S., Torng, E.: Nonclairvoyant scheduling. Theor. Comput. Sci. 130(1), 17‚Äì47 (1994). https://doi.org/10.1016/0304-3975(94)90151-1 26. Rinnooy Kan, A.H.G.: Machine Scheduling Problems: ClassiÔ¨Åcation, Complexity and Computations. Martinus NijhoÔ¨Ä (1976). https://doi.org/10.1007/978-1-46134383-7  
   
  260  
   
  S. J√§ger et al.  
   
  27. Shmoys, D.B., Wein, J., Williamson, D.P.: Scheduling parallel machines online. SIAM J. Comput. 24(6), 1313‚Äì1331 (1995). https://doi.org/10.1137/ S0097539793248317 28. Sitters, R.: Competitive analysis of preemptive single-machine scheduling. Oper. Res. Lett. 38(6), 585‚Äì588 (2010). https://doi.org/10.1016/j.orl.2010.08.012 29. Smith, W.E.: Various optimizers for single-stage production. Nav. Res. Logist. Q. 3(1‚Äì2), 59‚Äì66 (1956). https://doi.org/10.1002/nav.3800030106 30. van Stee, R., La Poutr√©, H.: Minimizing the total completion time on-line on a single machine, using restarts. J. Alg. 57(2), 95‚Äì129 (2005). https://doi.org/10. 1016/j.jalgor.2004.10.001 31. Vestjens, A.P.A.: On-line machine scheduling. Ph.D. thesis, Technische Universiteit Eindhoven (1997). https://doi.org/10.6100/IR500043, https://pure.tue.nl/ ws/Ô¨Åles/1545064/500043.pdf 32. Yao, A.C.C.: Probabilistic computations: toward a uniÔ¨Åed measure of complexity. In: 18th Annual IEEE Symposium on Foundations of Computer Science (SFCS), pp. 222‚Äì227 (1977). https://doi.org/10.1109/SFCS.1977.24  
   
  A Deterministic Better-than-3/2 Approximation Algorithm for Metric TSP Anna R. Karlin, Nathan Klein(B) , and Shayan Oveis Gharan University of Washington, Seattle, USA {karlin,nwklein,shayan}@cs.washington.edu  
   
  Abstract. We show that the max entropy algorithm can be derandomized (with respect to a particular objective function) to give a deterministic 3/2‚àí approximation algorithm for metric TSP for some  > 10‚àí36 . To obtain our result, we apply the method of conditional expectation to an objective function constructed in prior work which was used to certify that the expected cost of the algorithm is at most 3/2 ‚àí  times the cost of an optimal solution to the subtour elimination LP. The proof in this work involves showing that the expected value of this objective function can be computed in polynomial time (at all stages of the algorithm‚Äôs execution).  
   
  1  
   
  Introduction  
   
  One of the most fundamental problems in combinatorial optimization is the traveling salesperson problem (TSP), formalized as early as 1832 (c.f. [App+07, Ch 1]). In an instance of TSP we are given a set of n cities V along with their pairwise symmetric distances, c : V √óV ‚Üí R‚â•0 . The goal is to Ô¨Ånd a Hamiltonian cycle of minimum cost. In the metric TSP problem, which we study here, the distances satisfy the triangle inequality. Therefore, the problem is equivalent to Ô¨Ånding a closed Eulerian connected walk of minimum cost. It is NP-hard to approximate TSP within a factor of 123 122 [KLS15]. An algorithm of ChristoÔ¨Ådes-Serdyukov [Chr76,Ser78] from four decades ago gives a 3 2 -approximation for TSP. Over the years there have been numerous attempts to improve the ChristoÔ¨Ådes-Serdyukov algorithm and exciting progress has been made for various special cases of metric TSP, e.g., [OSS11,MS11,Muc12,SV12, HNR21,KKO20,HN19,Gup+21]. Recently, [KKO21] gave the Ô¨Årst improvement for the general case by demonstrating that the so-called ‚Äúmax entropy‚Äù algorithm of the third author, Saberi, and Singh [OSS11] gives a randomized 32 ‚àí  approximation for some  > 10‚àí36 . The method introduced in [KKO21] exploits the optimum solution to the following linear programming relaxation of metric TSP studied by [DFJ59,HK70, GB93], also known as the subtour elimination LP:  
   
  c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 261‚Äì274, 2023. https://doi.org/10.1007/978-3-031-32726-1_19  
   
  262  
   
  A. R. Karlin et al.  
   
  min  
   
    
   
  x{u,v} c(u, v)  
   
  u,v  
   
  s.t.,  
   
    
   
  x{u,v} = 2  
   
  ‚àÄv ‚àà V,  
   
  u  
   
    
   
  (1) x{u,v} ‚â• 2,  
   
  ‚àÄS  V, S = ‚àÖ  
   
  u‚ààS,v ‚ààS /  
   
  x{u,v} ‚â• 0  
   
  ‚àÄu, v ‚àà V.  
   
  However, [KKO21] had two shortcomings. First, it did not show that the integrality gap of the subtour elimination polytope is bounded below 32 . Second, it was randomized, and the analysis in that work was by nature ‚Äúnon-constructive‚Äù in the sense that it used the optimal solution; thus it was not clear how to to derandomize it using the method of conditional expectation. Other methods of derandomization seem at the moment out of reach and may require algorithmic breakthroughs. A followup work, [KKO22], remedied the Ô¨Årst shortcoming by showing an improved integrality gap. While it did not address the question of derandomization, a byproduct of that work is an analysis of the max entropy algorithm which is in principle polynomially-time computable as it avoids looking at OPT. The purpose of the present work is to show that this analysis can indeed be done in polynomial-time, from which the following can be deduced (remedying the second shortcoming of [KKO21]): Theorem 1. Let x be a solution to LP (1) for a TSP instance. For some absolute constant  > 10‚àí36 , there is a deterministic algorithm (in particular, a derandomized version of max entropy) which outputs a TSP tour with cost at most 32 ‚àí  times the cost of x. Thus, this work in some sense completes the exploratory program concerning whether the max entropy algorithm for TSP beats 3/2 (initiated by [OSS11] in 2011), as now the above two weaknesses of [KKO21] have been addressed. Of course, much work remains in determining the true approximation factor of the algorithm; in this regard we are only at the tip of the iceburg. Using the recent exciting work of Traub, Vygen, and Zenklusen reducing path TSP to TSP [TVZ20] our theorem also implies that there is a deterministic 32 ‚àí  approximation algorithm for path TSP. 1.1  
   
  High Level Proof Overview  
   
  The high level strategy for derandomizing the max entropy algorithm is to use the method of conditional expectation on an objective function given by the analysis in [KKO22]. The max entropy algorithm, similar to ChristoÔ¨Ådes‚Äô algorithm, Ô¨Årst selects a spanning tree and then adds a minimum cost matching on the odd vertices of the tree. While ChristoÔ¨Ådes selects a minimum cost spanning tree, here the spanning tree is sampled from a distribution. In particular, after solving the natural LP  
   
  A Deterministic Better-than-3/2 Approximation Algorithm for Metric TSP  
   
  263  
   
  relaxation for the problem to obtain a fractional solution x, a tree is sampled from the distribution Œº which has maximal entropy subject to the constraint PT ‚àºŒº [e ‚àà T ] = xe for all e ‚àà E (with possibly some exponentially small error in these constraints). [KKO21,KKO22] construct a so-called ‚Äúslack‚Äù vector which is used to show the expected cost of the matching (over the randomness of the trees) is at most 12 ‚àí  times the cost of an optimal solution to the LP. Given a solution x to LP (1) these works imply that there is a random vector m as a function of the tree T ‚àº Œº such that: (1) The cost of the minimum cost matching on the odd vertices of tree T is at most c(m) (with probability 1), and (2) ET ‚àºŒº [c(m)] ‚â§ ( 12 ‚àí )c(x). Let C = ET ‚àºŒº [c(T ) + c(m)]. This will be the objective function to which we will apply the method of conditional expectation. Since the expected cost of the tree T is c(x), as PT ‚àºŒº [e ‚àà T ] = xe , by (2) C is at most ( 32 ‚àí )c(x). Since by (1) for a given tree T , c(T ) + c(m) is an upper bound on the cost of the output of the algorithm (with probability 1), this shows that the expected cost of the algorithm is bounded strictly below 3/2. Ideally, one would like Œº to have polynomial sized support. Then one could simply check the cost of the output of the algorithm on every tree in the support, and the above would guarantee that some tree gives a better-than-3/2 approximation. However, the max entropy distribution can have exponential sized support, and it‚Äôs not clear how to Ô¨Ånd a similarly behaved distribution with polynomial sized support. Instead, let Tpartial be the family of all partial settings of the edges of the graph to 0 or 1 where the edges set to 1 are acyclic. For Set = {Xe1 , . . . , Xei } ‚àà Tpartial , and 1 ‚â§ j ‚â§ i, we use Xej to indicate whether ej is set to 1 or 0. The method of conditional expectations is then used as follows: Process the edges in an arbitrary order e1 , . . . , em and for each edge ei : (1) Assume we inductively have chosen a valid assignment Set ‚àà Tpartial to edges e1 , . . . , ei‚àí1 . (2) Let Set+ = Set ‚à™ {Xei = 1}. Compute C + = ET ‚àºŒº [c(T ) + c(m) | Set+ ]. Similarly, let Set‚àí = Set ‚à™ {Xei = 0} and compute C ‚àí = ET ‚àºŒº [c(T ) + c(m) | Set‚àí ]. (3) Let Set ‚Üê Set+ or Set ‚Üê Set‚àí depending on which quantity is smaller. After a tree is obtained, add the minimum cost matching on the odd vertices of T . The resulting algorithm is shown in Algorithm 3 (see Algorithm 2 for its instantiation in a simple case). As C ‚â§ ( 32 ‚àí )c(x), this algorithm succeeds with probability 1. We only need to show it can be made to run in polynomial time. Since we can compute the expected cost of the tree conditioned on Set using linearity of expectation and the matrix tree theorem (Sect. 2.2), it remains to show that ET ‚àºŒº [c(m)|Set] can be computed deterministically and eÔ¨Éciently for any Set ‚àà Tpartial . Key Contributions. The key contribution of this paper is to show how to do this computation eÔ¨Éciently, which is based on two observations:  
   
  264  
   
  A. R. Karlin et al.  
   
  (1) The Ô¨Årst is that the vector m (whose cost upper bounds the cost of the minimum cost matching on the odd vertices of the tree) can be written as the (weighted) sum of indicators of events that depend on the sampled tree T , and each of these events happens only when a constant number of (not necessarily disjoint) sets of edges have certain parities or certain sizes. (2) The second is that the probability of any such event can be deterministically computed in polynomial time by evaluating the generating polynomial of all spanning trees at certain points in CE , see Lemma 10. Structure of the Paper. After reviewing some preliminaries, in Sect. 3 we review the matrix tree theorem and show (as a warmup) how to compute the probability two (not necessarily disjoint) sets of edges both have an even number of edges in the sampled tree. In Sect. 4, we then give a complete description and proof of a deterministic algorithm for the special ‚Äúdegree cut‚Äù case of TSP. Unlike the subsequent sections of the paper, Sect. 4 is self-contained and thus directed towards readers looking for more high-level intuition or those not familiar with [KKO21,KKO22]. In Sect. 5 we show (2) from above and give the deterministic algorithm in the general case. The remainder of the paper then involves proving (1) for the general deÔ¨Ånition of m from [KKO21,KKO22].  
   
  2 2.1  
   
  Preliminaries Notation  
   
  For a set of edges A ‚äÜ E and (a tree) T ‚äÜ E, we write AT = |A ‚à© T |. For a tree T , we will say a cut S ‚äÜ V is odd in T if Œ¥(S)T is odd and even in T otherwise. If the tree is understood we will simply say even or odd. We use Œ¥(S) = {{u, v} ‚àà E : |{u, v} ‚à© S| = 1} to denote the set of edges that leave S, and E(S) = {{u, v} ‚àà E : |{u, v} ‚à© S| = 2} to denote the set of edges inside of S.  For a set A ‚äÜ E and a vector x ‚àà R|E| we write x(A) := e‚ààA xe . 2.2  
   
  Randomized Algorithm of [KKO21]  
   
  Let x0 be an optimum solution of LP (1). Without loss of generality we assume x0 has an edge e0 = {u0 , v0 } with x0e0 = 1, c(e0 ) = 0. (To justify this, consider the following process: given x0 , pick an arbitrary node, u, split it into two nodes u0 , v0 and set x{u0 ,v0 } = 1, c(e0 ) = 0 and assign half of every edge incident to u to u0 and the other half to v0 .) Let E0 = E ‚à™ {e0 } be the support of x0 and let x be x0 restricted to E and G = (V, E). By Lemma 1 x0 restricted to E is in the spanning tree polytope (2) of G. We write G = (V, E, x) to denote the (undirected) graph G together with special vertices u0 , v0 and the weight function x : E ‚Üí R‚â•0 . Similarly, let G0 = (V, E0 , x0 ) and let G/e0 = G0 /{e0 }, i.e. G/e0 is the graph G0 with the edge e0 contracted.  
   
  A Deterministic Better-than-3/2 Approximation Algorithm for Metric TSP  
   
  265  
   
  Definition 1. For a vector Œª : E ‚Üí R‚â•0 , a Œª-uniform distribution ŒºŒª over spanning trees of G = (V, E) is a distribution where for every spanning tree Œªe  . T ‚äÜ E, PŒºŒª [T ] =   e‚ààT  Œªe T  
   
  e‚ààT  
   
  Theorem 2 ([Asa+10]). Let z be a point in the spanning tree polytope (see (2)) of a graph G = (V, E). For any  > 0, a vector Œª : E ‚Üí R‚â•0 can be found such that the corresponding Œª-uniform spanning tree distribution, ŒºŒª , satisfies  PŒºŒª [T ] ‚â§ (1 + Œµ)ze , ‚àÄe ‚àà E, T ‚ààT :T e  
   
  i.e., the marginals are approximately preserved. In the above T is the set of all spanning trees of (V, E). The algorithm is deterministic and running time is polynomial in n = |V |, ‚àí log mine‚ààE ze and log(1/). [KKO22] showed that the following (randomized) max entropy algorithm has expected cost of the output is at most ( 32 ‚àí )c(x). Algorithm 1. (Randomized) Max Entropy Algorithm for TSP Find an optimum solution x0 of Eq. (1), and let e0 = {u0 , v0 } be an edge with x0e0 = 1, c(e0 ) = 0. Let E0 = E ‚à™ {e0 } be the support of x0 and x be x0 restricted to E and G = (V, E). Find a vector Œª : E ‚Üí R‚â•0 such that for any e ‚àà E, PT ‚àºŒºŒª [e ‚àà T ] = xe (1 ¬± 2‚àín ). Sample a tree T ‚àº ŒºŒª . Let M be the minimum cost matching on odd degree vertices of T . Output T ‚à™ M .  
   
  2.3  
   
  Polyhedral Background  
   
  For any graph G = (V, E), Edmonds [Edm70] gave the following description for the convex hull of spanning trees of a graph G = (V, E), known as the spanning tree polytope. z(E) = |V | ‚àí 1,  
   
  z(E(S)) ‚â§ |S| ‚àí 1 ‚àÄS ‚äÜ V,  
   
  ze ‚â• 0 ‚àÄe ‚àà E.  
   
  (2)  
   
  Edmonds [Edm70] proved that the extreme point solutions of this polytope are the characteristic vectors of the spanning trees of G. Lemma 1 ([KKO21, Fact 2.1]). Let x0 be a feasible solution of (1) such that x0e0 = 1 with support E0 = E ‚à™ {e0 }. Let x be x0 restricted to E; then x is in the spanning tree polytope of G = (V, E). Since c(e0 ) = 0, the following fact is immediate. Lemma 2. Let G = (V, E, x) where x is in the spanning tree polytope. If Œº is any distribution of spanning trees with marginals x then ET ‚àºŒº [c(T ‚à™ e0 )] = c(x).  
   
  266  
   
  A. R. Karlin et al.  
   
  To bound the cost of the min-cost matching on the set O(T ) of odd degree vertices of the tree T , we use the following characterization of the O(T )-join polyhedron due to Edmonds and Johnson [EJ73]. Proposition 1. For any graph G = (V, E), cost function c : E ‚Üí R+ , and a set O ‚äÜ V with an even number of vertices, the minimum weight of an O-join equals the optimum value of the following integral linear program. min c(y) y(Œ¥(S)) ‚â• 1  
   
  3  
   
  s.t. S ‚äÜ V, |S ‚à© O| odd  
   
  ye ‚â• 0 ‚àÄe ‚àà E  
   
  (3)  
   
  Computing Probabilities  
   
  The deterministic algorithm depends on the computation of various probabilities and conditional expectations. In this section (and additionally later in Sect. 5), we show to do these calculations eÔ¨Éciently. 3.1  
   
  Notation  
   
  Let BE be the set of all probability measures on the Boolean algebra 2|E| . Let Œº ‚àà BE . The generating polynomial gŒº : R[{ze }e‚ààE ] of Œº is deÔ¨Åned as follows:   gŒº (z) := Œº(S) ze . S  
   
  3.2  
   
  e‚ààS  
   
  Matrix Tree Theorem  
   
  Let G = (V, E) with |V | = n. For e = (u, v) we let Le = (1u ‚àí 1v )(1u ‚àí 1v )T be the Laplacian of e. Recall KirchhoÔ¨Ä‚Äôs matrix tree theorem: Theorem 3 (Matrix  tree theorem). For a graph G = (V, E) let gT ‚àà R[ze1 , . . . , zem ] = T ‚ààT z T be the generating polynomial of the spanning trees of G. Then, we have gT ({ze }e‚ààE ) =  
   
   1 det( ze Le + 11T /n). n e‚ààE  
   
   Given a vector Œª ‚àà R|E| and a set S ‚äÜ E, let ŒªS := i‚ààS Œªi . Recall that the Œª-uniform distribution ŒºŒª is the probability distribution over spanning trees where the probability of every tree T is ŒªT . Then the generating polynomial of ŒºŒª is     1 ŒªT z T = gT ({Œªe ze }e‚ààE ) = det ze Œªe Le + 11T /n gŒºŒª (z) = n T ‚ààT  
   
  e‚ààE  
   
  A Deterministic Better-than-3/2 Approximation Algorithm for Metric TSP  
   
  267  
   
  and can be evaluated at any z ‚àà CE eÔ¨Éciently using a determinant computation. Thus we can compute PT ‚àºŒº [e ‚àà T ] by computing the sum of the probabilities of trees in the graph G/{e}, i.e. the graph with e contracted, as follows:  / T] = 1 ‚àí ŒªT PT ‚àºŒº [e ‚àà T ] = 1 ‚àí PT ‚àºŒº [e ‚àà T ‚ààT :e‚ààT /  
   
  where to compute the sum in the RHS we evaluate gŒºŒª at ze = 0, zf = 1 for all f = e. Thus, Lemma 3. Given a Œª-uniform distribution ŒºŒª over spanning trees, for every edge e, we can compute PT ‚àºŒºŒª [e ‚àà T ] in polynomial time. Given some Set ‚àà Tpartial , we contract each edge e with Xe = 1 in Set and delete each edge e with Xe = 0 in Set. Let G be the resulting graph with n vertices, with corresponding Œªe ‚àù Œªe for all e ‚àà G normalized such that  T = 1. T  ‚ààG Œª  T 1/n‚àí1 Remark 1. A vector Œª ‚àà R|E| is normalized by setting Œªe = Œªe / T Œª  1/n‚àí1 i.e., Œªe = Œªe /gT ({Œªe }e‚ààE ) . Thus at the cost of another application of the matrix-tree theorem, we assume without loss of generality that we are always dealing with Œª values that are normalized. Putting the previous facts together, we obtain Lemma 4. Given a Œª-uniform distribution ŒºŒª and some Set ‚àà Tpartial , we can compute a vector Œª such that ŒºŒª = ŒºŒª|Set . 3.3  
   
  Computing Parities in a Simple Case  
   
  Lemma 5. Let A, B ‚äÜ E and ŒºŒª be a Œª-uniform distribution over spanning trees. Then, we can compute PT ‚àºŒºŒª [AT , BT even] in polynomial time. Proof. First observe that I {AT , BT even} =  
   
  1 (1 + (‚àí1)AT + (‚àí1)BT + (‚àí1)((AB)‚à™(BA))T ) 4  
   
  One can easily check that if AT and BT are even, this is 1, and otherwise it is 0. To compute PT ‚àºŒºŒª [A and B even in T ] it is enough to compute the expected value of this indicator. By linearity of expectation it is therefore enough to compute the expectation of (‚àí1)FT for any set F ‚äÜ E. We can do this using Theorem 3. Setting zeF = ‚àí1 if e ‚àà F and zeF = +1 otherwise, we exactly have:   
   
  (‚àí1)FT ŒªT = ET ‚àºŒºŒª (‚àí1)FT . gŒºŒª (z F ) = T ‚ààT  
   
  The lemma follows.  
   
  268  
   
  A. R. Karlin et al.  
   
  Remark 2. We can use the same approach to compute PT ‚àºŒºŒª [AT odd, BT even] or the probability that both are odd. All we need to do is to multiply (‚àí1)AT with a ‚àí1 if AT needs to be odd (and similarly for BT ), and (‚àí1)((AB)‚à™(BA))T with a ‚àí1 if we are looking for diÔ¨Äerent parities in AT , BT . Given some Set ‚àà Tpartial , by Lemma 4 we can compute ŒºŒª = ŒºŒª|Set . Applying the above lemma to ŒºŒª , it follows (after appropriately updating the parities to account for edges set to 1 in Set): Corollary 1. Let A, B ‚äÜ E. We can compute PT ‚àºŒº [A and B even in T | Set] in polynomial time.  
   
  4  
   
  A Deterministic Algorithm in the Degree Cut Case  
   
  As a warmup, in this section we show how to implement the deterministic algorithm for the so-called ‚Äúdegree cut case,‚Äù i.e., when for every set of vertices S with 2 ‚â§ |S| ‚â§ n ‚àí 2 we have x(Œ¥(S)) ‚â• 2 + Œ∑ for some absolute constant Œ∑ > 0. See Algorithm 2.  
   
  Algorithm 2 . A Deterministic Approximation Algorithm for Metric TSP in the Degree Cut Case 1: Given a solution x0 of the LP (1), with an edge e0 with xe0 = 1. 2: Let G be the support graph of x. 3: Find a vector Œª : E ‚Üí R‚â•0 such that for any e ‚àà E, PT ‚àºŒºŒª [e ‚àà T ] = xe (1 ¬± 2‚àín ) (see Section 2.2). 4: Initialize Set := ‚àÖ 5: while there exists e = e0 not set in Set do ‚àí 6: Let Set+ := Set ‚à™ {Xe = 1} and  let Set  := Set ‚à™ {Xe = 0};  + 7: if ET ‚àºŒºŒª c(T ) + c(m) | Set ‚â§ ET ‚àºŒºŒª c(T ) + c(m) | Set‚àí (m from DeÔ¨Ånition 2) then 8: Set := Set+ ; 9: else 10: Set := Set‚àí ; 11: end if 12: end while 13: Return T = {e : Xe = 1 in Set} together with min cost matching on odd degree vertices of T .  
   
  Construction of the Matching Vector. We describe a simple construction for the matching vector m : T ‚Üí R|E| for the degree cut case. It will ensure that for a tree T , m is in the O(T )-Join polyhedron where O(T ) is the set of odd vertices of T (we emphasize that m is a function of T ). Therefore, c(m) is an upper bound on the cost of the minimum cost matching on the odd vertices of T as desired.  
   
  A Deterministic Better-than-3/2 Approximation Algorithm for Metric TSP  
   
  269  
   
  Let p = 2 ¬∑ 10‚àí10 (note that we have not optimized this constant and in the degree cut case it can be greatly improved). We say that an edge e = (u, v) is good if PT ‚àºŒº [u, v both even in T ] ‚â• p, where we say a vertex v is even in a tree T if Œ¥(v)T is even. The vector m will consist of the convex combination of two feasible points in the O(T )-Join polyhedron, g and b (where g is for ‚Äúgood‚Äù edges and b is for ‚Äúbad‚Äù edges). For a tree T and an edge e = (u, v) we let: 1 xe If u and v are both even in T ge = 2+Œ∑ 1 Otherwise 2 xe Lemma 6. g is in the O(T )-Join polyhedron. Proof. First, consider any cut consisting of a single vertex v (or its complement). If v is odd, we need to ensure that g(Œ¥(v)) ‚â• 1. If v is odd, then ge = xe /2 for all e ‚àà Œ¥(v), so this follows from the fact that x(Œ¥(v)) = 2. Now consider any cut S with 2 ‚â§ |S| ‚â§ n‚àí2. We now argue that g(Œ¥(S)) ‚â• 1 with probability 1. This follows from the fact that: g(Œ¥(S)) ‚â•  
   
  1 1 x(Œ¥(S)) ‚â• (2 + Œ∑) = 1, 2+Œ∑ 2+Œ∑  
   
  where we use that every cut S with 2 ‚â§ |S| ‚â§ n ‚àí 2 has x(Œ¥(S)) ‚â• 2 + Œ∑. let:  
   
  We now design our second vector b. For a tree T and an edge e = (u, v) we 1+Œ∑ xe If e is good be = 2+Œ∑ 1 2+Œ∑ xe If e is bad We will crucially use the following:  
   
  Corollary 2 (Corollary of Theorem 5.14 from[KKO21]). Let v be a vertex. Then, if Gv is the set of good edges adjacent to v, x(Gv ) ‚â• 1. In [KKO21], it is shown that if xe is bounded away from 1/2, then e is a good edge. Furthermore, for any two edges e and f adjacent to v with xe ‚âà xf ‚âà 1/2, at least one is good. So, v can have only one bad edge which has fraction about 1/2, giving the above corollary (therefore it is even true that x(Gv ) ‚â• 3/2 ‚àí Œ≥ for some small Œ≥ > 0). Given this, we can show the following: Lemma 7. b is in the O(T )-Join polyhedron. Proof. For any non-vertex cut, similar to above, the O(T )-Join constraint is easily satisÔ¨Åed. For a vertex cut v, we use that by the above theorem the x weight of the set of good edges adjacent to v is at least 1. Therefore, b(v) ‚â• 1+Œ∑ 1 2+Œ∑ + 2+Œ∑ = 1.  
   
  270  
   
  A. R. Karlin et al.  
   
  Definition 2 (Matching vector m in the degree cut case). Let m = Œ±b + (1 ‚àí Œ±)g, for some 0 < Œ± < 1 we choose in the next subsection. Since b and g are both in the O(T )-Join polyhedron, so is m. Lemma 8. For any good edge e, E [ge ] ‚â§ ( 12 ‚àí  
   
  Œ∑p 4+2Œ∑ )xe .  
   
  Proof. Let pe = PT ‚àºŒº [u, v even]. We can compute:  
   
     pe p 1 1 ‚àí pe 1‚àíp Œ∑p + + ‚àí E [ge ] = xe ‚â§ xe = xe , 2+Œ∑ 2 2+Œ∑ 2 2 4 + 2Œ∑ as desired. Therefore, for any good edge e,  
   
    
   
   1+Œ∑ 1 Œ∑p E [me ] ‚â§ Œ± ‚àí + (1 ‚àí Œ±) xe 2+Œ∑ 2 4 + 2Œ∑ For any bad edge e, we have  
   
  E [me ] ‚â§ To make the two equal, we set Œ± =  
   
  E [me ] ‚â§  
   
  Œ± 1‚àíŒ± + 2+Œ∑ 2 p 2+p .  
   
   xe  
   
  Therefore,  
   
  p/(2 + p) 1 ‚àí p/(2 + p) + 2+Œ∑ 2  
   
   xe  
  0. Therefore the randomized algorithm has expected cost at most ( 32 ‚àí )c(x), which is enough to prove that Algorithm 2 deterministically Ô¨Ånds a tree plus a matching whose cost is at most ( 32 ‚àí )c(x). Thus the only remaining question is the computational complexity of Algorithm 2, which we address now. Computing E [c(T ) + c(m) | Set]. Now that we have explained the construction of m, we observe that there is a simple deterministic algorithm to compute E [c(T ) + c(m) | Set] in polynomial time. First, compute E [c(T ) | Set]. By linearity of expectation it is enough to compute P [e ‚àà T | Set] for all e ‚àà E. To do this, we Ô¨Årst apply Lemma 4 to Ô¨Ånd Œª such that ŒºŒª = ŒºŒª|Set and then apply Lemma 3. Now to compute E [c(m) | Set], it suÔ¨Éces to compute E [me | Set] for any Set ‚àà Tpartial , P [e ‚àà T | Set] and any e = (u, v). Given the deÔ¨Ånition of m, the only event depending on the tree is the event P [u, v even | Set]. This can be computed with Corollary 1.  
   
  A Deterministic Better-than-3/2 Approximation Algorithm for Metric TSP  
   
  271  
   
  Algorithm 3. A Deterministic Approximation Algorithm for Metric TSP 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12:  
   
  5  
   
  Given a solution x0 of the LP (1), with an edge e0 with xe0 = 1. Let G be the support graph of x. Find a vector Œª : E ‚Üí R‚â•0 such that for any e ‚àà E, PT ‚àºŒºŒª [e ‚àà T ] = xe (1 ¬± 2‚àín ) Perform Preprocessing Steps 1, 2, 3, 4, 5, and 6 Initialize Set := ‚àÖ. while there exists e = e0 not set in Set do Let Set+ := Set ‚à™ {Xe = 1} and let Set‚àí:= Set ‚à™ {Xe = 0}; Compute S + = EET ‚àº ŒºŒª c(T ) | Set+ + e‚ààE Ec(s‚àó ) (e, Set+ ) + Ec(s) (e, Set+ ). Compute S ‚àí = EET ‚àº ŒºŒª c(T ) | Set‚àí + e‚ààE Ec(s‚àó ) (e, Set‚àí ) + Ec(s) (e, Set‚àí ). If S + ‚â§ S ‚àí , let Set := Set+ . Otherwise let Set := Set‚àí . end while Return T = {e : Xe = 1 in Set} together with min cost matching on odd degree vertices of T .  
   
  General Case  
   
  The matching vector m in the general case, [KKO22, Thm 6.1], can be written as s + s‚àó + 12 x where s, s‚àó are functions of the tree T ‚àº ŒºŒª and some independent Bernoullis B. Roughly speaking, the (slack) vector s‚àó : E ‚Üí R‚â•0 takes care of matching constraints for near minimum cuts that are crossed and the (slack) vector s : E ‚Üí R takes care of the constraints corresponding to cuts which are not crossed. Most importantly, the guarantee is that for a Ô¨Åxed tree T the expectation of c(s) + c(s‚àó ) + 12 c(x) over the Bernoullis is at least c(M ) where M is the minimum cost matching on the odd vertices of T . Furthermore, E [c(s) + c(s‚àó )] ‚â§ ‚àíc(x) which is the necessary bound to begin applying the method of conditional expectation in Algorithm 3. Remark 3. The deÔ¨Ånitions of s and s‚àó , the proof that E [c(s) + c(s‚àó )] ‚â§ ‚àíc(x), and the proof that x/2 + E [s + s‚àó | T ] is in the O(T )-join polyhedron come from [KKO21,KKO22]. Here, we will review how to construct the random slack vectors s, s‚àó for a given spanning tree T and then explain how to eÔ¨Éciently compute E [c(s) + c(s‚àó ) | Set] deterministically for any Set ‚àà Tpartial . Unfortunately, a reader who has not read [KKO21,KKO22] may not be able to understand the motivation behind the details of the construction of s, s‚àó . However, ?? and ?? are self-contained in the sense that a reader should be able to verify that E [c(s) + c(s‚àó ) | Set] can be computed eÔ¨Éciently and deterministically. Our theorem boils down to showing the following two lemmas: Lemma 9. For any Set ‚àà Tpartial , there is a polynomial time deterministic algorithm that computes: (1) ET ‚àºŒºŒª [c(s‚àó ) | Set] (shown in Ec(s‚àó ) (e, Set)) (2) ET ‚àºŒºŒª [c(s) | Set] (shown in Ec(s) (e, Set))  
   
  272  
   
  A. R. Karlin et al.  
   
  The crux of proving the above lemma is to show that for a given edge e and any Set, each of E [s‚àóe | Set] and E [se | Set] can be written as the (weighted) sum of indicators of events that depend on the sampled tree T , and each of these events happens only when a constant number of (not necessarily disjoint) sets of edges have certain parities or certain sizes. Technically speaking, these weighted sums are non-trivial for some of the events deÔ¨Åned in [KKO21,KKO22]. Given that, the following is enough to prove Lemma 9, as it gives a deterministic algorithm to compute the probability that a collection of (not necessarily disjoint) sets of edges have certain parities or certain sizes. (1) of Lemma 9 is proved in ??, and (2) in ??. The algorithm for each part requires a series of preprocessing steps and function deÔ¨Ånitions that we have marked with gray boxes. In each section, the Ô¨Ånal procedure to calculate the expected cost of the slack vector is given in a yellow box at the end of the corresponding section. Lemma 10. Given a probability distribution Œº : 2[n] ‚Üí R‚â•0 and an oracle O that can evaluate gŒº (z1 , . . . , zn ) at any z1 , . . . , zn ‚àà C. Let E1 , . . . , Ek be a collection of (not necessarily disjoint) subsets of [n] and (œÉ1 , . . . , œÉk ) ‚àà Fm1 √ó ¬∑ ¬∑ ¬∑ √ó Fmk . Then, we can compute, PT ‚àºŒº [(Ei )T = œÉi (mod mi ), ‚àÄ1 ‚â§ i ‚â§ k] . in N := m1 . . . mk -many calls to the oracle.1  I{e‚ààEj } Proof. For each of the sets Ei , deÔ¨Åne a variable xi , and substitute j xj for ze into the polynomial gŒº and call the resulting polynomial g. Then g(x1 , . . . , xk ) =  
   
    
   
  P [S]  
   
  i=1  
   
  S‚ààsupp(Œº)  
   
  Where recall (Ei )S = |Ei ‚à© S|. Now, let œâi := e 1 m1 ¬∑ ¬∑ ¬∑ mk  
   
    
   
  k   
   
  (e1 ,...,ek )‚ààFr1 √ó¬∑¬∑¬∑√óFrk i=1  
   
  k   
   
  ‚àö 2œÄ ‚àí1 mi  
   
  (Ei )S  
   
  xi  
   
  . We claim that  
   
  œâi‚àíei œÉi g(œâ1e1 , . . . , œâkek )  
   
  = PS‚àºŒº [(Ei )S ‚â° œÉi mod mi , ‚àÄ1 ‚â§ i ‚â§ k] So the algorithm only needs to call the oracle N many times to compute the sum in the LHS.  
   
  1  
   
  Note that since we are dealing with irrational numbers, we will not be able to compute this probability exactly. However by doing all calculations with poly(n, N ) bits of precision we can ensure our estimate has exponentially small error which will suÔ¨Éce to get the bounds we need later.  
   
  A Deterministic Better-than-3/2 Approximation Algorithm for Metric TSP  
   
  273  
   
  To see this identity, notice that we can write the LHS as 1 m1 ¬∑ ¬∑ ¬∑ mk =  
   
    
   
  P [S]  
   
    
   
  P [S]  
   
   S‚ààsupp(Œº)  
   
  P [S]  
   
  k   
   
  ‚éõ  
   
  ‚éù 1 mi i=1 k   
   
    
   
  k  i=1  
   
  (e1 ,...,ek )‚ààFm1 √ó¬∑¬∑¬∑√óFmk S‚ààsupp(Œº)  
   
  S‚ààsupp(Œº)  
   
  =  
   
    
   
  ‚àíei œÉi +ei (Ei )S  
   
  œâi  
   
  ‚éû  
   
  ((Ei )S ‚àíœÉi )ei ‚é†  
   
  œâi  
   
  ei ‚ààFmi  
   
  I {(Ei )S ‚àí œÉi ‚â° 0 mod œÉi }  
   
  i=1  
   
  where the last equality uses that œâi is the mi ‚Äôth root of unity. The RHS is exactly equal to the probability that (Ei )S ‚â° œÉi mod mi for all i. Remark 4. When we apply this lemma in this paper, we will always let k be a constant and mi ‚â§ |V | for all i. Thus, it will always use a polynomial number of calls to an oracle evaluating the generating polynomial of a spanning tree distribution ŒºŒª . By Theorem 3, for any z ‚àà C|E| : gŒºŒª ({ze }e‚ààE ) =  
   
   1 det( Œªe ze Le + 11T /n), n e‚ààE  
   
  which can be computed in polynomial time. Corollary 3. Let ŒºŒª be a Œª-uniform spanning tree distribution and let Set ‚àà Tpartial . Then, let E1 , . . . , Ek be a collection of (not necessarily disjoint) subsets of [n] and (œÉ1 , . . . , œÉk ) ‚àà Fm1 √ó ¬∑ ¬∑ ¬∑ √ó Fmk . Then, we can compute, PT ‚àºŒºŒª [(Ei )T = œÉi  
   
  (mod mi ), ‚àÄ1 ‚â§ i ‚â§ k | Set] .  
   
  in N := m1 . . . mk -many calls to the oracle. Proof. Construct a new graph G by contracting all edges with Xe = 1 in Set and deleting all edges with Xe = 0. We then update all œÉi by subtracting the number of edges that are set to 1 in Ei by Set. Then we apply Lemma 10 to the Œª-uniform spanning tree distribution over G with the updated œÉ and the same m.  
   
  References [App+07] Applegate, D.L., Bixby, R.E., Chvatal, V., Cook, W.J.: The Traveling Salesman Problem: A Computational Study (Princeton Series in Applied Mathematics). Princeton University Press, Princeton, NJ, USA (2007) [Asa+10] Asadpour, A., Goemans, M.X., Madry, A., Gharan, S.O., Saberi, A.: An o(log n/ log log n) approximation algorithm for the asymmetric traveling salesman problem. In: SODA, pp. 379‚Äì389 (2010)  
   
  274  
   
  A. R. Karlin et al.  
   
  [Chr76] Nicos ChristoÔ¨Ådes. Worst case analysis of a new heuristic for the traveling salesman problem. Report 388, Graduate School of Industrial Administration, Carnegie-Mellon University, Pittsburgh, PA, 1976 [DFJ59] Dantzig, G.B., Fulkerson, D.R., Johnson, S.: On a linear programming combinatorial approach to the traveling salesman problem. OR 7, 58‚Äì66 (1959) [Edm70] Edmonds, J.: Submodular functions, matroids and certain polyhedra. In: Combinatorial Structures and Their Applications, pp. 69‚Äì87, New York, NY, USA (1970). Gordon and Breach [EJ73] Edmonds, J., Johnson, E.L.: Matching, Euler tours and the Chinese postman. Math. Program. 5(1), 88‚Äì124 (1973) [GB93] Goemans, M., Bertsimas, D.: Survivable network, linear programming relaxations and the parsimonious property. Math. Program. 60, 06 (1993) [Gup+21] Gupta, A., Lee, E., Li, J., Mucha, M., Newman, H., Sarkar, S.: Matroidbased TSP rounding for half-integral solutions. CoRR, abs/2111.09290 (2021) [HK70] Held, M., Karp, R.M.: The traveling salesman problem and minimum spanning trees. Oper. Res. 18, 1138‚Äì1162 (1970) [HN19] Haddadan, A., Newman, A.: Towards improving christoÔ¨Ådes algorithm for half-integer TSP. In: Bender, M.A., Svensson, O., Herman, G., editors, ESA, vol. 144 of LIPIcs, pp. 56:1‚Äì56:12. Schloss Dagstuhl - Leibniz-Zentrum f√ºr Informatik (2019) [HNR21] Haddadan, A., Newman, A., Ravi, R.: Shorter tours and longer detours: uniform covers and a bit beyond. Math. Program. 185(1‚Äì2), 245‚Äì273 (2021) [KKO20] Karlin, A.R., Klein, N., Gharan, S.O.: An improved approximation algorithm for TSP in the half integral case. In: Makarychev, K., Makarychev, Y., Tulsiani, M., Kamath, G., Chuzhoy, J., editors, STOC, pp. 28‚Äì39. ACM (2020) [KKO21] Karlin, A.R., Klein, N., Gharan, S.O.: A (slightly) improved approximation algorithm for metric tsp. In: STOC. ACM (2021) [KKO22] Karlin, A., Klein, N., Gharan, S.O.: A (slightly) improved bound on the integrality gap of the subtour LP for tsp. In: FOCS, pp. 844‚Äì855. IEEE Computer Society (2022) [KLS15] Karpinski, M., Lampis, M., Schmied, R.: New inapproximability bounds for TSP. J. Comput. Syst. Sci. 81(8), 1665‚Äì1677 (2015) [MS11] Moemke, T., Svensson, O.: Approximating graphic tsp by matchings. In: FOCS, pp. 560‚Äì569 (2011) -approximation for graphic TSP. In: STACS, pp. 30‚Äì41 (2012) [Muc12] Mucha, M.: 13 9 [OSS11] Gharan, S.O., Saberi, A., Singh, M.: A randomized rounding approach to the traveling salesman problem. In: FOCS, pp. 550‚Äì559. IEEE Computer Society (2011) [Ser78] Serdyukov, A.I.: O nekotorykh ekstremal‚Äônykh obkhodakh v grafakh. Upravlyaemye sistemy 17, 76‚Äì79 (1978) [SV12] Seb√∂, A., Vygen, J.: Shorter tours by nicer ears: CoRR abs/1201.1870 (2012) [TVZ20] Traub, V., Vygen, J., Zenklusen, R.: Reducing path TSP to TSP. In: Makarychev, K., Makarychev, Y., Tulsiani, M., Kamath, G., Chuzhoy, J., editors, STOC, pp. 14‚Äì27. ACM (2020)  
   
  Monoidal Strengthening of Simple V-Polyhedral Disjunctive Cuts Aleksandr M. Kazachkov1(B)  
   
  and Egon Balas2  
   
  1  
   
  2  
   
  University of Florida, Gainesville, FL, USA [email protected]  Carnegie Mellon University, Pittsburgh, PA, USA [email protected]   
   
  Abstract. Disjunctive cutting planes can tighten a relaxation of a mixed-integer linear program. Traditionally, such cuts are obtained by solving a higher-dimensional linear program, whose additional variables cause the procedure to be computationally prohibitive. Adopting a Vpolyhedral perspective is a practical alternative that enables the separation of disjunctive cuts via a linear program with only as many variables as the original problem. The drawback is that the classical approach of monoidal strengthening cannot be directly employed without the values of the extra variables appearing in the extended formulation. We derive how to compute these values from a solution to the linear program generating V-polyhedral disjunctive cuts. We then present computational experiments with monoidal strengthening of cuts from disjunctions with as many as 64 terms. Some instances are dramatically impacted, with strengthening increasing the gap closed by the cuts from 0 to 100%. However, for larger disjunctions, monoidal strengthening appears to be less eÔ¨Äective, for which we identify a potential cause.  
   
  1  
   
  Introduction  
   
  Disjunction-based cutting planes, or disjunctive cuts, are a strong class of valid inequalities for mixed-integer programming problems, which can be used as a framework for analyzing or generating general-purpose cuts [8]. Their strength comes at a high computational cost, due to which only very special cases of disjunctive cuts have been deployed in optimization solvers. As a step towards practicality, Balas and Kazachkov [10] introduce a relaxation-based V-polyhedral paradigm for disjunctive cuts, which trades oÔ¨Ä some theoretical strength for computational eÔ¨Éciency. The approach selects a small number of points and rays whose convex hull forms a relaxation of the disjunction; as a result, some potential cuts are no longer valid, but strong cuts are nevertheless guaranteed to be E. Balas passed away during the preparation of this manuscript, which started when both authors were at Carnegie Mellon University. The core ideas and early results are documented in the PhD dissertation of Kazachkov [37, Chapter 5]. A.M. Kazachkov completed the computational experiments, analysis, and writing independently. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 275‚Äì290, 2023. https://doi.org/10.1007/978-3-031-32726-1_20  
   
  276  
   
  A. M. Kazachkov and E. Balas  
   
  obtainable. Further, cuts from this relaxation, called V-polyhedral (disjunctive) cuts (VPCs), can be generated via a relatively compact linear program, called the point-ray linear program (PRLP), compared to the usual higher-dimensional cut-generating linear program (CGLP) for disjunctive cuts [8,14,15]. Hence, with VPCs, it is more computationally eÔ¨Écient to improve the disjunction by adding terms and increase the relaxation quality, thereby accessing disjunctive cuts that diÔ¨Äer substantially from the families of cuts typically applied in solvers. VPCs improve the average (integrality) gap closed substantially relative to Gomory mixed-integer cuts (GMICs) and other standard cuts in solvers. However, the computational experiments by Balas and Kazachkov [10] reveal a curiosity: there are instances for which GMICs (which can be derived as cuts from a two-term disjunction) remain stronger than VPCs even when using large variable disjunctions. For example, for the instance 10teams, originally part of the 3rd Mixed Integer Programming Library (MIPLIB) [18], GMICs close 100% of the integrality gap, while VPCs from a 64-term disjunction close 0% of the gap. A potential explanation for this phenomenon is that GMICs beneÔ¨Åt from a strengthening procedure that cannot be directly applied to VPCs. SpeciÔ¨Åcally, the GMIC two-term disjunction can be obtained via monoidal strengthening of a disjunction on a single variable [9,12,38]. Monoidal strengthening of cuts from more general disjunctions is also possible, but the procedure ostensibly requires a simple disjunction, where each term only imposes a single new constraint. This is not a theoretical barrier, as any cut from a general disjunction can also be derived from a simple disjunction obtained from the general one by aggregating the constraints deÔ¨Åning each disjunctive term. The multipliers for this aggregation are precisely the Farkas certiÔ¨Åcate for the validity of the cut. The key challenge for VPCs is that this certiÔ¨Åcate is not readily available, because the PRLP only has variables for the cut coeÔ¨Écients, compared to the CGLP that explicitly includes variables for the Farkas multipliers. Our contributions, summarized next, are to identify a way to eÔ¨Éciently apply monoidal strengthening for the particular version of the VPC framework introduced in Balas and Kazachkov [10], as well as to implement and computationally evaluate this strengthening idea. Contributions. Given a VPC, one can solve the CGLP with cut coeÔ¨Écients Ô¨Åxed and retrieve the required values of the aggregation multipliers, in order to apply monoidal strengthening. Unfortunately, the computational eÔ¨Äort associated to this is likely to be prohibitive. Our Ô¨Årst contribution, discussed in Sect. 3, is observing that solving the CGLP is unnecessary: it suÔ¨Éces to use the inverse of an easily-identiÔ¨Åed nonsingular matrix per disjunctive term. Furthermore, for the type of simple VPCs proposed and tested by Balas and Kazachkov [10], this inverse is readily available within the cut generation process. Next, in Sect. 4, we discuss computational experiments with strengthening simple VPCs on a set of benchmark instances. We compare the strength to unstrengthened VPCs and to GMICs, for disjunctions ranging in size up to 64 terms. We Ô¨Ånd that strengthening can signiÔ¨Åcantly improve the gap closed for some instances. Furthermore, we see that GMICs and unstrengthened VPCs tend to be complementary in terms of which instances they beneÔ¨Åt, but applying  
   
  Monoidal Strengthening of Simple V-Polyhedral Disjunctive Cuts  
   
  277  
   
  monoidal strengthening enables the two families to be simultaneously eÔ¨Äective for more instances. The results are most striking for two-term disjunctions, in which strengthened VPCs close 40% more gap than unstrengthened VPCs, on average. For example, returning to the instance 10teams, the VPCs from a single variable disjunction close 0% of the integrality gap, but this value goes to 100% after strengthening the cuts. However, as the size of the disjunction increases, the relative improvement by strengthening becomes smaller. Our Ô¨Ånal contribution, in Sect. 5, is identifying a theoretical source of this weakness. Related Work. A focal point in the literature on monoidal strengthening for disjunctive cuts [9] (see also Balas [8, Section 7]) is the special case of split disjunctions, which are parallel two-term disjunctions that are used for GMICs and related cut families. In this context, the use of the CGLP leads to lift-andproject cuts (L&PCs) [14], to which monoidal strengthening can be applied [15, Section 2.4]. The family of strengthened L&PCs is equivalent to GMICs, as shown by Balas and Perregaard [12], and to mixed-integer rounding inequalities [45,46], as discussed in Cornu√©jols and Li [23]. Balas and Perregaard [12] provide an appealing geometric interpretation of this connection via intersection cuts [7]: every undominated L&PC can be derived as an intersection cut from a basis in the original problem space. As a result, L&PCs can be generated without explicitly building the CGLP and without hindering a posteriori strengthening of the cuts. Bonami [19] presents a diÔ¨Äerent method for separating L&PCs in the original space of variables that is also amenable to strengthening. Avoiding formulating the higher-dimensional CGLP is the key advance that has enabled the eÔ¨Äective inclusion of L&PCs in several solvers. Sidestepping the CGLP continues to be crucial to move beyond split disjunctions. However, the aforementioned approaches [12,19] rely on properties of the split set; for example, with general disjunctions, there exist cuts that dominate all intersection cuts [5,11,40], so one cannot hope to merely pivot among bases in the original space. Nonetheless, a stream of work [20,36,40] extends cut generation in the original space to general two-term disjunctions, and monoidal strengthening applies to the resulting cuts [28]. No further extension of this technique to more general disjunctions has been reported in the literature. This motivates the use of VPCs, due to the PRLP‚Äôs advantage of having the same number of variables as the original problem. The diÔ¨Éculty is that a description of a polyhedron using points and rays may be exponentially larger than using inequalities, causing exponentially many constraints in the PRLP. This naturally leads to row generation in prior work by Perregaard and Balas [48] and Louveaux et al. [44] when invoking the V-polyhedral perspective. In the experiments by Perregaard and Balas [48], for disjunctions with 16 terms, separating cuts via the PRLP with row generation is an order of magnitude faster than via the CGLP. Nonetheless, row generation is time consuming, as multiple PRLPs must be solved to Ô¨Ånd one valid inequality. The remedy by Balas and Kazachkov [10] is to construct a relaxation of each disjunctive term, where the resulting PRLP has few rows and immediately produces valid cuts. This is successful at quickly generating cuts from large  
   
  278  
   
  A. M. Kazachkov and E. Balas  
   
  disjunctions, but the average gap closed by the cuts alone is less than that from GMICs. It is only when VPCs and GMICs are used together that a marked improvement in gap closed is observed, which shows that VPCs aÔ¨Äect a diÔ¨Äerent region of the relaxation than GMICs. However, as mentioned with the 10teams instance in which GMICs close all of the gap, while VPCs close none, the results also suggest that the absence of strengthening for VPCs is a signiÔ¨Åcant deÔ¨Åciency. As discussed, the vanilla monoidal strengthening presented by Balas and Jeroslow [9] does not directly apply to VPCs due to the lack of the values of the aggregation multipliers. Balas and Qualizza [13, Section 6] show that a crosspolytope disjunction, arising from using multiple rows of the simplex tableau, can be strengthened by modularizing the inequalities deÔ¨Åning the disjunction, replacing the coeÔ¨Écients of integer-restricted nonbasic variables, and they prove the form of the optimal strengthening for the two-row case. An alternative to monoidal strengthening is the group-theoretic approach [32, 35], equivalent to monoidal strengthening under some conditions. SpeciÔ¨Åcally, ‚Äútrivial lifting‚Äù has been applied to simple disjunctions [16,24‚Äì26,49]. Evaluating the trivial lifting is expensive in general [30], and it does not directly apply to arbitrary disjunctive cuts. While this paper exclusively approaches disjunctive cut generation via the VPC framework, there exist other methods for producing strong disjunctive cuts without solving the higher-dimensional CGLP. Any such approach could potentially beneÔ¨Åt from the eÔ¨Écient computation of a Farkas certiÔ¨Åcate. For example, a common technique in the literature is to use a disjunction to strengthen cuts via tilting, which has been applied to linear and nonlinear integer optimization problems [37,39,42,47].  
   
  2  
   
  Notation and Background  
   
  Our target is to Ô¨Ånd strong valid cuts to tighten the natural linear relaxation of the mixed-integer linear program below, given rational data: min  
   
  x‚ààRn  
   
  cT x Ai¬∑ x ‚â• bi  
   
  for i ‚àà [q],  
   
  xj ‚â• 0 xj ‚àà Z  
   
  for j ‚àà [n], for j ‚àà I.  
   
  (IP)  
   
  Here, [n] ..= {1, . . . , n} for any integer n, and I ‚äÜ [n] is the set of integerrestricted variables. For a given matrix A, we denote the ith row by ‚ÄúAi¬∑ ‚Äù and the jth column by ‚ÄúA¬∑j ‚Äù. Let PI denote the feasible region of (IP), and let P ..= {x ‚àà Rn‚â•0 : Ax ‚â• b}. One way to strengthen the formulation P (with respect to PI ) is to use logical conditions to formulate a disjunction, from which valid inequalities for PI can then be derived. Suppose ‚à®t‚ààT (Dt x ‚â• D0t ) is a valid disjunction, in the sense that PI ‚äÜ ‚à™t‚ààT {x ‚àà Rn : Dt x ‚â• D0t }. Let Qt ..= {x ‚àà P : Dt x ‚â• D0t }. This is an H-polyhedral (inequality) description. We assume Qt = ‚àÖ for all t ‚àà T .  
   
  Monoidal Strengthening of Simple V-Polyhedral Disjunctive Cuts  
   
  279  
   
  Let P t ..= {x ‚àà Rn : At x ‚â• bt } denote a relaxation of Qt , where At x ‚â• bt is deÔ¨Åned by a subset of the constraints deÔ¨Åning Qt . For the VPC procedure, we must ensure that P t has relatively few extreme points and rays, i.e., it has a compact V-polyhedral description (P t , Rt ), so that P t = conv(P t ) + cone(Rt ). DeÔ¨Åne the disjunctive hull PD ..= cl conv(‚à™t‚ààT P t ), which can be described by the point-ray collection (P, R) ..= (‚à™t‚ààT P t , ‚à™t‚ààT Rt ). For t ‚àà T , let qt be the number of rows of At . We Ô¨Årst summarize some important disjunctive programming concepts and the two cut-generating paradigms that we are relating. CGLP. One way to generate valid cuts for PD is through the CGLP, which is an application of disjunctive programming duality [8, Section 4]. SpeciÔ¨Åcally, an inequality Œ±T x ‚â• Œ≤ is valid for PD if and only if the inequality is valid for each P t , t ‚àà T . Consequently, by Farkas‚Äôs lemma [27], Œ±T x ‚â• Œ≤ is valid for PD if and only if the following system is feasible, in variables (Œ±, Œ≤, {v t }t‚ààT ), where  v t ‚àà R1√óqt is a row vector of appropriate length for each t ‚àà T : ‚é´ Œ±T = v t At ‚é¨ t t Œ≤‚â§v b for all t ‚àà T . (1) ‚é≠ qt v t ‚àà R‚â•0 We refer to {v t }t‚ààT as the Farkas certificate for the validity of Œ±T x ‚â• Œ≤ for PD . To generate cuts with (1), one typically maximizes the violation with respect to a PI -infeasible point, after adding a normalization, which can be a crucial choice [29]. For example, the constant of the cut can be Ô¨Åxed to Œ≤¬Ø ‚àà R:   ¬Ø {v t }t‚ààT ) is feasible to (1) . ¬Ø (Œ±, {v t }t‚ààT ) : (Œ±, Œ≤, (CGLP(Œ≤))  
   
  PRLP. An alternative way to generate disjunctive cuts is through the reverse polar of PD [8, Section 5], which is deÔ¨Åned with respect to a given Œ≤¬Ø ‚àà R as   Œ± ‚àà Rn : Œ±T x ‚â• Œ≤¬Ø for all x ‚àà PD . Clearly this captures all of the valid inequalities for PD whose constant is equal ¬Ø Since x ‚àà PD if and only if x ‚àà conv(P) + cone(R), it holds that Œ±T x ‚â• Œ≤¬Ø to Œ≤. is valid for PD if and only if it is satisÔ¨Åed by all of the points and rays in (P, R). ¬Ø ¬Ø in variables Œ± ‚àà Rn , for a Ô¨Åxed Œ≤: This yields the system (PRLP(Œ≤)), Œ±T p ‚â• Œ≤¬Ø  
   
  for all p ‚àà P  
   
  Œ± r‚â•0  
   
  for all r ‚àà R.  
   
  T  
   
  ¬Ø (PRLP(Œ≤))  
   
  ¬Ø are what we refer to as VPCs. The feasible solutions to (PRLP(Œ≤)) ¬Ø over (CGLP(Œ≤)) ¬Ø is the absence of As discussed, the advantage of (PRLP(Œ≤)) the Farkas multipliers as variables, so VPCs are generated without requiring a ¬Ø is that these missing lifted space. As we see next, the disadvantage to (PRLP(Œ≤)) variables are used in strengthening the cuts after they are generated.  
   
  280  
   
  A. M. Kazachkov and E. Balas  
   
  Monoidal Strengthening. Balas and Jeroslow [9] strengthen cuts with a monoid:   |T | . mt ‚â• 0 . (M) M .= m ‚àà Z : t‚ààT  
   
  It is also assumed that, for each t ‚àà T , there exists a Ô¨Ånite lower bound vector t such that Dt x ‚â• t for all x ‚àà PI . Let Œît ..= D0t ‚àí t . To strengthen the cut, we improve the underlying disjunction. SpeciÔ¨Åcally, given a valid disjunction ‚à®t‚ààT (Dt x ‚â• D0t ), for any m ‚àà M and k ‚àà I, the Àú tx ‚â• D Àú t ) is also valid, where D Àú t ..= Dt + Œît mt , and D Àút = disjunction ‚à®t‚ààT (D 0 ¬∑j ¬∑k ¬∑k t D¬∑j for all j = k. The strengthened cut is obtained by applying the Farkas certiÔ¨Åcate of the unstrengthened cut to the strengthened disjunction. Let qt denote the number of constraints in Dt x ‚â• D0t for term t ‚àà T . Given 1√óqt √ó R‚â•0 , deÔ¨Åne row vectors (ut , ut0 ) ‚àà R1√óq ‚â•0 t Œ±kt ..= ut A¬∑k + ut0 D¬∑k .  
   
  (Œ±kt )  
   
  Then (using an appropriate CGLP) the cut Œ±T x ‚â• Œ≤ is valid for PD , where Œ±k ..= max{Œ±kt } t‚ààT  
   
  and  
   
  Œ≤ ..= min{ut b + ut0 D0t }. t‚ààT  
   
  (The above applies to cuts valid for ‚à®t‚ààT Qt ; for PD , assume a value of zero for the ÀÜtk ..= Œ±k ‚àíŒ±kt . multipliers on constraints of Qt that are not present in P t .) DeÔ¨Åne u T We now apply monoidal strengthening to the cut Œ± x ‚â• Œ≤. 1√óqt Theorem 1 ([9, Theorem 3]). Given (ut , ut0 ) ‚àà R1√óq √ó R‚â•0 for t ‚àà T , the ‚â•0 T .  
   
  k .= Œ±k for k ‚àà / I, and, for k ‚àà I, inequality Œ±  
   
  x ‚â• Œ≤ is valid for PI , where Œ±    t  uk + ut0 Œît mt . Œ±  
   
  k ..= inf max Œ±kt + ut0 Œît mt = Œ±k + inf max ‚àíÀÜ m‚ààM t‚ààT  
   
  m‚ààM t‚ààT  
   
  Thus, the Farkas certiÔ¨Åcate {(ut , ut0 )}t‚ààT is used for monoidal strengthening. Computing these values without solving the CGLP is our next target.  
   
  3  
   
  Correspondence Between PRLP and CGLP Solutions  
   
  Let Œ± ¬Ø T x ‚â• Œ≤¬Ø be a valid inequality for PD , corresponding to a feasible solution ¬Ø Our goal is to compute Farkas multipliers certifying the cut‚Äôs to (PRLP(Œ≤)). validity without explicitly solving the CGLP. While one can solve for values ¬Ø T = v t At , Œ≤¬Ø = v t bt , v t ‚â• 0, we provide an improvement via v t that satisfy Œ± basic linear programming concepts. We Ô¨Årst present a special case in Sects. 3.1 and 3.2, when the disjunctive terms P t are not primal degenerate, a condition that is satisÔ¨Åed by the VPC procedure implemented for our experiments. Then, Sect. 3.3 discusses a challenge posed by the general case. We assume that Œ± ¬Ø T x ‚â• Œ≤¬Ø is supporting for all terms in T . This is for ease of notation, as otherwise we would need to add an index t to the constant side. Concretely, the assumption is without loss of generality because, for any term  
   
  Monoidal Strengthening of Simple V-Polyhedral Disjunctive Cuts  
   
  281  
   
  t ‚àà T , we can increase the constant side of the cut until we obtain an inequality Œ± ¬Ø T x ‚â• Œ≤¬Øt that is supporting for term t, though perhaps invalid for other terms. ¬Ø with The value of Œ≤¬Øt can be quickly calculated by taking the dot product of Œ± ¬Ø T x ‚â• Œ≤¬Øt every point in P t . We can then Ô¨Ånd a certiÔ¨Åcate v t of the validity of Œ± ¬Ø We for P t , which also serves as a certiÔ¨Åcate for the weaker inequality Œ± ¬Ø T x ‚â• Œ≤. state, without proof, a slightly more general version of this in Lemma 2. Lemma 2. For t ‚àà T , let C t ‚äá P t and Œ≤¬Øt ‚â• Œ≤¬Ø such that Œ± ¬Ø T x ‚â• Œ≤¬Øt is valid for ¬Ø T x ‚â• Œ≤¬Øt C t . Then, given any Farkas certificate for the validity of the inequality Œ± for C t , the same multipliers certify that Œ± ¬Ø T x ‚â• Œ≤¬Ø is valid for P t . For convenience, we introduce extra notation to refer to the feasible region of Qt as AÀÜt x ‚â• ÀÜbt , and we deÔ¨Åne the number of these constraints as qÀÜt ..= q + qt + n. For N ‚äÜ [ÀÜ qt ], deÔ¨Åne AÀÜtN x ‚â• ÀÜbtN as the constraints of Qt indexed by N . 3.1  
   
  Simple VPCs  
   
  Our experimental setup in Sect. 4 follows that of Balas and Kazachkov [10], who focus on a variant of the VPC framework called simple VPCs. Let pt be a vertex of Qt , for t ‚àà T . There exists a cobasis for pt , a set of n linearly independent qt ] denote constraints among those deÔ¨Åning Qt that are tight at pt . Let N t ‚äÜ [ÀÜ the indices of these n constraints, and deÔ¨Åne the basis cone C t ..= {x ‚àà Rn : AÀÜtN t x ‚â• ÀÜbtN t }. The inequality Œ± ¬Ø T x ‚â• Œ≤¬Ø is a simple VPC if P t is a basis cone for each term. The (translated) cone C t has a particularly easy V-polyhedral representation: there is a single extreme point pt , and there are n extreme rays {ri }i‚àà[n] . The ith extreme ray of C t corresponds to increasing the ‚Äúslack‚Äù on the ith constraint deÔ¨Åning C t [21, Chapter 6]. Lemma 3 states that, for simple ¬Ø can be computed via VPCs, the values of the variables {v t }t‚ààT to (CGLP(Œ≤)) the dot product of the cut coeÔ¨Écients with the rays of C t . Lemma 3. Let C t be a basis cone defined by N t , the indices of n linearly inde¬Ø T x ‚â• Œ≤¬Ø is valid for C t , then the multiplier on pendent constraints of Qt . If Œ± t ¬Ø T ri , where ri is column i of (AÀÜtN t )‚àí1 . constraint i ‚àà [n] of C has value vit = Œ± Proof. Add nonnegative slack variables stN t for each row indexed by N t , so that AÀÜtN t x ‚àí stN t = btN t . Then observe that, being a cobasis, AÀÜtN t is invertible, so x = (AÀÜtN t )‚àí1 btN t + (AÀÜtN t )‚àí1 stN t = pt + i‚ààN t ri sti . The last equality follows from the derivation of the rays of C t ; see, for example, Conforti et al. [21, Chapter 6]. Therefore, for simple VPCs, the Farkas certiÔ¨Åcate can be computed with no extra eÔ¨Äort when given the point-ray representation of PD . Moreover, Balas and Kazachkov [10] obtain simple VPCs from the leaf nodes of a partial branch-andbound tree and use pt as the optimal solution to the linear relaxation at each leaf; implemented carefully, this can further reduce the computational load for generating then strengthening VPCs, as the values of the rays can be read from the optimal tableau, which is typically readily available from a solver.  
   
  282  
   
  3.2  
   
  A. M. Kazachkov and E. Balas  
   
  Relaxations Without Primal Degeneracy  
   
  Suppose the relaxation P t ‚äá Qt is a simple polyhedron, in which every extreme point and ray is deÔ¨Åned by a unique basis [50]. The basis cone C t used for simple VPCs is one example. While the basis cone setting may seem quite narrow, it turns out to encompass more general situations. SpeciÔ¨Åcally, there always exists ¬Ø T x ‚â• Œ≤¬Ø is valid and supporting for C t . a basis cone C t ‚äá P t such that Œ± Lemma 4. Let P t be a simple polyhedron, and suppose the point-ray collection ¬Ø T x ‚â• Œ≤¬Ø be a valid inequality (P t , Rt ) satisfies P t = conv(P t ) + cone(Rt ). Let Œ± t t t ¬Ø T x ‚â• Œ≤¬Ø is valid for the for P . Then there exists a vertex p ‚àà P such that Œ± t t basis cone C associated to p , defined with respect to the constraints of P t . Proof. Let pt be an optimal solution to minx {¬Ø Œ±T x : x ‚àà P t } = minp {¬Ø Œ±T p : p ‚àà t T t . ¬Ø . ¬Ø p. Note that the rays in R need not be considered, as the P }. DeÔ¨Åne Œ≤t = Œ± optimization problem must be bounded since Œ± ¬Ø T x ‚â• Œ≤¬Øt is valid for all x ‚àà P t . t The point p has a unique basis, so the basis cone C t is deÔ¨Åned by the (precisely) n constraints of P t that are tight at pt . Optimality of pt implies all reduced costs ¬Ø ¬Ø T pt = Œ≤¬Øt ‚â• Œ≤, are nonnegative. It follows that Œ± ¬Ø T r ‚â• 0 every ray r ‚àà C t . Since Œ± T t ¬Ø the inequality Œ± ¬Ø x ‚â• Œ≤ is valid for C . Therefore, we can invoke Lemmas 2 and 3 to Ô¨Ånd the Farkas certiÔ¨Åcate for this case. Note that, when the given point-ray collection only contains extreme points and rays, the rays of C t for any basis cone of the simple polyhedron P t can be computed as the rays Rt , along with the directions p ‚àí pt for every point p ‚àà P t that is adjacent (one pivot away) from pt . 3.3  
   
  Relaxations with Primal Degeneracy  
   
  Up to now, we have made the convenient assumption that the relaxation P t is a simple polyhedron. More generally, there always exists a basis cone C t , such that a cut valid for P t is valid for C t . With Example 5, we illustrate the complication if Œ± ¬Ø T x ‚â• Œ≤¬Ø is supporting at a primal degenerate point of P t : a basis for that point needs to be chosen carefully, as the inequality may not be valid for some basis cones. It can be computationally involved to Ô¨Ånd a valid basis in these situations, which prevents a direct application of our approach relying on simple polyhedra. The purpose of this example is to highlight a crucial obstacle to a complete correspondence between PRLP and CGLP solutions, but we do not further investigate the nondegenerate case in this paper. Example 5. Figure 1 shows a polyhedron P , deÔ¨Åned as the feasible solutions to ‚àí(13/8)x1 ‚àí (1/4)x2 ‚àí x3 ‚â• ‚àí15/8 (1/2)x1 + x2 ‚â• 1/2 (1/2)x1 ‚àí x3 ‚â• ‚àí3/4 (1/2)x1 ‚àí x2 ‚â• ‚àí1/2 x2 ‚â• 0.  
   
  (c1) (c2) (c3) (c4) (c5)  
   
  Monoidal Strengthening of Simple V-Polyhedral Disjunctive Cuts  
   
  283  
   
  Fig. 1. Example 5: Disjunctive terms with primal degeneracy, despite a nondegenerate initial polyhedron. The VPC is the red wavy line in the second panel. (Color Ô¨Ågure online)  
   
  A valid cut from the disjunction (‚àíx1 ‚â• 0) ‚à® (x1 ‚â• 1) has coeÔ¨Écients Œ± ¬ØT = (‚àí5/8, ‚àí1/4, ‚àí1) and constant Œ≤¬Ø = ‚àí7/8. The cut, depicted in the right panel, is incident to point p1 = (0, 1/2, 3/4) on P 1 ..= {x ‚àà P : ‚àíx1 ‚â• 0}. This point is tight for four inequalities: three deÔ¨Åning P (constraints (c2)-(c4)), and the disjunction-deÔ¨Åning inequality ‚àíx1 ‚â• 0. Note that P is simple, but P 1 is not. To construct the cobasis N 1 , such that the inequality is valid for the associated basis cone C 1 , we must select three linearly independent constraints among those that are tight at p1 . One of the inequalities must be ‚àíx1 ‚â• 0, as otherwise we have not imposed the disjunction at all (but we also know the cut is not valid for P ). It can be veriÔ¨Åed that the only valid choice for this example is N 1 containing the indices for (c3), (c4), and the disjunctive inequality ‚àíx1 ‚â• 0.   
   
  4  
   
  Computational Experiments  
   
  We implement monoidal strengthening for simple VPCs, building on the code used by Balas and Kazachkov [10] from https://github.com/akazachk/vpc. Our goal for the computational study is to measure the eÔ¨Äect of monoidal strengthening on the percent integrality gap closed by VPCs, compared to unstrengthened VPCs and GMICs, and evaluated across diÔ¨Äerent disjunction sizes. The code is run on HiPerGator, a shared cluster through Research Computing at the University of Florida. The computational setup is nearly identical to the one described in Balas and Kazachkov [10, Section 5 and Appendix C]. We select instances from the union of the MIPLIB [4,17,18,31,41], CORAL [22], and NEOS sets, restricted to those with at most 5,000 rows and columns and based on other criteria given in [10, Appendix C]. This yields 332 instances suitable for gap closed comparisons. However, we only report on 274 of these  
   
  284  
   
  A. M. Kazachkov and E. Balas  
   
  332 instances, due to memory resource constraints on the cluster. Despite this reduced dataset, we can identify recurring patterns in how monoidal strengthening aÔ¨Äects instances. Instances are presolved with Gurobi [34], but cut generation is done via the C++ interface to COIN-OR [43], using Clp [3] for solving linear programs and Cbc [1] for constructing disjunctions based on partial branch-andbound trees. We test six diÔ¨Äerent disjunction sizes, stopping branching when the number of leaf nodes (disjunctive terms) is 2 for  ‚àà [6]. Thus, we report results with monoidal strengthening of disjunctive cuts from up to 64-term disjunctions, though only one disjunction is used at a time. One GMIC is generated per fractional integer variable at an optimal solution to the linear programming relaxation, and the number of GMICs is also used as the limit for the number of VPCs we generate for that instance per Ô¨Åxed choice of disjunction. One round of cuts is used for both procedures. GMICs are generated through CglGMI [2], while the VPC generation procedure is identical to that of Balas and Kazachkov [10], with strengthening applied afterwards. While Lemma 3 enables us to calculate the values of the Farkas multipliers via the rays of each relaxation P t , and these values are readily available based on how we built the PRLP, we do not avail of this connection. Instead, we calculate ¬Ø T (At )‚àí1 . This approach is still more direct than solving a feasibility version vt = Œ± ¬Ø with Œ± of (CGLP(Œ≤)) ¬Ø Ô¨Åxed. We opt for numerical safety for this exploratory investigation, so we use the Eigen library [33] to recompute the inverse of At rather than reading from the Cbc / Clp internal basis inverse for each term. We report the average percent integrality gap closed by VPCs and GMICs in Table 1. The Ô¨Årst six data rows contain the results for each Ô¨Åxed disjunction size. The penultimate data row, labeled ‚ÄúBest‚Äù, uses the highest gap closed per instance across all disjunctions. The last data row, labeled ‚ÄúWins‚Äù, reports the number of instances for which the ‚ÄúBest‚Äù gap closed is at least 10‚àí3 higher than the gap closed by GMICs. In the columns, we refer to GMICs by ‚ÄúG‚Äù, unstrengthened VPCs by ‚ÄúV‚Äù, strengthened VPCs by ‚ÄúV+ ‚Äù. The columns ‚ÄúG+V‚Äù and ‚ÄúG+V+ ‚Äù refer to GMICs applied together with VPCs. There are two sets of instances: ‚ÄúAll‚Äù reports on all 274 instances, while ‚Äú‚â•10%‚Äù reports on the 97 instances for which unstrengthened VPCs alone close at least 10% of the integrality gap for the ‚ÄúBest‚Äù values. In terms of overall gap closed, despite the monoidal strengthening procedure, as reported by Balas and Kazachkov [10], VPCs alone do not outperform GMICs for the ‚ÄúAll‚Äù set, but using VPCs and GMICs together provides around 40% improvement in gap closed relative to GMICs alone. While adding VPCs with GMICs might double the number of cuts, one round of VPCs continues to close substantial more gap even after multiple rounds of solver-default cuts [10]. Hence, VPCs tighten the relaxation in diÔ¨Äerent regions relative to GMICs. This is also highlighted by the ‚Äú‚â•10%‚Äù set, which are instances for which VPCs have strong performance; for this set, GMICs are relatively weaker, with the best VPCs per instance (used alone) providing a 75% improvement in average percent gap closed over GMICs alone. We also see this in the ‚ÄúWins‚Äù row: for the ‚Äú‚â•10%‚Äù set, VPCs alone outperform GMICs for 73 of the 97 instances in the set.  
   
  Monoidal Strengthening of Simple V-Polyhedral Disjunctive Cuts  
   
  285  
   
  Table 1. Average percent gap closed by VPCs and GMICs according to the number of leaf nodes used to construct the partial branch-and-bound tree. ‚ÄúBest‚Äù refers to the maximum gap closed per instance across all partial tree sizes. All G  
   
  V  
   
  V+  
   
  ‚â•10% G+V G+V+ G V  
   
  V+  
   
  G+V G+V+  
   
  17.95 18.37 18.98 20.54 22.31 23.72  
   
  6.47 8.35 11.16 16.05 22.28 25.85  
   
  18.13 19.14 20.66 24.86 29.59 32.90  
   
  2 leaves 4 leaves 8 leaves 16 leaves 32 leaves 64 leaves  
   
  17.21 2.28 3.25 17.21 3.35 3.72 17.21 4.51 4.76 17.21 6.41 6.57 17.21 8.78 8.97 17.21 10.46 10.57  
   
  18.18 18.54 19.15 20.67 22.48 23.83  
   
  Best Wins  
   
  17.21 11.93 12.57 24.67 24.89 103 104 185 190  
   
  16.29 16.29 16.29 16.29 16.29 16.29  
   
  5.34 7.81 10.84 15.81 21.82 25.59  
   
  18.59 19.48 20.91 25.04 29.97 33.14  
   
  16.29 29.26 29.53 35.27 35.59 73 73 94 94  
   
  Next, we summarize observations about the eÔ¨Äect of monoidal strengthening. We start with the Ô¨Årst data row, in which VPCs are derived from one split disjunction per instance. For the set ‚ÄúAll‚Äù, monoidal strengthening aÔ¨Äects the gap closed by VPCs for 87 instances and increases the average gap closed by VPCs by ~1% from 2.28% to 3.25%, a 40% relative improvement. For the set ‚Äú‚â•10%‚Äù, the corresponding relative improvement is 20%. Although the two-term case is encouraging, and a similar relative improvement in gap closed would be substantial for larger disjunctions, this unfortunately does not materialize. From Table 1, we see that as the disjunction size increases, the contribution of monoidal strengthening tends to further diminish, with an absolute improvement in gap closed of only 0.1% for VPCs from a 64-term disjunction. We will discuss a potential cause for this in the next section. We now compare the columns ‚ÄúG+V+ ‚Äù to ‚ÄúG+V‚Äù. On the set ‚ÄúAll‚Äù, even for split disjunctions, the eÔ¨Äect of strengthening is minimal when VPCs are combined with GMICs, with strengthening only yielding an additional 0.23% in percent gap closed, preserving around 23% of the improvement between ‚ÄúV+ ‚Äù and ‚ÄúV‚Äù. For larger disjunctions, while the absolute increase in gap closed by strengthened VPCs is small, over 80% of that improvement is preserved when adding GMICs together with VPCs. A closer examination of the results supports the hypothesis that monoidal strengthening is a key factor enabling GMICs to close more gap than VPCs. We sort the instances by the increase in gap closed by strengthened VPCs compared to unstrengthened ones, using the best gap closed across all disjunction sizes, per column. Table 2 shows the top ten instances, sorted by the last column, which calculates the diÔ¨Äerence between ‚ÄúV+ ‚Äù and ‚ÄúV‚Äù. The table includes the instance 10teams discussed earlier, as well as six other instances for which unstrengthened VPCs close at most 5% of the gap. We see that monoidal strengthening of VPCs bridges a large portion of the diÔ¨Äerence with GMICs for these instances. For neos-1281048, the situation is reversed: 121 GMICs close no gap while 29  
   
  286  
   
  A. M. Kazachkov and E. Balas  
   
  Table 2. Percent gap closed for instances where strengthening VPCs works best. Instance  
   
  G  
   
  V  
   
  V+  
   
  G+V  
   
  G+V+ V+ ‚àíV  
   
  10teams 100.00 0.00 100.00 100.00 100.00 100.00 neos-1281048 0.00 17.09 29.36 17.09 29.36 12.27 neos-1599274 34.65 0.00 11.19 34.65 34.65 11.19 f2gap401600 62.97 2.53 11.34 63.31 71.77 8.80 prod2 2.31 27.60 35.90 27.63 35.91 8.29 neos-942830 6.25 0.00 6.25 6.25 6.25 6.25 p0548 48.62 3.28 9.03 49.03 55.11 5.75 mkc 6.08 2.60 6.56 6.35 9.61 3.96 f2gap201600 60.27 8.58 12.13 60.27 60.27 3.56 neos-4333596-skien 20.84 7.05 9.83 20.84 20.85 2.78  
   
  unstrengthened VPCs close 17% of the gap, which is further improved to 29% after strengthening. From this table, we also observe the phenomenon that the value in column ‚ÄúG+V‚Äù is typically either entirely due to GMICs or to VPCs, but which cuts are more important varies by instance. The situation remains similar for the column ‚ÄúG+V+ ‚Äù, though now we Ô¨Ånd several cases (f2gap401600, p0548, mkc) in which the two cut families add to each other. While running time is not our focus, and the shared computing environment makes wall clock times unreliable, Table 3 provides the average number of seconds for a single run of each instance, including generating then strengthening VPCs. On average, cut generation takes, in total, from less than a second for two-term disjunctions to 50 s for 16-term disjunctions, 150 s for 32-term disjunctions, and nearly 9 min for 64-term disjunctions. The time per cut, on average, is less than 0.1 s for two-term disjunctions, ranging up to 9 s for 32 terms and over 30 s for 64 terms.  
   
  5  
   
  Choosing a Relaxation Amenable to Strengthening  
   
  In this section, we examine a potential cause of the diminishing eÔ¨Äect of monoidal strengthening with larger disjunctions. From Theorem 1, given an initial cut Œ±T x ‚â• Œ≤, we can strengthen coeÔ¨Écient Œ±k , k ‚àà I, to  t  Œ±  
   
  k = Œ±k + inf max ‚àíÀÜ uk + ut0 Œît mt , m‚ààM t‚ààT  
   
  Table 3. Average time (seconds) to generate the cuts in column V+ of Table 1. Statistic  
   
  Set  
   
  2 leaves 4 leaves 8 leaves 16 leaves 32 leaves 64 leaves  
   
  Cut time (s) All 0.76 ‚â•10% 0.92  
   
  6.39 9.31  
   
  15.33 21.06  
   
  49.90 130.45  
   
  149.84 273.51  
   
  525.78 521.99  
   
  Time/cut (s) All 0.08 ‚â•10% 0.07  
   
  0.39 0.35  
   
  0.97 0.79  
   
  2.65 2.46  
   
  9.00 7.75  
   
  30.54 20.19  
   
  Monoidal Strengthening of Simple V-Polyhedral Disjunctive Cuts  
   
  287  
   
  t where u ÀÜtk = Œ±k ‚àí (ut A¬∑k + ut0 D¬∑k ) is the slack on the CGLP constraint Œ±k ‚â• t . Equivalently, u ÀÜtk is the Farkas multiplier for the nonnegativity ut A¬∑k + ut0 D¬∑k constraint xk ‚â• 0. The next lemma restates the (known) reason that a nonbasic integral variable k is required for monoidal strengthening.  
   
  Lemma 6. If u ÀÜtk = 0, then Œ±  
   
  k = Œ±k . Proof. In this case, Œ±  
   
  k = Œ±k + inf m‚ààM maxt‚ààT {ut0 Œît mt } . Since for every m ‚àà M, and ut0 Œît ‚â• 0, the optimal solution is m = 0.  
   
  t‚ààT  
   
  mt ‚â• 0  
   
  In the correspondence in Sect. 3, we ultimately Ô¨Ånd a point pt ‚àà P t such that Œ±T p : p ‚àà P t }. We then compute a basis cone at pt Œ± ¬Ø pt = Œ≤¬Øt , where Œ≤¬Øt = minp {¬Ø for which the cut is valid and use this (translated) cone to compute the values of the Farkas certiÔ¨Åcate. However, by complementary slackness, if ptk > 0, then necessarily u ÀÜtk = 0. Although at Ô¨Årst this appears simultaneously unfortunate and unavoidable, there are two potential remedies. First, there may be dual degeneracy in the choice of pt : each such point can lead to a diÔ¨Äerent Farkas certiÔ¨Åcate and therefore a diÔ¨Äerent strengthening. Second, as observed by Balas and Qualizza [6], ‚Äúsometimes weaking a disjunction helps the strengthening‚Äù. Though in that context, the weakening involves adding terms to the disjunction, the sentiment ¬Ø then one can seek a diÔ¨Äerent, potentially applies to our setting as well: if Œ≤¬Øt > Œ≤, t ¬Ø T x ‚â• Œ≤¬Ø infeasible, basis of Q in which more integer variables are nonbasic and Œ± is still valid for the associated basis cone. The computational results support the above intuition. When VPCs are generated from a split disjunction, on average, around 95% of the generated cuts per instance have any coeÔ¨Écient strengthened with the monoidal technique. This decreases to 85% for 64-term disjunctions. Furthermore, on average among VPCs to which strengthening has been applied, 20% of the cut coeÔ¨Écients are strengthened for split disjunctions, while this value steadily decreases as disjunction size increases, so among the analogous VPCs from 64-term disjunctions, only 10% of the coeÔ¨Écients are strengthened. T  
   
  6  
   
  Conclusion  
   
  We show that strengthening cuts from general disjunctions is possible without explicitly solving a higher-dimensional CGLP, and that this strengthening can have a high impact for certain instances. However, several challenges are also highlighted for future work. First, the strengthening does not work well on average for larger disjunctions. While we propose a viable explanation and remedy, it is computationally demanding and requires development. Second, the optimal monoidal strengthening involves solving an integer program per cut; this is a relatively small and easy problem, but it nonetheless can be slow for larger disjunctions, as suggested by Table 3, which includes strengthening time. One can reduce this load by selectively strengthening only the most promising cuts, identiÔ¨Åed by theoretical properties or good heuristics, or to forego optimality in  
   
  288  
   
  A. M. Kazachkov and E. Balas  
   
  the strengthened cut coeÔ¨Écients. Our computational results indicate that VPCs and GMICs seem to have complementary aÔ¨Äects; understanding this better is an opportunity to more widely adopt disjunctive cuts.  
   
  References 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12.  
   
  13. 14. 15. 16. 17. 18. 19. 20. 21.  
   
  22.  
   
  COIN-OR Branch and Cut. https://github.com/coin-or/Cbc COIN-OR Cut Generation Library. https://github.com/coin-or/Cgl COIN-OR Linear Programming. https://github.com/coin-or/Clp Achterberg, T., Koch, T., Martin, A.: MIPLIB 2003. Oper. Res. Lett. 34(4), 361‚Äì 372 (2006) Andersen, K., Cornu√©jols, G., Li, Y.: Split closure and intersection cuts. Math. Program., 102(3, Ser. A), 457‚Äì493 (2005) Balas, E., Qualizza, A.: Monoidal cut strengthening revisited. Discrete Optim. 9(1), 40‚Äì49 (2012) Balas, E.: Intersection cuts‚Äìa new type of cutting planes for integer programming. Oper. Res. 19(1), 19‚Äì39 (1971) Balas, E.: Disjunctive programming. Ann. Discrete Math. 5, 3‚Äì51 (1979) Balas, E., Jeroslow, R.G.: Strengthening cuts for mixed integer programs. Eur. J. Oper. Res. 4(4), 224‚Äì234 (1980) Balas, E., Kazachkov, A.M.: V-polyhedral disjunctive cuts (2022). https://arxiv. org/abs/2207.13619 Balas, E., Kis, T.: On the relationship between standard intersection cuts, lift-andproject cuts and generalized intersection cuts. Math. Program., 1‚Äì30 (2016) Balas, E., Perregaard, M.: A precise correspondence between lift-and-project cuts, simple disjunctive cuts, and mixed integer Gomory cuts for 0-1 programming. Math. Program. 94(2‚Äì3, Ser. B), 221‚Äì245 (2003). The Aussois 2000 Workshop in Combinatorial Optimization Balas, E., Qualizza, A.: Intersection cuts from multiple rows: a disjunctive programming approach. EURO J. Computat. Optim. 1(1), 3‚Äì49 (2013) Balas, E., Ceria, S., Cornu√©jols, G.: A lift-and-project cutting plane algorithm for mixed 0-1 programs. Math. Program. 58(3, Ser. A), 295‚Äì324 (1993) Balas, E., Ceria, S., Cornu√©jols, G.: Mixed 0-1 programming by lift-and-project in a branch-and-cut framework. Man. Sci. 42(9), 1229‚Äì1246 (1996) Basu, A., Bonami, P., Cornu√©jols, G., Margot, F.: Experiments with two-row cuts from degenerate tableaux. INFORMS J. Comput. 23(4), 578‚Äì590 (2011) Bixby, R.E., Boyd, E.A., Indovina, R.R.: MIPLIB: a test set of mixed integer programming problems. SIAM News 25, 16 (1992) Bixby, R.E., Ceria, S., McZeal, C.M., Savelsbergh, M.W.P.: An updated mixed integer programming library: MIPLIB 3.0. Optima, 58, 12‚Äì15, 6 (1998) Bonami, P.: On optimizing over lift-and-project closures. Math. Program. Comput. 4(2), 151‚Äì179 (2012) Bonami, P., Conforti, M., Cornu√©jols, G., Molinaro, M., Zambelli, G.: Cutting planes from two-term disjunctions. Oper. Res. Lett. 41(5), 442‚Äì444 (2013) Conforti, M., Cornu√©jols, G., Zambelli, G.: Integer Programming, vol. 271 of Graduate Texts in Mathematics. Springer, Cham (2014). https://doi.org/10.1007/9783-319-11008-0 CORAL. Computational Optimization Research at Lehigh. MIP instances. https://coral.ise.lehigh.edu/data-sets/mixed-integer-instances/ (2020). Accessed Sept 2020  
   
  Monoidal Strengthening of Simple V-Polyhedral Disjunctive Cuts  
   
  289  
   
  23. Cornu√©jols, G., Li, Y.: Elementary closures for integer programs. Oper. Res. Lett. 28(1), 1‚Äì8 (2001) 24. Dey, S.S., Wolsey, L.A.: Two row mixed-integer cuts via lifting. Math. Program. 124(1‚Äì2, Ser. B), 143‚Äì174 (2010) 25. Dey, S.S., Lodi, A., Tramontani, A., Wolsey, L.A.: On the practical strength of two-row tableau cuts. INFORMS J. Comput. 26(2), 222‚Äì237 (2014) 26. Espinoza, D.G.: Computing with multi-row Gomory cuts. Oper. Res. Lett. 38(2), 115‚Äì120 (2010) 27. Farkas, J.: Theorie der einfachen Ungleichungen. J. Reine Angew. Math. 124, 1‚Äì27 (1902) 28. Fischer, T., Pfetsch, M.E.: Monoidal cut strengthening and generalized mixedinteger rounding for disjunctions and complementarity constraints. Oper. Res. Lett. 45(6), 556‚Äì560 (2017) 29. Fischetti, M., Lodi, A., Tramontani, A.: On the separation of disjunctive cuts. Math. Program. 128(1‚Äì2, Ser. A), 205‚Äì230 (2011) 30. Fukasawa, R., Poirrier, L., Xavier, √Å.S.: The (not so) trivial lifting in two dimensions. Math. Program. Comp. 11(2), 211‚Äì235 (2019) 31. Gleixner, A., et al.: MIPLIB 2017: Data-Driven compilation of the 6th mixedinteger programming library. Math. Prog. Comp., (2021) 32. Gomory, R.E., Johnson, E.L.: Some continuous functions related to corner polyhedra. Math. Program. 3(1), 23‚Äì85 (1972) 33. Guennebaud, G., et al.: Eigen v3. http://eigen.tuxfamily.org (2010) 34. Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual (2022) 35. Johnson, E.L.: On the group problem for mixed integer programming. Math. Program. Stud. 2, 137‚Äì179 (1974) 36. J√∫dice, J.J., Sherali, H.D., Ribeiro, I.M., Faustino, A.M.: A complementarity-based partitioning and disjunctive cut algorithm for mathematical programming problems with equilibrium constraints. J. Global Optim. 36(1), 89‚Äì114 (2006) 37. Kazachkov, A.M.: Non-Recursive Cut Generation. PhD thesis, Carnegie Mellon University (2018) 38. Kazachkov, A.M., Serrano, F.: Monoidal cut strengthening. In: Prokopyev, O., Pardalos, P.M., editors, Encyclopedia of Optimization. Springer, US, Boston, MA. Under review 39. Kƒ±lƒ±n√ß, M., Linderoth, J., Luedtke, J., Miller, A.: Strong-branching inequalities for convex mixed integer nonlinear programs. Comput. Optim. Appl. 59(3), 639‚Äì665 (2014). https://doi.org/10.1007/s10589-014-9690-8 40. Kis, T.: Lift-and-project for general two-term disjunctions. Discrete Optim. 12, 98‚Äì114 (2014) 41. Koch, T., Achterberg, T., Andersen, E., Bastert, O., Berthold, T., Bixby, R.E., et al.: MIPLIB 2010: mixed integer programming library version 5. Math. Program. Comput. 3(2), 103‚Äì163 (2011) 42. Kronqvist, J., Misener, R.: A disjunctive cut strengthening technique for convex MINLP. Optim. Eng. 22(3), 1315‚Äì1345 (2021) 43. Lougee-Heimer, R.: The Common Optimization INterface for Operations Research: promoting open-source software in the operations research community. IBM J. Res. Dev. 47 (2003) 44. Louveaux, Q., Poirrier, L., Salvagnin, D.: The strength of multi-row models. Math. Program. Comput. 7(2), 113‚Äì148 (2015) 45. Nemhauser, G.L., Wolsey, L.A.: Integer and combinatorial optimization. WileyInterscience Series in Discrete Mathematics and Optimization. John Wiley & Sons Inc, New York (1988)  
   
  290  
   
  A. M. Kazachkov and E. Balas  
   
  46. Nemhauser, G.L., Wolsey, L.A.: A recursive procedure to generate all cuts for 0-1 mixed integer programs. Math. Program. 46(1), 379‚Äì390 (1990) 47. Perregaard, M.: Generating Disjunctive Cuts for Mixed Integer Programs. PhD thesis, Carnegie Mellon University, 9 (2003) 48. Perregaard, M., Balas, E.: Generating cuts from multiple-term disjunctions. In: Aardal, K., Gerards, B. (eds.) IPCO 2001. LNCS, vol. 2081, pp. 348‚Äì360. Springer, Heidelberg (2001). https://doi.org/10.1007/3-540-45535-3_27 49. Xavier, √Å.S., Fukasawa, R., Poirrier, L.: Multirow intersection cuts based on the inÔ¨Ånity norm. INFORMS J. Comput. 33(4), 1624‚Äì1643 (2021) 50. Ziegler, G.M.: Lectures on Polytopes, vol. 152 of Graduate Texts in Mathematics. Springer-Verlag, New York (1995). https://doi.org/10.1007/978-1-4613-8431-1  
   
  Optimal General Factor Problem and Jump System Intersection Yusuke Kobayashi(B) Kyoto University, Kyoto, Japan [email protected]   
   
  Abstract. In the optimal general factor problem, given a graph G = (V, E) and a set B(v) ‚äÜ Z of integers for each v ‚àà V , we seek for an edge subset F of maximum cardinality subject to dF (v) ‚àà B(v) for v ‚àà V , where dF (v) denotes the number of edges in F incident to v. A recent crucial work by Dudycz and Paluch shows that this problem can be solved in polynomial time if each B(v) has no gap of length more than one. While their algorithm is very simple, its correctness proof is quite complicated. In this paper, we formulate the optimal general factor problem as the jump system intersection, and reveal when the algorithm by Dudycz and Paluch can be applied to this abstract form of the problem. By using this abstraction, we give another correctness proof of the algorithm, which is simpler than the original one. We also extend our result to the valuated case.  
   
  1 1.1  
   
  Introduction General Factor Problem  
   
  Matching in graphs is one of the most well-studied topics in combinatorial optimization. Since a maximum matching algorithm was proposed by Edmonds [6] in 1960s, a lot of generalizations of the matching problem have been proposed and studied in the literature. Among them, we focus on the general factor problem, which contains several important problems as special cases. In the general factor problem (or also called B-factor problem), we are given a graph G = (V, E) and a set B(v) ‚äÜ Z of integers for each v ‚àà V . The objective is to Ô¨Ånd an edge subset F ‚äÜ E such that dF (v) ‚àà B(v) for any v ‚àà V if it exists, where dF (v) denotes the number of edges in F incident to v. Such an edge set is called a B-factor. Since the general factor problem is NP-hard in general (e.g. it contains the 3-edge-coloring problem [13]), polynomially solvable special cases have attracted attention. A B-factor amounts to a perfect matching if B(v) = {1} for each v ‚àà V , and it is called a b-factor if B(v) = {b(v)} for each v ‚àà V , where b : V ‚Üí Z. For a, b : V ‚Üí Z, if B(v) = {a(v), a(v) + 1, a(v) + 2, . . . , b(v) ‚àí 1, b(v)} (resp, B(v) = {a(v), a(v) + 2, a(v) + 4, . . . , b(v) ‚àí 2, b(v)}) for v ‚àà V , then a B-factor is called an (a, b)-factor (resp. an (a, b)-parity factor ). It is well-known The full version is available at arXiv [10]. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 291‚Äì305, 2023. https://doi.org/10.1007/978-3-031-32726-1_21  
   
  292  
   
  Y. Kobayashi  
   
  that, in the above cases, we can Ô¨Ånd a B-factor in polynomial time by using a maximum matching algorithm; see [13] and [23, Section 35]. Note that the parity constraint can be dealt with by adding 12 (b(v) ‚àí a(v)) self-loops to each v ‚àà V and modifying B(v). Another special case is the antifactor problem, in which B(v) = {0, 1, 2, . . . , dE (v)} \ {Œ±v } for some Œ±v ‚àà {0, 1, 2, . . . , dE (v)}, that is, exactly one value is forbidden for each v ‚àà V . Graphs with an antifactor were characterized by Lov¬¥ asz [14]. The edge-and-triangle partitioning problem is to cover all the vertices in a graph by edges and triangles that are mutually disjoint, which can be easily reduced to the general factor problem with B(v) = {1}, {0, 2}, or {0, 2, 3}. The edge-and-triangle partitioning problem is known to be solvable in polynomial time [4]. All the above polynomially solvable cases have a property that each B(v) has no gap of length more than one. Here, B(v) ‚äÜ Z is said to have a gap of length p if there exists Œ± ‚àà B(v) such that Œ±+1, Œ±+2, . . . , Œ±+p ‚àà B(v) and Œ±+p+1 ‚àà B(v). It turns out that this is a key property to design a polynomial-time algorithm. Indeed, Cornu¬¥ejols [3] gave a polynomial-time algorithm for the general factor problem with this property and SebÀù o [24] gave a good characterization. An optimization variant of the general factor problem has also attracted attention, which we call the optimal general factor problem (or the optimal general matching problem). In the problem, given a graph G = (V, E) and a set B(v) ‚äÜ Z of integers for each v ‚àà V , we seek for a B-factor of maximum cardinality. It is the maximum matching problem if B(v) = {0, 1}, and is the maximum b-matching problem if B(v) = {0, 1, . . . , b(v)}, both of which can be solved in polynomial time. In the same way as the search problem described above, we can Ô¨Ånd a maximum (a, b)-factor (or (a, b)-parity factor) in polynomial time; see [23, Section 35]. The optimization variant of the edge-and-triangle partitioning problem was studied with the name of the simplex matching problem, and a polynomial-time algorithm was designed for this problem [1]; see also [22]. Recently, Dudycz and Paluch [5] showed that the optimal general factor problem can be solved in polynomial time if each B(v) has no gap of length more than one. This is deÔ¨Ånitely a crucial result in this area, because it is a generalization of all the above results. While their algorithm is very simple, its correctness proof is quite complicated. 1.2  
   
  Jump System Intersection  
   
  In this paper, we introduce an abstract form of the optimal general factor problem by using the concept of jump systems introduced by Bouchet and Cunningham [2] (see also [9,17]). Let V be a Ô¨Ånite set. For x, y ‚àà ZV , we say that s ‚àà ZV is an (x, y)-step if s1 = 1 and (x + s) ‚àí y1 = x ‚àí y1 ‚àí 1. A non-empty subset J ‚äÜ ZV is called a jump system if it satisÔ¨Åes the following property: (JUMP) For any x, y ‚àà J and for any (x, y)-step s, either x + s ‚àà J or there exists an (x + s, y)-step t such that x + s + t ‚àà J. Typical examples of jump systems include matroids, delta-matroids, integral polymatroids (or submodular systems [7]), and degree sequences of subgraphs.  
   
  Optimal General Factor Problem and Jump System Intersection  
   
  293  
   
  When J ‚äÜ Z is one-dimensional, one can see that J is a jump system if and only if it has no gap of length more than one. One can also see that the direct product of one-dimensional jump systems is also a jump system. We consider the optimization problem over the intersection of two jump systems, where one is the direct product of one-dimensional jump systems. Jump System Intersection Input. A jump system J ‚äÜ ZV , a Ô¨Ånite one-dimensional jump system B(v) ‚äÜ Z for each v ‚àà V , and a vector c ‚àà ZV . Problem. Find a vector x ‚àà J ‚à© B maximizing c x, where B ‚äÜ ZV is the direct product of B(v)‚Äôs. If J consists of degree sequences of subgraphs, i.e., J = {dF ‚àà ZV | F ‚äÜ E}, and c(v) = 1 for v ‚àà V , then the problem amounts to the optimal general factor problem, which can be solved in polynomial time [5]. On the other hand, if J is a 2-polymatroid and B(v) = {0, 2} for each v ‚àà V , then the problem amounts to the matroid matching problem [15] or the matroid parity problem [12]. This implies that the problem cannot be solved in polynomial time if J is given as a membership oracle [8,16]; see also [18]. A similar problem is to determine whether the intersection of two jump systems J1 and J2 is empty or not, which is also hard in general. This problem was studied in [17] as a membership problem of J1 ‚àí J2 := {x ‚àí y | x ‚àà J1 , y ‚àà J2 }, because J1 ‚à© J2 = ‚àÖ if and only if 0 ‚àà J1 ‚àí J2 . 1.3  
   
  Our Contribution: Jump System with SBO Property  
   
  A natural question is why the optimal general factor problem can be solved eÔ¨Éciently, while the general setting of Jump System Intersection is hard. In this paper, we answer this question by revealing the properties of J that are essential in the argument in [5]. For a positive integer , we denote {1, 2, . . . , } by []. For x, y ‚àà ZV , we say that a multiset {p1 , . . . , p } of vectors is a 2-step decomposition ofy ‚àí x if pi ‚àà ZV and pi 1 = 2 for each i ‚àà [], y ‚àí x1 = 2, and y ‚àí x = i‚àà[] pi . A non-empty subset J ‚äÜ ZV is called a jump system with SBO property1 if it satisÔ¨Åes the following property: (SBO-JUMP) For any x, y ‚àà J, there exists a 2-step decomposition {p1 , . . . , p } of y ‚àí x such that x + i‚ààI pi ‚àà J for any I ‚äÜ []. We can see that (SBO-JUMP) implies (JUMP). To see this, for given x, y ‚àà J, suppose that there exist vectors p1 , . . . , p ‚àà ZV satisfying the conditions in (SBO-JUMP). Then, for any (x, y)-step s, there exists an (x + s, y)-step t such that s + t = pi for some i ‚àà [], and hence x + s + t = x + pi ‚àà J. Therefore, if J is a jump system with SBO property, then it is a jump system such that v‚ààV x(v) has the same parity for any x ‚àà J, which is called a constant parity jump system. See [21] for a characterization of constant parity jump systems. We now give a few examples of jump systems with SBO property. 1  
   
  SBO stands for strongly base orderable (see Example 1).  
   
  294  
   
  Y. Kobayashi  
   
  Example 1. A matroid M = (S, B) with a ground set S and a base family B is called strongly base orderable if, for any bases B1 , B2 ‚àà B, there exists a bijection f : B1 \B2 ‚Üí B2 \B1 such that (B1 \X)‚à™{f (x) | x ‚àà X} ‚àà B for any X ‚äÜ B1 \B2 (see e.g., [23, Section 42.6c]). By deÔ¨Ånition, the characteristic vectors of the bases of a strongly base orderable matroid satisfy (SBO-JUMP). Note that the characteristic vectors of the bases do not satisfy (SBO-JUMP) if the matroid is not strongly base orderable, which implies that the class of jump systems with SBO property is strictly smaller than that of constant parity jump systems. By merging some elements in Example 1, we obtain the following example, which was studied for linear matroids in a problem similar to Jump System Intersection [25]. Example 2. Let M = (S, B) be a strongly base orderable matroid and let (S1 , S2 , . . . , Sn ) be a partition of S. Then, J = {x ‚àà Zn | B ‚àà B, x(i) = |B ‚à© Si | for i ‚àà [n]} satisÔ¨Åes (SBO-JUMP). Another example is the set of the degree sequences of subgraphs. Example 3. Let G = (V, E) be a graph and let J be the set of the degree sequences of subgraphs, i.e., J = {dF | F ‚äÜ E}. Then, J satisÔ¨Åes (SBO-JUMP). To see this, for x, y ‚àà J, let M, N ‚äÜ E be edge sets with dM = x and dN = y. Then, the symmetric diÔ¨Äerence of M and N can be decomposed into alternating paths P1 , . . . , P and alternating cycles such that {dN ‚à©Pi ‚àí dM ‚à©Pi | i ‚àà []} is a 2-step decomposition of y ‚àí x. Note that each Pi is regarded  as an edge subset. ‚äÜ [], x + i‚ààI pi is the degree Let pi := dN ‚à©Pi ‚àí dM ‚à©Pi for i ‚àà []. For any I  sequence of the symmetric diÔ¨Äerence of M and i‚àà[I] Pi , and hence it is in J. Our contribution is to introduce the jump system with SBO property and show that (SBO-JUMP) is crucial when we apply the algorithm in [5] for Jump System Intersection. For Œ±, Œ≤ ‚àà Z with Œ± ‚â§ Œ≤ that have the same parity, a set {Œ±, Œ± + 2, . . . , Œ≤ ‚àí 2, Œ≤} is called a parity interval. The main result in this paper is stated as follows. Theorem 1. There is an algorithm for Jump System Intersection whose    running time is polynomial in v‚ààV Œ±‚ààB(v) log(|Œ±| + 1) + v‚ààV log(|c(v)| + 1) if the following properties hold: (C1) a feasible solution x0 ‚àà J ‚à© B is given, (C2) J satisfies (SBO-JUMP), and (C3) for any direct product B  ‚äÜ ZV of parity intervals, there is an oracle for finding a vector x ‚àà J ‚à© B  maximizing c x. Note that no explicit representation of J is required in this theorem. We only need the oracle in Condition (C3). Note also that Condition (C3) implies the existence of the membership oracle of J. When J is the set of the degree sequences of subgraphs, we see that J satisÔ¨Åes (C1)‚Äì(C3) as follows. It was shown by Cornu¬¥ejols [3] that a feasible solution x0 ‚àà J‚à©B in (C1) can be found in polynomial time, and (C2) holds by Example 3.  
   
  Optimal General Factor Problem and Jump System Intersection  
   
  295  
   
  The subproblem in (C3) is to Ô¨Ånd a maximum (a, b)-parity factor, which can be solved in polynomial time. Our proof for Theorem 1 is based on the argument of Dudycz and Paluch [5]. While their algorithm is very simple, the correctness proof is quite complicated. In particular, an involved case analysis is required to prove a key lemma [5, Lemma 2]. Our technical contribution in this paper is to give a new simpler proof of this lemma in a slightly diÔ¨Äerent form (Lemma 1). In our proof, we use several properties that are peculiar to our problem formulation (see Sect. 4.1), which is an advantage of introducing the abstract form of the optimal general factor problem. We also show that a scaling technique used in [5] is not required in the algorithm, which is another contribution of this paper. We also introduce a quantitative extension of (SBO-JUMP), and extend Theorem 1 to a valuated variant of Jump System Intersection; see Theorem 2. 1.4  
   
  Organization  
   
  The rest of this paper is organized as follows. Some preliminaries are given in Sect. 2. In Sect. 3, we describe our algorithm and prove its correctness by using a key technical lemma (Lemma 1). A proof of Lemma 1 is given in Sect. 4, where properties shown in Sect. 4.1 play important roles to simplify the argument. In Sect. 5, we extend our results to the valuated case and show that a polynomial-time algorithm for the weighted general factor problem is derived from our results. Proofs of theorems/lemmas marked with () are omitted due to the page limitation and given in the full version [10].  
   
  2  
   
  Preliminaries  
   
  Let V be a Ô¨Ånite set. For v ‚àà V , let œáv ‚àà ZV denote the characteristic vector of v, that is, œáv (v) = 1 and œáv (u) = 0 for u ‚àà V \ {v}. For each v ‚àà V , we are given a non-empty Ô¨Ånite set B(v) ‚äÜ Z that has no gap of length more than one, i.e., B(v) is a one-dimensional jump system. Throughout this paper, let B ‚äÜ ZV be the direct product of B(v)‚Äôs, i.e., B := {x ‚àà ZV | x(v) ‚àà B(v) for any v ‚àà V }. For x ‚àà ZV , we denote min B ‚â§ x ‚â§ max B if min B(v) ‚â§ x(v) ‚â§ max B(v) for every v ‚àà V . For x ‚àà ZV , we deÔ¨Åne q(x) = |{v ‚àà V | x(v) ‚àà B(v)}|. Note that, if min B ‚â§ x ‚â§ max B, then q(x) := miny‚ààB x ‚àí y1 , because each B(v) has no gap of length greater than one. Recall that a parity interval is a subset of Z that is of the form {Œ±, Œ± + 2, . . . , Œ≤ ‚àí 2, Œ≤}. For v ‚àà V , we see that B(v) is uniquely partitioned into inclusionwise maximal parity intervals (see Fig. 1), which we call maximal parity intervals of B(v). For Œ±, Œ≤ ‚àà Z with min B(v) ‚â§ Œ± ‚â§ Œ≤ ‚â§ max B(v), we deÔ¨Åne distB(v) (Œ±, Œ≤) as the number of maximal parity intervals of B(v) intersecting [Œ±, Œ≤] minus one. In other words, distB(v) (Œ±, Œ≤) is the number of pairs of consecutive integers in B(v) ‚à© [Œ±, Œ≤]. We also deÔ¨Åne  
   
  296  
   
  Y. Kobayashi  
   
  Fig. 1. Blue circles are elements in B(v) and red arrows are maximal parity intervals. (Color Ô¨Ågure online)  
   
  distB(v) (Œ≤, Œ±) := distB(v) (Œ±, Œ≤). For x, y ‚àà ZV with min B ‚â§ x, y ‚â§ max B, we deÔ¨Åne distB (x, y) := v‚ààV distB(v) (x(v), y(v)); see Fig. 2. Note that distB satisÔ¨Åes the triangle inequality.  
   
  Fig. 2. In this two-dimensional example, distB(v1 ) (x(v1 ), y(v1 )) = 3, distB(v2 ) (x(v2 ), y(v2 )) = 2, distB (x, y) = 5, x ‚àí y1 = 14, q(x) = 1, and q(y) = 0.  
   
  3  
   
  Algorithm and Correctness  
   
  Our algorithm for Jump System Intersection is basically the same as [5]. We Ô¨Årst initialize the vector x := x0 , where x0 is as in Condition (C1) in Theorem 1. In each iteration, we compute a vector x ‚àà J ‚à© B maximizing c x subject to distB (x, x ) ‚â§ 2. If c x = c x, then the algorithm terminates by returning x. Otherwise, we replace x with x and repeat the procedure. See Algorithm 1 for a pseudocode of the algorithm. In the correctness proof, we use the following key lemma, whose proof is given in Sect. 4. Note again that giving a simpler proof for this lemma is a technical contribution of this paper. Lemma 1. Let x, y ‚àà B be vectors with distB (x, y) = 4, let {p1 , . . . , p } be a 2-step decomposition of y ‚àí x,  and let wi ‚àà R for i ‚àà []. Then, there exists a set I ‚äÜ [] such that z := x + i‚ààI pi is contained in B, distB (x, z) = 2, and   i‚ààI wi ‚â• min{0, i‚àà[] wi }.  
   
  Optimal General Factor Problem and Jump System Intersection  
   
  297  
   
  Algorithm 1: Algorithm for Jump System Intersection 1 2 3 4 5 6  
   
  Input: J, B, c, and x0 . Output: x ‚àà J ‚à© B maximizing c x. x ‚Üê x0 ; while true do Find a vector x ‚àà J ‚à© B maximizing c x subject to distB (x, x ) ‚â§ 2; if c x = c x then return x x ‚Üê x ;  
   
  Remark 1. In Lemma 1, the roles of x and y are symmetric by changing the signs of pi and wi , because I¬Ø := [] \ I satisÔ¨Åes the following:   ‚Äì x + i‚ààI pi = y + i‚ààI¬Ø(‚àípi ), ‚Äì  distB (x, z) = 2 ‚áê‚áí  z) = 2,and     distB (y, ‚áê‚áí ‚Äì w ‚â• min 0, w i i i‚ààI i‚àà[] i‚ààI¬Ø(‚àíwi ) ‚â• min 0, i‚àà[] (‚àíwi ) .  Let w ‚àà R be the vector consisting of wi ‚Äôs, and denote w(I) := i‚ààI wi for I ‚äÜ []. We next show the following lemma. Note that almost the same result is shown for degree sequences in [5, Lemma 1]. Lemma 2. Let k be a positive integer. Let x, y ‚àà B be vectors with distB (x, y) = 2k and let {p1 , . . . , p } be a 2-step decomposition of y ‚àíx. Then,  there exist index sets ‚àÖ = I0  I1  I2  ¬∑ ¬∑ ¬∑  Ik = [] such that zj := x + i‚ààIj pi is contained in B and distB (zj‚àí1 , zj ) = 2 for j ‚àà [k]. Proof. It suÔ¨Éces to construct I1 ‚äÜ [] satisfying the conditions, because I2 , I3 , . . . , Ik‚àí1 can be constructed in this order in the same way. By changing the direction of axes if necessary, we may assume that x(v) ‚â§ y(v) for every v ‚àà V . Then, each pi is equal to œáa + œáb for some a, b ‚àà V (possibly a = b). For z ‚àà ZV , we denote œÜ(z) := (distB (x, z), q(z)) ‚àà Z2‚â•0 . In order to construct I1 , we start with I := I0 = ‚àÖ and add an element one by one to I. During  the procedure, we keep œÜ(z) ‚àà {(0, 0), (0, 2), (1, 1), (2, 0)}, where z := x + i‚ààI pi . Note that œÜ(z) = (0, 0) when I is initialized to I0 . If œÜ(z) = (2, 0), then I1 := I clearly satisÔ¨Åes the conditions. Otherwise, it holds that œÜ(z) ‚àà {(0, 0), (0, 2), (1, 1)}. In this case, we show that there exists an index i ‚àà [] \ I such that œÜ(z + pi ) ‚àà {(0, 0), (0, 2), (1, 1), (2, 0)} by the following case analysis. ‚Äì Suppose that œÜ(z) = (0, 0). Let i be an arbitrary index in [] \ I. Then, pi = œáa + œáb for some a, b ‚àà V (possibly a = b). We see that œÜ(z + œáa ) ‚àà {(0, 1), (1, 0)}, and hence œÜ(z + pi ) = œÜ(z + œáa + œáb ) ‚àà {(0, 0), (0, 2), (1, 1), (2, 0)}. ‚Äì Suppose that œÜ(z) = (0, 2). Then, z + œáa + œáb ‚àà B for some distinct a, b ‚àà V such that z(a) < y(a) and z(b) < y(b). Let i be an index in [] \ I such that pi = œáa + œác for some c ‚àà V (possibly c = a or c = b). Then, we see that œÜ(z +œáa ) = (0, 1), and hence œÜ(z +pi ) = œÜ(z +œáa +œác ) ‚àà {(0, 0), (0, 2), (1, 1)}.  
   
  298  
   
  Y. Kobayashi  
   
  ‚Äì Suppose that œÜ(z) = (1, 1). Then, z+œáa ‚àà B for some a ‚àà V with z(a) < y(a). Let i be an index in [] \ I such that pi = œáa + œáb for some b ‚àà V (possibly b = a). Then, we see that œÜ(z + œáa ) = (1, 0), and hence œÜ(z + pi ) = œÜ(z + œáa + œáb ) ‚àà {(1, 1), (2, 0)}. If œÜ(z + pi ) = (2, 0), then I1 := I ‚à™ {i} satisÔ¨Åes the conditions. Otherwise, we replace I with I ‚à™ {i} and repeat the procedure. Since [] is Ô¨Ånite, this process   terminates by Ô¨Ånding a desired index set I1 , which completes the proof. By using Lemmas 1 and 2, we can evaluate the improvement of the objective value in each iteration of Algorithm 1 as follows. Lemma 3. Let J be a jump system with SBO property, let x‚àó ‚àà J ‚à© B be an optimal solution of Jump System Intersection, and let x ‚àà J ‚à© B be a vector with x = x‚àó . Let x ‚àà J ‚à© B be a vector maximizing c x subject to 2 (c x‚àó ‚àí c x). distB (x, x ) ‚â§ 2. Then, c x ‚àí c x ‚â• x‚àó ‚àíx 1 Proof. If distB (x, x‚àó ) ‚â§ 2, then the inequality is obvious. Since distB (x, x‚àó ) is even, suppose that distB (x, x‚àó ) ‚â• 4. Since x, x‚àó ‚àà J, there exists a 2-step decomposition {p1 , . . . , p } of x‚àó ‚àíx that satisÔ¨Åes the conditions in (SBO-JUMP).  ‚àó  For i ‚àà [], we deÔ¨Åne wi = c pi ‚àí c x ‚àíc x + Œµ, where Œµ is a suÔ¨Éciently small 1 positive number (e.g. Œµ = (+1) 2 ) that is used to break ties. Observe that, for   I, I ‚äÜ [] with |I| = |I |, w(I) = w(I  ) holds because of Œµ. By Lemma  2, there exist index sets ‚àÖ = I0  I1  I2  ¬∑ ¬∑ ¬∑  Ik = [] such that zj := x + i‚ààIj pi is contained in B and distB (zj‚àí1 , zj ) = 2 for j ‚àà [k]. We choose I1 , I2 , . . . , Ik‚àí1 so that (w(I1 ), w(I2 ), . . . , w(Ik‚àí1 )) is lexicographically maximum. Note that zj ‚àà J for j ‚àà [k] by (SBO-JUMP). Let j ‚àà [k] be the minimum index such that w(Ij‚àí1 ) < w(Ij ). Note that such j must exist, because w(I0 ) = 0 < Œµ = w(Ik ). Assume that j = 1. Then, the minimality of j shows that w(Ij‚àí2 ) > w(Ij‚àí1 ) < w(Ij ), where we note that w(Ij‚àí2 ) = w(Ij‚àí1 ) as |Ij‚àí2 | = |Ij‚àí1 |. By applying Lemma 1 to a 2-step decomposition {pi | i ‚àà Ij \I j‚àí2 } of zj ‚àízj‚àí2 , we obtain an index set I ‚äÜ Ij \Ij‚àí2  such that zj‚àí1 := zj‚àí2 + i‚ààI pi is contained in B, distB (zj‚àí2 , z j‚àí1 ) = 2, and   := Ij‚àí2 ‚à™ I. By zj‚àí1 = x + i‚ààI  pi and w(I) ‚â• min{0, w(Ij \ Ij‚àí2 )}. Let Ij‚àí1 j‚àí1  (SBO-JUMP), we see that zj‚àí1 ‚àà J. Furthermore, we obtain  w(Ij‚àí1 ) = w(Ij‚àí2 ) + w(I) ‚â• min {w(Ij‚àí2 ), w(Ij )} > w(Ij‚àí1 ),  
   
  which contradicts the choice of Ij‚àí1 . Therefore, we obtain j = 1, that is, 0 = w(I0 ) < w(I1 ). Since 0 < w(I1 ) =  
   
   i‚ààI1  
   
  c pi ‚àí  
   
   c x‚àó ‚àí c x +Œµ   
   
  = c z1 ‚àí c x ‚àí  
   
   c x‚àó ‚àí c x   
   
   ‚àí Œµ |I1 |  
   
  Optimal General Factor Problem and Jump System Intersection  
   
  299  
   
  and Œµ is suÔ¨Éciently small, we obtain c z1 ‚àí c x ‚â•  
   
  (c x‚àó ‚àí c x)|I1 | .   
   
  We also see that c x ‚â• c z1 , because z1 ‚àà J ‚à© B and distB (x, z1 ) ‚â§ 2. By ‚àó 1 combining these inequalities with |I1 | ‚â• 1 and  = x ‚àíx , we obtain c x ‚àí 2 2   ‚àó    c x ‚â• x‚àó ‚àíx 1 (c x ‚àí c x). This implies that the global optimality is guaranteed by the local optimality. Corollary 1. In an instance of Jump System Intersection with (C2), a feasible solution x ‚àà J ‚à© B maximizes c x if and only if c x ‚â• c x for any x ‚àà J ‚à© B with distB (x, x ) ‚â§ 2. We are now ready to prove the correctness of Algorithm 1. Proof (Proof of Theorem 1). We Ô¨Årst show that each iteration of Algorithm 1 runs in polynomial time. For x, x ‚àà B with distB (x, x ) ‚â§ 2, we see that x(v) and x (v) are contained in the same maximal parity interval of B(v) for any v ‚àà V except at most two elements. Thus, for x ‚àà B, {x ‚àà B | distB (x, x ) ‚â§ 2} can be partitioned into O(n2 ) sets, each of which is a direct product of parity intervals. Therefore, we can Ô¨Ånd a vector x ‚àà J ‚à© B maximizing c x subject to distB (x, x ) ‚â§ 2 by using the oracle in Condition (C3), O(n2 ) times. We next evaluate the number of iterations inthe algorithm. Let OPT be the optimal value of the problem and let Bsize := v‚ààV |B(v)|. Since J is a jump system with SBO property by Condition (C2), we can apply Lemma 3. By this lemma, if x is replaced with x in line 6 of Algorithm 1, then  

  2 1 OPT ‚àí c x ‚â§ 1 ‚àí ‚àó (OPT ‚àí c x) ‚â§ 1 ‚àí (OPT ‚àí c x), x ‚àí x1 Bsize 1 that is, the gap to the optimal value decreases by a factor of at most 1 ‚àí Bsize .  Therefore, by repeating this procedure O(Bsize log(OPT‚àíc x0 )) times, the algorithm terminates and returns an optimal solution. This shows that Algorithm 1 solves Jump System Intersection in polynomial time.    
   
  4 4.1  
   
  Outline of the Proof of Lemma 1 Minimal Counterexample  
   
  This section gives an outline of the proof of Lemma 1. A tuple (x, y, (pi )i‚àà[] , w) is called an instance and a set I satisfying the conditions is called a solution. To derive a contradiction, assume that Lemma 1 does not hold. Suppose that (x, y, (pi )i‚àà[] , w) is a counterexample that minimizes y ‚àí x1 . Among such counterexamples, we choose one that minimizes |{(pi , wi ) | i ‚àà []}|, that is, we  
   
  300  
   
  Y. Kobayashi  
   
  minimize the number of diÔ¨Äerent (pi , wi ) pairs. Such (x, y, (pi )i‚àà[] , w) is called a minimal counterexample. DeÔ¨Åne U ‚äÜ V as U := {v ‚àà V | distB(v) (x(v), y(v)) ‚â• 1}. By changing the direction of axes if necessary, we may assume that x(v) ‚â§ y(v) for every v ‚àà V . Then, each pi is equal to œáa +œáb for some a, b ‚àà V (possibly a = b). We show some properties of the minimal counterexample. Our argument becomes simpler with the aid of these properties. Lemma 4. For any i ‚àà [], pi = œáa + œáb for some a, b ‚àà U (possibly a = b). Consequently, x(v) = y(v) for all v ‚àà V \ U . Proof. Assume to the contrary that there exists i ‚àà [] such that pi = œáa + œác for some a ‚àà V and for some c ‚àà V \ U . Suppose that a = c, i.e., pi = 2œác . We consider a new instance by removing pi and replacing y with y ‚àí 2œác ‚àà B. By the minimality of the counterexample, the obtained instance has a solution I ‚äÜ [] \ {i}, which implies that w(I) ‚â• 0 or w(I) ‚â• w([] \ {i}). Then, I  := I is a solution of the original instance in the former case and I  := I ‚à™ {i} is a solution of the original instance in the latter case, which is a contradiction. Suppose next that a = c. Since distB(c) (x(c), y(c)) = 0 and x(c), y(c) ‚àà B(c), we see that x(c) and y(c) have the same parity. Thus, there exists i ‚àà [] \ {i} such that pi = œáb + œác for some b ‚àà V \ {c}. We merge pi and pi as follows: replace pi and pi with a new vector pi := œáa + œáb whose weight is wi + wi , and replace y with y ‚àí 2œác ‚àà B. By the minimality of the counterexample, the obtained instance has a solution I ‚äÜ ([] \ {i, i }) ‚à™ {i }. Then, we see that the set (I \ {i }) ‚à™ {i, i } if i ‚àà I,  I := I otherwise is a solution of the original instance, which is a contradiction.  
   
     
   
  Lemma 5. () For any i ‚àà [], pi = 2œáa for a ‚àà U with distB(a) (x(a), y(a)) = 1. Lemma 6. For any i, j ‚àà [] with pi = pj , it holds that wi = wj . Proof. Let (x, y, (pi )i‚àà[] , w) be a minimal counterexample of Lemma 1, and assume that pi = pj does not imply wi = wj . Let I ‚àó ‚äÜ [] be a maximal index set such that pi = pj for any i, j ‚àà I ‚àó and wi = wj for some i, j ‚àà I ‚àó . We denote I ‚àó = {i1 , i2 , . . . , it }, where wi1 ‚â• wi2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• wit . Let w‚àó := 1t w(I ‚àó ). DeÔ¨Åne wi := w‚àó for i ‚àà I ‚àó and wi := wi for i ‚àà [] \ I ‚àó . We note that w ([]) = w([]). If there exists a solution I  ‚äÜ [] for a new instance (x, y, (pi )i‚àà[] , w ), then I := (I  \ I ‚àó ) ‚à™ {i1 , i2 , . . . , i|I  ‚à©I ‚àó | } is a solution for the original instance (x, y, (pi )i‚àà[] , w), because wi1 + wi2 + ¬∑ ¬∑ ¬∑ + wi|I  ‚à©I ‚àó | ‚â• |I  ‚à© I ‚àó | ¬∑ w‚àó = w (I  ‚à© I ‚àó ) implies that w(I) ‚â• w(I  ). This shows that instance (x, y, (pi )i‚àà[] , w ) has no solution, and hence it is a counterexample. Since |{(pi , wi ) | i ‚àà []}| < |{(pi , wi ) | i ‚àà []}|, this contradics the minimality of   (x, y, (pi )i‚àà[] , w).  
   
  Optimal General Factor Problem and Jump System Intersection  
   
  Let I + := {i ‚àà [] | wi > 0} and z + := x+ the following.  
   
   i‚ààI +  
   
  301  
   
  pi . By Lemma 6, we observe  
   
  Observation 1. For any i ‚àà I + and for any j ‚àà [] \ I + , it holds that pi = pj . Since x(v) = y(v) = z + (v) for v ‚àà V \ U by Lemma 4, it holds that q(z + ) ‚â§ |U | ‚â§ distB (x, y) = 4. We derive a contradiction for the cases when |U | = 4, |U | = 3, and |U | ‚â§ 2, separately. In this extended abstract we only consider the case when |U | = 3 as a demonstration. The other cases are dealt with in the full version [10]. In the case analysis, we use the following lemma, which is obtained by the same argument as Lemma 2. Here, we denote œÜ(z) := (distB (x, z), q(z)) ‚àà Z2‚â•0 for z ‚àà ZV .  Lemma 7. Let I0 ‚äÜ [] be an index set such that z0 := x + i‚ààI0 pi satisfies (2, 0)}. Then, there exists an index set I ‚äÜ [] with œÜ(z0 ) ‚àà {(0, 0), (0, 2), (1, 1), I0 ‚äÜ I such that z := x + i‚ààI pi is contained in B and distB (x, z) = 2, i.e., œÜ(z) = (2, 0). 4.2  
   
  Part of Case Analysis: |U | = 3  
   
  In this extended abstract, we only consider the case when |U | = 3. Let U = {v1 , v2 , v3 } such that distB(v1 ) (x(v1 ), y(v1 )) = distB(v2 ) (x(v2 ), y(v2 )) = 1 and distB(v3 ) (x(v3 ), y(v3 )) = 2. By Lemmas 4 and 5, for any i ‚àà [], either pi = œáa +œáb for some distinct a, b ‚àà U or pi = 2œáv3 . Since distB (x, z + )+distB (y, z + ) = 4, by changing the roles of x and y if necessary (see Remark 1), we may assume that distB (x, z + ) ‚â§ 2.2 Furthermore, since x ‚àí z + 1 is even, we see that distB (x, z + ) + q(z + ) is even. Therefore, the pair œÜ(z + ) := (distB (x, z + ), q(z + )) is one of the following: (0, 0), (0, 2), (1, 1), (1, 3), (2, 0), and (2, 2), where we note that q(z + ) ‚â§ |U | = 3. We derive a contradiction by considering each case separately. Case 1: œÜ(z + ) = (0, 0), (0, 2), (1, 1), or (2, 0). By Lemma 7, there exists an index set I ‚äÜ [] with I + ‚äÜ I such that  z := x + i‚ààI pi is contained in B and distB (x, z) = 2. Since wi ‚â§ 0 for each i ‚àà [] \ I, we obtain w(I) ‚â• w([]), and hence I is a solution of Lemma 1. This is a contradiction. Case 2: œÜ(z + ) = (1, 3). In this case, z + (v) ‚àà B(v) for v ‚àà U . Since z + (v1 ) = y(v1 ), there exists i ‚àà []\I + such that pi = œáv1 +œáu for some u ‚àà {v2 , v3 }. Since œÜ(z + +pi ) = (1, 1), + by Lemma  7, there exists an index set I ‚äÜ [] with I ‚à™ {i} ‚äÜ I such that z := x + j‚ààI pj is contained in B and distB (x, z) = 2. We see that such I is a solution of Lemma 1 in the same way as Case 1, which is a contradiction. 2  
   
   If we change the roles of x and y, then I ‚àí := {i ‚àà [] | wi < 0} and z ‚àí := y‚àí i‚ààI ‚àí pi + + + play the roles of I and z , respectively. We see that if distB (x, z ) ‚â• 3, then distB (y, z ‚àí ) ‚â§ distB (y, z + ) = 4 ‚àí distB (x, z + ) ‚â§ 1.  
   
  302  
   
  Y. Kobayashi  
   
  Case 3: œÜ(z + ) = (2, 2). Since q(z + ) = 2 and |U | = 3, at least one of z + (v1 ) ‚àà B(v1 ) and z + (v2 ) ‚àà B(v2 ) holds. By changing the roles of v1 and v2 if necessary, we may assume that z + (v1 ) ‚àà B(v1 ). Let v ‚àó ‚àà {v2 , v3 } be the other element such that z + (v ‚àó ) ‚àà B(v ‚àó ). Since z + (v1 ) = x(v1 ), there exists i1 ‚àà I + such that pi1 = œáv1 + œáu for some u ‚àà {v2 , v3 }. Similarly, since z + (v1 ) = y(v1 ), there exists i2 ‚àà [] \ I + such that pi2 = œáv1 + œáu for some u ‚àà {v2 , v3 }. By Observation 1, either pi1 = œáv1 + œáv‚àó or pi2 = œáv1 + œáv‚àó holds (Fig. 3). If pi1 = œáv1 + œáv‚àó , then I := I + \ {i1 } is a solution, because w(I) ‚â• 0, which is a contradiction; see Fig. 3 (left two). If pi2 = œáv1 + œáv‚àó , then I := I + ‚à™ {i2 } is a solution, because w(I) ‚â• w([]), which is a contradiction; see Fig. 3 (right two).  
   
  Fig. 3. Possible situations in Case 3. A blue edge (u, v) corresponds to an element i ‚àà [] \ I + with pi = œáu + œáv , a red dashed edge (u, v) corresponds to an element i ‚àà I + with pi = œáu + œáv , and a vertex v ‚àà V in a rectangle satisÔ¨Åes that z + (v) ‚àà B(v). (Color Ô¨Ågure online)  
   
  5  
   
  Extension to Valuated Problem  
   
  In this section, we consider a valuated version of Jump System Intersection. Valuated Jump System Intersection Input. A function f : J ‚Üí Z on a jump system J ‚äÜ ZV and a Ô¨Ånite onedimensional jump system B(v) ‚äÜ Z for each v ‚àà V . Problem. Find a vector x ‚àà J ‚à© B maximizing f (x), where B ‚äÜ ZV is the direct product of B(v)‚Äôs. Note that f and J may be given in an implicit way, e.g., by an oracle. To simplify the notation, we extend the domain of f to ZV by setting f (x) = ‚àí‚àû for x ‚àà ZV \ J. The following property is a quantitative extension of (SBO-JUMP). (SBO-M-JUMP) For any x, y ‚àà J, there exist real values  g1 , . . . , g and a2-step such that f (x+ i‚ààI pi ) ‚â• f (x)+ i‚ààI gi decomposition {p1 , . . . , p } of y ‚àíx for any I ‚äÜ [] and f (y) = f (x) + i‚àà[] gi . Note that we use ‚ÄúM‚Äù in the name of the exchange axiom, because it deÔ¨Ånes a subclass of M-concave functions on constant parity jump systems [21]; see Remark 2 below. We can see that if f satisÔ¨Åes (SBO-M-JUMP), then its eÔ¨Äective domain J := {x ‚àà ZV | f (x) > ‚àí‚àû} satisÔ¨Åes (SBO-JUMP). By using (SBO-MJUMP), we generalize Theorem 1 as follows.  
   
  Optimal General Factor Problem and Jump System Intersection  
   
  303  
   
  Theorem 2. () There is an algorithm for Valuated Jump System Inter  section whose running time is polynomial in v‚ààV Œ±‚ààB(v) log(|Œ±| + 1) + maxx‚ààJ log(|f (x)| + 1) if the following properties hold: (C1‚Äô) a vector x0 ‚àà J ‚à© B is given, (C2‚Äô) f satisfies (SBO-M-JUMP), and (C3‚Äô) for any direct product B  ‚äÜ ZV of parity intervals, there is an oracle for finding a vector x ‚àà J ‚à© B  maximizing f (x). Remark 2. Functions with (SBO-M-JUMP) form a subclass of M-concave functions on constant parity jump systems studied in the context of discrete convex analysis [11,19‚Äì21]. For J ‚äÜ ZV , a function f : J ‚Üí Z is called an Mconcave function on a constant parity jump system [21] if it satisÔ¨Åes the following exchange axiom. (M-JUMP) For any x, y ‚àà J and for any (x, y)-step s, there exists an (x + s, y)step t such that f (x + s + t) + f (y ‚àí s ‚àí t) ‚â• f (x) + f (y). We can see that (SBO-M-JUMP) implies (M-JUMP) as follows. For x, y ‚àà J, suppose that there exist a 2-step decomposition {p1 , . . . , p } of y ‚àí x and gi ‚àà R for i ‚àà [] satisfying the conditions in (SBO-M-JUMP). For any (x, y)-step s, there exists an (x + s, y)-step t such that s + t = pi for some i ‚àà []. Such t satisÔ¨Åes the conditions in (M-JUMP), because    f (x + s + t) + f (y ‚àí s ‚àí t) = f (x + pi ) + f x + pj   
   
  j‚àà[]\{i}  
   
  ‚â• (f (x) + gi ) + f (x) +  
   
    
   
   gj = f (x) + f (y).  
   
  j‚àà[]\{i}  
   
  6  
   
  Weighted Optimal General Factor Problem  
   
  It was shown by Dudycz and Paluch [5] that the edge-weighted variant of the optimal general factor problem can also be solved in polynomial time if each B(v) has no gap of length more than one. Formally, in the weighted optimal general factor problem, given a graph G = (V, E), an edge weight w(e) ‚àà Z for e ‚àà E, and a set B(v) ‚äÜ Z of integers F ‚äÜ E that  for each v ‚àà V , we seek for a B-factor  maximizes its total weight e‚ààF w(e), where we denote w(F ) := e‚ààF w(e). Their algorithm consists of local improvement steps used in Algorithm 1 and a scaling technique. In what follows in this section, we show that the polynomial solvability of the weighted optimal general factor problem is derived from Theorem 2. Theorem 3 (Dudycz and Paluch [5]). The weighted optimal general factor problem can be solved in polynomial time if each B(v) has no gap of length more than one.  
   
  304  
   
  Y. Kobayashi  
   
  Proof. Let G = (V, E), w, and B be an instance of the weighted optimal general factor problem such that each B(v) has no gap of length more than one. Let J := {dF | F ‚äÜ E}, and deÔ¨Åne f : J ‚Üí Z by f (x) := max{w(F ) | dF = x, F ‚äÜ E} for x ‚àà J. We now show (C1‚Äô), (C2‚Äô), and (C3‚Äô) in Theorem 2. Since an edge set F0 ‚äÜ E with dF0 ‚àà B can be found in polynomial time by the algorithm of Cornu¬¥ejols [3] (if it exists), we obtain x0 := dF0 satisfying the condition in (C1‚Äô). The subproblem in (C3‚Äô) is to Ô¨Ånd an (a, b)-factor with parity constraints that maximizes the total edge weight, which can be solved in polynomial time; see [23, Section 35]. To see (C2‚Äô), for x, y ‚àà J, let M, N ‚äÜ E be edge sets such that dM = x, dN = y, w(M ) = f (x), and w(N ) = f (y). As in Example 3, the symmetric diÔ¨Äerence of M and N can be decomposed into alternating paths P1 , . . . , P and alternating cycles such that {dN ‚à©Pi ‚àí dM ‚à©Pi | i ‚àà []} is a 2-step decomposition of y ‚àí x. For i ‚àà [], let pi := dN ‚à©Pi ‚àí dM ‚à©Pi and ‚à© Pi ) ‚àí w(M ‚à© Pi ). For I ‚äÜ [], letFI ‚äÜ E be the symmetric diÔ¨Äerence gi := w(N   of M and i‚àà[I] Pi . Then, since dFI = x + i‚ààI pi and w(FI ) = f (x) + i‚ààI gi ,   we obtain f (x + i‚ààI pi ) ‚â• f (x) + i‚ààI gi . This shows (C2‚Äô). By Theorem 2, we can Ô¨Ånd x‚àó ‚àà J ‚à© B maximizing f (x‚àó ) in polynomial time. Furthermore, an edge set F ‚àó ‚äÜ E satisfying w(F ‚àó ) = f (x‚àó ) and dF ‚àó = x‚àó can also be found in polynomial time by a weighted b-factor algorithm. By deÔ¨Ånition, such F ‚àó is an optimal solution of the weighted optimal general factor problem.    
   
  7  
   
  Concluding Remarks  
   
  In this paper, we have revealed that (SBO-JUMP) is a key property to obtain a polynomial time-algorithm for Jump System Intersection, which is an abstract form of the optimal general factor problem. By using this abstraction, we have obtained a simpler correctness proof for the polynomial solvability of the optimal general factor problem. We have also extended the results to the valuated case. There are some possible directions for future research. It is nice if we obtain more examples of jump systems satisfying (SBO-JUMP) other than Examples 1‚Äì 3. It is open whether Jump System Intersection can be solved in polynomial time if each B(v) is given as a union of parity intervals. It is also a natural open problem whether we can obtain a strongly polynomial-time algorithm for the weighted general factor problem. Finally, it is interesting to Ô¨Ånd a new property of J other than (SBO-JUMP) that enables us to design a diÔ¨Äerent polynomialtime algorithm. Acknowledgements. The author thanks Kenjiro Takazawa for his helpful comments. This work is supported by JSPS KAKENHI grant numbers 20K11692 and 20H05795, Japan.  
   
  Optimal General Factor Problem and Jump System Intersection  
   
  305  
   
  References 1. Anshelevich, E., Karagiozova, A.: Terminal backup, 3D matching, and covering cubic graphs. SIAM J. Comput. 40, 678‚Äì708 (2011) 2. Bouchet, A., Cunningham, W.H.: Delta-matroids, jump systems, and bisubmodular polyhedra. SIAM J. Discret. Math. 8, 17‚Äì32 (1995) 3. Cornu¬¥ejols, G.: General factors of graphs. J. Comb. Theory Ser. B 45(2), 185‚Äì198 (1988) 4. Cornu¬¥ejols, G., Hartvigsen, D., Pulleyblank, W.: Packing subgraphs in a graph. Oper. Res. Lett. 1(4), 139‚Äì143 (1982) 5. Dudycz, S., Paluch, K.E.: Optimal general matchings. In: WG 2018. LNCS, vol. 11159, pp. 176‚Äì189. Springer, Cham (2018). arXiv version is available at http:// arxiv.org/abs/1706.07418. https://doi.org/10.1007/978-3-030-00256-5 15 6. Edmonds, J.: Paths, trees, and Ô¨Çowers. Can. J. Math. 17, 449‚Äì467 (1965) 7. Fujishige, S.: Submodular Functions and Optimization, 2nd edn, vol. 58. Annals of Discrete Mathematics. Elsevier, Amsterdam (2005) 8. Jensen, P.M., Korte, B.: Complexity of matroid property algorithms. SIAM J. Comput. 11, 184‚Äì190 (1982) 9. Kabadi, S.N., Sridhar, R.: Œî-matroid and jump system. J. Appl. Math. Decis. Sci. 2005(2), 95‚Äì106 (2005) 10. Kobayashi, Y.: Optimal general factor problem and jump system intersection. arXiv:2209.00779 (2022) 11. Kobayashi, Y., Murota, K., Tanaka, K.: Operations on M-convex functions on jump systems. SIAM J. Discret. Math. 21, 107‚Äì129 (2007) 12. Lawler, E.L.: Combinatorial Optimization ‚Äì Networks and Matroids. Holt, Rinehalt, and Winston, New York (1976) 13. Lov¬¥ asz, L.: The factorization of graphs. II. Acta Mathematica Academiae Scientiarum Hungarica 23, 223‚Äì246 (1972) 14. Lov¬¥ asz, L.: Antifactors of graphs. Period. Math. Hung. 4, 121‚Äì123 (1973) 15. Lov¬¥ asz, L.: The matroid matching problem. Algebr. Methods Graph Theory Colloq. Math. Soc. J¬¥ anos Bolyai 25, 495‚Äì517 (1978) 16. Lov¬¥ asz, L.: Matroid matching and some applications. J. Comb. Theory Ser. B 28, 208‚Äì236 (1980) 17. Lov¬¥ asz, L.: The membership problem in jump systems. J. Comb. Theory Ser. B 70, 45‚Äì66 (1997) 18. Lov¬¥ asz, L., Plummer, M.D.: Matching Theory. North-Holland, Amsterdam (1986) 19. Minamikawa, N., Shioura, A.: Time bounds of basic steepest descent algorithms for M-convex function minimization and related problems. J. Oper. Res. Soc. Jpn. 64(2), 45‚Äì60 (2021) 20. Murota, K.: Discrete Convex Analysis. SIAM, Philadelphia (2003) 21. Murota, K.: M-convex functions on jump systems: a general framework for minsquare graph factor. SIAM J. Discret. Math. 20, 213‚Äì226 (2006) 22. Pap, G.: A TDI description of restricted 2-matching polytopes. In: Bienstock, D., Nemhauser, G. (eds.) IPCO 2004. LNCS, vol. 3064, pp. 139‚Äì151. Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-25960-2 11 23. Schrijver, A.: Combinatorial Optimization. Polyhedra and EÔ¨Éciency. Algorithms and Combinatorics, vol. 24. Springer, Heidelberg (2003) 24. SebÀù o, A.: General antifactors of graphs. J. Comb. Theory Ser.B 58(2), 174‚Äì184 (1993) 25. Szab¬¥ o, J.: Matroid parity and jump systems: A solution to a conjecture of Recski. SIAM J. Discret. Math. 22(3), 854‚Äì860 (2008)  
   
  Decomposition of Probability Marginals for Security Games in Abstract Networks Jannik Matuschke(B) KU Leuven, Leuven, Belgium [email protected]  Abstract. Given a set system (E, P), let œÄ ‚àà [0, 1]P be a vector of requirement values on the sets and let œÅ ‚àà [0, 1]E be a vector of probability marginals with e‚ààP œÅe ‚â• œÄP for all P ‚àà P. We study the question under which conditions the marginals œÅ can be decomposed into a probability distribution on the subsets of E such that the resulting random set intersects each P ‚àà P with probability at least œÄP . Extending a result by Dahan, Amin, and Jaillet [3] motivated by a network security game in directed acyclic graphs, we show that such a distribution exists if P  is an abstract network and the requirements are of the form œÄP = 1 ‚àí e‚ààP Œºe for some Œº ‚àà [0, 1]E . Our proof yields an explicit description of a feasible distribution that can be computed eÔ¨Éciently. As a consequence, equilibria for the security game studied in [3] can be eÔ¨Éciently computed even when the underlying digraph contains cycles. As a subroutine of our algorithm, we provide a combinatorial algorithm for computing shortest paths in abstract networks, partially answering an open question by McCormick [14]. We further show that a conservation law proposed in [3] for requirement functions in partially ordered sets can be reduced to the setting of aÔ¨Éne requirements described above.  
   
  1  
   
  Introduction  
   
  Consider a set system (E, P), where E is a Ô¨Ånite ground set and P ‚äÜ 2E is a collection of subsets of E. Given probability marginals œÅ ‚àà [0, 1]E and requirements œÄ ‚àà [0, 1]P , we are interested in Ô¨Ånding a probability distribution on the power set 2E of E that is consistent with these marginals and that ensures that each set in P ‚àà P is hit with probability at least œÄP . In other words, we are looking for a solution x to the system    
   
  xS = œÅe  
   
  ‚àÄ e ‚àà E,  
   
  (1)  
   
  xS ‚â• œÄP S‚äÜE xS = 1,  
   
  ‚àÄ P ‚àà P,  
   
  (2) (3)  
   
  ‚â• 0  
   
  ‚àÄ S ‚äÜ E.  
   
  (4)  
   
  S‚äÜE:e‚ààS S‚äÜE:S‚à©P =‚àÖ  
   
    
   
  xS  
   
  Throughout this paper, we will call a distribution x fulÔ¨Ålling (1) to (4) a feasible decomposition of œÅ for (E, P) and œÄ, and we will say that the marginals œÅ are feasible for (E, P) and œÄ if such a feasible decomposition exists. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 306‚Äì318, 2023. https://doi.org/10.1007/978-3-031-32726-1_22  
   
  Decomposition of Probability Marginals in Abstract Networks  
   
  307  
   
  A necessary condition for the existence of a feasible decomposition is that the marginals suÔ¨Éce to cover each set of the system individually, i.e.,  ‚àÄ P ‚àà P. () e‚ààP œÅe ‚â• œÄP We are particularly interested in identifying classes of systems and requirement functions for which () is not only a necessary but also a suÔ¨Écient condition. For such systems, the set of distributions on 2E fulÔ¨Ålling (2) can be described by the corresponding polytope of feasible marginals deÔ¨Åned by (), which is of exponentially lower dimension. 1.1  
   
  Motivation  
   
  A natural application for feasible decompositions in the setting described above lies in network security games; see, e.g., [1‚Äì3,8,16,17] for various examples and applications of network security games. In fact, such a game was also the motivation of Dahan, Amin, and Jaillet [3], who originally introduced the decomposition setting described above. We will discuss their game in detail in Sect. 5. Here, we describe a simpler yet relevant problem as an illustrative example. Consider the following game played on a set system (E, P), where each element e ‚àà E is equipped with a usage cost ce ‚â• 0 and an inspection cost de ‚â• 0. A defender D determines a random subset S of elements from E to inspect at  cost e‚ààS de (e.g., a set of links of a network at which passing traÔ¨Éc is monitored). She anticipates that an attacker A is planning to carry out an illegal action, where A chooses a set in P ‚àà P (e.g., a route in the network  along which he smuggles contraband), for which he will receive utility U1 ‚àí e‚ààP ce for some constant U1 > 0. However, if P intersects with the random set S of elements inspected by D, then A is discovered while carrying out his illegal action, reducing his utility by a penalty U2 ‚â• U1 . The attacker also has the option to not carry out any attack, resulting in utility 0. Thus, A will  refrain from using P ‚àà P if the probability that S ‚à© P = ‚àÖ exceeds œÄP := (U1 ‚àí e‚ààP ce )/U2 . A natural goal for D is to discourage A from attempting any attack at all, while keeping the incurred inspection cost as small as possible. Note that the randomized  that achieve this goal correspond exactly to vectors x that strategies minimize S‚äÜE e‚ààS de xS subject to constraints (2) to (4). Unfortunately, the corresponding LP has both an exponential number of variables and an exponential number of constraints in the size of the ground set E. However, assume that we can establish the following three properties for our set system: (i) condition () is suÔ¨Écient for the feasibility of marginals, (ii) we can eÔ¨Éciently compute the corresponding feasible decompositions, and  Œ≥ . Then (i)allows us (iii) given Œ≥ ‚àà RE e + , we can eÔ¨Éciently solve minP ‚ààP e‚ààP to formulate D‚Äôs problem in terms of the marginals, i.e., minœÅ‚àà[0,1]E e‚ààE de œÅe subject to constraints (), (iii) allows us to separate the linear constraints () and obtain optimal marginals œÅ, and (ii) allows us to turn these marginals into a distribution corresponding to an optimal inspection strategy for the defender D. In this paper, we will establish all three conditions for a generic type of set systems called abstract networks.  
   
  308  
   
  1.2  
   
  J. Matuschke  
   
  Abstract Networks  
   
  An abstract network consists of a set system (E, P) where each set P ‚àà P (also referred to as an (abstract) path) is equipped with an internal linear order P of its elements, such that for all P, Q ‚àà P and all e ‚àà P ‚à© Q there is a abstract path R ‚àà P with R ‚äÜ {p ‚àà P : p P e} ‚à™ {q ‚àà Q : e Q q}. Given P, Q ‚àà P and e ‚àà P ‚à© Q, we use the notation P √óe Q to denote an arbitrary but Ô¨Åxed feasible choice for such an R ‚àà P. Intuitively, this deÔ¨Ånition is an abstraction of the property of digraphs that one can construct a new path by concatenating a preÔ¨Åx and a suÔ¨Éx of two intersecting paths. Interesting special cases of abstract networks include P being the set of maximal chains in a partially ordered set (E, ) (here, P is simply the restriction of to P ) and P being the set of simple s-t-paths in a digraph D = (V, A) (here, E = V ‚à™ A and each path is identiÔ¨Åed with the sequence of its nodes and arcs). We remark that in both cases, the order P √óe Q is consistent with P and Q , which is not a general requirement for abstract networks; see, e.g., [10] for examples of abstract networks where this is not the case. Abstract networks were introduced by Hofmann [7] to illustrate the generality of Ford and Fulkerson‚Äôs [6] max-Ô¨Çow/min-cut theorem.1 McCormick [14] provided a combinatorial algorithm for computing maximum Ô¨Çows in abstract networks using a membership oracle that, given F ‚äÜ E, returns P ‚àà P with P ‚äÜ F together with the corresponding order P , or certiÔ¨Åes that no such P exists. Martens and McCormick [12] later extended this result by giving a combinatorial algorithm for a weighted version of the problem, using a stronger oracle. Applications of abstract networks include, e.g., line planning for public transit systems [11] and route assignment in evacuation planning [9,15]. 1.3  
   
  Previous Results  
   
  Dahan et al. [3] studied the case where P is the set of maximal chains of a partially ordered set (poset), or, equivalently, the set of s-t-paths in a directed acyclic graph (DAG). They showed that () is suÔ¨Écient for the existence of a feasible distribution when the requirements fulÔ¨Åll the following conservation law: œÄP + œÄQ = œÄP √óe Q + œÄQ√óe P  
   
  ‚àÄ P, Q ‚àà P, e ‚àà P ‚à© Q.  
   
  (C)  
   
  Although their result is algorithmic, the corresponding algorithm requires explicitly enumerating all maximal chains and hence does not run in polynomial time in the size of E. However, Dahan et al. [3] provide a polynomial-time algorithm for the case of aÔ¨Éne requirements, in which there exists a vector Œº ‚àà [0, 1]E such that the requirements are of the form  œÄP = 1 ‚àí e‚ààP Œº(e) ‚àÄ P ‚àà P. (A) 1  
   
  + Given an abstract network (E,  P) with capacities u ‚àà RE + , a Ô¨Çow is a vector f ‚àà R fulÔ¨Ålling capacity constraints f ‚â§ u for all e ‚àà E. The maximum e P ‚ààP:e‚ààP P  abstract Ô¨Çow problem asks for a Ô¨Çow of maximum value P ‚ààP fP . HoÔ¨Äman [7] showed that the corresponding dual linear program is totally dual integral (even in a more general weighted setting), thus generalizing the max-Ô¨Çow/min-cut theorem.  
   
  Decomposition of Probability Marginals in Abstract Networks  
   
  309  
   
  As a consequence of this latter result, the authors were able to characterize Nash equilibria for their network security game (which is a Ô¨Çow-interdiction game played on s-t-paths in a digraph) by means of a compact arc-Ô¨Çow LP formulation and compute such equilibria in polynomial time, under the condition that the underlying digraph is acyclic. Indeed, this positive result is particularly surprising, as similar‚Äîand seemingly simpler‚ÄîÔ¨Çow-interdiction games had previously been shown to be NP-hard, even on DAGs [5]. 1.4  
   
  Our Results  
   
  We extend the results of Dahan et al. [3] for posets/DAGs in multiple directions: 1. For the aÔ¨Éne requirements case (A), we show that () is a suÔ¨Écient condition for the feasibility of marginals when (E, P) is an abstract network, by providing an explicit description of a feasible decomposition for this case, based on a natural generalization of shortest-path distances to abstract networks (see Sect. 2). The described solutions have the property that the sets in their support can be represented by an interval matrix. A special case of this result is the case where P is the set of s-t-paths in a digraph (which is not necessarily acyclic). In this case, a feasible decomposition can be computed eÔ¨Éciently by a single run of a standard shortest-path algorithm. 2. We also provide an algorithm for eÔ¨Éciently computing the corresponding feasible decompositions for the general case of an arbitrary abstract network given by a membership oracle (see Sect. 3). This algorithm makes use of the following result as a subroutine. 3. We provide a combinatorial strongly polynomial algorithm for computing shortest paths in abstract networks when P is given by a membership oracle (see Sect. 4). Beyond its relevance for the present work, this result also gives a partial answer to an open question by McCormick [14], who conjectured that such an algorithm might enable a strongly polynomial algorithm for computing maximum Ô¨Çows in abstract networks. 4. As a consequence of our results, Nash equilibria for the network security game studied by Dahan et al. [3] can be described by a compact polyhedron and computed eÔ¨Éciently even when the game is played on an abstract network, including the case of a digraph with cycles (see Sect. 5). 5. We further show that the conservation law (C) proposed in [3] for maximal chains in posets can be reduced to the aÔ¨Éne requirements case (A) (see Sect. 6). We provide a polynomial-time algorithm for computing the corresponding weights Œº when the requirements œÄ are given by a value oracle. As a consequence, the corresponding feasible decompositions can be computed eÔ¨Éciently in this case as well. 6. Finally, we discuss other types of set systems (see Sect. 7). We observe that () is not suÔ¨Écient for the feasibility of the marginals when P consists of the bases of a matroid, perfect matchings of a bipartite graph, or paths in a multicommodity network. We further show that deciding whether a given set of marginals is feasible is NP-hard in general, even when P is given by an explicit list of small sets and the requirements are all equal to 1.  
   
  310  
   
  1.5  
   
  J. Matuschke  
   
  Notation  
   
  Before we discuss our results in detail, we introduce some useful notation concerning abstract networks. Let (E, P) be an abstract network. For P ‚àà P and e ‚àà P , we use the following notation to denote preÔ¨Åxes of P ending at e and suÔ¨Éxes of P starting at e, respectively: [P, e] := {p ‚àà P : p P e} (P, e) := {p ‚àà P : p ‚â∫P e}  
   
  [e, P ] := {p ‚àà P : e P p} (e, P ) := {p ‚àà P : e ‚â∫P p}  
   
  For any path P ‚àà P, we further denote the minimal and maximal element of P with respect to P by sP and tP , respectively. Throughout the paper, proofs of results marked with () can be found in the full version [13].  
   
  2  
   
  Feasible Decompositions in Abstract Networks  
   
  In this section we prove the following theorem, providing an explicit description of feasible decompositions of marginals in abstract networks assuming that requirements are of the form (A) and fulÔ¨Åll the necessary condition (). The construction, described in the following theorem, is based on a natural generalization of shortest-path distances in abstract networks. Theorem 1. Let (E, and let œÅ, Œº ‚àà [0, 1]E fulÔ¨Ålling   P) be an abstract network condition (), i.e., e‚ààP œÅe ‚â• œÄP := 1 ‚àí e‚ààP Œºe for all P ‚àà P. DeÔ¨Åne   Œ±e := min and Œ±e := min {Œ±e , 1 ‚àí œÅe } f ‚àà(Q,e) Œºf + œÅf : Q ‚àà P, e ‚àà Q for e ‚àà E. For œÑ ‚àº U [0, 1] drawn uniformly at random from [0, 1], let SœÑ := {e ‚àà E : Œ±e ‚â§ œÑ < Œ±e + œÅe }. Then x deÔ¨Åned by xS := Pr [SœÑ = S] for S ‚äÜ E is a feasible decomposition of œÅ for (E, P) and œÄ. Intuitively, the values Œ±e for e ‚àà E in the construction above correspond to the ‚Äúshortest-path distance‚Äù to element e in the abstract network (E, P), with the truncation of Œ±e at 1 ‚àí œÅe ensuring that [Œ±e , Œ±e + œÅe ] ‚äÜ [0, 1]. Before we prove Theorem 1, let us Ô¨Årst discuss some of its implications. Interval Structure and Explicit Computation of x. Given the vector Œ±, the nonzero entries of x can be easily determined in polynomial time. Indeed, note that the set Œõ := {Œ±e , Œ±e + œÅe : e ‚àà E} induces a partition of [0, 1] into at most 2|E| + 1 intervals (each with two consecutive values from Œõ ‚à™ {0, 1} as its endpoints), such that SœÑ  = SœÑ  whenever œÑ  and œÑ  are in the same interval. Thus, there are at most 2|E| + 1 non-zero entries in x, whose values can be determined by sorting Œõ, determining all corresponding intervals, computing SœÑ for one œÑ in each of these intervals, and then, for each occurring set S, setting xS to the total length of all intervals in which this set is attained.  
   
  Decomposition of Probability Marginals in Abstract Networks  
   
  311  
   
  Special Case: Directed Graphs. Consider the case where P is the set of simple s-t-paths in a digraph D = (V, A) and E = V ‚à™ A. For v ‚àà V , let Psv denote the set of simple s-v-paths in D. If we are given explicit access to D (rather than accessing P via a membership oracle), we can compute feasible decompositions as follows. Without loss of generality, we can assume that for any v ‚àà V and Q ‚àà Psv there is Q ‚àà Pst with Q ‚äÜ Q .2 Then Œ±v = minQ‚ààPsv f ‚ààQ\{v} Œºf +œÅf  for v ‚àà V and Œ±a = minQ‚ààPsv f ‚ààQ Œºf + œÅf for a = (v, w) ‚àà A. Hence, the vector Œ± corresponds to shortest-path distances in D with respect to œÅ + Œº (with costs on both arcs and nodes). Both Œ± and the corresponding feasible decomposition of œÅ can be computed by a single run of Dijkstra‚Äôs [4] algorithm. Computing feasible decompositions in the general case of arbitrary abstract networks is more involved. We show how this can be achieved in Sect. 3. Proof of Theorem 1. We show that x as constructed in Theorem 1 is a feasible decomposition. Note that  x fulÔ¨Ålls (3) and (4) by construction. Note further that x fulÔ¨Ålls (1) because S‚äÜE:e‚ààS xS = Pr [e ‚àà SœÑ ] = Pr [Œ±e ‚â§ œÑ < Œ±e + œÅe ] = œÅe for all e ‚àà E, where the second identity follows from 0 ‚â§ Œ±e ‚â§ 1 ‚àí œÅe . It remains to prove that x fulÔ¨Ålls (2). The following lemma will be helpful in this endeavour. Lemma 2. Given (E, P), œÅ, Œº, and Œ± as described in Theorem 1, the following two conditions are fulÔ¨Ålled for every P ‚àà P: 1. Œ±tP + ŒºtP + œÅtP ‚â• 1 and 2. for every e ‚àà P \ {tP } there is e ‚àà (e, P ) with Œ±e ‚â§ Œ±e + Œºe + œÅe . Proof. We Ô¨Årst show statement 1. By contradiction assume Œ±tP + ŒºtP + œÅtP < 1. Let Q ‚àà P with tP ‚àà Q and f ‚àà(Q,tP ) Œºf + œÅf = Œ±tP and let R := Q √ótP P .  Note  tP ] and hence e‚ààR Œºe + œÅe ‚â§ Œ±tP + ŒºtP + œÅtP < 1, implying  that R ‚äÜ [Q, e‚ààR œÅe < 1 ‚àí e‚ààR Œºe , a contradiction to (). We now turn to statement 2. If Œ±e ‚â• 1 ‚àí Œºe ‚àí œÅe , then the statement follows with e = tP because Œ±tP‚â§ 1 ‚â§ Œ±e + Œºe + œÅe . Thus assume Œ±e < 1 ‚àí Œºe ‚àí œÅe and let Q ‚àà P with Œ±e = f ‚àà(Q,e) Œºf + œÅf . Let R := Q √óe P . By () we observe  that f ‚ààR Œºf + œÅf ‚â• 1 > Œ±e + Œºe + œÅe , which implies R \ [Q, e] = ‚àÖ because Œº, œÅ ‚â• 0. Thus, let e ‚àà R \ [Q, e] be minimal with respect to ‚â∫R . Observe that R \ [Q, e] ‚äÜ (e, P ) and hence e ‚àà (e, P ). The statement then follows from   Œ±e ‚â§ f ‚àà(R,e ) Œºf + œÅf ‚â§ f ‚àà[Q,e] Œºf + œÅf = Œ±e + Œºe + œÅe , where the second inequality is due to the fact that (R, e ) ‚äÜ [Q, e] by choice of  e and the fact that Œº, œÅ ‚â• 0. With the help of Lemma 2, we can prove that x fulÔ¨Ålls (2) as follows. Let P ‚àà P. For e ‚àà P deÔ¨Åne  Œºf . œÜ(e) := Pr [SœÑ ‚à© [P, e] = ‚àÖ ‚àß œÑ ‚â§ Œ±e + œÅe ] + f ‚àà[P,e] 2  
   
  This can be ensured by introducing arcs (v, t) with Œº(v,t) = 1 and œÅ(v,t) = 0 for all v ‚àà V \ {t}. Note that this does not change the set of feasible decompositions of œÅ.  
   
  312  
   
  J. Matuschke  
   
  Let F := {e ‚àà P : œÜ(e) ‚â• Œ±e + Œºe + œÅe }. We will show that tP ‚àà F . Note that this suÔ¨Éces to prove (2), because the deÔ¨Ånition of F together with statement 1 of Lemma 2 imply œÜ(tP ) ‚â• Œ±tP + ŒºtP + œÅtP ‚â• 1, which in turn yields    xS = Pr [SœÑ ‚à© P = ‚àÖ] ‚â• œÜ(tP ) ‚àí Œºf ‚â• 1 ‚àí Œºf = œÄP . S‚äÜE:S‚à©P =‚àÖ  
   
  f ‚ààP  
   
  f ‚ààP  
   
  We proceed to show tP ‚àà F . By contradiction assume this is not the case. Note that F = ‚àÖ because Œ±sP = 0 and œÜ(sP ) = Pr [sP ‚àà SœÑ ] + ŒºsP = œÅsP + ŒºsP . Thus let e ‚àà F be maximal with respect to ‚â∫P . Because e = tP , we can invoke statement 2 of Lemma 2 and obtain e ‚àà (e, P ) with Œ±e ‚â§ Œ±e + Œºe + œÅe .  
   
  (5)  
   
  We will show that e ‚àà F , contradicting our choice of e. Note that the deÔ¨Ånition of œÜ and the fact that e P e imply œÜ(e ) ‚â• œÜ(e) + Pr [e ‚àà SœÑ ‚àß œÑ > Œ±e + œÅe ] + Œºe ‚â• Œ±e + Œºe + œÅe + Pr [e ‚àà SœÑ ‚àß œÑ > Œ±e + œÅe ] + Œºe ,  
   
  (6)  
   
  where the second inequality follows from e ‚àà F . Moreover, observe that e ‚àà SœÑ if and only if Œ±e ‚â§ œÑ < Œ±e + œÅe and hence Pr [e ‚àà SœÑ ‚àß œÑ > Œ±e + œÅe ] = Œ±e + œÅe ‚àí max{Œ±e , Œ±e + œÅe } ‚â• Œ±e + œÅe ‚àí (Œ±e + Œºe + œÅe ), where the inequality follows from (5). Combining this bound with (6) yields œÜ(e ) ‚â• Œ±e + Œºe + œÅe and hence e ‚àà F , contradicting our choice of e and completing the proof of Theorem 1.    
   
  3  
   
  Computing Feasible Decompositions  
   
  Complementing our existence result from the previous section, we now discuss how to compute corresponding feasible decompositions. We will assume that the ground set E is given explicitly, while the set of abstract paths P is given by a membership oracle that, given F ‚äÜ E, either returns P ‚àà P with P ‚äÜ F and the corresponding order P , or conÔ¨Årms that no P ‚àà P with P ‚äÜ F exists. By our arguments in Sect. 2, it suÔ¨Éces to compute the values of Œ±e for all e ‚àà E. Unfortunately, a complication arises in that even Ô¨Ånding a path containing a certain element e ‚àà E is NP-hard.3 However, as we show below, it is possible to identify a subset U ‚äÜ E for which we can compute the values of Œ±, while the elements in E \U turn out to be redundant w.r.t. the feasibility of the marginals. From this, we obtain the following theorem. 3  
   
  Note that even for the special case where P corresponds to the set of simple s-t-paths in a digraph, Ô¨Ånding P ‚àà P containing a certain arc e is equivalent to the 2-disjoint path problem (for P to be simple, its preÔ¨Åx up to the tail of e and its suÔ¨Éx starting from the head of e must be disjoint). Simply side-stepping this issue by introducing additional elements as done in the second remark after Theorem 1 is not possible here, because we are restricted to accessing P only via the membership oracle.  
   
  Decomposition of Probability Marginals in Abstract Networks  
   
  313  
   
  Theorem 3. There is an algorithm that, given an  abstract network (E, P) via a membership oracle and œÅ, Œº ‚àà [0, 1]E such that e‚ààP œÅe ‚â• œÄP := 1 ‚àí e‚ààP Œºe for all P ‚àà P, computes a feasible decomposition of œÅ for (E, P) and œÄ in time O(|E|3 ¬∑TP ), where TP denotes the time for a call to the membership oracle of P. The Algorithm. Theorem 3 is established via Algorithm 1, which computes values Œ± ¬Ø e for elements e in a subset U ‚äÜ E as follows.Starting from U = ‚àÖ, the algorithm iteratively computes a path P minimizing f ‚ààP ‚à©U Œºf + œÅf and adds the Ô¨Årst element e of P \ U to U , determining Œ± ¬Ø e based on the length of (P, e). Algorithm 1: Computing a feasible decomposition Initialize U := ‚àÖ.  while minP ‚ààP f ‚ààP ‚à©U Œºf + œÅf < 1 do  Let P ‚àà argminP ‚ààP f ‚ààP ‚à©U Œºf + œÅf . Let e := minP P \ U .   Set U := U ‚à™ {e} and Œ± ¬Ø e := min f ‚àà(P,e) Œºf + œÅf , 1 ‚àí œÅe . return Œ±, ¬Ø U  
   
  Analysis. First note  that in every iteration of the while loop, the set P \ U is nonempty because f ‚ààP Œºf + œÅf ‚â• 1 by the assumption on the input in Theorem 3. Hence the algorithm is well-deÔ¨Åned and terminates  after at most |E| iterations. We further remark that Ô¨Ånding P ‚àà P minimizing e‚ààP ‚à©U œÅf + Œºf can be done in time O(|E|2 TP ) using the Algorithm 2 described in Sect. 4. The following lemma then suÔ¨Éces to complete the proof of Theorem 3. Lemma 4 (). Let Œ± ¬Ø , U be the output of Algorithm 1 and deÔ¨Åne œÅ¬Øe := œÅe and ¬Øe := 0 for e ‚àà E \ U . Then Œº ¬Øe := Œºe for e ‚àà U and œÅ¬Øe := 0 and Œº   ¬Øe ‚â• œÄ ¬ØP := 1 ‚àí e‚ààP Œº ¬Øe for all P ‚àà P and 1. e‚ààP œÅ  ¬Øf + œÅ¬Øf : Q ‚àà P, e ‚àà Q ‚à™ {1 ‚àí œÅ¬Øe } for all e ‚àà U . 2. Œ± ¬Ø e = min f ‚àà(Q,e) Œº Indeed, observe that Lemma 4 together with Theorem 1 implies that Œ± ¬Ø induces a feasible decomposition x ¬Ø of œÅ¬Ø for (E, P) and œÄ ¬Ø . Because œÅ¬Øe ‚â§ œÅe for all e ‚àà E and œÄ ¬ØP ‚â• œÄP for all P ‚àà P, this decomposition can be extended to a feasible decomposition of œÅ for (E, P) and œÄ by arbitrarily incorporating the elements from E \ U . This completes the proof of Theorem 3.  
   
  4  
   
  Computing Shortest Paths in Abstract Networks  
   
  In this section, we consider the following natural generalization of the classic shortest s-t-path problem in digraphs: Given an abstractnetwork (E, P) and a cost vector Œ≥ ‚àà RE + , Ô¨Ånd a path P ‚àà P minimizing e‚ààP Œ≥e . We provide a combinatorial, strongly polynomial algorithm for this problem, accessing P only via a membership oracle. In fact, the question for such an algorithm was  
   
  314  
   
  J. Matuschke  
   
  already raised by McCormick [14], who conjectured that it can be used to turn (an adaptation of) his combinatorial, but only weakly polynomial algorithm for the maximum abstract Ô¨Çow problem into a strongly polynomial one. Our results show that such a shortest-path algorithm indeed exists, but leave it open how to use it to improve the running time of the maximum abstract Ô¨Çow algorithm. Theorem 5. There is an algorithm that, given an abstract network (E, P) via  a membership oracle and Œ≥ ‚àà RE + computes P ‚àà P minimizing e‚ààP Œ≥e in time O(|E|2 ¬∑TP ), where TP denotes the time for a call to the membership oracle of P. The Algorithm. For notational convenience, we assume that there is s, t ‚àà E with sP = s and tP = t for all P ‚àà P. Note that this assumption is without loss of generality, as it can be ensured by adding dummy elements s and t to E and including them at the start and end of each path, respectively. The algorithm is formally described as Algorithm 2. It can be seen as a natural extension of Dijkstra‚Äôs [4] algorithm in that it maintains for each element e ‚àà E a (possibly inÔ¨Ånite) label œàe indicating the length of the shortest segment [Qe , e] for some Qe ‚àà P with e ‚àà Qe found so far, and in that its outer loop iteratively chooses an element with currently smallest label for processing. However, updating these labels is more involved, as an abstract network does not provide local concepts such as ‚Äúthe set of arcs leaving a node‚Äù. In its inner loop, the algorithm therefore carefully tries to extend the segment Qe for the currently processed element e to Ô¨Ånd new shortest segments Qe for other elements e . Algorithm 2: Computing a shortest path in an abstract network Initialize T := ‚àÖ, œàs := Œ≥s , and œàe := ‚àû for all e ‚àà E \ {s}. Let Qs ‚àà P. while œàt > minf ‚ààE\T œàf do Let e ‚àà argminf ‚ààE\T œàf . Let F := (E \ T ) ‚à™ [Qe , e]. while there is P ‚àà P with P ‚äÜ F do Let e := minP P \ [Qe , e]. SetF := F \ {e }.  if f ‚àà[P,e ] Œ≥f < œàe then  Set œàe := f ‚àà[P,e ] Œ≥f and Qe := P . Set T := T ‚à™ {e}. return Qt  
   
  Analysis. The proof of the correctness of Algorithm 2 crucially relies on the following lemma, which essentially certiÔ¨Åes that the algorithm does not overlook any shorter path segments when processing an element. Lemma 6 (). Algorithm 2 maintains the following invariant: For all P ‚àà P, there is e ‚àà P with [e, P ] ‚à© T = ‚àÖ and œàe ‚â§ f ‚àà[P,e] Œ≥f .  
   
  Decomposition of Probability Marginals in Abstract Networks  
   
  315  
   
  Proof of Theorem 5. When Algorithm 2 terminates, œàt ‚â§ œàf for all f ‚àà E \ T by the termination criterion of the outer while  loop. Let P ‚àà P. By Lemma 6 there is an element e ‚àà P \ T with œàe ‚â§ f ‚àà[P,e] Œ≥f . Note that this implies    f ‚ààQt Œ≥f = œàt ‚â§ œàe = f ‚àà[P,e] Œ≥f ‚â§ f ‚ààP Œ≥f , where the last inequality uses the fact that Œ≥f ‚â• 0 for all f ‚àà E. We conclude that the path Qt returned by the algorithm is indeed a shortest path. To see that the algorithm terminates in polynomial time, observe that the outer while loop stops after at most |E| ‚àí 1 iterations, as in each iteration an element from E \ {t} is added to T and the termination criterion is fulÔ¨Ålled if T = E \ {t}. Furthermore, each iteration of the inner while loop removes an element from F and hence after at most |F | ‚â§ |E| iterations no path P ‚äÜ F exists anymore, implying that the inner while loop terminates.    
   
  5  
   
  Dahan et al.‚Äôs Network Security Game  
   
  Dahan et al. [3] studied the following network security game. The input is a E set system (E, P) with capacities u ‚àà RE + , transportation cost c ‚àà R+ and E the routing entity R, whose interdiction costs d ‚àà R+ . There are two players:  : strategy space is the set of Ô¨Çows F := {f ‚àà RP + P ‚ààP:e‚ààP fP ‚â§ ue ‚àÄe ‚àà E}, and the interdictor I, who selects a subset of elements S ‚äÜ E to interdict, with the intuition that all Ô¨Çow on interdicted elements is disrupted. Given strategies f ‚àà F and S ‚äÜ E, the payoÔ¨Äs for R and I, respectively, are given by    PR (f, S) := P ‚ààP:P ‚à©S=‚àÖ fP ‚àí P ‚ààP e‚ààP ce fP and   PI (f, S) := P ‚ààP:P ‚à©S=‚àÖ fP ‚àí e‚ààS de , respectively. That is, R‚Äôs payoÔ¨Ä is the total amount of non-disrupted Ô¨Çow, reduced by the cost for sending Ô¨Çow f , while I‚Äôs payoÔ¨Ä is the total amount of Ô¨Çow that is disrupted, reduced by the interdiction cost for the set S. We are interested in Ô¨Ånding (mixed) Nash equilibria (NE) for this game, i.e., random distributions œÉR and œÉI over the strategy spaces of I and R, respectively, such that no player can improve their expected payoÔ¨Ä by unilateral deviation. However, the eÔ¨Écient computation of such equilibria is hampered by the fact that the strategy spaces of both players are of exponential size/dimension in the size of the ground set E. To overcome this issue, Dahan et al. [3] proposed to consider the following pair of primal and dual linear programs:   œÄPc fP [LPI ] min ue Œºe +de œÅe [LPR ] max P ‚ààP  
   
  s.t.  
   
  e‚ààE  
   
    
   
  fP ‚â§ ue ‚àÄe ‚àà E  
   
  P ‚ààP:e‚ààP  
   
    
   
  fP P ‚ààP:e‚ààP where œÄPc := 1 ‚àí  
   
    
   
  Œºe + œÅe ‚â• œÄPc ‚àÄP ‚àà P  
   
  e‚ààP  
   
  ‚â§ de ‚àÄe ‚àà E  
   
  f ‚â•0  
   
  a‚ààP  
   
  s.t.  
   
    
   
  Œº‚â•0 œÅ ‚â• 0,  
   
  ca . Dahan et al. [3] showed the following result.  
   
  316  
   
  J. Matuschke  
   
  Theorem 7 (Dahan et al. [3]). Let f ‚àó and (Œº‚àó , œÅ‚àó ) be optimal solutions Let œÉI be a feasible decomposition of œÅ‚àó for to [LPR ] and [LPI ], respectively.  c ‚àó (E,  P) and œÄP :=‚àó œÄP ‚àí e‚ààP Œºe and let œÉR be a distribution over F with f ‚ààF œÉR,f fP = fP . Then (œÉR , œÉI ) is a Nash equilibrium. In particular, note that any  feasible solution to [LPI ] deÔ¨Ånes marginals œÅ that fulÔ¨Ål () for œÄP := œÄPc ‚àí e‚ààP Œºe . Hence, if condition () is suÔ¨Écient for feasibility of marginals in the set system P under aÔ¨Éne requirements, any pair of optimal solutions to the LPs induces a Nash equilibrium. If we can moreover eÔ¨Éciently compute optimal solutions to the LPs and the corresponding feasible decompositions, we can eÔ¨Éciently Ô¨Ånd a Nash equilibrium. Dahan et al. [3] showed that this is possible when P is the set of s-t-paths in a DAG. Hence NE for the game can be found eÔ¨Éciently in that setting. This positive result is particularly interesting because NE are hard to compute for the variant of the game in which the interdictor is limited by a budget, even when interdiction costs are uniform, transportation costs are zero, and the game is played on a DAG [5]. Our results in Sects. 2 to 4 imply that all three conditions for the computability of NE are also met when (E, P) is an abstract network (note that we can use Algorithm 2 to separate the constraints of [LPI ]). Hence we can compute Nash equilibria for the above game when (E, P) is an abstract network given by a membership oracle, in time polynomial in |E|, including the case where the game is played on a digraph with cycles. We remark that Dahan et al. [3] also showed that, if there is at least one dual solution with a decomposition that assigns positive probability to the empty set, then all NE of the game are of the form described in Theorem 7. They showed that this condition is always fulÔ¨Ålled in the DAG case when all transportation costs are positive. Via a small adjustment to our construction in Sect. 2, the same result can be proven for the case of abstract networks ().  
   
  6  
   
  The Conservation Law for Partially Ordered Sets  
   
  As discussed in Sect. 1, Dahan et al. [3] established the suÔ¨Éciency of () in partially ordered sets not only for the case of aÔ¨Éne requirements (A) but also for the case where requirements fulÔ¨Åll the conservation law (C). However, they left it open whether it is possible to eÔ¨Éciently compute the corresponding decompositions in the latter case. In this section, we resolve this question by showing that the conservation law (C) for maximal chains in a poset can be reduced to the case of aÔ¨Éne requirements (A) in the corresponding Hasse diagram,4 for which a feasible decomposition then can be computed eÔ¨Éciently. Theorem 8 (). Let D = (V, A) be a directed acyclic graph, let s, t ‚àà V , and let P ‚äÜ 2V ‚à™A be the set of s-t-paths in D. Let œÄ ‚àà [0, 1]P such that (C) is fulÔ¨Ålled. Then there exists Œº ‚àà [0, 1]V ‚à™A such that œÄP = 1 ‚àí e‚ààP Œºe . Furthermore, Œº can be computed in strongly polynomial time in |V | and |A| when œÄ is given by an oracle that, given P ‚àà P, returns œÄP . 4  
   
  See [13] for details on why the transformation to the Hasse diagram is necessary.  
   
  Decomposition of Probability Marginals in Abstract Networks  
   
  317  
   
  Proof(sketch). By Farkas‚Äô lemma, the existence of Œºis equivalent to showing that P ‚ààP (1 ‚àí œÄP )yP ‚â• 0 for every y ‚àà RP with P ‚ààP:e‚ààP yP ‚â• 0 for all e ‚àà V ‚à™ A. This property can be established by iteratively applying (C) to   transform y into a nonnegative vector without changing P ‚ààP (1 ‚àí œÄP )yP .   
   
  7  
   
  Other Set Systems  
   
  The results in this paper lead to the question whether suÔ¨Éciency of () and computability of feasible decompositions can be established for other set systems, beyond abstract networks. We give negative answers for several natural candidates of such systems and point out interesting questions for future research. SuÔ¨Éciency of () (). There are simple counterexamples for the suÔ¨Éciency of () in the following cases, even when assuming that œÄ ‚â° 1: when P is the set of bases of a matroid; when P is the set of perfect matchings in a bipartite graph; when P is the set of si -ti -paths in a digraph with multiple terminal pairs (si , ti ). An interesting question in this context is whether we can describe the systems for which () is suÔ¨Écient by means of forbidden substructures. Approximately Feasible Decompositions (). Given the non-existence result mentioned above, one may be interested in Ô¨Ånding decompositions that satisfy the requirements at least approximately. We say a decomposition x of marginals œÅ is Œ≤-approximately feasible, for Œ≤ ‚àà [0, 1], if it fulÔ¨Ålls (1), (3), (4)  and S‚äÜE:S‚à©P =‚àÖ xS ‚â• Œ≤ ¬∑ œÄP for all P ‚àà P. Indeed, if marginals œÅ fulÔ¨Åll () for requirements œÄ, a (1 ‚àí 1/e)-approximately feasible decomposition always exists: Simply include each element e ‚àà E in the random set independently with probability œÅe . An interesting question for future research is whether better guarantees may be achieved for some classes of systems. Computing Feasible Decompositions and Optimization (). For a given instance, we may also be interested in Ô¨Ånding a decomposition of the given marginals that is Œ≤-approximately feasible for the largest possible value of Œ≤. Note that this also includes the case of Ô¨Ånding a feasible decomposition if it exists (resulting in Œ≤ = 1). Unfortunately, this latter problem is NP-complete, even in quite restricted cases, as evidenced by the theorem below. However, note that this hardness result still leaves room for approximating the best possible Œ≤. Theorem 9 (). The following decision problem is NP-complete: Given a set system (E, P) with |P | = 3 for all P ‚àà P and marginals œÅ ‚àà [0, 1]E , is there a feasible decomposition of œÅ for (E, P) and requirement vector œÄ ‚â° 1? Acknowledgements. The author thanks three anonymous reviewers for numerous helpful suggestions that improved the manuscript. This work has been supported by the special research fund of KU Leuven (project C14/22/026).  
   
  318  
   
  J. Matuschke  
   
  References 1. Bertsimas, D., Nasrabadi, E., Orlin, J.B.: On the power of randomization in network interdiction. Oper. Res. Lett. 44, 114‚Äì120 (2016) 2. Correa, J., Harks, T., Kreuzen, V.J., Matuschke, J.: Fare evasion in transit networks. Oper. Res. 65, 165‚Äì183 (2017) 3. Dahan, M., Amin, S., Jaillet, P.: Probability distributions on partially ordered sets and network interdiction games. Math. Oper. Res. 47, 458‚Äì484 (2022) 4. Dijkstra, E.W.: A note on two problems in connexion with graphs. Numer. Math. 269, 271 (1959) 5. Disser, Y., Matuschke, J.: The complexity of computing a robust Ô¨Çow. Oper. Res. Lett. 48, 18‚Äì23 (2020) 6. Ford, L.R., Fulkerson, D.R.: Maximal Ô¨Çow through a network. Can. J. Math. 8, 399‚Äì404 (1956) 7. HoÔ¨Äman, A.J.: A generalization of max Ô¨Çow‚Äìmin cut. Math. Program. 6, 352‚Äì359 (1974) 8. Holzmann, T., Smith, J.C.: The shortest path interdiction problem with randomized interdiction strategies: Complexity and algorithms. Oper. Res. 69, 82‚Äì99 (2021) 9. Kappmeier, J.P.W.: Generalizations of Ô¨Çows over time with applications in evacuation optimization, Ph. D. thesis, TU Berlin (2015) 10. Kappmeier, J.P.W., Matuschke, J., Peis, B.: Abstract Ô¨Çows over time: a Ô¨Årst step towards solving dynamic packing problems. Theoret. Comput. Sci. 544, 74‚Äì83 (2014) 11. Karbstein, M.: Line planning and connectivity, Ph. D. thesis, TU Berlin (2013) 12. Martens, M., McCormick, S.T.: A polynomial algorithm for weighted abstract Ô¨Çow. In: Lodi, A., Panconesi, A., Rinaldi, G. (eds.) IPCO 2008. LNCS, vol. 5035, pp. 97‚Äì111. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-68891-4 7 13. Matuschke, J.: Decomposition of probability marginals for security games in abstract networks. Tech. rep., arXiv:2211.04922 (2022) 14. McCormick, S.T.: A polynomial algorithm for abstract maximum Ô¨Çow. In: Proceedings of the 7th annual ACM-SIAM Symposium on Discrete Algorithms, pp. 490‚Äì497 (1996) 15. Pyakurel, U., Khanal, D.P., Dhamala, T.N.: Abstract network Ô¨Çow with intermediate storage for evacuation planning. Eur. J. Oper. Res. 305, 1178‚Äì1193 (2022) 16. Szeszl¬¥er, D.: Security games on matroids. Math. Program. 161, 347‚Äì364 (2017) 17. Tambe, M.: Security and game theory: Algorithms, deployed systems, lessons learned. Cambridge University Press (2011)  
   
  Set Selection Under Explorable Stochastic Uncertainty via Covering Techniques Nicole Megow  
   
  and Jens Schl√∂ter(B)  
   
  Faculty of Mathematics and Computer Science, University of Bremen, Bremen, Germany {nmegow,jschloet}@uni-bremen.de Abstract. Given subsets of uncertain values, we study the problem of identifying the subset of minimum total value (sum of the uncertain values) by querying as few values as possible. This set selection problem falls into the Ô¨Åeld of explorable uncertainty and is of intrinsic importance therein as it implies strong adversarial lower bounds for a wide range of interesting combinatorial problems such as knapsack and matchings. We consider a stochastic problem variant and give algorithms that, in expectation, improve upon these adversarial lower bounds. The key to our results is to prove a strong structural connection to a seemingly unrelated covering problem with uncertainty in the constraints via a linear programming formulation. We exploit this connection to derive an algorithmic framework that can be used to solve both problems under uncertainty, obtaining nearly tight bounds on the competitive ratio. This is the Ô¨Årst non-trivial stochastic result concerning the sum of unknown values without further structure known for the set. With our novel methods, we lay the foundations for solving more general problems in the area of explorable uncertainty.  
   
  Keywords: explorable uncertainty  
   
  1  
   
  ¬∑ queries ¬∑ set selection ¬∑ set cover  
   
  Introduction  
   
  In the setting of explorable uncertainty, we consider optimization problems with uncertainty in numeric input parameters. Instead of having access to the precise numeric values, we are given uncertainty intervals that contain the precise values. Each uncertainty interval can be queried, which reveals the corresponding precise value. The goal is to adaptively query intervals until we have suÔ¨Écient information to optimally (or approximately) solve the underlying optimization problem, while minimizing the number of queries. We mainly consider the set selection problem (MinSet) under explorable uncertainty. In this problem, we are given a set of n uncertain values represented by uncertainty intervals I = {I1 , . . . , In } and a family of m sets S = {S1 , . . . , Sm } with S ‚äÜ I for all S ‚àà S. A value wi lies in its uncertainty interval Ii , is initially  unknown, and can be revealed via a query. The value of an S ‚àà S is w(S) = Ii ‚ààS wi . The goal is to determine a subset of minimum value c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 319‚Äì333, 2023. https://doi.org/10.1007/978-3-031-32726-1_23  
   
  320  
   
  N. Megow and J. Schl√∂ter  
   
  as well as its value by using a minimal number of queries. It can be seen as an integer linear program (ILP) with uncertainty in the coeÔ¨Écients of the objective function:  m min j=1 xj Ii ‚ààSj wi m (1) s.t. j=1 xj = 1 ‚àà {0, 1} ‚àÄj ‚àà {1, . . . , m}. xj Since the wi ‚Äôs are uncertain, we might have to execute queries to determine an optimal solution to (1). We refer to this problem as MinSet under uncertainty. In this paper, we consider the stochastic problem variant, where all values wi are drawn independently at random from their intervals Ii according to unknown distributions di . As there are instances that cannot be solved without querying the entire input, we analyze an algorithm ALG in terms of its competitive ratio: for the set of problem instances J , it is deÔ¨Åned as maxJ‚ààJ E[ALG(J)]/E[OPT(J)], where ALG(J) is the number of queries needed by ALG to solve instance J, and OPT(J) is the minimum number of queries necessary to solve the instance. MinSet is a fundamental problem and of intrinsic importance within the Ô¨Åeld of explorable uncertainty. The majority of existing works considers the adversarial setting, where query outcomes are not stochastic but returned in a worst-case manner. Selection type problems have been studied in the adversarial setting and constant (matching) upper and lower bounds are known, e.g., for selecting the minimum [19], the k-th smallest element [13,19], a minimum spanning tree [10,12,18,23], sorting [17] and geometric problems [5]. However, these problems essentially boil down to comparing single uncertainty intervals and identifying the minimum of two unknown values. Once we have to compare two (even disjoint) sets and the corresponding sums of unknown values, no deterministic algorithm can have a better adversarial competitive ratio than n, the number of uncertainty intervals. This has been shown by Erlebach et al. [11] for MinSet, and it implies strong adversarial lower bounds for classical combinatorial problems, such as, knapsack and matchings [25], as well as solving ILPs with uncertainty in the cost coeÔ¨Écients as in (1) [25]. As a main result, we provide substantially better algorithms for MinSet under stochastic uncertainty. This is a key step for breaching adversarial lower bounds for a wide range of problems. For the stochastic setting, the only related results we are aware of concern sorting [6] and the problem of Ô¨Ånding the minimum in each set of a given collection of sets [2]. Asking for the sum of unknown values is substantially diÔ¨Äerent. The Covering Point of View. Our key observation is that we can view MinSet as a covering problem with uncertainty in the constraints. To see this, we focus on the structure of the uncertainty intervals and how a query aÔ¨Äects it. We assume that each interval Ii ‚àà I is either open (non-trivial ) or trivial, i.e., Ii = (Li , Ui ) or Ii = {wi }; a standard technical assumption in explorable uncertainty. In the latter case, Li = Ui = wi . We call Li and Ui lower  and upper limit. For a set S ‚àà S,  we deÔ¨Åne the initial lower limit LS = Ii ‚ààS Li and initial upper limit US = Ii ‚ààS Ui . Clearly, w(S) ‚àà (LS , US ). As the intervals (LS , US ) of the sets S ‚àà S can overlap, we might have to execute queries to determine the set of minimum value. A query to an interval  
   
  Set Selection Under Explorable Stochastic Uncertainty  
   
  321  
   
  Ii reveals the precise value wi and, thus, replaces both, Li and Ui , with wi . In a sense, a query to an Ii ‚àà S reduces the range (LS , US ) in which w(S) might lie by increasing LS by wi ‚àí Li and decreasing US by Ui ‚àí wi . Let LS (Q) and US (Q) denote the limits of set S after querying a set of intervals Q ‚äÜ I. For a MinSet instance (I, S), let w‚àó = minS‚ààS w(S) be the initially uncertain minimum set value. To solve the problem, we have to adaptively query a set of intervals Q until US ‚àó (Q) = LS ‚àó (Q) = w‚àó holds for some S ‚àó ‚àà S and LS (Q) ‚â• w‚àó holds for all S ‚àà S. Only then, we know for sure that w‚àó is indeed the minimum set value and that S ‚àó achieves this value. The following ILP with ai = wi ‚àí Li for all Ii ‚àà I and bS = w‚àó ‚àí LS for all S ‚àà S formulates this problem:  min Ii ‚ààI xi s.t. (MinSetIP) Ii ‚ààS xi ¬∑ ai ‚â• bS ‚àÄS ‚àà S xi ‚àà {0, 1} ‚àÄIi ‚àà I Observe that this ILP is a special case of the multiset multicover problem (see, e.g., [26]). If ai = wi ‚àí Li = 1 for all Ii ‚àà I and bS = w‚àó ‚àí LS = 1 for all S ‚àà S, then the problem is exactly the classical SetCover problem with I corresponding to the SetCover sets and S corresponding to the SetCover elements. The optimal solution to (MinSetIP) is the optimal query set for the corresponding MinSet instance. Under uncertainty however, the coeÔ¨Écients ai = wi ‚àí Li and right-hand sides bS = w‚àó ‚àí LS are unknown. We only know that ai ‚àà (Li ‚àí Li , Ui ‚àí Li ) = (0, Ui ‚àí Li ) as ai = (wi ‚àí Li ) and wi ‚àà (Li , Ui ). In a sense, to solve MinSet under uncertainty, we have to solve (MinSetIP) with uncertainty in the coeÔ¨Écients and irrevocable decisions. For the rest of the paper, we interpret MinSet under uncertainty in exactly that way: We have to solve (MinSetIP) without knowing the coeÔ¨Écients in the constraints. Whenever we irrevocably add an interval Ii to our solution (i.e., set xi to 1), the information on the coeÔ¨Écients (in form of wi ) is revealed. Our goal is to add elements to our solution until it becomes feasible for (MinSetIP), and to minimize the number of added elements. In this interpretation, the terms ‚Äúquerying an element‚Äù and ‚Äúadding it to the solution‚Äù are interchangeable, and we use them as such. Our main contribution is an algorithmic framework that exploits techniques for classical covering problems and adapts them to handle uncertainty in the coeÔ¨Écients ai and the right-hand sides bS . This framework allows us to obtain improved results for MinSet under uncertainty and other covering problems. Our Results. We design a polynomial-time algorithm for MinSet under stochastic uncertainty with competitive ratio O( œÑ1 ¬∑ log2 m), where m is the number of sets (number of constraints in (MinSetIP)) and parameter œÑ characterizes how ‚Äúbalanced‚Äù the distributions of values within the given intervals are. More precisely, œÑ = minIi ‚ààI œÑi and œÑi is the probability that wi is larger than the center of Ii (e.g., for uniform distributions œÑ = 12 ). This is the Ô¨Årst stochastic result in explorable uncertainty concerning the sum of unknown values and it builds on new methods that shall be useful for solving more general problems in this  
   
  322  
   
  N. Megow and J. Schl√∂ter  
   
  Ô¨Åeld. The ratio is independent of the number of elements, n. In particular for a small number of sets, m, this is a signiÔ¨Åcant improvement upon the adversarial lower bound of n [11]. Dependencies on parameters such as œÑ are quite standard and necessary [3,4,15,22,29]. For example, in [22] the upper bounds depend on the probability to draw the largest value of the uncertainty interval, which is an even stricter assumption that does not translate to open intervals. Our results translate also to the maximization variant of MinSet; see full version [24]. We remark that the hidden constants in the performance bounds depend on the upper limits of the given intervals. Assuming those to be constant is also a common assumption; see, e.g., [22]. Even greedy algorithms for covering problems similar to (MinSetIP) without uncertainty have such dependencies [9,26,28]. As MinSet contains the classical SetCover, an approximation factor better than O(log m) is unlikely, unless P=NP [8]. We show that this holds also in the stochastic setting, even for uniform distributions. We also show a lower bound of œÑ1 for MinSet under stochastic explorable uncertainty, even for pairwise disjoint sets. Thus, the dependency on log m and œÑ1 in our results is necessary. In the special case that all given sets are disjoint, we provide a simpler algorithm with competitive ratio œÑ2 . This is a gigantic improvement compared to the adversarial setting, where the lower bound of n holds even for disjoint sets [11]. Algorithmically, we exploit the covering point of view to introduce a class of greedy algorithms that use the same basic strategy as the classical SetCover greedy algorithm [7]. However, we do not have suÔ¨Écient information to compute and query an exact greedy choice under uncertainty as this choice depends on uncertain parameters. Instead, we show that it is suÔ¨Écient to query a small number of elements that together achieve a similar greedy value to the exact greedy choice. If we do this repeatedly and the number of queries per iteration is small in expectation, then we achieve guarantees comparable to the approximation factor of a greedy algorithm with full information. It is worth noting that this way of comparing an algorithm to the optimal solution is a novelty in explorable uncertainty as all previous algorithms for adversarial explorable uncertainty (MinSet and other problems) exploit witness sets. A witness set is a set of queries Q such that each feasible solution has to query at least one element of Q, which allows to compare an algorithm with an optimal solution. Our results translate to other covering problems under uncertainty. In particular, for (MinSetIP) under uncertainty with deterministic right-hand sides, we give a simpliÔ¨Åed algorithm with improved competitive ratio O( œÑ1 ¬∑ log m). For a slightly diÔ¨Äerent balancing parameter, this holds even for the more general variant, where a variable can have diÔ¨Äerent coeÔ¨Écients for diÔ¨Äerent constraints, each with an individual uncertainty interval and distribution; see full version [24]. All missing proofs are provided in the full version [24]. Further Previous Work. For adversarial MinSet under uncertainty, Erlebach et al. [11] show a (best possible) competitive ratio of 2d, where d is the cardinality of the largest set. In the lower bound instances, d ‚àà Œ©(n). The algorithm repeatedly queries disjoint witness sets of size at most 2d. This result was stated for the setting, in which it is not necessary to determine the value of the minimal set; if the value has to be determined, the bounds change to d.  
   
  Set Selection Under Explorable Stochastic Uncertainty  
   
  323  
   
  Further related work on MinSet includes the result by Yamaguchi and Maehara [22], who consider packing ILPs with stochastic uncertainty in the cost coeÔ¨Écients, which can be queried. They present a framework for solving several problems and bound the absolute number of iterations that it requires to solve them, instead of the competitive ratio. However, we show in the full version [24] that their algorithm has competitive ratio Œ©(n) for MinSet, even for uniform distributions. Thus, it does not improve upon the adversarial lower bound. Wang et al. [30] also consider selection-type problems in a somewhat related model. They consider diÔ¨Äerent constraints on the set of queries that, in a way, imply a budget on the number of queries. They solve optimization problems with respect to this budget, which has a very diÔ¨Äerent Ô¨Çavor than our setting. While we are not aware of previous work on covering problems with valuequeries and uncertainty in the constraints, there is related work on queries that reveal the existence of edges in a graph instead of numeric values [3,4,15,29]. Furthermore, there is related work on covering problems in diÔ¨Äerent stochastic settings (see, e.g., [1,14,16,27]).  
   
  2  
   
  Algorithmic Framework  
   
  In this section, we present our algorithmic framework. To illustrate the main ideas, we Ô¨Årst consider the oÔ¨Ñine variant of MinSet and give hardness results. 2.1  
   
  OÔ¨Ñine Problems and Hardness of Approximation  
   
  We refer to the problem of solving (MinSetIP) with full knowledge of the precise values wi (and w‚àó ) as oÔ¨Ñine. For MinSet under uncertainty, we say a solution is optimal, if it is an optimal solution for the corresponding oÔ¨Ñine problem. We use OPT to refer to an optimal solution and its objective value. OÔ¨Ñine MinSet contains SetCover and, thus, is as hard to approximate. This result transfers to the stochastic setting, even with uniform distributions. Thus, an approximation factor better than O(log m) is unlikely, unless P=NP [8]. On the positive side, we can approximate oÔ¨Ñine MinSet by adapting covering results [7,9,20,21,26]. In particular, we want to use greedy algorithms that iteratively and irrevocably add elements to the solution that are selected by a certain greedy criterion. Recall that ‚Äúadding an element to the solution‚Äù corresponds to both, setting the variable xi of an interval Ii ‚àà I in (MinSetIP) to one and querying Ii . As the greedy criterion for adding an element depends on previously added elements, we deÔ¨Åne a version of the ILP parametrized by the set Q of elements that have already been added to the solution and adjust the right-hand sides to the remaining covering requirements after adding Q. To that  end, let bS (Q) = max{bS ‚àí Ii ‚ààQ‚à©S ai , 0} for ai = wi ‚àí Li and bS = w‚àó ‚àí LS .  min Ii ‚ààI\Q xi  s.t. (MinSetIP-Q) Ii ‚ààS\Q xi ¬∑ ai ‚â• bS (Q) ‚àÄS ‚àà S xi ‚àà {0, 1} ‚àÄIi ‚àà I \ Q  
   
  324  
   
  N. Megow and J. Schl√∂ter  
   
   Based on the ILP and the sum of right-hand sides b(Q) = S‚ààS bS (Q), we adjust an algorithm for multiset multicover by Dobson [9] to our setting. The OÔ¨Ñine Algorithm scales the coeÔ¨Écients to a and b such that all nonzero left-hand side coeÔ¨Écients are at least 1 (we refer to such instances as scaled ). Then it greedily adds the element to the solution that reduces the right-hand sides the most, i.e., the interval Ii ‚àà I \ Q that maximizes the greedy value gc (Q, Ii ) = b (Q) ‚àí b (Q ‚à™ {Ii }). For a subset G ‚äÜ I, we deÔ¨Åne gc (Q, G) = b (Q) ‚àí b (Q ‚à™ G). After bS (Q) < 1 for all S ‚àà S, we can exploit that all non-zero coeÔ¨Écients ai are at least one. This means that adding an element Ii ‚àà I \Q satisÔ¨Åes all remaining constraints of sets S with Ii ‚àà S. Thus, the remaining problem reduces to a SetCover instance, which can be solved by using the classical greedy algorithm by Chvatal [7]. This algorithm greedily adds the element Ii that maximizes greedy value gs (Q, Ii ) = A(Q)‚àíA(Q‚à™{Ii }) with A(Q) = |{S ‚àà S | bS (Q) > 0}|, i.e., the element that satisÔ¨Åes the largest number of constraints that are not already satisÔ¨Åed by Q. For subsets G ‚äÜ I, we deÔ¨Åne gs (Q, G) = A(Q)‚àíA(Q‚à™G). Theorem 1 (Follows from [9]). The OÔ¨Ñine Algorithm is a polynomial-time O(log m)-approximation for oÔ¨Ñine MinSet. The precise approximation factor is œÅ(Œ≥) = ln(Œ≥ ¬∑m¬∑maxS (w‚àó ‚àíLS ))+ln(m) with smin = minIi ‚ààI : (wi ‚àíLi )>0 (wi ‚àí Li ), Œ≥ = 1/smin and m = |S|. We will state the competitive ratios of our algorithms in terms of œÅ. To that end, deÔ¨Åne œÅ¬Ø(Œ≥) = ln(Œ≥ ¬∑ m ¬∑ maxS,S  (US ‚àí LS  ) + ln(m), which is an upper bound on œÅ(Œ≥). Under uncertainty, we compare against œÅ¬Ø to avoid the random variable w‚àó . For constant Ui ‚Äôs, œÅ¬Ø and œÅ are asymptotically the same. 2.2  
   
  Algorithmic Framework  
   
  To solve MinSet under uncertainty, we ideally would like to apply the OÔ¨Ñine Algorithm. However, since the coeÔ¨Écients ai = wi ‚àí Li and bS = w‚àó ‚àí LS are unknown, we cannot do so as we cannot compute the greedy values gc or gs . While we cannot precisely compute the greedy choice, our strategy is to approximate it and to show that approximating it is suÔ¨Écient to obtain the desired guarantees. To make this more precise, consider an iterative algorithm for (MinSetIP) that iteratively adds  pairwise disjoint subsets G1 , . . . , Gh of I to the solution. For each j, let Qj = 1‚â§j  ‚â§j‚àí1 Gj  , i.e., Qj contains the elements that have been added to the solution before Gj . If the combined greedy value of Gj is within a factor of Œ± to the best greedy value for the problem instance after adding Qj , then we say that Gj Œ±-approximates the greedy choice. The following deÔ¨Ånition makes this more precise while taking into account that there are two diÔ¨Äerent greedy values gc and gs (cf. the OÔ¨Ñine Algorithm). DeÔ¨Ånition 1. For a scaled instance of (MinSetIP), some Q ‚äÜ I, and an Œ± ‚â• 1, a subset G ‚äÜ I \ Q Œ±-approximates the current greedy choice (as characterized by Q) if one of the following conditions holds:  
   
  Set Selection Under Explorable Stochastic Uncertainty  
   
  325  
   
  1. bS (Q) < 1 for all S ‚àà S and gs (Q, G) ‚â• Œ±1 ¬∑ maxIi ‚ààI\Q gs (Q, Ii ). 2. b (Q) ‚â• 1 and gc (Q, G) ‚â• Œ±1 ¬∑ maxIi ‚ààI\Q gc (Q, Ii ). We bound the number of iterations j in which Gj Œ±-approximates the current greedy choice via an adjusted SetCover greedy analysis. Lemma 1. Consider an arbitrary algorithm for (MinSetIP) that scales the coeÔ¨Écients by factor Œ≥ and iteratively adds disjoint subsets G1 , . . . , Gh of I to the solution until the instance is solved. The number of groups Gj that Œ±-approximate the current greedy choice (after adding Qj ) is at most Œ± ¬∑ œÅ(Œ≥) ¬∑ OPT. The lemma states that the number of such groups Gj is within a factor of Œ± of the performance guarantee œÅ(Œ≥) of the oÔ¨Ñine greedy algorithm. If each Gj Œ±-approximates its greedy choice, the iterative algorithm achieves an approximation factor of maxj |Gj | ¬∑ Œ± ¬∑ œÅ(Œ≥). Thus, approximating the greedy choices by a constant factor using a constant group size is suÔ¨Écient to only lose a constant factor compared to the oÔ¨Ñine greedy algorithm. This insight gives us a framework to solve MinSet under uncertainty. Recall that the wi ‚Äôs (and by extension the ai ‚Äôs and bS ‚Äôs) are uncertain and only revealed once we irrevocably add an Ii ‚àà I to the solution. We refer to a revealed wi as a query result, and to a Ô¨Åxed set of revealed wi ‚Äôs for all Ii ‚àà I as a realization of query results. Consider an iterative algorithm. The sets Gj can be computed and queried adaptively and are allowed to depend on (random) query results from previous iterations. Hence, Xj = |Gj | is a random variable. Let Yj be an indicator variable denoting whether the algorithm executes iteration j (Yj = 1) or terminates beforehand (Yj = 0). We deÔ¨Åne the following class of iterative algorithms and show that algorithms from this class achieve certain guarantees. DeÔ¨Ånition 2. An iterative algorithm is (Œ±, Œ≤, Œ≥)-Greedy if it satisÔ¨Åes: 1. For every realization of query results; each Gj Œ±-approximates the greedy choice as characterized by Qj on the instance with coeÔ¨Écients scaled by Œ≥. 2. E[Xj | Yj = 1] ‚â§ Œ≤ holds for all iterations j. Theorem 2. Each (Œ±, Œ≤, Œ≥)-Greedy algorithm for MinSet under uncertainty achieves a competitive ratio of Œ± ¬∑ Œ≤ ¬∑ œÅ¬Ø(Œ≥) ‚àà O(Œ± ¬∑ Œ≤ ¬∑ log m). Proof. Consider an(Œ±, Œ≤, Œ≥)-Greedy algorithm ALG for MinSet. Its expected  cost is E[ALG] = j E[Xj ] = j P[Yj = 1] E[Xj | Yj = 1] + P[Yj = 0] E[Xj | Yj = 0]. As E[Xj | Yj = 0] = 0 (if the algorithm terminates before iteration j, E[ALG] = it adds no more elements and, thus, Xj = 0), the equality reduces to  P[Y = 1] E[X | Y = 1]. By DeÔ¨Ånition 2, this implies E[ALG] ‚â§ Œ≤ j j j j j P[Yj = 1].  It remains to bound j P[Yj = 1], which is the expected number of iterations of ALG. Consider a Ô¨Åxed realization of query results. By the Ô¨Årst property of Definition 2, each Gj Œ±-approximates its greedy choice for the (MinSetIP) instance of the realization scaled by factor Œ≥. Thus, Lemma 1 implies that the number of iterations is at most Œ±œÅ(Œ≥)OPT, which is upper bounded by Œ±¬Ø œÅ(Œ≥)OPT. As  
   
  326  
   
  N. Megow and J. Schl√∂ter  
   
  Algorithm 1: MinSet with deterministic right-hand sides. 1 2 3 4 5 6  
   
  Input: Instance of MinSet with deterministic right-hand sides. 2 Q = ‚àÖ; Scale a and b by smin to a and b for smin = minIi ‚ààI : Ui ‚àíLi >0 Ui ‚àí Li ; while the problem is not solved do if b (Q) ‚â• 1 then g = g¬Øc else g = g¬Øs ; repeat Ii ‚Üê arg maxIj ‚ààI\Q g(Q, Ij ); Query Ii ; Q ‚Üê Q ‚à™ {Ii }; until the problem is solved or wi ‚àí Li ‚â• 12 ¬∑ (Ui ‚àí Li );  
   
  this upper bound on the number of iterations holds for every realization and OPT is the only random variable of that term (as we substituted œÅ by œÅ¬Ø), we œÅ(Œ≥) E[OPT], which implies E[ALG] ‚â§ Œ±Œ≤ œÅ¬Ø(Œ≥) E[OPT].  
   
  get j P[Yj = 1] ‚â§ Œ±¬Ø  
   
  3  
   
  MinSet with Deterministic Right-Hand Sides  
   
  We consider a variant of MinSet under uncertainty, where the right-hand sides bS of the ILP representation (MinSetIP) are deterministic and explicit part of the input. Thus, only the coeÔ¨Écients ai = (wi ‚àí Li ) remain uncertain within the interval (0, Ui ‚àí Li ). For this variant, the instance might have no feasible solution. In that case, we require every algorithm (incl. OPT) to reduce the covering requirements as much as possible. Recall that in the stochastic setting i ]. the balancing parameter is œÑ = minIi ‚ààI œÑi for œÑi = P[wi ‚â• Ui +L 2 Theorem 3. There is a polynomial-time algorithm for MinSet under uncertainty with deterministic right-hand sides and a competitive ratio of œÑ2 ¬∑ œÅ(Œ≥) ‚àà O( œÑ1 ¬∑ log m) with Œ≥ = 2/smin for smin = minIi ‚ààI : Ui ‚àíLi >0 Ui ‚àí Li . The algorithm loses only a factor œÑ2 compared to the greedy approximation factor œÅ(Œ≥) on the oÔ¨Ñine problem. We show the theorem by proving that Algo2 with rithm 1 is an (Œ±, Œ≤, Œ≥)-Greedy algorithm for Œ± = 2, Œ≤ = œÑ1 and Œ≥ = smin smin = minIi ‚ààI : Ui ‚àíLi >0 Ui ‚àí Li . Using Theorem 2, this implies the theorem. The algorithm scales the instance by factor Œ≥; a and b refer to the scaled coeÔ¨Écients. The idea is to execute the OÔ¨Ñine Algorithm under the assumption that ai = Ui ‚àí Li (and ai = Œ≥ai ) for all Ii ‚àà I that were not yet added to the solution. As ai = (wi ‚àí Li ) ‚àà (0, Ui ‚àí Li ), this means that we assume ai to have the largest possible value. Consequently, smin is the smallest (non-zero) coeÔ¨Écient ai under this assumption. The algorithm computes the greedy choice based on the optimistic greedy values  g¬Øc (Q, Ii ) = bS (Q) ‚àí max{0, bS (Q) ‚àí Œ≥(Ui ‚àí Li )} S‚ààS : Ii ‚ààS  
   
  (if b (Q) ‚â• 1) and g¬Øs (Q, Ii ) = |{S ‚àà S : Ii ‚àà S | bS (Q) > 0 ‚àß bS (Q) ‚àí Œ≥(Ui ‚àí Li ) ‚â§ 0}|  
   
  Set Selection Under Explorable Stochastic Uncertainty  
   
  327  
   
  (otherwise). These are the greedy values under the assumption ai = Ui ‚àí Li . We call them optimistic as they might overestimate but never underestimate the actual greedy values. For subsets G ‚äÜ I, we deÔ¨Åne g¬Øs (Q, G) and g¬Øc (Q, G) analogously. In contrast to gs and gc , Algorithm 1 has suÔ¨Écient information to compute g¬Øs and g¬Øc , and the best greedy choice based on the optimistic greedy values. The algorithm is designed to Ô¨Ånd, in each iteration, an element Ii with wi ‚àí i Li ‚â• Ui ‚àíL . We show that (i) this ensures that each iteration 2-approximates 2 the greedy choice and (ii) that Ô¨Ånding such an element takes only œÑ1 tries in expectation. This suÔ¨Éces to apply Theorem 2. i) implies wi ‚àí Li ‚â• 12 (Ui ‚àí Li ) To show (ii), we can observe that wi ‚â• (Ui +L 2 (Ui +Li ) and that P[wi ‚â• ] ‚â• œÑ holds by assumption. Thus, we Ô¨Ånd an interval Ii 2 satisfying wi ‚àí Li ‚â• 12 (Ui ‚àí Li ) with probability at least œÑ . This implies that, given an iteration of the while-loop is started, it in expectation takes œÑ1 tries to Ô¨Ånd such an interval Ii . This is exactly the second property of DeÔ¨Ånition 2. To prove (i), we use the next lemma, which shows that the optimistic greedy value of an Ii with wi ‚àí Li ‚â• 12 (Ui ‚àí Li ) is close to the actual greedy value. 2 Lemma 2. Consider an instance of (MinSetIP) scaled by Œ≥ = smin and some Q ‚äÜ I. If wi ‚àí Li ‚â• (Ui ‚àí Li )/2 for an Ii ‚àà I \ Q, then gc (Q, Ii ) ‚â• g¬Øc (Q, Ii )/2. If additionally b (Q) < 1, then gs (Q, Ii ) = g¬Øs (Q, Ii )  
   
  Proof. The statement regarding gc and g¬Øc holds directly by deÔ¨Ånition. To show the second statement, we use the assumption b (Q) < 1 and the choice of Œ≥. i , we have From b (Q) < 1 follows bS (Q) < 1 for all S ‚àà S. As wi ‚àí Li ‚â• Ui ‚àíL 2 Ui ‚àíLi smin 2  ai = wi ‚àí Li ‚â• 2 ‚â• 2 and, therefore, ai = Œ≥ai = smin ai ‚â• 1. This means that adding Ii to the solution satisÔ¨Åes all constraints for sets S with Ii ‚àà S that are not yet satisÔ¨Åed by Q. Thus, the optimistic greedy value g¬Øs (Q, Ii ) and the real greedy value gs (Q, Ii ) are the same, i.e., g¬Øs (Qj , Ii ) = gs (Q, Ii ), as adding Ii cannot satisfy more constraints even if the coeÔ¨Écient ai was Ui ‚àí Li .  
   
  As the algorithm always queries the interval Ii with the best optimistic greedy value, Lemma 2 shows that the last query of the iteration 2-approximates the greedy choice after querying the set Q of all previous queries. This implies (i) and Property 1 of DeÔ¨Ånition 2. By Theorem 2 this suÔ¨Éces to prove Theorem 3.  
   
  4  
   
  MinSet Under Uncertainty  
   
  We consider MinSet under uncertainty and prove the following main result. Theorem 4. There is a polynomial-time algorithm for MinSet under uncertainty with a competitive ratio of O( œÑ1 log m ¬∑ œÅ¬Ø(Œ≥)) ‚äÜ O( œÑ1 ¬∑ log2 m) with Œ≥ = 2/smin for smin = minIi ‚ààI : Ui ‚àíLi >0 Ui ‚àí Li . Exploiting Theorem 2, we prove the statement by providing Algorithm 2 and showing that it is an (Œ±, Œ≤, Œ≥)-Greedy algorithm for Œ± = 2, Œ≥ = 2/smin and  
   
  328  
   
  N. Megow and J. Schl√∂ter  
   
  Œ≤ = œÑ1 (log1.5 (m ¬∑ 2(maxIi ‚ààI (Ui ‚àí Li ))/smin ) + log2 (m)). Note that Œ± and Œ≥ are deÔ¨Åned as in the previous section and will be used analogously. For Œ≤ on the other hand, we require a larger value to adjust for the additional uncertainty in the right-hand sides bS = w‚àó ‚àí LS as the minimum set value w‚àó is unknown. Notice that we do not have suÔ¨Écient information to execute Algorithm 1, since we need the right-hand side values to compute even the optimistic greedy values. To handle this additional uncertainty, we want to ensure that each iteration of our algorithm Œ±-approximates the greedy choice for each possible value of w‚àó . To do so, we compute and query the best optimistic greedy choice for several carefully selected possible values w‚àó . To state our algorithm, we deÔ¨Åne a parametrized variant of (MinSetIP) that states the problem under the assumptions that w‚àó = w for some w and that the set Q ‚äÜ I has already been queried. The coeÔ¨Écients  are scaled to ai = (2/smin )(wi ‚àí Li ) and    b S (Q, w) = max{(2/smin )(w ‚àí LS ) ‚àí Ii ‚ààQ‚à©S ai , 0}. As before, let b (Q, w) =  S‚ààS bS (Q, w).  min Ii ‚ààI\Q xi    s.t. (MinSetIP-Qw) Ii ‚ààS\Q xi ¬∑ ai ‚â• bS (Q, w) ‚àÄS ‚àà S xi ‚àà {0, 1} ‚àÄIi ‚àà I As the right-hand sides are unknown, we deÔ¨Åne the greedy values for every possible value w for w‚àó . To that end, let gc (Q, Ii , w) = b (Q, w) ‚àí b (Q ‚à™ {Ii }, w) and gs (Q, Ii , w) = A(Q, w) ‚àí A(Q ‚à™ {Ii }, w), where A(Q, w) = |{S ‚àà S | bS (Q, w) > 0}|. As before, gc (Q, Ii , w) and gs (Q, Ii , w) describe how much adding Ii to the solution reduces the sum of right-hand sides and the number of nonsatisÔ¨Åed constraints, respectively; now under the assumption that w‚àó = w. The optimistic greedy values g¬Øc (Q, Ii , w) and g¬Øs (Q, Ii , w) for an Ii ‚àà I are deÔ¨Åned analogously but again assume that ai = Ui ‚àí Li . Similar to Algorithm 1, we would like to repeatedly compute and query the i (cf. best optimistic greedy choice until the queried Ii satisÔ¨Åes wi ‚àí Li ‚â• Ui ‚àíL 2 the repeat-statement). However, we cannot decide which greedy value, g¬Øc or g¬Øs , to use as deciding whether bS (Q, w‚àó ) < 1 depends on the unknown w‚àó . Instead, we compute and query the best optimistic greedy choice for both greedy values (cf. the for-loop). Even then, the best greedy choice still depends on the unknown right-hand sides. Thus, we compute and query the best optimistic greedy choice for several carefully selected values w (cf. the inner while-loop) to make sure that the queries of the iteration approximate the greedy choice for every possible w‚àó . Additionally, we want to ensure that we use at most Œ≤ queries in expectation. Consider an iteration of the outer while-loop with g = g¬Øc , and let Q denote the set of queries that were executed before the start of the iteration. Since we only care about the greedy value gc if there exists some S ‚àà S with bS (Q) > 1 (otherwise we use g¬Øs and gs instead), we assume that this is the case. If not, we use a separate analysis for the for-loop iteration with g = g¬Øs . ¬Ø that 2-approximates Our goal for the iteration is to query a set of intervals Q the best greedy choice I ‚àó after querying Q, i.e., it has a greedy value ¬Ø Q, w‚àó ) ‚â• 1 gc (I ‚àó , Q, w‚àó ). To achieve this for the unknown w‚àó , the algogc (Q, 2 rithm uses the parameter d, which is initialized with 1 (cf. Line 5), the minimum  
   
  Set Selection Under Explorable Stochastic Uncertainty  
   
  329  
   
  Algorithm 2: Algorithm for MinSet under uncertainty. 1 2 3 4 5 6 7 8 9 10 11 12 13 14  
   
  Input: Instance of MinSet under uncertainty. Scale all coeÔ¨Écients with Œ≥ = 2/smin for smin = minIi ‚ààI : (Ui ‚àíLi )>0 (Ui ‚àí Li ); Q ‚Üê ‚àÖ, wmin ‚Üê minimum possible value w‚àó (keep up-to-date); while the problem is not solved do foreach g from the ordered list g¬Øc , g¬Øs do d ‚Üê 1; Q ‚Üê Q; if g = g¬Øc then wmax ‚Üê max possible value w‚àó ; else wmax ‚Üê max w s.t. bS (Q, w) < 1 for all S ‚àà S; while ‚àÉwmin ‚â§ w ‚â§ wmax such that maxIh ‚ààI\Q g(Q, Ih , w) ‚â• d do repeat w ‚Üê min wmin ‚â§ w ‚â§ wmax s.t. maxIh ‚ààI\Q g(Q, w, Ih ) ‚â• d ; Ii ‚Üê arg maxIh ‚ààI\Q g(Q, Ih , w); Query Ii ; Q ‚Üê Q ‚à™ {Ii }; U ‚àíL Q1/2 ‚Üê {Ij ‚àà Q \ Q | wj ‚àí Lj ‚â• j 2 j };  if g = g¬Øc then d ‚Üê gc (Q , Q1/2 , w) else d ‚Üê gs (Q , Q1/2 , w); i until wi ‚àí Li ‚â• Ui ‚àíL or w ‚â§ wmax : maxIh ‚ààI\Q g(Q, w, Ih ) ‚â• d ; 2  
   
  possible value for g¬Øc (I ‚àó , Q, w‚àó ) under the assumption that there exists some S ‚àà S with bS (Q) > 1. In an iteration of the inner while-loop, the algorithm repeatedly picks the minimal value w such that the best current optimistic greedy choice has an optimistic greedy value of at least d (cf. Line 10). If no such value exists, then the loop terminates (cf. Lines 8, 14). Afterwards, it queries the corresponding best optimistic greedy choice Ii for the selected value w (cf. Line 11). Similar to the algorithms of the previous section, this is done repeatedly until wi ‚àí Li ‚â• (Ui ‚àí Li )/2. The key idea to achieve the 2-approximation with an expected number of queries that does not exceed Œ≤, is to always reset the value d to gc (Q , Q1/2 , w), where Q1/2 is the subset of all intervals Ij that have already been queried in the current iteration of the outer while-loop and satisfy wj ‚àí Lj ‚â• (Uj ‚àí Lj )/2 (cf. Lines 12, 13). This can be seen as an implicit doubling strategy to search for an unknown value. It leads to an exponential increase of d over the iterations of the inner while-loop, which will allow us to bound their number. With the following lemma, we prove that this choice of d also ensures that the queries of the iteration indeed 2-approximate the best greedy choice for w‚àó if there exists a S ‚àà S with bS (Q , w‚àó ) ‚â• 1. If there is no such set, we can use a similar proof. For an iteration j of theouter while-loop, let Gj be the set of queries during the iteration and let Qj = j  0 m for all Œ≤ ‚àà D \{Œ≤ 0 }. The point (x0 , y0 ) is called an exposing point of Œì (Œ≤ 0 ) x‚àí Œ≤  y ‚â• 0. In terms of Theorem 5, an exposing point is equivalent to an exposing ‚àû sequence for Œì (Œ≤ 0 ) x‚àíŒ≤  0 y ‚â• 0 deÔ¨Åned by the constant sequence ((x0 , y0 ))t=1 . m Let Œ≤ 0 ‚àà D and set (x0 , y0 ) := (Œì (Œ≤ 0 ), Œ≤ 0 ). It is easy to verify that m (x0 , y0 ) ‚àà Q and Œì (Œ≤ 0 ) x0 ‚àí Œ≤  0 y0 = 0. Furthermore, for each Œ≤ ‚àà D \ {Œ≤ 0 },   we have Œì (Œ≤) x0 ‚àí Œ≤ y0 > 0 because Œì is strictly non-expansive. Thus, Theorem 3.1 in [24] implies that CŒì is a maximal Q-free set. The existence of an exposing point immediately implies that CŒì is full-dimensional. For example, any strict convex combination of two exposing points is in the interior of CŒì .  
   
  6  
   
  Preliminary Results on Non-expansive Functions  
   
  In this section we collect a variety of lemmata to prove the main theorems. Throughout this section, assume Œì : Dm ‚Üí Dn is non-expansive. Lemma 2. Let I ‚äÜ Dm be a finite set of pairwise isometric points. The following properties hold true:   Œ≤ ‚â• 0 for each Œ≤ ‚àà I, then Œ≤‚ààI Œ≤ Œì (Œ≤) ‚àà Dn 1. If Œ≤‚ààI Œ≤ Œ≤ ‚àà Dm , where  and Œì ( Œ≤‚ààI Œ≤ Œ≤) = Œ≤‚ààI Œ≤ Œì (Œ≤).  2. If Œ≤‚ààI Œ≤ Œì (Œ≤) ‚àà Dn , where Œ≤ ‚â• 0 for each Œ≤ ‚àà I, then Œ≤‚ààI Œ≤ Œ≤ ‚àà Dm and Œì ( Œ≤‚ààI Œ≤ Œ≤) = Œ≤‚ààI Œ≤ Œì (Œ≤).  :=  Proof. Set Œ≤ Œ≤‚ààI Œ≤ Œ≤. Using the isometry of points in I, we have   
   
    
   
  2 Œ≤‚ààI Œ≤ Œì (Œ≤)  
   
  =  
   
    
   
     Œ≤,Œ≤  ‚ààI Œ≤ Œ≤ Œì (Œ≤) Œì (Œ≤ )  
   
  =  
   
    
   
   Œ≤,Œ≤  ‚ààI Œ≤ Œ≤ Œ≤  
   
    
   
   2. Œ≤  = Œ≤  
   
   ‚àà Dm , so the previous equation In the case ofProperty 1, we assume Œ≤ proves that  Œ≤‚ààI Œ≤ Œì (Œ≤) = 1. In the case of Property 2, we assume   ‚àà Dm . There Œ≤‚ààI Œ≤ Œì (Œ≤)2 = 1, so the previous equation proves that Œ≤   = fore, it remains to show that Œì (Œ≤) Œ≤‚ààI Œ≤ Œì (Œ≤) in both cases; we prove these simultaneously.  
   
  Towards a Characterization of Maximal Quadratic-Free Sets  
   
  343  
   
  Using the non-expansive property of Œì and the nonnegativity of Œ≤ , we have =  Œ≤ 1=Œ≤  
   
    
   
          Œ≤ ‚â§  Œ≤‚ààI Œ≤ Œì (Œ≤) Œì (Œ≤) = Œì (Œ≤) Œ≤‚ààI Œ≤ Œì (Œ≤) .  
   
  Œ≤‚ààI Œ≤ Œ≤  
   
    ( The Cauchy-Schwarz inequality implies that 1 = Œì (Œ≤) Œ≤‚ààI Œ≤ Œì (Œ≤)). Since  = both vectors have unit norm, we conclude that Œì (Œ≤)   Œ≤‚ààI Œ≤ Œì (Œ≤). The following lemma is helpful when analyzing full-dimensional Q-free sets. Lemma 3. Define CŒì  as in (2). Assume CŒì is full-dimensional and let I ‚äÜ m be a finite set. If D Œ≤‚ààI ŒªŒ≤ Œ≤ = 0, where ŒªŒ≤ > 0 for each Œ≤ ‚àà I, then  Œª Œì (Œ≤) =  0. Œ≤ Œ≤‚ààI  Proof. Assume to the contrary that Œ≤‚ààI ŒªŒ≤ Œì (Œ≤) = 0. Fix Œ≤  ‚àà I. We have   Œ≤  = ‚àí Œ≤‚ààI\{Œ≤ } (ŒªŒ≤/ŒªŒ≤ )Œ≤ and Œì (Œ≤  ) = ‚àí Œ≤‚ààI\{Œ≤ } (ŒªŒ≤/ŒªŒ≤ )Œì (Œ≤). The following inequalities are both valid for CŒì :   
   
    
   
   ( Œ≤‚ààI\{Œ≤ }  Œì (Œ≤  ) x ‚àí Œ≤   y = ‚àí( Œ≤‚ààI\{Œ≤ }  
   
  ŒªŒ≤ ŒªŒ≤ ŒªŒ≤ ŒªŒ≤  
   
   Œì (Œ≤)) x ‚àí ( Œ≤‚ààI\{Œ≤ }  Œì (Œ≤)) x + ( Œ≤‚ààI\{Œ≤ }  
   
  ŒªŒ≤ ŒªŒ≤ ŒªŒ≤ ŒªŒ≤  
   
  Œ≤) y ‚â• 0 Œ≤) y ‚â• 0.  
   
  Thus, CŒì satisÔ¨Åes an equation contradicting that it is full-dimensional.  
   
     
   
  The next lemma, which is known from convexity theory, will allow us to simplify the description of CŒì in the proofs of Theorems 3 and 4. The proof follows from Theorem 17.3 [26]. Lemma 4. Define CŒì as in (2). If CŒì is full-dimensional and Œ≥  x ‚àí Œ≤  y ‚â• 0 is valid for CŒì , then (Œ≥, Œ≤) ‚àà cone({(Œì (Œ≤), Œ≤) : Œ≤ ‚àà Dn }). Our Ô¨Ånal lemma states that if an inequality Œì (Œ≤) x ‚àí Œ≤  y ‚â• 0 is implied by other inequalities of the same form indexed by I ‚äÜ Dm , then Œ≤ must be isometric with Œ≤ ‚àà I. This will be used in the proof of Theorem 4 to help establish that we have a covering of Dm by isometric points. m Lemma 5. Let Œ≤ ‚àà D , I ‚äÜ Dm be a finite set, and ŒªŒ≤ > 0 for each Œ≤ ‚àà I be such that (Œì (Œ≤), Œ≤) = Œ≤‚ààI ŒªŒ≤ (Œì (Œ≤), Œ≤). Then Œ≤ and Œ≤ are isometric for each Œ≤ ‚àà I.  
   
  Proof. Notice that    ŒªŒ≤ Œì (Œ≤)  Œì (Œ≤) ‚àí Œ≤ Œ≤‚ààI ŒªŒ≤ Œ≤      = Œ≤‚ààI ŒªŒ≤ Œì (Œ≤) Œì (Œ≤) ‚àí Œ≤ Œ≤ ,  
   
  0 = Œì (Œ≤) Œì (Œ≤) ‚àí Œ≤  Œ≤ =  
   
    
   
  Œ≤‚ààI  
   
  where the Ô¨Årst equality follow from Œ≤ ‚àà Dm and Œì (Œ≤) ‚àà Dn . Due to the nonexpansiveness of Œì , every summand is non-negative. Since the sum is 0, every   summand must be 0. As ŒªŒ≤ > 0, we conclude that Œì (Œ≤) Œì (Œ≤) = Œ≤  Œ≤.  
   
  344  
   
  7  
   
  G. MuÀú noz et al.  
   
  A Proof of Theorem 3  
   
  From our discussion in Section 1.1, we know the set CŒì is always Q-free. Since we assume CŒì is a polyhedron, it admits a Ô¨Ånite description using facet inequalities. By applying Lemma 4 to each facet inequality, we can assume that there is a Ô¨Ånite set I ‚äÜ Dm such that CŒì = {(x, y) ‚àà Rn √ó Rm : Œì (Œ≤) x ‚àí Œ≤  y ‚â• 0 ‚àÄ Œ≤ ‚àà I}. We assume that Œì (Œ≤) x ‚àí Œ≤  y ‚â• 0 deÔ¨Ånes a facet of CŒì for each Œ≤ ‚àà I. We will prove that this representation of CŒì suÔ¨Éces to prove maximality in Theorem 3 using Theorem 5. To this end, let Œ≤ ‚àà I. According to Theorem 5, it suÔ¨Éces to show that Œì (Œ≤) x ‚àí Œ≤  y ‚â• 0 has an exposing sequence. For t ‚àà N, deÔ¨Åne xt := Œì (Œ≤) +  
   
  ‚àö 2t+1 t  
   
  Œì  
   
  and  
   
  yt := (1 + 1t ) Œ≤,  
   
  (6)  
   
  where Œì ‚àà Dn will be chosen in Claim 1 so that Œì (Œ≤) Œì = 0. Using this property, the inclusion Œ≤ ‚àà Dm and Œì , Œì (Œ≤) ‚àà Dn , we see that (xt , yt ) ‚àà Q. Consider a bounded sequence of inequalities Œ≥ t x ‚àí Œ±t y ‚â• 0, where t ‚àà N, that are satisÔ¨Åed by points in CŒì and such that 0 ‚â• Œ≥ t xt ‚àí Œ±t yt . By the Farkas Lemma, there exist numbers ŒªŒ≤,t ‚â• 0 for each Œ≤ ‚àà I such that  (Œ≥ t , Œ±t ) = Œ≤‚ààI ŒªŒ≤,t (Œì (Œ≤), Œ≤). (7) After normalizing (Œ≥ t , Œ±t ), we may assume (Œ≥ t , Œ±t ) = (Œì (Œ≤), Œ≤) for all t. Furthermore, according to Carath¬¥eodory‚Äôs theorem we may assume that for each t the set {(Œì (Œ≤), Œ≤) : ŒªŒ≤,t > 0} is linearly independent. Consequently, there exists œÑ > 0 such that ŒªŒ≤,t ‚â§ œÑ for each Œ≤ ‚àà I and t ‚àà N. In order to demonstrate that ((xt , yt ))‚àû t=1 is an exposing sequence, we prove  limt‚Üí‚àû Œ≤‚ààI ŒªŒ≤,t (Œì (Œ≤), Œ≤) = (Œì (Œ≤), Œ≤). To this end, it suÔ¨Éces to prove limt‚Üí‚àû ŒªŒ≤,t = 0 for each Œ≤ ‚àà I \ {Œ≤}. We will choose Œì so that this condition is met. Note that ‚àö    0 ‚â• Œ≥ t xt ‚àíŒ±t yt = Œ≤‚ààI ŒªŒ≤,t (Œì (Œ≤) Œì (Œ≤)‚àíŒ≤  Œ≤)+ 2t+1 Œì  Œì (Œ≤)‚àí 1t Œ≤  Œ≤ . t Multiplying through by t, we have   ‚àö  0 ‚â• Œ≤‚ààI ŒªŒ≤,t t(Œì (Œ≤) Œì (Œ≤) ‚àí Œ≤  Œ≤) + 2t + 1 Œì  Œì (Œ≤) ‚àí Œ≤  Œ≤ .  
   
  (8)  
   
  Claim 1. Œì can be chosen such that Œì  Œì (Œ≤) = 0 and for each Œ≤ ‚àà I \ {Œ≤} ‚àö limt‚Üí‚àû t(Œì (Œ≤) Œì (Œ≤) ‚àí Œ≤  Œ≤) + 2t + 1 Œì  Œì (Œ≤) ‚àí Œ≤  Œ≤ = ‚àû. Proof of Claim. Regardless of Œì , if Œ≤ is such that Œì‚àö(Œ≤) Œì (Œ≤) ‚àí Œ≤  Œ≤ > 0, then the limit goes to ‚àû because the term t dominates 2t + 1.  
   
  Towards a Characterization of Maximal Quadratic-Free Sets  
   
  345  
   
  In what remains, we need to choose Œì so that Œì (Œ≤) Œì > 0 for all Œ≤ ‚àà I \{Œ≤} satisfying Œì (Œ≤) Œì (Œ≤) ‚àí Œ≤  Œ≤ ‚àö = 0. If we establish this, then the limit tends to inÔ¨Ånity because of the term 2t + 1. DeÔ¨Åne J := {Œ≤ ‚àà I \ {Œ≤} : Œì (Œ≤) Œì (Œ≤) ‚àí Œ≤  Œ≤ = 0}. We consider two cases: whether Œì (Œ≤) ‚àà cone(Œì (J)) or not. Case 1. Assume Œì (Œ≤) ‚àà cone{Œì (Œ≤) : Œ≤ ‚àà J}. DeÔ¨Åne K := cone(Œì (J ‚à™ {Œ≤})). If K is not pointed, then 0 ‚àà Rn is a non-trivial conic combination of the generators of K. By Lemma 2, 0 ‚àà Rm can be obtained using the same conic multipliers applied to vectors in J ‚à™ {Œ≤}. However, Lemma 3 implies that CŒì is not full-dimensional, which is a contradiction. Hence, K is pointed. The fact that J is Ô¨Ånite together with Œì (Œ≤) ‚àà cone{Œì (Œ≤) : Œ≤ ‚àà J} implies that Œì (Œ≤) generates an extreme ray of K and there is no Œ≤ ‚àà J that generates the same extreme ray. Thus, by the separating hyperplane theorem there exists some Œì ‚àà Dn such that Œì  Œì (Œ≤) = 0 < Œì  Œì (Œ≤) for all Œ≤ ‚àà J, as desired. Case 2. Assume Œì (Œ≤) ‚àà cone{Œì (Œ≤) : Œ≤ ‚àà J}. Thereexists a set H ‚äÜ J and numbers Œ≤ > 0 for each Œ≤ ‚àà H such that Œì (Œ≤) = Œ≤‚ààH Œ≤ Œì (Œ≤). DeÔ¨Åne  :=  Œ≤ Œ≤. Using the non-expansive property of Œì , we have Œ≤ Œ≤‚ààH  
   
   2= Œ≤  
   
    
   
   Œ≤,Œ≤  ‚ààH Œ≤ Œ≤ Œ≤  
   
    
   
  Œ≤ ‚â§  
   
    
   
     Œ≤,Œ≤  ‚ààH Œ≤ Œ≤ Œì (Œ≤) Œì (Œ≤ )  
   
  = Œì (Œ≤)2 = 1.  
   
  Using the isometry with Œ≤ and each Œ≤ ‚àà H, we then have      Œ≤ =  Œ≤ Œ≤‚ààH Œ≤ Œ≤ Œ≤ = Œ≤‚ààH Œ≤ Œì (Œ≤) Œì (Œ≤) = Œì (Œ≤) Œì (Œ≤) = 1.   Œ≤| ‚â§ Œ≤Œ≤  Thus, we have equality in the Cauchy-Schwarz inequality |Œ≤ ‚â§ 1,  1   so Œ≤ = Œ≤ . Thus, (Œì (Œ≤), Œ≤) = Œ≤‚ààH Œ≤ (Œì (Œ≤), Œ≤) contradicting that Œì (Œ≤) x ‚àí Œ≤  y ‚â• 0 deÔ¨Ånes a facet of CŒì . Choose Œì ‚àà Dn according to Claim 1. For each t ‚àà N, we have   ‚àö ŒªŒ≤,t t(Œì (Œ≤) Œì (Œ≤) ‚àí Œ≤  Œ≤) + 2t + 1 Œì  Œì (Œ≤) ‚àí Œ≤  Œ≤ = ‚àíŒªŒ≤,t ‚â• ‚àíœÑ. Together with (8), this implies   ‚àö  0 ‚â• ‚àíœÑ + Œ≤‚ààI\{Œ≤} ŒªŒ≤,t t(Œì (Œ≤) Œì (Œ≤) ‚àí Œ≤  Œ≤) + 2t + 1 Œì  Œì (Œ≤) ‚àí Œ≤  Œ≤ . (9) For Œ≤ ‚àà I \ {Œ≤}, if ŒªŒ≤,t does not go to 0 as t tends to ‚àû, then Claim 1 implies that the righthand side of (9) will go to ‚àû, which is a contradiction. Hence, (7) tends to (Œì (Œ≤), Œ≤) as t tends to ‚àû.   Remark 2. As we mentioned in Section 1.1, we conjecture that Theorem 3 is generalizable to a set CŒì that is not necessarily a polyhedron. With this in mind, a natural question is how reliant on polyhedrality the proof of this section is. Various points of the proof can be adapted to handle a non-polyhedral case: for example, a similar expression to (7) can be obtained for an inÔ¨Ånite I. However, one the key steps that heavily uses Ô¨Åniteness is the construction of Œì using a strict separating hyperplane in Case 1 of Claim 1. It is not clear if such Œì exists in a general case, and the proof may need a diÔ¨Äerent approach.  
   
  346  
   
  8  
   
  G. MuÀú noz et al.  
   
  A Proof of Theorem 4  
   
  (‚áê) We show that if Œ≤ ‚àà Dm \ I, then Œì (Œ≤) x ‚àí Œ≤  y ‚â• 0 is implied by the inequalities indexed by I. Let Œ≤ ‚àà Dm \I. By assumption, there exists a set J ‚äÜ I of pairwise isometric points satisfying  Œ≤ ‚àà cone(J). Hence, thereexist ŒªŒ≤ ‚â• 0 for each Œ≤ ‚àà J such that Œ≤ = Œ≤‚ààJ ŒªŒ≤ Œ≤. We have Œì (Œ≤) = Œ≤‚ààJ ŒªŒ≤ Œì (Œ≤) by Lemma 2. This shows that (Œì (Œ≤), Œ≤) ‚àà cone({(Œì (Œ≤), Œ≤) : Œ≤ ‚àà J}). Hence, Œì (Œ≤) x ‚àí Œ≤  y ‚â• 0 is implied by the inequalities indexed by I. (‚áí) CŒì is a polyhedron, so by Lemma 4 there is a Ô¨Ånite representation   CŒì = (x, y) ‚àà Rn √ó Rn : Œì (Œ≤) x ‚àí Œ≤  y ‚â• 0 ‚àÄ Œ≤ ‚àà I . Let Œ≤ ‚àà Dm \ I. The inequality Œì (Œ≤) x ‚àí Œ≤  y ‚â• 0 is valid for CŒì , so there exists a set J ‚äÜ I and positive coeÔ¨Écients ŒªŒ≤ for each Œ≤ ‚àà J such that  (Œì (Œ≤), Œ≤) = Œ≤‚ààJ ŒªŒ≤ (Œì (Œ≤), Œ≤). Lemma 5 states that Œ≤  Œ≤ = Œì (Œ≤) Œì (Œ≤) for all Œ≤ ‚àà J. For each Œ≤  ‚àà J, we have  Œì (Œ≤  ) Œì (Œ≤) ‚àí Œ≤   Œ≤ = Œ≤‚ààJ ŒªŒ≤ (Œì (Œ≤  ) Œì (Œ≤) ‚àí Œ≤   Œ≤). The left-hand side is 0 because Œ≤ and Œ≤  are isometric, and every summand on the right-hand side is nonnegative because ŒªŒ≤ > 0 and Œì (Œ≤  ) Œì (Œ≤) ‚àí Œ≤   Œ≤ ‚â• 0 by the non-expansive property of Œì . Hence, Œì (Œ≤  ) Œì (Œ≤) = Œ≤   Œ≤ for all Œ≤ ‚àà J. As Œ≤  was arbitrarily chosen in J, we see that all elements of J are pairwise isometric and Œ≤ ‚àà cone(J). Acknowledgements. The second author was supported by a Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant [RGPIN-202102475]. The authors would like to thank the three anonymous reviewers for their valuable feedback.  
   
  References 1. Andersen, K., Jensen, A.N.: Intersection cuts for mixed integer conic quadratic sets. In: Goemans, M., Correa, J. (eds.) IPCO 2013. LNCS, vol. 7801, pp. 37‚Äì48. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-36694-9 4 2. Andersen, K., Louveaux, Q., Weismantel, R., Wolsey, L.: Cutting planes from two rows of the simplex tableau. In: Proceedings of Integer Programming and Combinatorial Optimization (IPCO), pp. 1‚Äì15 (2007) 3. Averkov, G.: A proof of Lov¬¥ asz‚Äôs theorem on maximal lattice-free sets. Contrib. Algebra Geom. (2013) 4. Averkov, G., Basu, A., Paat, J.: Approximation of corner polyhedra with families of intersection cuts. SIAM J. Optim. 28(1), 904‚Äì929 (2018) 5. Averkov, G.: On maximal s-free sets and the Helly number for the family of s-convex sets. SIAM J. Discret. Math. 27(3), 1610‚Äì1624 (2013)  
   
  Towards a Characterization of Maximal Quadratic-Free Sets  
   
  347  
   
  6. Baes, M., Oertel, T., Weismantel, R.: Duality for mixed-integer convex minimization. Math. Program. 158, 547‚Äì564 (2016) 7. Balas, E.: Intersection cuts - a new type of cutting planes for integer programming. Oper. Res. (1971) 8. Barvinok, A.: A course in convexity. Am. Math. Soc. (2002) 9. Basu, A., Conforti, M., Cornu¬¥ejols, G., Weismantel, R., Weltge, S.: Optimality certificates for convex minimization and Helly numbers. Oper. Res. Lett. 45(6), 671‚Äì674 (2017) 10. Basu, A., Dey, S., Paat, J.: Nonunique lifting of integer variables in minimal inequalities. SIAM J. Discret. Math. (2019) 11. Basu, A., Conforti, M., Cornu¬¥ejols, G., Zambelli, G.: Maximal lattice-free convex sets in linear subspaces. Math. Oper. Res. 35(3), 704‚Äì720 (2010) 12. Basu, A., Conforti, M., Cornu¬¥ejols, G., Zambelli, G.: Minimal inequalities for an infinite relaxation of integer programs. SIAM J. Discret. Math. 24(1), 158‚Äì168 (2010) 13. Bienstock, D., Chen, C., MuÀú noz, G.: Intersection cuts for polynomial optimization. In: Lodi, A., Nagarajan, V. (eds.) IPCO 2019. LNCS, vol. 11480, pp. 72‚Äì87. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-17953-3 6 14. Bienstock, D., Chen, C., MuÀú noz, G.: Outer-product-free sets for polynomial optimization and oracle-based cuts. Math. Program. 183, 105‚Äì148 (2020) 15. Chmiela, A., MuÀú noz, G., Serrano, F.: On the implementation and strengthening of intersection cuts for QCQPs. Math. Program. 1‚Äì38 (2022) 16. Conforti, M., Cornu¬¥ejols, G., Daniilidis, A., Lemar¬¥echal, C., Malick, J.: Cutgenerating functions and S-free sets. Math. Oper. Res. (2014) 17. Conforti, M., Cornu¬¥ejols, G., Zambelli, G.: A geometric perspective on lifting. Oper. Res. 59(3), 569‚Äì577 (2011) 18. Conforti, M., Cornu¬¥ejols, G., Zambelli, G.: Integer Programming. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-11008-0 19. Conforti, M., Summa, M.D.: Maximal s-free convex sets and the Helly number. SIAM J. Discret. Math. 30(4), 2206‚Äì2216 (2016) 20. Dey, S., Wolsey, L.: Two row mixed-integer cuts via lifting. Math. Program. 124, 143‚Äì174 (2010) 21. Lov¬¥ asz, L.: Geometry of numbers and integer programming. In: Iri, M., Tanabe, K. (eds.) Mathematical Programming: Recent Developments and Applications, pp. 177‚Äì201. Kluwer Academic Publishers, Amsterdam (1989) 22. Modaresi, S., Kƒ±lƒ±n¬∏c, M., Vielma, J.: Intersection cuts for nonlinear integer programming convexification techniques for structured sets. Math. Program. (2016) 23. MuÀú noz, G., Serrano, F.: Maximal quadratic-free sets. In: Proceedings of the International Conference on Integer Programming and Combinatorial Optimization, pp. 307‚Äì321 (2020) 24. MuÀú noz, G., Serrano, F.: Maximal quadratic-free sets. Math. Program. 192, 229‚Äì 270 (2022) 25. Paat, J., Schl¬® oter, M., Speakman, E.: Constructing lattice-free gradient polyhedra in dimension two. Math. Program. 192(1), 293‚Äì317 (2022) 26. Rockafellar, R.T.: Convex Analysis. Princeton University Press, Princeton (1970) 27. Tuy, H.: Concave minimization under linear constraints with special structure. Dokl. Akad. Nauk SSSR 159, 32‚Äì35 (1964)  
   
  Compressing Branch-and-Bound Trees ¬¥ Gonzalo MuÀú noz1 , Joseph Paat2(B) , and Alinson S. Xavier3 1  
   
  2  
   
  Institute of Engineering Sciences, Universidad de O‚ÄôHiggins, Rancagua, Chile [email protected]  Sauder School of Business, University of British Columbia, Vancouver, BC, Canada [email protected]  3 Energy Systems and Infrastructure Analysis Division, Argonne National Laboratory, Lemont, IL, USA [email protected]   
   
  Abstract. A branch-and-bound (BB) tree certiÔ¨Åes a dual bound on the value of an integer program. In this work, we introduce the tree compression problem (TCP): Given a BB tree T that certiÔ¨Åes a dual bound, can we obtain a smaller tree with the same (or stronger) bound by either (1) applying a diÔ¨Äerent disjunction at some node in T or (2) removing leaves from T ? We believe such post-hoc analysis of BB trees may assist in identifying helpful general disjunctions in BB algorithms. We initiate our study by considering computational complexity and limitations of TCP. We then conduct experiments to evaluate the compressibility of realistic branch-and-bound trees generated by commonly-used branching strategies, using both an exact and a heuristic compression algorithm.  
   
  1  
   
  Introduction  
   
  Consider an integer linear programming (IP) problem min{c x : x ‚àà P ‚à© Zn },  
   
  (1)  
   
  where c ‚àà Qn and P := {x ‚àà Rn : Ax ‚â§ b} for A ‚àà Qm√ón and b ‚àà Qm . Primal bounds on (1) can be certiÔ¨Åed by integer feasible solutions z ‚àà P ‚à© Zn . Dual bounds on (1), on the other hand, are typically certiÔ¨Åed using branch-andbound (BB) trees. A BB tree is a graph-theoretical tree T where each node v corresponds to a polyhedron Q(v), with the root corresponding to P. Moreover, v is either a leaf, or it has exactly two children corresponding to the polyhedra deÔ¨Åned by applying a disjunction (œÄ  x ‚â§ œÄ0 )‚à®(œÄ  x ‚â• œÄ0 +1) to Q(v), where we call œÄ ‚àà Zn the branching direction and œÄ0 ‚àà Z. If we solve the corresponding linear programs over all leaves of T , then the smallest value obtained over all leaves yields a dual bound for (1). See Sect. 2 for a formal deÔ¨Ånition of BB trees and the dual bound. In order to generate a BB tree, one must identify a strategy for selecting a leaf of the tree and a strategy for selecting a disjunction to apply. See [22] for a survey on diÔ¨Äerent strategies. In practical implementations of the BB method, c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 348‚Äì362, 2023. https://doi.org/10.1007/978-3-031-32726-1_25  
   
  Compressing Branch-and-Bound Trees  
   
  349  
   
  the only allowed directions are typically {e1 , . . . , en }, in which case we say the algorithm uses variable disjunctions. However, many results explore the beneÔ¨Åt of additional directions: various subsets of {‚àí1, 0, 1}n are explored in [25,27,30]; directions derived from mixed integer Gomory cuts are explored in [9,19]; directions derived using basis reduction techniques are explored in [1,26]; Mahajan and Ralphs [23] solve a subproblem to Ô¨Ånd a disjunction that closes the duality gap by a certain amount. The largest set of directions is the set Zn , in which case the algorithm uses general disjunctions. Although a larger set of allowable directions provides more Ô¨Çexibility, it has been repeatedly veriÔ¨Åed that searching through this set during the execution of the algorithm can be computationally expensive [15,23]. The work in this paper is motivated by a diÔ¨Äerent approach to identify meaningful directions. Given a tree T produced using some set of allowable directions D ‚äÜ Zn , we ask if T can be ‚Äúcompressed‚Äù into a smaller tree with the same (or stronger) dual bound by using a potentially larger set of directions D ‚äá D, and a limited set of transformations. This post-hoc compression analysis is more restricted and allows one to use a global view of the tree to identify potentially meaningful branching directions, as opposed to the dynamic approach. We believe this compression question may help produce small trees to be used as better certiÔ¨Åcates [7] or as training data for learn-to-branch strategies. Related Work. To the best of our knowledge, this is the Ô¨Årst piece of work to study the tree compression problem. A related question is the minimum size of a BB tree certifying optimality or infeasibility of (1); we use some of these results in our own work. Chv¬¥ atal [8] and Jeroslow [18] gives examples of IPs that require a BB tree whose size is exponential in the number of variables n when only variable directions D = {e1 , . . . , en } are used to generate disjunctions. There are examples where an exponential lower bound in n cannot be avoided even with general disjunctions [10,11]. Basu et al. [3] consider the set Ds of directions whose support is at most s; they prove that if s ‚àà O(1), then a BB tree proving infeasibility of Jeroslow‚Äôs instance has exponential in n many nodes [3]. For an interesting perspective on provable upper bounds, Dey et al. [12] relate the size of BB trees generated using full strong branching and variable disjunctions to the additive integrality gap for certain classes of instances like vertex cover. For complexity results, Pfetsch et al. [16] show that is it NP-hard to Ô¨Ånd the smallest BB tree generated using only variable disjunctions. Mahajan and Ralphs [24] show that it is NP-complete to decide whether there is a general disjunction proving infeasibility at the root node. They also provide a MIP that can be solved at a node in a BB tree to yield a disjunction maximizing the dual bound improvement. The tree compression problem is a post-hoc analysis of a BB tree. A similar kind of analysis is done in backdoor branching, where one explores a tree T to Ô¨Ånd small paths from the root to the optimal solution with the ultimate aim to identify good branching decisions to make next time the algorithm is run on a similar IP [14,20]. The major diÔ¨Äerence between backdoor branching and the compression question is that the former only considers Ô¨Ånding a path  
   
  350  
   
  G. MuÀú noz et al.  
   
  existing in the tree while the latter considers how to modify a tree to create short paths. Another form of post-hoc analysis is tree balancing, where the goal is to transform a tree T proving integer infeasibility into a new tree with the same dual bound whose size is polynomial in |T | and whose depth is polylogarithmic in |T |; see, e.g., [4] for a discussion on balancing and stabbing planes. A major diÔ¨Äerence between the balancing question and the compression question is that the former is allowed to grow the tree along branches while the latter is not. Our Contributions. We introduce the tree compression problem in Sect. 2. In Theorem 1, we show that the problem is NP-Complete when D = Zn and c = 0. We then demonstrate in Theorem 2 that tree compression does not always give the smallest BB tree meeting a certain dual bound. In fact, we give an example of a BB tree T of size |T | ‚â• 2n+1 ‚àí 1 that cannot be compressed to a BB tree n with fewer than (2 ‚àí 1)/n nodes, but there is a diÔ¨Äerent BB tree with the same root and dual bound with only 7 nodes. These results appear in Sect. 3. We next provide extensive computational results on the compression problem. We look at BB trees from MIPLIB 3.0 [6] instances generated using full strong branching, the state-of-the-art variable branching strategy with respect to tree size, and reliability branching with plunging, often considered the state-of-the-art branching strategy with respect to running time. We Ô¨Årst compress these trees using a computationally-expensive exact algorithm based on a MIP formulation by Mahajan and Ralphs [23,24]. We then evaluate how much of this compression is achievable in a short amount of time, by applying a heuristic algorithm based on the iterative procedure introduced by Owen and Mehrota [27]. Overall, we see that many MIPLIB 3.0 trees can be signiÔ¨Åcantly compressed. Moreover, we Ô¨Ånd that the heuristic procedure achieves good compression. These algorithms and results are described in Sects. 4 and 5, respectively.  
   
  2  
   
  The Tree Compression Problem (TCP)  
   
  We deÔ¨Åne a branch-and-bound (BB) tree as a graph-theoretical rooted tree where each node v corresponds to a polyhedron Q(v), and the root node r corresponds to Q(r) = P. Furthermore, each node v is either a leaf, or it has exactly two children corresponding to the polyhedra Q(v) ‚à© {x ‚àà Rn : œÄ  x ‚â§ œÄ0 }  
   
  and Q(v) ‚à© {x ‚àà Rn : œÄ  x ‚â• œÄ0 + 1},  
   
  (2)  
   
  where œÄ ‚àà Zn is called the branching direction and œÄ0 ‚àà Z. The dual bound relative to c ‚àà Qn provided by a BB tree T is d(T, c) := minv‚ààL(T ) min{c x : x ‚àà Q(v)}, where L(T ) is the set of leaves of T . DeÔ¨Åne d(T, c) = ‚àû if Q(v) = ‚àÖ for each v ‚àà L(T ), and d(T, c) = ‚àí‚àû if x ‚Üí c x is unbounded from below over Q(v) for some v ‚àà L(T ). For simplicity, our deÔ¨Ånition allows BB trees that have multiple nodes corresponding to the same polyhedron, although such trees would typically not be generated by well-designed BB algorithms. We also do not require the tree  
   
  Compressing Branch-and-Bound Trees  
   
  351  
   
  to certify infeasibility or optimality of (1), to allow trees generated by partial (e.g. time- or node-limited) runs of the BB method. Let T be a BB tree and v ‚àà T be a non-leaf node. Our notion of compression is based on two operations on T . For (œÄ, œÄ0 ) ‚àà Zn √ó Z, let replace(T, v, œÄ, œÄ0 ) denote the BB tree obtained from T by replacing all descendants of v with the two new children deÔ¨Åned by applying the disjunction (œÄ  x ‚â§ œÄ0 ) ‚à® (œÄ  x ‚â• œÄ0 + 1) to Q(v), i.e., the two new children are the polyhedra in (2). We use drop(T, v) to denote the BB tree obtained from T by removing all descendants of v. We refer to the number of nodes in T as the size of T and denote it by |T |. We say that a BB tree T  is a compression of T if there exists a sequence of BB trees T1 = T, T2 , . . . , Tk = T  such that for each i ‚àà {2, . . . , k} we have that 1. Either Ti = drop(Ti‚àí1 , v) for some v ‚àà Ti‚àí1 , or Ti = replace(Ti‚àí1 , v, œÄ, œÄ0 ) for some v ‚àà Ti‚àí1 and (œÄ, œÄ0 ) ‚àà Zn √ó Z. 2. |Ti | < |Ti‚àí1 | and d(Ti , c) ‚â• d(Ti‚àí1 , c). Note that the deÔ¨Ånition of compression depends on the dual bound of T . Also, observe that the replacement operation only acts on non-leaf nodes and thus only produces children of non-leaf nodes. Consequently, leaf nodes of a BB tree will either remain leaf nodes or disappear from the tree during the compression process. Given that the replacement operation creates two new nodes that are leaves themselves, the previous discussion implies that any new (potentially dense) disjunctions introduced in the compression process appear near the bottom of the tree. See [5,15,28] for comments on potential drawbacks of dense inequalities. As an example of these deÔ¨Ånitions, consider P := [0, 1/5]2 and the following BB tree T (disjunctions are indicated on edges and polyhedra in the nodes): x1 ‚â§ 0  
   
  {0}  
   
  x2 ‚â§  
   
  0  
   
  P  
   
  x1 ‚â• 1  
   
  {0} √ó [0, 1/5] x 2 ‚â•1 ‚àÖ  
   
  ‚àÖ  
   
  x1  
   
  ‚â§0  
   
  ‚àÖ  
   
  x1  
   
  ‚â•1  
   
  ‚àÖ  
   
  Let c = (‚àí1, ‚àí1); we have d(T, c) = 0. We can compress T with the drop operation at the right child v2 of the root r; see Ô¨Ågure (a). We can compress T with the replace operation at the root with œÄ = ‚àíc and œÄ0 = 0; see Ô¨Ågure (b). It can be checked that d(drop(T, v2 ), c) = d(replace(T, r, œÄ, 0), c) = 0.  
   
  352  
   
  G. MuÀú noz et al.  
   
  x1  
   
  {0}  
   
  x2 ‚â§  
   
  0  
   
  ‚â§0 P  
   
  x1  
   
  ‚â•1  
   
  {0} √ó [0, 1/5] x 2 ‚â• 1  
   
  ‚àÖ  
   
  {0}  
   
  x2 x1 +  
   
  ‚â§ 0 P x1 + x 2 ‚â• 1 ‚àÖ  
   
  ‚àÖ  
   
  (a) The BB tree drop(T, v2 )  
   
  (b) The BB tree replace(T, r, œÄ, 0)  
   
  The example illustrates that strict dual improvement is not necessary in the compression process. It is possible for the dual bound to improve during the compression process; e.g., use the same example except replace P by the triangle with vertices (‚àí1/2, ‚àí1/2), (‚àí1/2, 1), (1, ‚àí1/2). For an example of an invalid compression operation, one can replace 1/5 in the original example by 1/2; here replace(T, r, œÄ, 0) would no longer be a compression because we deteriorate the lower bound. The tree compression problem (TCP) with respect to a set of allowable directions D is deÔ¨Åned as follows: Given a BB tree T and an objective vector c ‚àà Qn , is there a compression of T where the replacement operation only uses branching directions in D? There is also an optimization version of this question in which we try to compress T as much as possible. Section 3 considers this decision problem (showing this is NP-Complete) and the optimization problem (showing limitations of compression). Our computational results in Sects. 4 and 5 consider the optimization problem. As seen in the previous example, the choice of D inÔ¨Çuences the compression question; the BB tree in Ô¨Ågure (a) is the best compression if D only contains unit vectors while the BB tree in Ô¨Ågure (b) is the best compression if D contains the all-ones vector.  
   
  3  
   
  Complexity Results and Lower Bounds  
   
  We show (TCP) is NP-Complete when D = Zn and c = 0. Our proof uses a reduction from the NP-Complete problem of disjunctive infeasibility (DI) [24, Proposition 3.2]: Given A ‚àà Qm√ón and b ‚àà Qn defining a polyhedron S = {x ‚àà Rn : Ax ‚â§ b}, decide if there exists œÄ ‚àà Zn \ {0} and œÄ0 ‚àà Z such that S ‚äÜ {x ‚àà Rn : œÄ0 < œÄ  x < œÄ0 + 1}. Keep in mind that the input to (DI) is a single polyhedron whereas the input to (TCP) is an entire BB tree. For this reason, if D is a smaller set in the (TCP) deÔ¨Ånition, e.g., vectors of bounded support, then (TCP) can be solved in polynomial time by solving a series of Ô¨Åxed dimension MIPs, one at each node of T ; see [24, ¬ß2.1]. Theorem 1. (TCP) is NP-Complete when D = Zn and c = 0. Proof. We brieÔ¨Çy argue (TCP) is in NP when D = Zn and c = 0. Let T be a BB tree that can be compressed. Either d(T, 0) = 0, which happens if Q(v) = ‚àÖ for some v ‚àà L(T ), or d(T, 0) = ‚àû, which happens if Q(v) = ‚àÖ for all v ‚àà L(T ). If a non-leaf node v of T satisÔ¨Åes Q(v) = ‚àÖ, then T  = drop(T, v) is a compression of T whose size is polynomial in the size of T . Suppose d(T, 0) = ‚àû. Since T  
   
  Compressing Branch-and-Bound Trees  
   
  353  
   
  can be compressed, there exists v ‚àà T and (œÄ, œÄ0 ) ‚àà Zn √ó Z such that applying the disjunction (œÄ  x ‚â§ œÄ0 ) ‚à® (œÄ  x ‚â• œÄ0 + 1) to Q(v) will yield two empty polyhedra. Mahajan and Ralphs demonstrate that Ô¨Ånding such a disjunction is in NP [24, ¬ß3]. In particular, there is a compression T  = replace(T, v, œÄ, œÄ0 ) of T whose size is polynomial in the size of T . This shows that (TCP) is in NP. Consider an instance (A, b) of (DI). Let x‚àó ‚àà S \ Zn ; this can be found in polynomial time unless S is empty (in which case the answer to (DI) is ‚Äòyes‚Äô) or a single integer vector (in which case the answer is ‚Äòno‚Äô). Without loss of generality, x‚àó1 ‚àà Z. We lift S into Rn+1 to create an instance of (TCP). We write a point in Rn+1 as (x, y) ‚àà Rn √ó R. DeÔ¨Åne P := conv ({(x‚àó , 0), (x‚àó , 1)} ‚à™ {(x, 1/2) : x ‚àà S}) We build a BB tree T with root node r and Q(r) = P. Branch on the disjunction (y ‚â§ 0) ‚à® (y ‚â• 1) at r to obtain v1 and v2 : Q(v1 ) := {(x, y) ‚àà P : y ‚â§ 0} = {(x‚àó , 0)} Q(v2 ) := {(x, y) ‚àà P : y ‚â• 1} = {(x‚àó , 1)}. Branch on v1 and v2 using (x1 ‚â§ x‚àó1 ) ‚à® (x1 ‚â• x‚àó1 ) to obtain v3 , v4 , v5 , v6 : Q(v3 ) := {(x, y) ‚àà P Q(v4 ) := {(x, y) ‚àà P Q(v5 ) := {(x, y) ‚àà P Q(v6 ) := {(x, y) ‚àà P  
   
  : : : :  
   
  y y y y  
   
  ‚â§0 ‚â§0 ‚â•1 ‚â•1  
   
  and and and and  
   
  x1 x1 x1 x1  
   
  ‚â§ x‚àó1 } = ‚àÖ ‚â• x‚àó1 } = ‚àÖ ‚â§ x‚àó1 } = ‚àÖ ‚â• x‚àó1 } = ‚àÖ.  
   
  T has 7 nodes, and the four leaves v3 , v4 , v5 , v6 have corresponding polyhedra that are empty. The encoding size of T is polynomial in the encoding size of S. If (DI) has a ‚Äòyes‚Äô answer with certiÔ¨Åcate œÄ ‚àà Zn \ {0} and œÄ0 ‚àà Z, then P ‚äÜ S √ó R ‚äÜ {(x, y) ‚àà Rn √ó R : œÄ0 < œÄ  x < œÄ0 + 1}. Hence, the answer to (TCP) is ‚Äòyes‚Äô because replace(T, r, (œÄ, 0), œÄ0 ) is a compression of T . Assume (TCP) has a ‚Äòyes‚Äô answer. The drop operation can only be applied to r, v1 or v2 , and doing so to any of these does not compress the tree because the dual bound decreases. So, the ‚Äòyes‚Äô answer must come from the replace operation. In order to decrease the size of the tree, which is required for compression, the replace operation must be applied at r. Therefore, there is (œÄ, œÄn+1 ) ‚àà Zn √ó Z and œÄ0 ‚àà Z such that œÄ0 < œÄ  x + œÄn+1 y < œÄ0 + 1 for all (x, y) ‚àà P. Note that œÄ = 0 and œÄn+1 = 0 as otherwise (x‚àó , 0) or (x‚àó , 1) violates  one of these inequalities. The tuple (œÄ, œÄ0 ) provides a ‚Äòyes‚Äô answer to (DI).  Note that (TCP) can be answered in polynomial time if the set D of directions allowed in the replacement operation is polynomial in the size of T , e.g., D = {e1 , . . . , en }. Indeed, one can try the drop operation at each node and the replace operation for each node-direction pair (v, d); this requires polynomial time because the size of D is polynomial in the size of T . The next theorem shows that tree compression does not always yield the smallest tree for a given dual bound.  
   
  354  
   
  G. MuÀú noz et al.  
   
  Theorem 2. For n ‚â• 2, there exists a polytope P ‚äÜ Rn+1 and a BB tree T with root polyhedron P such that 1. |T | ‚â• 2n+1 ‚àí 1 and d(T, 0) = ‚àû. n 2. T cannot be compressed to a tree with fewer than (2 ‚àí 1)/n nodes.   3. There exists a tree T with root P, |T | = 7 and d(T, 0) = d(T  , 0). Proof. Let P ‚äÜ [0, 1]n be a polytope satisfying P ‚à© Zn = ‚àÖ and if a tree T with root P satisÔ¨Åes d(T , 0) = ‚àû, then |T | ‚â• 2n+1 ‚àí 1. One such P comes from [11, Proposition 3]. Let T be a BB tree of minimal size with root P and d(T , 0) = ‚àû. Minimality implies that only the leaves correspond to empty polyhedra. There exist (|T | ‚àí 1)/2 ‚â• 2n ‚àí 1 non-empty non-leaf nodes in T . For each non-empty node v ‚àà T , we have Q(v) \ Zn = Q(v) = ‚àÖ. So, there exists i‚àó ‚àà {1, . . . , n} such n that at least (2 ‚àí 1)/n nodes in T whose corresponding polyhedron contains a point with i‚àó th component in (0, 1). We denote the set of these nodes as N := {v ‚àà T : ‚àÉ x ‚àà Q(v) with xi‚àó ‚àà (0, 1)}. For each v ‚àà N , arbitrarily choose a point in Q(v) whose i‚àó th component is in (0, 1) and call this point x(v). DeÔ¨Åne P := conv({(x(v), t) : v ‚àà N and t ‚àà {0, 1}} ‚à™ (P √ó {1/2})). Note that P ‚à© Zn+1 = ‚àÖ. Create a BB tree T  with root polyhedron P and d(T  , 0) = ‚àû by Ô¨Årst branching on (xn+1 ‚â§ 0) ‚à® (xn+1 ‚â• 1); the polyhedra of the resulting children are conv{(x(v), j) : v ‚àà N } for j ‚àà {0, 1}. Given that x(v)i‚àó ‚àà (0, 1) for each v ‚àà N , we can branch on each conv{(x(v), j) : v ‚àà N } using (xi‚àó ‚â§ 0) ‚à® (xi‚àó ‚â• 1) to obtain all empty children nodes. This proves 3. We deÔ¨Åne T in the theorem by lifting T . More precisely, extend every disjunction (œÄ  x ‚â§ œÄ0 ) ‚à® (œÄ  x ‚â• œÄ0 + 1) in T to a disjunction (œÄ  x ‚â§ œÄ0 ) ‚à® (œÄ  x ‚â• œÄ0 + 1), where œÄ := (œÄ, 0). Thus, |T | = |T | ‚â• 2n+1 ‚àí 1. Furthermore, P ‚äÜ P √ó R, so d(T, 0) = ‚àû because d(T , 0) = ‚àû. Thus, T satisÔ¨Åes 1. It remains to prove 2. Every point in P is of the form (x , Œ±), where x in P and Œ± ‚àà [0, 1]. Assume to the contrary that T can be compressed via the drop operation. The corresponding node in T can also be dropped. However, this contradicts the minimality of T . We claim that if v ‚àà T corresponds to a node in v ‚àà N , then T cannot be compressed at v using the replace operation. Suppose there exists v ‚àà T corresponding to a node v ‚àà N and a disjunction (œÄ  x + œÄn+1 xn+1 ‚â§ œÄ0 ) ‚à® (œÄ  x + œÄn+1 xn+1 ‚â• œÄ0 + 1) that we can use to compress T at v via the replace operation, i.e., Q(v) ‚äÜ {(x, Œ±) ‚àà Rn √ó R : œÄ0 < œÄ  x + œÄn+1 Œ± < œÄ0 + 1}. If œÄn+1 = 0, then this disjunction can be projected to T to compress it, contradicting the minimality of T . So, œÄn+1 = 0. For each Œ± ‚àà [0, 1] the point (x(v), Œ±) satisÔ¨Åes œÄ0 ‚àí œÄ  x(v) < œÄn+1 Œ± < œÄ0 ‚àí œÄ  x(v) + 1. But this cannot be satisÔ¨Åed if we plug in Œ± = 0 and Œ± = 1 because œÄn+1 ‚àà Z. Thus, the replace operation can only be applied to nodes in T that do not n correspond to nodes in N . We have |N | ‚â• (2 ‚àí 1)/n, so T cannot be compressed n   to fewer than (2 ‚àí 1)/n nodes, which proves 2.  
   
  Compressing Branch-and-Bound Trees  
   
  4  
   
  355  
   
  Compression Algorithms  
   
  In this section we introduce two compression algorithms and later evaluate their performance. Let T be a BB tree and c ‚àà Qn . For both algorithms, the general approach we follow is: (1) Traverse T starting from the root. We may skip leaves, since these are not compressible; (2) If the minimum of x ‚Üí c x over Q(v) is greater than or equal to d(T, c) then we apply drop(T, v); (3) Otherwise, we search for (œÄ, œÄ0 ) ‚àà Zn √ó Z such that T  = replace(T, v, œÄ, œÄ0 ) satisÔ¨Åes d(T, c) ‚â• d(T  , c). In the following, we provide two methods for Step (3), which is the bottleneck of the procedure. 4.1  
   
  An Exact Method  
   
  A BB tree replace(T, v, œÄ, œÄ0 ) is a compression of T if and only if min{c x : x ‚àà Q(v), œÄ  x ‚â§ œÄ0 } ‚â• d(T, c) and min{c x : x ‚àà Q(v), œÄ  x ‚â• œÄ0 + 1} ‚â• d(T, c). Mahajan and Ralphs [23] proposed MIP formulation that can be used to Ô¨Ånd such (œÄ, œÄ0 ); the only diÔ¨Äerence is that they used it for Ô¨Ånding a general disjunction that could provide the best possible dual improvement when branching, but we can easily adapt it to our compression task. The resulting model we use is ‚é´ ‚éß A p ‚àí sL c ‚àí œÄ = 0, p b ‚àí d(T, c)sL ‚àí œÄ0 ‚â• Œ¥ ‚é¨ ‚é® (3) Œ¥ : A q ‚àí sR c + œÄ = 0, q b ‚àí d(T, c)sR ‚àí œÄ0 ‚â• Œ¥ ‚àí 1 max ‚é≠ Œ¥,p,q,œÄ, ‚é© œÄ0 ,sL ,sR p, q ‚â• 0, sL , sR ‚â• 0, œÄ ‚àà Zn , œÄ0 ‚àà Z Any feasible solution with Œ¥ > 0 produces a tuple (œÄ, œÄ0 ) that we can use in the replace operation. Conversely, if no such Œ¥ exists, neither does a suitable disjunction; see [23]. Model (3) can be costly to solve in practice. However, if given enough time, one can be certain that it will yield an algorithm capable of compressing T as much as possible. 4.2  
   
  A Heuristic Method  
   
  Many heuristic methods for Ô¨Ånding good branching directions have been proposed in the literature (e.g. [9,16,19,27]) and can be readily used for tree compression. Here, we adapt a procedure in Owen and Mehrota [27] that iteratively improves variable directions by changing one coeÔ¨Écient at a time. To outline the method, assume we have solved the LP relaxation at a node v. The Ô¨Årst step is to Ô¨Ånd the best variable direction œÄ ‚àà {e1 , . . . , en }. Suppose œÄ  x ‚â§ œÄ0 is the side of the disjunction with the smallest optimal value. We add this constraint to the node LP and re-solve it to obtain a fractional solution x. For each fractional component xi , we then evaluate the branching directions œÄ + ei and œÄ ‚àí ei . If one of these directions yields a better dual bound than œÄ, then we replace œÄ by it and repeat the procedure until œÄ can no longer be improved. At the end, if the bound provided by œÄ is better than the tree bound, we apply replace(T, v, œÄ, œÄ0 ). Unlike the previous exact method, this iterative method provides no guarantees that a suitable disjunction will be found, even if it exists, and therefore may not achieve the best compression. However, it is typically much faster.  
   
  356  
   
  5  
   
  G. MuÀú noz et al.  
   
  Computational Experiments  
   
  In this section, we attempt to compress MIPLIB 3.0 trees using the methods described in the previous section. Our main goal is to evaluate, without taking running time into consideration, how compressible are realistic BB trees generated by two commonly-used branching strategies ‚Äî full strong branching (FSB) and reliability branching with plunging (RB). Our secondary goal is to estimate how much of this compression can be achieved in shorter and more practical running times. 5.1  
   
  Methodology  
   
  For each branching strategy and for each MIPLIB 3.0 instance, we started by generating a BB tree using a custom textbook implementation of the BB method. We chose MIPLIB 3.0, instead of larger benchmark sets, so that we could compute large FSB trees for all instances and could obtain accurate results for the exact compression method. We used a custom implementation of the BB method, instead of exporting the tree generated by a commercial MIP solver, so that we could easily understand how exactly the tree is generated and control every aspect of the algorithm. The implementation is written in Julia 1.8 and has been made publicly available as part of the open-source MIPLearn software package [29]. It relies on an external LP solver, accessed through JuMP [13] and MathOptInterface [21], to solve the LP relaxation of each BB node and to evaluate strong branching decisions. In our experiments, we used Gurobi 9.5 [17] with default settings as the LP solver. When generating the trees, we provided the optimal value to the BB method and imposed a 10,000-node limit. No time limit was imposed, and no presolve or cutting planes were applied. After the trees were generated, they were then compressed by the exact and the heuristic methods described in Sect. 4. Both methods were implemented in Python 3.10 and gurobipy. The nodes were traversed using depth-Ô¨Årst search. For the exact method, we imposed a 24-h limit on the entire procedure and a 20min limit on each individual MIP. For the heuristic method, we imposed a 15-min limit on the entire procedure and no time limits on individual nodes. All MIPs and LPs were solved with Gurobi 9.5 with default settings. The experiments were run on a dedicated desktop computer (AMD Ryzen 9 7950X, 4.5/5.7 GHz, 16 cores, 32 threads, 128 GB DDR5), and 32 trees were compressed in parallel at a time; each compression was single-threaded. 5.2  
   
  Full Strong Branching Results  
   
  Full strong branching (FSB) is a strategy which solves, at each node of the BB tree, two LPs for each fractional variable, then picks as the branching variable the one that presents the best overall improvement to dual bound [2]. FSB is often paired, as we do in our experiments, with best-bound node selection rule, which always picks, as the BB node to process next, an unexplored leaf node that has minimal optimal value. Although computationally expensive, FSB is typically  
   
  Compressing Branch-and-Bound Trees  
   
  357  
   
  Fig. 1. Compressibility of FSB trees (exact method, 24-h limit).  
   
  considered the state-of-the-art branching strategy in terms of node count, so one would naturally expect such trees to be hard to compress. Figure 1 shows the exact compressibility of FSB trees under diÔ¨Äerent restrictions on the support size of the disjunction. SpeciÔ¨Åcally, supp:inf corresponds to the exact method based on Model (3), whereas supp:1 and supp:2 use the same model, but impose the additional constraint that at most 1 or 2 coeÔ¨Écients of œÄ, respectively, can be non-zero. Method drop is the method in which we are only allowed to drop nodes, not replace them. In the chart, the compressibility of diÔ¨Äerent methods is superimposed, with the weaker methods in the foreground and the stronger methods in the background. The y-axis indicates how small is the resulting tree, with larger values indicating higher compression. For example, on instance vmp1, methods drop, supp:2 and supp:inf were able to reduce the tree by 22.2%, 67.1% and 80.9%, respectively. Method supp:1 does not appear in the chart because it was not able to improve upon drop. The line shows the average compression obtained by the strongest method across all instances. Our Ô¨Årst insight from Fig. 1 is that many FSB trees can be signiÔ¨Åcantly compressed, despite the notorious tree-size eÔ¨Éciency of this branching rule. On average, supp:inf was able to reduce tree size by 35.2%, with the ratio exceeding 50% for 20 (out of 59) instances. We also note, from the Ô¨Ågure, that a large support size is required for obtaining the best results, although a restricted support size still provides signiÔ¨Åcant compression. On average, supp:2 compressed the trees by 24.0%, which is still considerable, although being well below supp:inf. Method supp:1, on the other hand, never outperformed drop. This was expected, as it can be easily shown that trees generated by FSB (with best-bound) on a particular set of candidate branching directions can never be compressed (beyond dropping nodes) based on the same set of directions. Also as a direct consequence of using the best-bound node selection rule, we observed that, for the vast majority of instances, few nodes could be dropped. On average, drop was only able  
   
  358  
   
  G. MuÀú noz et al.  
   
  Fig. 2. Compressibility of FSB trees (heuristic method, 15-min limit).  
   
  to compress the trees by 12.1% on average, with the compression being near zero for 50 instances. Finally, despite the positive average compression results for supp:inf, we do note that a large number of trees could not be meaningfully compressed. SpeciÔ¨Åcally, supp:inf presented a compression ratio below 5% for 19 instances, which may indicate that trees for certain classes of problems are hard to compress. Furthermore, supp:inf took an exceedingly long average time of 47,153 s, with 25 instances hitting the 24-h limit. We now focus on more practical tree compression algorithms. Figure 2 shows the performance of the heuristic method, outlined in Subsect. 4.2, on the same BB trees, with a 15-min limit. We see that the heuristic method is able to obtain compression ratios comparable to supp:inf in a reasonable amount of time. On average, heuristic took 493 s to run (95x faster than the exact method), and reduced tree size by 27.7% (7.5% points lower). We conclude that FSB trees are compressible not only in a theoretical sense, but also in practice. We also note that heuristic outperformed supp:inf for 12 instances, sometimes by a signiÔ¨Åcant margin. Notable examples include instances bell5, bell3a, vpm2, p0282 and mas74, where the margin exceeded 15% points. This is possible due to the time limits imposed on supp:inf. 5.3  
   
  Reliability Branching with Plunging  
   
  Reliability branching (RB) is a strategy that attempts to accelerate FSB by skipping strong branching computations for variables that already have reliable pseudocosts [2]. In our experiments, the pseudocost of a variable is considered reliable if it is based on 10 or more strong branching evaluations. RB has been shown to perform well on a variety of real-world instances and it is often considered the state-of-the-art branching rule in terms of running time. Plunging is a modiÔ¨Åcation to node selection which attempts to exploit the fact that sequentially solving two LPs that are similar can done much faster than solving two LPs  
   
  Compressing Branch-and-Bound Trees  
   
  359  
   
  Fig. 3. Compressibility of RB trees (exact method, 24-h limit).  
   
  that are signiÔ¨Åcantly diÔ¨Äerent. When plunging is enabled, the BB method picks, as the node to explore next, one of the children of the most-recently explored node, falling back to best-bound node selection when both children are pruned. Our motivation for analyzing RB trees with plunging is that we expect such trees to resemble the ones generated by typical state-of-the-art MIP solvers. Figure 3 shows the exact compressibility of RB trees, under diÔ¨Äerent support size restrictions. The Ô¨Årst notable fact is that RB trees are, as expected, much more compressible than FSB trees. On average, drop, supp:1, supp:2 and supp:inf were able to reduce tree size by 51.9%, 57.3%, 61.5% and 66.3%, respectively. Method supp:inf presented compression ratio above 50% for 42 (out of 59) instances, and above 80% for 34 instances. The strong performance of drop can be directly attributed to plunging. While the technique may be helpful when solving MIPs, we observed that it leads to the exploration of areas in the tree that do not contribute to its overall dual bound, and which can be dropped in a post-hoc analysis. As with previous experiments, the best compression results were obtained with larger support sizes, although, in this case, the beneÔ¨Åts of unbounded support were not as large as before, in relative terms. Method supp:1, unlike in previous experiments, provided signiÔ¨Åcant compression in a number of instances (e.g. gen, l152lav, qnet1 o), and a modest average improvement over drop. We attribute this to suboptimal variable branching decisions made by RB, which is also expected. As in the previous case, we do note that supp:inf failed to meaningfully compress a few instances, and it was overall prohibitively slow, requiring 45,256 s on average. Finally, Fig. 4 shows the performance of the heuristic method on RB trees. Similarly to the results in the previous section, the heuristic method presented very strong performance, obtaining compression ratios that approached or even exceed those of the exact method, in much smaller running times. Method heuristic took an average of 335 s (134x faster) and obtained an average com-  
   
  360  
   
  G. MuÀú noz et al.  
   
  Fig. 4. Compressibility of RB trees (heuristic method, 15-min limit).  
   
  pression ratio of 63.7% (2.5% points lower). We conclude that BB trees generated by node and variable selection rules that focus on MIP solution time tend to be highly compressible, in both a theoretical and a practical sense.  
   
  6  
   
  Future Work  
   
  We have formally introduced the tree compression problem, and we demonstrated through experiments how much trees can be compressed. There are many open questions that we believe warrant future research. First, is there a family of problems for which BB trees generated, say using strong branching, can be provably compressed? Second, for a tree generated using branching directions in a set D, how compressible is the tree using directions in the Minkowski Sum D + D? In particular when D is the set of variable disjunctions, a positive result may indicate sparse disjunctions that are useful in a BB tree. This would complement our current computational results on disjunctions of support size 2. Third, given that the compression algorithm is based (partially) on general disjunctions which can be seen as splits, is there a relationship between the strength of split cuts at the root and the compressibility of a BB tree? Finally, could the general disjunctions found by the compression algorithm be useful in solving similar MIP instances? Acknowledgements. J. Paat was supported by a Natural Sciences and Engineering ¬¥ Research Council of Canada Discovery Grant [RGPIN-2021-02475]. A.S. Xavier was partially supported by the U.S. Department of Energy OÔ¨Éce of Electricity. The authors want to thank the referees, whose comments improved the overall presentation of the paper, led to better bounds in Theorem 2, and identiÔ¨Åed directions of future work.  
   
  Compressing Branch-and-Bound Trees  
   
  361  
   
  References 1. Aardal, K., Lenstra, A.: Hard equality constrained integer knapsacks. Math. Oper. Res. 29, 724‚Äì738 (2004) 2. Achterberg, T., Koch, T., Martin, A.: Branching rules revisited. Oper. Res. Lett. 33(1), 42‚Äì54 (2005) 3. Basu, A., Conforti, M., Di Summa, M., Jiang, H.: Complexity of branch-and-bound and cutting planes in mixed-integer optimization - II. In: Singh, M., Williamson, D.P. (eds.) IPCO 2021. LNCS, vol. 12707, pp. 383‚Äì398. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-73879-2 27 4. Beame, P., et al.: Stabbing planes. In: Karlin, A.R. (ed.) 9th Innovations in Theoretical Computer Science Conference (ITCS 2018). Leibniz International Proceedings in Informatics (LIPIcs), vol. 94, pp. 10:1‚Äì10:20. Schloss Dagstuhl-LeibnizZentrum fuer Informatik, Dagstuhl (2018). https://doi.org/10.4230/LIPIcs.ITCS. 2018.10, http://drops.dagstuhl.de/opus/volltexte/2018/8341 5. Bixby, R.: Solving real-world linear programs: a decade and more of progress. Oper. Res. 50(1), 3‚Äì15 (2002) 6. Bixby, R., Boyd, E., Indovina, R.: MIPLIB: a test set of mixed integer programming problems. SIAM News (1992) 7. Cheung, K.K.H., Gleixner, A., SteÔ¨Äy, D.E.: Verifying integer programming results. In: Eisenbrand, F., Koenemann, J. (eds.) IPCO 2017. LNCS, vol. 10328, pp. 148‚Äì 160. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-59250-3 13 8. Chv¬¥ atal, V.: Hard knapsack problems. Oper. Res. 28, 1402‚Äì1411 (1980) 9. Cornu¬¥ejols, G., Liberti, L., Nannicini, G.: Improved strategies for branching on general disjunctions. Math. Program. 130, 225‚Äì247 (2011) 10. Dadush, D., Tiwari, S.: On the complexity of branching proofs. In: Saraf, S. (ed.) 35th Computational Complexity Conference (CCC 2020). Leibniz International Proceedings in Informatics (LIPIcs), vol. 169, pp. 34:1‚Äì34:35. Schloss DagstuhlLeibniz-Zentrum f¬® ur Informatik, Dagstuhl (2020) 11. Dey, S., Dubey, Y., Molinaro, M.: Lower bounds on the size of general branch-andbound trees. Math. Program. (2022) 12. Dey, S., Dubey, Y., Molinaro, M., Shah, P.: A theoretical and computational analysis of full strong-branching. arXiv:2110.10754 (2021) 13. Dunning, I., Huchette, J., Lubin, M.: Jump: a modeling language for mathematical optimization. SIAM Rev. 59(2), 295‚Äì320 (2017). https://doi.org/10.1137/ 15M1020575 14. Fischetti, M., Monaci, M.: Backdoor branching. INFORMS J. Comput. 25(4), 693‚Äì700 (2018) 15. Gamrath, G., Melchiori, A., Berthold, T., Gleixner, A.M., Salvagnin, D.: Branching on multi-aggregated variables. In: Michel, L. (ed.) CPAIOR 2015. LNCS, vol. 9075, pp. 141‚Äì156. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-180083 10 16. Gl¬® aser, M., Pfetsch, M.: On the complexity of Ô¨Ånding shortest variable disjunction branch-and-bound proofs. In: Aardal, K., Sanit` a, L. (eds.) IPCO 2022. LNCS, vol. 13265, pp. 291‚Äì304. Springer, Cham (2022). https://doi.org/10.1007/978-3-03106901-7 22 17. Gurobi Optimization: Gurobi Optimizer (Version 9.5). https://www.gurobi.com/ products/gurobi-optimizer/. Accessed 4 Nov 2022 18. Jeroslow, R.: Trivial integer programs unsolvble by branch-and-bound. Math. Program. 6, 105‚Äì109 (1974)  
   
  362  
   
  G. MuÀú noz et al.  
   
  19. Karamanov, M., Cornu¬¥ejols, G.: Branching on general disjunctions. Math. Program. 128, 403‚Äì436 (2011) 20. Khalil, E., Vaezipoor, P., Dilkina, B.: Finding backdoors to integer programs: a Monte Carlo tree search framework. In: Proceedings of AAAI (2022) 21. Legat, B., Dowson, O., Dias Garcia, J., Lubin, M.: MathOptInterface: a data structure for mathematical optimization problems. INFORMS J. Comput. 34(2), 672‚Äì 689 (2021). https://doi.org/10.1287/ijoc.2021.1067 22. Linderoth, J., Savelsbergh, M.: A computational study of search strategies for mixed integer programming. INFORMS J. Comput. 11(2), 173‚Äì187 (1999) 23. Mahajan, A., Ralphs, T.: Experiments with branching using general disjunctions. In: Proceedings of Operations Research and Cyber-Infrastructure, pp. 101‚Äì118 (2009) 24. Mahajan, A., Ralphs, T.: On the complexity of selecting disjunctions in integer programming. SIAM J. Optim. 20(5), 2181‚Äì2198 (2010) 25. Mahmoud, H., Chinneck, J.: Achieving MILP feasibility quickly using general disjunctions. Comput. Oper. Res. 40, 2094‚Äì2102 (2013) 26. Mehrotra, S., Li, Z.: Branching on hyperplane methods for mixed integer linear and convex programming using adjoint lattices. J. Glob. Optim. (2010) 27. Owen, J., Mehrotra, S.: Experimental results on using general disjunctions in branch-and-bound for general-integer linear programs. Comput. Optim. Appl. 20, 159‚Äì170 (2001) 28. Walter, M.: Sparsity of lift-and-project cutting planes. In: Helber, S., et al. (eds.) Operations Research Proceedings 2012, pp. 9‚Äì14. Springer, Cham (2014). https:// doi.org/10.1007/978-3-319-00795-3 2 29. Xavier, A.S., Qiu, F.: MIPLearn: a framework for learning-enhanced mixedinteger optimization (Julia interface) (2022). https://github.com/ANL-CEEESA/ MIPLearn.jl 30. Yang, Y., Boland, N., Savelsbergh, M.: Multivariable branching: a 0‚Äì1 knapsack problem case study. INFORMS J. Comput. 33(4), 1354‚Äì1367 (2021)  
   
  Exploiting the Polyhedral Geometry of Stochastic Linear Bilevel Programming Gonzalo MuÀú noz(B) , David Salas, and Anton Svensson Instituto de Ciencias de la Ingenier¬¥ƒ±a, Universidad de O‚ÄôHiggins, Rancagua, Chile {gonzalo.munoz,david.salas,anton.svensson}@uoh.cl Abstract. We study linear bilevel programming problems whose lowerlevel objective is given by a random cost vector with known distribution. We consider the case where this distribution is nonatomic, allowing to pose the problem of the leader using vertex-supported beliefs in the sense of [29]. We prove that, under suitable assumptions, this formulation turns out to be piecewise aÔ¨Éne over the so-called chamber complex of the feasible set of the high point relaxation. We propose two algorithmic approaches to solve general problems enjoying this last property. The Ô¨Årst one is based on enumerating the vertices of the chamber complex. The second one is a Monte-Carlo approximation scheme based on the fact that randomly drawn points of the domain lie, with probability 1, in the interior of full-dimensional chambers, where the problem (restricted to this chamber) can be reduced to a linear program. Keywords: Bilevel Programming ¬∑ Bayesian Approach ¬∑ Chamber complex ¬∑ Enumeration algorithm ¬∑ Monte-Carlo algorithm  
   
  1  
   
  Introduction  
   
  Stackelberg games, also referred to as bilevel programming problems, were Ô¨Årst introduced by H. von Stackelberg in [31]. In this seminal work, an economic equilibrium problem between two Ô¨Årms was studied, under the particularity that one of them, the leader, is able to anticipate the decisions of the other one, the follower. Bilevel programming is an active Ô¨Åeld of research, and we refer the reader to the monographs [10,11] for comprehensive introductions, and to [12] for recent developments. In the last decade, researchers have started to consider uncertainty in Stackelberg games. A recent survey by Beck, Ljubi¬¥c and Schmidt [3] provides an overview of new questions and recent contributions on this topic. One model that considers uncertainty in Stackelberg games is the Bayesian approach [26,29]. The starting point is that for any given leader‚Äôs decision x, the leader only knows that the reaction y of the follower is selected from a set Y (x), The Ô¨Årst author was supported by FONDECYT Iniciaci¬¥ on 11190515 (ANID-Chile). The second author was supported by the Center of Mathematical Modeling (CMM) FB210005 BASAL funds for centers of excellence (ANID-Chile), and the grant FONDECYT Iniciaci¬¥ on 11220586 (ANID-Chile). The third author was supported by the grant FONDECYT postdoctorado 3210735 (ANID-Chile). c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 363‚Äì377, 2023. https://doi.org/10.1007/978-3-031-32726-1_26  
   
  364  
   
  G. MuÀú noz et al.  
   
  hence being y a decision-dependent uncertainty parameter. The leader endows the set Y (x) with a probability distribution Œ≤x which models how the leader believes that the possible responses of the follower are distributed. Note that the classical optimistic and pessimistic approaches of bilevel programming are included in this setting, under quite mild assumptions (see [29]). Uncertainty in the data of the lower-level has been considered by Claus for linear bilevel programming from a variational perspective considering risk measures (see the survey [6] by Burtscheidt, and the references therein, and the recent works [7,8]). In [19], Ivanov considered the cost function of the follower as a bilinear form Ax + Œæ(œâ), y. Recently, in [5], Buchheim, Henke and Irmai considered a bilevel version of the continuous knapsack problem with uncertainty on the follower‚Äôs objective. In this work, we consider a linear bilevel programming problem where the lower-level objective is uncertain for the leader but follows a prior known distribution (as the particular case studied in [5]). We study the problem from a Bayesian approach perspective [29], and by means of the so-called chamber complex of a polytope (see Sect. 4), which subdivides the space of the leader‚Äôs decisions in a meaningful way. The idea of using the chamber complex to understand geometrical properties of optimization problems under uncertainty is not new, but it is recent. To the best of our knowledge, the Ô¨Årst work that does this is [14] (see also [13]), on which multistage stochastic linear optimization is studied. However, the techniques there cannot be extended to Stackelberg games. Due to space constraints, we do not provide all details in this extended abstract. We refer the reader to our full-length preprint [27]. 1.1  
   
  Problem Formulation and Contributions  
   
  Our study focuses on the setting of linear bilevel programming, i.e., the objective functions and constraints of the problem are all linear. More precisely, we aim to study the problem where the leader decides a vector x ‚àà Rnx that solves ‚éß min d1 , x + E[d2 , y(x, œâ)] ‚é™ ‚é™ x ‚é™ ‚é™ ‚é™ ‚é™ ‚é™ ‚é® s.t. x ‚àà X ‚éß (P) := (1) ‚é™ c(œâ), y ‚é™ ‚é® min ‚é™ y ‚é™ ‚é™ ny ‚é™ y(x, œâ) solves œâ ‚àà Œ© a.s. ‚é™ ‚é™ ‚é™ s.t. y ‚àà R , ‚é© ‚é© Ax + By ‚â§ b, where A ‚àà Rm√ónx , B ‚àà Rm√óny , b ‚àà Rm , d1 ‚àà Rnx , d2 ‚àà Rny , c : Œ© ‚Üí Sny is a random vector over a probability space (Œ©, Œ£, P) with values in the unit sphere of Rny , and X ‚äÇ Rnx is a nonempty polytope. The notation carries the usual ambiguity of bilevel problems, which appears whenever the lower-level optimal response y(x, œâ) is not uniquely determined for some x ‚àà X. However, we focus our attention here on costs whose distributions are nonatomic (in a sense we will specify later on) which implies that, with probability 1, y(x, œâ) is unique for all x ‚àà X.  
   
  Exploiting the Polyhedral Geometry of Stochastic Linear  
   
  365  
   
  Our main contributions regarding this problem are: (a) To rewrite (1) using a Bayesian approach formulation and a sample average approximation for it; (b) to show the structure of the leader‚Äôs objective function and its relation to the chamber complex of the feasible set of the high-point relaxation; and (c) to exploit these structures in a mixed-integer-programming-based algorithm and in a Monte-Carlo algorithm that can tackle (1).  
   
  2  
   
  Preliminaries  
   
  For an integer n ‚àà N, we write [n] := {1, . . . , n}. Throughout this work, we will consider Euclidean spaces Rn endowed with their usual norm ¬∑ and their inner product ¬∑, ¬∑. We denote by Sn the unit sphere of Rn . For a set O ‚äÇ Rn , we denote the aÔ¨Éne dimension of O as dim(O), which corresponds to the dimension of the aÔ¨Éne envelope of O, denoted by aÔ¨Ä(O). The relative interior of O is denoted by ri(O). We write 1O to denote the indicator function of a set O ‚äÇ Rn , having value 1 on O and 0 elsewhere. In general, we follow the standard notation of mathematical programming. For a polyhedron P , we denote by F (P ) the collection of all faces of P , and by ext(P ) the vertices (extreme points) of P . For any k ‚àà {0, . . . , n} we will write (2) F‚â§k (P ) := {F ‚àà F (P ) : dim(F ) ‚â§ k}. If there is no ambiguity, we might simply write F and F‚â§k . For a convex set C ‚äÇ Rn and a point x ‚àà C, we write NC (x) to denote the normal cone of C at x, i.e., (3) NC (x) = {d ‚àà Rn : d, y ‚àí x ‚â§ 0 ‚àÄy ‚àà C}. Motivated by the structure of problem (1), we deÔ¨Åne the polyhedron D as the feasible region of the high-point relaxation of the problem (see, e.g., [21]), i.e., D = {(x, y) ‚àà Rnx √ó Rny : Ax + By ‚â§ b} .  
   
  (4)  
   
  It will be assumed throughout the paper that D is full dimensional. We do not lose generality since it is always possible to embed D into Rdim(D) . We will also assume that D is compact, i.e. it is a polytope. Finally, by moving the leader‚Äôs constraints to the follower‚Äôs problem, we assume without losing any generality that   (5) X = x ‚àà Rnx : ‚àÉy ‚àà Rny such that Ax + By ‚â§ b . In the latter assumption about X we are simply stating that the lower-level problem is feasible for any feasible choice of x and restricting ‚Äòunilaterally‚Äô the x coordinate in D which does not change the lower-level problem. We deÔ¨Åne the ambient space Y for the follower‚Äôs decision vector as   Y = y ‚àà Rny : ‚àÉx ‚àà Rnx such that Ax + By ‚â§ b . (6) Note that, since D is full-dimensional and compact in Rnx √ó Rny , both X and Y are full-dimensional (in Rnx and Rny , respectively) and compact as well. We  
   
  366  
   
  G. MuÀú noz et al.  
   
  write œÄ : Rnx √óRny ‚Üí Rnx to denote the parallel projection given by œÄ(x, y) = x. In particular, equation (5) can be written as X = œÄ(D). Given nonempty sets U ‚äÇ Rnx and V ‚äÇ Rny we write M : U ‚áí V to denote a set-valued map, i.e., a function assigning to each x ‚àà U a (possibly empty) set M (x) in V . In this work, we consider the set-valued map S : X ‚áí Rny deÔ¨Åned as (7) S(x) = {y ‚àà Rny : (x, y) ‚àà D}. We call S the Ô¨Åber map of D (through the projection œÄ). Clearly, S has nonempty convex and compact values, and S(X) = Y , as given by equation (6).  
   
  3  
   
  Vertex-Supported Beliefs and Bayesian Formulation  
   
  In what follows, we write B(Y ) to denote the Borel œÉ-algebra of Y and P(Y ) to denote the family of all (Borel) probability measures on Y . We endow P(Y ) with the topology of weak convergence (see, e.g., [22, Chapter 13]). Accordingly, we will say that a measure-valued map h : X ‚Üí P(Y ) is weak continuous if it is so for this topology (which coincides with the weak* topology when looking the space of measures as the dual space of the space of continuous functions, see [22, Remark 13.14]). Recall from [29] that for a set-valued map M : X ‚áí Y with closed and nonempty values, a map Œ≤ : X ‚Üí P(Y ) is said to be a belief over M if for every x ‚àà X, the measure Œ≤(x) = Œ≤x concentrates on M (x), i.e., Œ≤x (M (x)) = 1. By identifying P(M (x)) with its natural injection into P(Y ), a belief over M can be understood as a selection of {P(M (x)) : x ‚àà X}. Let (Œ©, Œ£, P) be a probability space. To model the cost function of the lowerlevel problem in (1), we will consider only random vectors c : Œ© ‚Üí Sny with nonatomic distributions, in the sense that ‚àÄO ‚àà B(Sny ) :  
   
  Hny ‚àí1 (O) = 0 =‚áí P[c(œâ) ‚àà O] = 0,  
   
  (8)  
   
  where B(Sny ) stands for the Borel œÉ-algebra of Sny , and Hny ‚àí1 denotes the (ny ‚àí 1)-HausdorÔ¨Ä measure over (Sny , B(Sny )). In other words, the probability measure P ‚ó¶ c‚àí1 is absolutely continuous with respect to Hny ‚àí1 . Note that with this deÔ¨Ånition, any random vector c : Œ© ‚Üí Rny , which has an absolutely continuous distribution with respect to the Lebesgue measure Lny , induces an equivalent c(œâ) . This new random vector is random vector c¬Ø : Œ© ‚Üí Sny given by c¬Ø(œâ) = c(œâ) well-deÔ¨Åned almost surely in Œ©, except for the negligible set N = c‚àí1 (0), and using c(¬∑) or c¬Ø(¬∑) in problem (1) is equivalent. Now, to understand the distribution of the optimal response y(x, œâ) induced by the random vector c : Œ© ‚Üí Sny , we consider a belief Œ≤ : X ‚Üí P(Y ) over the Ô¨Åber map S : X ‚áí Y given by dŒ≤x (y) = pc (x, y) := P[‚àíc(œâ) ‚àà NS(x) (y)].  
   
  (9)  
   
  Note that P[‚àíc(œâ) ‚àà NS(x) (y)] = P[‚àíc(œâ) ‚àà int(NS(x) (y)) ‚à© Sny ] for any point (x, y) ‚àà D, and that int(NS(x) (y)) is nonempty only if y is an extreme point  
   
  Exploiting the Polyhedral Geometry of Stochastic Linear  
   
  367  
   
  of S(x). By putting together both observations, one can easily deduce that for each x ‚àà X, p(x, ¬∑) is a discrete density function whose support is contained in ext(S(x)). Therefore, the belief Œ≤ is given by  pc (x, y)1O (y), ‚àÄO ‚àà B(Y ). (10) Œ≤x (O) = y‚ààext(S(x))  
   
  We call Œ≤ the vertex-supported belief induced by c. With this construction, we can rewrite problem (1) as (P) := min d1 , x + EŒ≤x [d2 , y] x‚ààX  
   
  (11)  
   
  where EŒ≤x [d2 , y] = y‚ààext(S(x)) d2 , ypc (x, y). Our goal in this work is to study problem (1) by proÔ¨Åting from the Bayesian formulation (11), in the sense of [29]. 3.1  
   
  Sample Average Formulation  
   
  Problem (11) has an intrinsic diÔ¨Éculty which consists in how to evaluate the objective function Œ∏(x) = d1 , x + EŒ≤x [d2 , y]. To make an exact evaluation of Œ∏ at a point x ‚àà X one would require to compute the set of all vertices y1 , . . . , yk of S(x) (having a positive probability of being optimal for c(œâ)) and to compute the corresponding probabilities pc (x, y1 ), . . . , pc (x, yk ), deÔ¨Åned as the ‚Äúsizes‚Äù of the respective normal cones at each vertex yi . This is not always possible. To deal with this issue, we consider the well-known sample average approximation (SAA) method for stochastic optimization (see, e.g., [18,30]). That is, we take an i.i.d sample {ÀÜ c1 , . . . , cÀÜN0 } of the random lower-level cost c(¬∑), where each sample unit is drawn following the (known) distribution of c(¬∑), and try to solve the (now deterministic) problem ‚éß N0 minx d1 , x + N10 i=1 d2 , yi (x) ‚é™ ‚é™ ‚é™ ‚é™ ‚é® ÀÜ := (P) (12) s.t. x ‚àà X  
   
  ‚é™ ‚é™ ‚é™ min ÀÜ c , y y i ‚é™ ‚é© ‚àÄi ‚àà [N0 ], yi (x) solves s.t. Ax + By ‚â§ b. Proposition 1 ([27]). Assume that c(¬∑) has a nonatomic distribution over Sny , in the sense of (8). Then, with probability 1, we have that for each i ‚àà [N0 ] and ci , y : Ax + By ‚â§ b} is a singleton. each x ‚àà X the set argminy {ÀÜ Based on the above proposition, for the sample {ÀÜ c1 , . . . , cÀÜN0 } we can, for each i ‚àà [N0 ], deÔ¨Åne the mapping x ‚Üí yi (x) where yi (x) is the unique ci , y : Ax + By ‚â§ b}. Thus, almost surely, problem (12) is element argminy {ÀÜ well-deÔ¨Åned. We close this section by stating that problem (12) is a consistent approximation of the original problem (11).  
   
  368  
   
  G. MuÀú noz et al.  
   
  Proposition 2 ([27]). Let x‚àóN0 be an optimal solution of (12) for a sample of ‚àó be the associated optimal value. Let S be the set of solutions size N0 , and let ŒΩN 0 N ‚Üí‚àû  
   
  0 of (1) and let ŒΩ¬Ø be its optimal value. Then, d(x‚àóN0 , S) := inf x‚ààS x‚àóN0 ‚àíx ‚àí‚àí‚àí ‚àí‚àí‚Üí  
   
  N ‚Üí‚àû  
   
  0 ‚àó 0, and ŒΩN ‚àí‚àí‚àí ‚àí‚àí‚Üí ŒΩ¬Ø, both with probability 1. 0  
   
  4  
   
  Geometrical Structure of Vertex-Supported Beliefs  
   
  Here, we recall the deÔ¨Ånition of a chamber complex, frequently used in some Ô¨Åelds of mathematics, like computational geometry (see, e.g., [9]). Definition 1 (Chamber complex). Let D ‚äÇ Rnx √ó Rny be a polyhedron as described in (4). For each x ‚àà X = œÄ(D), we deÔ¨Åne the chamber of x as   œÄ(F ) : F ‚àà F (D), x ‚àà œÄ(F ) . (13) œÉ(x) = The chamber complex, is then given by the (Ô¨Ånite) collection of chambers, i.e., C (D) = {œÉ(x) : x ‚àà X}. For a more comprehensive exposition of the chamber complex C (D) and their many interesting properties, we refer the reader to [9,13,14,27] and references therein. The next proposition shows that to compute a chamber it is enough to consider faces of D with dimensions up to nx instead of the collection of all faces. Proposition 3 ([27]). For any x ‚àà X, one has that œÉ(x) = {œÄ(F ) : F ‚àà F , x ‚àà œÄ(F ), dim(F ) ‚â§ nx }. While the previous result narrows the class of faces that are needed to compute a chamber, we may still need faces of drastically diÔ¨Äerent dimensions. The next example illustrates this phenomenon. Example 1. Consider the polytope D := {(x, y) ‚àà R2 √ó R : |x1 | ‚â§ y ‚â§ 1 ‚àí |x2 |}, whose vertices are (0, ¬±1, 0) and (¬±1, 0, 1). Clearly, (0, 0) and (1, 0) are minimal chambers, however, (0, 0) is not a projection of a vertex of D, while (1, 0) cannot be obtained using only projections of facets. It is well-known (see, e.g., [14]) that the family {ri(K) : K ‚àà C(D)} is a partition of X. With this in mind, we introduce the following deÔ¨Ånition. Definition 2. A function f : X ‚Üí R is said to be piecewise linear over the chamber complex C (D) if there exists a sequence of pairs {(dK , aK ) : K ‚àà C (D)} ‚äÇ Rnx √ó R such that  f (x) = (dK , x + aK )1ri(K) (x), ‚àÄx ‚àà X. (14) K‚ààC (D)  
   
  Exploiting the Polyhedral Geometry of Stochastic Linear  
   
  369  
   
  Fig. 1. The polytope D of Example 1 and its chamber complex whose vertex (0, 0) is not a projection of a vertex of D.  
   
  In what follows, it will be useful to distinguish minimal chambers and maximal chambers, with respect to the inclusion ordering. The former are characterized by having nonempty interior, while the latter are the vertices in the chamber complex. Definition 3. Let D ‚äÇ Rnx √óRny be a polyhedron as described in (4). We deÔ¨Åne K (D) := {K ‚àà C (D) : int(K) = ‚àÖ} , V (D) := {v ‚àà X : {v} ‚àà C (D)} .  
   
  (15) (16)  
   
  We call K (D) the family of full-dimensional chambers of D, and V (D), the vertices of the chamber complex C (D) (which correspond to the family of zerodimensional chambers of D). It is worth mentioning that K (D) is a covering of X, i.e., X ‚äÇ K (D). This follows from the fact that each chamber is contained in a full-dimensional chamber. A very straightforward proposition (see [27]) is that  
   
  ext(K). (17) V (D) = K‚ààK (D)  
   
  A direct implication of this observation is the following corollary, which is one of the keystones of the enumeration algorithm that we propose. Corollary 1. If a function f : X ‚Üí R is continuous and piecewise linear over the chamber complex C (D), then it has at least one minimizer in V (D). We Ô¨Ånish this section with our main theorem. Theorem 1 (See [27]). Consider a random cost c with nonatomic distribution and Œ≤ : X ‚Üí P(Y ) the vertex-supported belief induced by c over S as deÔ¨Åned in (10). Then, 1. Œ≤ is weakly continuous, and thus for any lower semicontinuous function f : X √ó Y ‚Üí R, the problem min EŒ≤x [f (x, ¬∑)] has a solution. x‚ààX  
   
  2. The function x ‚àà X ‚Üí d1 , x+EŒ≤x [d2 , y] is continuous and piecewise linear over C (D). 3. For almost every sample {ÀÜ c1 , . . . , cÀÜN0 }, the function x ‚àà X ‚Üí d1 , x + N0 1 i=1 d2 , yi (x) is well-deÔ¨Åned, continuous and piecewise linear over C (D). N0 In particular, problem (1) has at least one solution over V (D).  
   
  370  
   
  5  
   
  G. MuÀú noz et al.  
   
  Algorithms  
   
  The rest of the work is focused on algorithms to solve problem (11) in the cases when we can evaluate the objective function x ‚Üí d1 , x + EŒ≤x [d2 , y], or solve problem (12), otherwise. Theorem 1 shows that both problems have the form min Œ∏(x),  
   
  (18)  
   
  x‚ààX  
   
  where Œ∏ : X ‚Üí R is a continuous function and it is piecewise linear over the chamber complex C (D) (with probability 1, in the case of problem (12)). Thus, we provide algorithms to solve this generic problem. 5.1  
   
  Enumeration Algorithm  
   
  Corollary 1 gives us a natural strategy to solve problem (18): It is enough to compute the chamber vertices V (D) and evaluate the corresponding objective function Œ∏ at each one of them. In this section we provide an enumeration algorithm to compute V (D) by sequentially solving mixed-integer programming problems which are formulated using F‚â§nx := {F ‚àà F : dim(F ) ‚â§ nx }, as shown in Proposition 3. We remind the reader that, due to the discussion in Example 1, we may need faces of diÔ¨Äerent dimensions to compute V (D). This is why we rely on the full set F‚â§nx . Remark 1. Computing V (D) is at least as hard as computing all vertices of a polytope. Indeed, given an arbitrary (full-dimensional) polytope P ‚äÜ Rn , one may consider D = P √ó [0, 1]. The vertices of P correspond exactly to V (D). To the best of our knowledge, the complexity of Ô¨Ånding all vertices of a polytope P is currently unknown; however, for a polyhedron P (not necessarily bounded), it is known that it is NP-complete to decide, given a subset of vertices of P , if there is a new vertex of P to add to the collection [20]. Therefore, we can expect that computing V (D) will be computationally expensive. For x ‚àà X, let us deÔ¨Åne the label of x as the set (x) := {F ‚àà F‚â§nx : x ‚àà œÄ(F )}. Endowing the set of all (Ô¨Ånitely many) labels with the order of the inclusion, one can show (see [27]) that x ‚àà V (D) ‚áê‚áí (x) is a maximal label.  
   
  (19)  
   
  Intuitively, this states that a vertex of a chamber is obtained by intersecting as many projections of faces as possible (of dimension ranging from 0 to nx ). Using this result, we can generate an element of V (D) through a MILP formulation that Ô¨Ånds an x such that (x) is a maximal label. The following formulation achieves this.  zF (20a) max z,x,y  
   
  s.t.  
   
  F ‚ààF ‚â§nx  
   
  Ax + ByF ‚â§ b AF x + BF yF ‚â• bF ‚àí M (1 ‚àí zF )  
   
  ‚àÄF ‚àà F‚â§nx ‚àÄF ‚àà F‚â§nx  
   
  (20b) (20c)  
   
  zF ‚àà {0, 1}  
   
  ‚àÄF ‚àà F‚â§nx  
   
  (20d)  
   
  Exploiting the Polyhedral Geometry of Stochastic Linear  
   
  371  
   
  Here, y and z stand for the vectors (yF : F ‚àà F‚â§nx ) and (zF : F ‚àà F‚â§nx ), respectively. For each F ‚àà F‚â§nx , AF , BF and bF are submatrices of A, B and b such that F = {(x, y) ‚àà D : AF x + BF y = b}. Finally, M is a vector of m positive values such that Ai x+Bi y ‚àíbi ‚â• ‚àíMi , for all (x, y) ‚àà D. This vector M is well deÔ¨Åned when D is a polytope and can be easily computed using m linear programs. Formulation (20) is straightforward. It tries to ‚Äúactivate‚Äù as many faces as possible such that the intersection of their projection is non-empty. Remark 2. We note that, while conceptually simple, (20) depends on the enumeration of all faces F‚â§nx which can be a highly challenging task. Other approaches below also rely on this. We have not yet devised a mechanism around this potentially expensive enumeration; moreover, in many of our computational experiments (Sect. 6), our approaches are actively making use of a signiÔ¨Åcant portion of F‚â§nx . For this reason, we can only handle small-size problems in this work. We discuss potential future avenues for solving these limitations in Sect. 6. Note that these diÔ¨Éculties are in line with the discussion of Remark 1. Let (z ‚àó , x‚àó , y ‚àó ) be an optimal solution of (20). It is not hard to see that x‚àó is an element of V (D), thus, we can collect it and focus on generating a new element of V (D). Noting that (x‚àó ) = {F ‚àà F‚â§nx : zF‚àó = 1}, we see that such new element can be obtained by adding the following inequality to (20)  zF ‚â• 1. (21) ‚àó =0 F ‚ààF ‚â§nx : zF  
   
  Since (x‚àó ) is a maximal label, we can easily see that constraint (21) is removing only the single element x‚àó of V (D) from (20). This is a so-called ‚Äúno-good cut‚Äù. This procedure can be iterated until the optimization problem becomes infeasible. In our computational experiments, however, we noted that detecting infeasibility was particularly challenging for the optimization solver, and thus devised an alternative strategy: we add a new binary variable s that can relax (21) when needed, and whose value will deÔ¨Åne the stopping criterion. Under these considerations, we next present the precise model we use. Suppose we have partially generated a set V ‚äÜ V (D), we generate an element of V (D) \ V or determine V = V (D) using the following optimization problem  zF (22a) max z,s,x,y  
   
  s.t.  
   
  F ‚ààF ‚â§nx  
   
  Ax + ByF ‚â§ b  
   
  ‚àÄF ‚àà F‚â§nx  
   
  (22b)  
   
  AF x + BF yF ‚â• bF ‚àí M (1 ‚àí zF )  zF + s ‚â• 1  
   
  ‚àÄF ‚àà F‚â§nx  
   
  (22c)  
   
  ‚àÄv ‚àà V  
   
  (22d)  
   
  F ‚àà(v)  
   
    
   
  zF ‚â§ |F‚â§nx |(1 ‚àí s)  
   
  (22e)  
   
  F ‚ààF ‚â§nx  
   
  zF ‚àà {0, 1} s ‚àà {0, 1}  
   
  ‚àÄF ‚àà F‚â§nx  
   
  (22f) (22g)  
   
  372  
   
  G. MuÀú noz et al.  
   
  Algorithm 1: Chamber vertex enumeration algorithm 1 2 3 4 5 6 7 8 9 10 11  
   
  Input: A, B, b deÔ¨Åning a polytope D = {(x, y) ‚àà Rnx √ó Rny : Ax + By ‚â§ b}; Set V = ‚àÖ, s‚àó = 0; Compute F‚â§nx (D); while true do Solve problem (22) and obtain an optimal solution (z ‚àó , s‚àó , x‚àó , y ‚àó ); if s‚àó = 0 then break ; end V ‚Üê V ‚à™ {x‚àó }; Store (x‚àó ) = {F ‚àà F‚â§nx (D) : zF‚àó = 1}; end Result: The set V = V (D)  
   
  Lemma 1 (see [27]). Problem (22) is always feasible provided that D = ‚àÖ. Moreover, in an optimal solution (z ‚àó , s‚àó , x‚àó , y ‚àó ), then one (and only one) of the following situations hold (a) s‚àó = 0 and x‚àó ‚àà V (D) \ V , or (b) s‚àó = 1 and V (D) = V . In Algorithm 1 we formalize our enumeration procedure. To solve problem (18), it is enough to run Algorithm 1, and evaluate Œ∏ over the set V = V (D). 5.2  
   
  Monte-Carlo Approximation Scheme  
   
  The previous enumeration algorithm of Sect. 5.1 has several drawbacks. First, it requires (in practice) computing the whole collection of faces of D, which might depend exponentially on the whole dimension nx + ny . And even with the collection of faces at hand, computing all chamber vertices in V (D) can be hard. Moreover, V (D) might be exponentially large. Another approach, that we explore in this section, is to try to compute the collection of full-dimensional chambers K (D). Despite the fact that K (D) might still be exponentially large, in some cases it is considerably smaller than C (D). Moreover drawing points x uniformly over X yield that œÉ(x) ‚àà K (D) almost surely. Indeed, this follows from Proposition 2.6 in [27] and the facts that there are Ô¨Ånitely many chambers in K (D), and only those in K (D) are not negligible. To simplify the exposition, we will assume that X ‚äÇ [0, 1]nx and we will write c X = [0, 1]nx \ X. To be able to consider samples in [0, 1]nx , we identify (x) = ‚àÖ for all x ‚àà X c . We base our algorithm in the following lemma. Lemma 2 (see [27]). Let = (¬Ø x) for some x ¬Ø ‚àà X and assume that K := œÉ(¬Ø x) is a full-dimensional chamber. Then, (x) ‚äÇ Fnx := {F ‚àà F : dim(F ) = nx }. Moreover, for each j ‚àà [nx ], the following linear problem  max t t, (yF )F ‚àà (23) (¬Ø x + tej , yF ) ‚àà F, ‚àÄF ‚àà , s.t.  
   
  Exploiting the Polyhedral Geometry of Stochastic Linear  
   
  373  
   
  Algorithm 2: Monte-Carlo algorithm 1 2 3 4 5 6 7 8 9 10 11  
   
  Input: A, B, b deÔ¨Åning a polytope D = {(x, y) ‚àà Rnx √ó Rny : Ax + By ‚â§ b}, Œ∏ : X ‚Üí R continuous and piecewise linear over C (D); Generate a (uniformly iid) training sample S of size N over [0, 1]nx ; Set List = ‚àÖ, x ÀÜ = N aN , Œ∏ÀÜ = ‚àû; Compute Fnx ; foreach Œæ ‚àà S do Compute  = {F ‚àà Fnx : Œæ ‚àà œÄ(F )}; if  ‚àà List or  = ‚àÖ then continue; end List ‚Üê List ‚à™ {}; Compute d as in Lemma 2; Solve the linear problem   
   
  min  
   
  d , x  
   
  s.t.  
   
  (x, yF ) ‚àà F, ‚àÄF ‚àà   
   
  x,(yF )F ‚àà  
   
  12 13 14 15  
   
  Ô¨Ånding a solution x ÀÜ and set the value Œ∏ÀÜ = Œ∏(ÀÜ x ); if Œ∏ÀÜ < Œ∏ÀÜ then x ÀÜ‚Üêx ÀÜ , Œ∏ÀÜ ‚Üê Œ∏ÀÜ ; end end ÀÜ for problem (18). Result: The pair solution-value (ÀÜ x, Œ∏)  
   
  has a solution t‚àój > 0. Finally, for every function Œ∏ : X ‚Üí R continuous and piecewise linear over the chamber complex C (D), the vector d := dK ‚àà Rnx   such that Œ∏ K = d , ¬∑ + aK (for some aK ‚àà R) can be computed as   
   
  d =  
   
    
   
  Œ∏(¬Ø x + t‚àónx enx ) ‚àí Œ∏(¬Ø x) Œ∏(¬Ø x + t‚àó1 e1 ) ‚àí Œ∏(¬Ø x) ,¬∑¬∑¬∑ , ‚àó t1 t‚àónx  
   
    
   
  .  
   
  With this result, we can establish a Monte-Carlo algorithm (see Algorithm 2) to approximate the solution of problem (18): we randomly draw points x ¬Ø x), and use Lemma 2 to optimize Œ∏ over œÉ(¬Ø x) via an from [0, 1]nx , compute (¬Ø LP formulation. Note that, to compute labels, we only need (with probability 1) access to Fnx , which might depend exponentially only on the lower-level dimension ny and not nx + ny (see [27]). The main drawback of Algorithm 2 is ÀÜ is an optimal solution of that we cannot ensure in general that the result (ÀÜ x, Œ∏) problem (18) or not. A measurement in terms of unseen chambers is proposed ÀÜ might be. in [27] to quantify how good the solution (ÀÜ x, Œ∏)  
   
  6  
   
  Numerical Experiments  
   
  We implemented both Algorithms 1 and 2 in Julia 1.8.2 [4], using Polymake [15] to compute the faces of a polytope and Gurobi 9.5.2 [16] to solve (22)  
   
  374  
   
  G. MuÀú noz et al.  
   
  and any auxiliary LP. Our code is publicly available in https://github.com/gmunoz/bilevelbayesian. All experiments were run single-threaded on a Linux machine with an Intel Xeon Silver 4210 2.2G CPU and 128 GB RAM. The main objectives behind these experiments are (1) to determine how Algorithm 1 scales and (2) how well the Monte-Carlo algorithm performs in comparison to the exact method. A global time limit of 15 min was set for Algorithm 1; in case this time limit is met, only the chamber vertices that were found are used. We focus our attention in sample average formulations, as in (12), where the lower-level cost is assumed to have a uniform distribution over the unit sphere. We use instances from two publicly available libraries: BOLib [32] and the bilevel instances in [1], which we call CoralLib. Since our approach relies on computing a (possibly exponentially) large number of faces, we can only consider low-dimensional instances at the moment: we restrict to nx + ny ‚â§ 10. Additionally, we consider randomly generated instances of the stochastic bilevel continuous knapsack problem [5]. These instances have the form: max ‚àíŒ¥x + d y x  
   
  s.t. x ‚àà [L, U ]  
   
  ‚éß c(œâ), y ‚é™ ‚é® min y y(x, œâ) solves a y ‚â§ x, ‚é™ ‚é© s.t. y ‚àà [0, 1]ny  
   
  (24) œâ ‚àà Œ© a.s.  
   
  In our experiments, we consider a to be a random non-negative vector, Œ¥ = 1/4 and d the vector of ones. We call Knapsack i an instance generated for ny = i. While these instances have a more eÔ¨Écient algorithm for them than the one presented here (see [5]), they are helpful in showing how well our generalpurpose Monte-Carlo algorithm performs. In all experiments, we used a sample of size 100 for the follower‚Äôs cost vector. The same sample is used in both algorithms to better compare their performance. Additionally, in Algorithm 2 we used samples of size 200 for the domain X. In Table 1, we compare the performance of both methods. The gap measures how far the value of the Monte-Carlo algorithm is from the exact method, i.e., if vali is the value obtained by Algorithm i, then the gap is Gap = |val1 |‚àí1 (val2 ‚àí val1 ). Since we ran Algorithm 1 with a time limit, it may be that Gap < 0, which indicates the Monte-Carlo algorithm performing better than the exact method. The results in Table 1 clearly shows an advantage of the Monte-Carlo approach over the exact method. The Monte-Carlo approach was able to meet or surpass the value of the exact method in almost all cases. In the largest examples, the Monte-Carlo had a much better performance, in some cases providing much better solutions than the exact method in shorter running times. The main (and clear) challenge for this work is scalability: while these results show short running times, these are all instances of small dimensions. The main bottleneck currently is the enumeration of the faces of a polytope. In the case of Algorithm 1, there does not seem to be much hope in improving this substan-  
   
  Exploiting the Polyhedral Geometry of Stochastic Linear  
   
  375  
   
  Table 1. Summary of results for Algorithms 1 and 2 for selected BOLib instances [32], CoralLib instances [1] and Knapsack instances [5]. The ‚ÄúSize‚Äù of the instance is (nx + ny , m). The ‚ÄúObj gap‚Äù column shows the gap between the values found for both algorithms; a negative gap indicates the stochastic method performed better. The ‚ÄúError‚Äù column shows the upper estimation of the volume of unseen chambers during the sampling process (see [27] for the details). The columns labeled ‚ÄúComputation Times‚Äù contain the running times (in seconds) for the computation of all the faces, the execution of Algorithm 1 and of Algorithm 2. The columns labeled ‚ÄúUsed faces‚Äù contain the number of faces that were explicitly used during the execution of each algorithm. Computation Times Used Faces Instance  
   
  Size  
   
  |F‚â§nx | |Fnx | Obj gap Error Faces Alg. 1 Alg. 2 Alg. 1 Alg 2.  
   
  BOLib/AnandalinghamWhite1990 (2,7)  
   
  12  
   
  6  
   
  0%  
   
  0%  
   
  1.4  
   
  3.8  
   
  7.2  
   
  12  
   
  5  
   
  BOLib/Bard1984a  
   
  (2,6)  
   
  10  
   
  5  
   
  0%  
   
  0%  
   
  3.1  
   
  6.8  
   
  7.9  
   
  10  
   
  5  
   
  BOLib/Bard1984b  
   
  (2,6)  
   
  10  
   
  5  
   
  0%  
   
  0%  
   
  1.4  
   
  4.1  
   
  7.5  
   
  10  
   
  5  
   
  BOLib/Bard1991Ex2  
   
  (3,6)  
   
  14  
   
  9  
   
  0%  
   
  0%  
   
  1.4  
   
  4.2  
   
  8.2  
   
  14  
   
  6  
   
  BOLib/BardFalk1982Ex2  
   
  (4,7)  
   
  45  
   
  17  
   
  0%  
   
  45%  
   
  1.7  
   
  4.3  
   
  7.6  
   
  45  
   
  5  
   
  BOLib/BenAyedBlair1990a  
   
  (3,6)  
   
  20  
   
  12  
   
  0%  
   
  0%  
   
  1.4  
   
  4.2  
   
  8.5  
   
  20  
   
  4  
   
  BOLib/BenAyedBlair1990b  
   
  (2,5)  
   
  6  
   
  3  
   
  0%  
   
  0%  
   
  1.5  
   
  4.3  
   
  8.0  
   
  6  
   
  3  
   
  BOLib/BialasKarwan1984a  
   
  (3,8)  
   
  20  
   
  12  
   
  0%  
   
  0%  
   
  1.6  
   
  4.4  
   
  8.4  
   
  20  
   
  10  
   
  BOLib/BialasKarwan1984b  
   
  (2,7)  
   
  12  
   
  6  
   
  0%  
   
  0%  
   
  1.5  
   
  4.3  
   
  8.0  
   
  12  
   
  5  
   
  BOLib/CandlerTownsley1982  
   
  (5,8)  
   
  111  
   
  48  
   
  1%  
   
  37%  
   
  1.8  
   
  7.9  
   
  18.5  
   
  111  
   
  16  
   
  BOLib/ClarkWesterberg1988  
   
  (2,3)  
   
  6  
   
  3  
   
  0%  
   
  0%  
   
  1.5  
   
  4.3  
   
  8.0  
   
  6  
   
  3  
   
  BOLib/ClarkWesterberg1990b  
   
  (3,7)  
   
  15  
   
  9  
   
  0%  
   
  0%  
   
  1.5  
   
  4.4  
   
  8.3  
   
  15  
   
  9  
   
  BOLib/GlackinEtal2009  
   
  (3,6)  
   
  20  
   
  5  
   
  0%  
   
  48%  
   
  1.6  
   
  4.8  
   
  8.0  
   
  20  
   
  3  
   
  BOLib/HaurieSavardWhite1990  
   
  (2,4)  
   
  8  
   
  4  
   
  0%  
   
  0%  
   
  1.6  
   
  4.5  
   
  8.3  
   
  8  
   
  4  
   
  BOLib/HuHuangZhang2009  
   
  (3,6)  
   
  20  
   
  12  
   
  0%  
   
  0%  
   
  1.6  
   
  4.4  
   
  8.6  
   
  20  
   
  7  
   
  BOLib/LanWenShihLee2007  
   
  (2,8)  
   
  14  
   
  7  
   
  0%  
   
  1%  
   
  1.6  
   
  4.5  
   
  8.2  
   
  14  
   
  6  
   
  BOLib/LiuHart1994  
   
  (2,5)  
   
  10  
   
  5  
   
  0%  
   
  0%  
   
  1.6  
   
  4.4  
   
  8.6  
   
  10  
   
  4  
   
  BOLib/MershaDempe2006Ex1  
   
  (2,6)  
   
  8  
   
  4  
   
  0%  
   
  0%  
   
  1.7  
   
  4.8  
   
  8.6  
   
  8  
   
  4  
   
  BOLib/MershaDempe2006Ex2  
   
  (2,7)  
   
  10  
   
  5  
   
  0%  
   
  64%  
   
  1.5  
   
  4.3  
   
  6.4  
   
  10  
   
  3  
   
  BOLib/TuyEtal1993  
   
  (4,7)  
   
  45  
   
  17  
   
  0%  
   
  54%  
   
  1.7  
   
  4.5  
   
  7.6  
   
  45  
   
  5  
   
  BOLib/TuyEtal1994  
   
  (4,8)  
   
  72  
   
  24  
   
  0%  
   
  46%  
   
  1.6  
   
  4.5  
   
  7.5  
   
  72  
   
  6  
   
  BOLib/VisweswaranEtal1996  
   
  (2,6)  
   
  8  
   
  4  
   
  0%  
   
  0%  
   
  1.4  
   
  4.3  
   
  7.9  
   
  8  
   
  4  
   
  BOLib/WangJiaoLi2005  
   
  (3,7)  
   
  23  
   
  14  
   
  0%  
   
  0%  
   
  1.5  
   
  4.4  
   
  8.6  
   
  23  
   
  5  
   
  CoralLib/linderoth  
   
  (6,15)  
   
  545  
   
  51  
   
  0%  
   
  74%  
   
  1.4  
   
  148.0  
   
  7.4  
   
  545  
   
  7  
   
  CoralLib/moore90 2  
   
  (2,7)  
   
  12  
   
  6  
   
  0%  
   
  0%  
   
  1.4  
   
  4.0  
   
  7.6  
   
  12  
   
  5  
   
  CoralLib/moore90  
   
  (2,8)  
   
  8  
   
  4  
   
  0%  
   
  0%  
   
  1.6  
   
  4.4  
   
  8.1  
   
  8  
   
  4  
   
  Knapsack 6  
   
  (7,15)  
   
  574  
   
  447  
   
  0%  
   
  0%  
   
  3.2  
   
  117.1  
   
  19.2  
   
  574  
   
  255  
   
  Knapsack 7  
   
  (8,17)  
   
  1278  
   
  1023  
   
  0%  
   
  0%  
   
  2.5  
   
  626.9  
   
  38.5  
   
  1278  
   
  575  
   
  Knapsack 8  
   
  (9,19)  
   
  2814  
   
  2303  
   
  -14%  
   
  0%  
   
  4.9  
   
  914.7  
   
  82.4  
   
  1990  
   
  1279  
   
  Knapsack 9  
   
  (10,21) 6142  
   
  5119  
   
  -380%  
   
  0%  
   
  15.4  
   
  984.2  
   
  181.4  
   
  3346  
   
  2815  
   
  tially: note in Table 1 that in all but the bottom two entries1 , Algorithm 1 used all available faces. This is because the algorithm heavily relies on maximal labels, which is important in our procedure to not repeat chambers when enumerating. 1  
   
  The last two entries of Table 1 correspond to cases where F‚â§nx was not fully computed due to the time limit.  
   
  376  
   
  G. MuÀú noz et al.  
   
  Nonetheless, we still believe Algorithm 1 can be useful as a baseline that has optimality guarantees. Algorithm 2, however, could potentially be improved signiÔ¨Åcantly. First of all, recall that this approach only uses the faces of dimension nx (i.e. Fnx ), which can be considerably smaller than F‚â§nx (see columns 3 and 4 of Table 1). Therefore, a more intricate enumeration that exploits this could be devised. Additionally, and perhaps more importantly, note that in the instances where |Fnx | is not too small (say, more than 40) Algorithm 2 only uses a fraction of Fnx in its execution. This indicates that one could heavily restrict the faces to consider initially and generate more on-the-Ô¨Çy, much like in a column generation approach. Another potential improvement path is exploiting more structure of a particular family of instances, which may indicate which are the faces that one would truly need.  
   
  References 1. Coral bilevel optimization problem library. https://coral.ise.lehigh.edu/data-sets/ bilevel-instances/ Accessed 3 Nov 2022 2. Bazaraa, M.S., Sherali, H.D., Shetty, C. M.: Nonlinear programming: theory and algorithms. John Wiley Sons (2013) 3. Beck, Y., Ljubi¬¥c, I., Schmidt, M.: A survey on bilevel optimization under uncertainty. European J. Oper. Res., (2023) (In Press) 4. Bezanson, J., Edelman, A., Karpinski, S., Shah, V.B.: Julia: A fresh approach to numerical computing. SIAM review 59(1), 65‚Äì98 (2017) 5. Buchheim, C., Henke, D., Irmai, J.: The stochastic bilevel continuous knapsack problem with uncertain follower‚Äôs objective. J. Optim. Theory. Appl. 194, 521‚Äì 542 (2022) 6. Burtscheidt, J., Claus, M.: Bilevel Linear Optimization Under Uncertainty. In: Dempe, S., Zemkoho, A. (eds.) Bilevel Optimization. SOIA, vol. 161, pp. 485‚Äì511. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-52119-6 17 7. Claus, M.: On continuity in risk-averse bilevel stochastic linear programming with random lower level objective function. Oper. Res. Lett. 49(3), 412‚Äì417 (2021) 8. Claus, M.: Existence of solutions for a class of bilevel stochastic linear programs. European J. Oper. Res. 299(2), 542‚Äì549 (2022) 9. De Loera, J., Rambau, J., Santos, F.: Triangulations: structures for algorithms and applications, volume 25. Springer Science Business Media (2010) https://doi.org/ 10.1007/978-3-642-12971-1 10. Dempe, S.: Foundations of bilevel programming. Springer Science Business Media (2002) https://doi.org/10.1007/b101970 11. Dempe, S., Kalashnikov, V., P¬¥erez-Vald¬¥es, G.A., Kalashnykova, N.: Bilevel programming problems. Energy Systems. Springer, Heidelberg, 2015. Theory, algorithms and applications to energy networks https://doi.org/10.1007/978-3-66245827-3 12. Dempe, S., Zemkoho, A. (eds.): SOIA, vol. 161. Springer, Cham (2020). https:// doi.org/10.1007/978-3-030-52119-6 13. Forcier, M.: Multistage stochastic optimization and polyhedral geometry. PhD. ¬¥ Thesis, Ecole de Ponts - ParisTech (2022)  
   
  Exploiting the Polyhedral Geometry of Stochastic Linear  
   
  377  
   
  14. Forcier, M., Gaubert, S., Lecl`ere, V.: Exact quantization of multistage stochastic linear problems (2021) (preprint - arXiv:2107.09566) 15. Gawrilow, E., Joswig, M., polymake: a framework for analyzing convex polytopes. In Polytopes‚Äìcombinatorics and computation of DMV Sem (Oberwolfach, 1997), 29, pp. 43‚Äì73. Birkh¬® auser, Basel, (2000) 16. Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual (2022) 17. Hiriart-Urruty, J.-B., Lemar¬¥echal, J.-B.: Convex analysis and minimization algorithms I: Fundamentals, volume 305. Springer science business media (2013) https://doi.org/10.1007/978-3-662-02796-7 18. Homem-de Mello, T., Bayraksan, G.: Monte Carlo sampling-based methods for stochastic optimization. Surv. Oper. Res. Manag. Sci., 19(1), 56‚Äì85 (2014) 19. Ivanov, S.V.: A bilevel programming problem with random parameters in the follower‚Äôs objective function. Diskretn. Anal. Issled. Oper. 25(4), 27‚Äì45 (2018) 20. Khachiyan, L., Boros, E., Borys, K., Gurvich, V., Elbassioni, K.:Generating all vertices of a polyhedron is hard. In 20th Anniversary Volume, 1‚Äì17. Springer (2009) https://doi.org/10.1007/s00454-008-9050-5 21. Kleinert, T., Labb¬¥e, M., Ljubi¬¥c, I., Schmidt, M.: A survey on mixed-integer programming techniques in bilevel optimization. EURO J. Comput. Optim. 9, 100007 (2021) 22. Klenke, A: Probability Theory: a Comprehensive Course. Springer (2014) https:// doi.org/10.1007/978-1-4471-5361-0 23. Leobacher, G., Pillichshammer, F.: Introduction to quasi-Monte Carlo integration and applications. Compact Textbooks in Mathematics. Birkh¬® auser/Springer, Cham (2014) 24. Lu, S., Robinson, S.M.: Normal fans of polyhedral convex sets: structures and connections. Set-Valued Anal. 16(2‚Äì3), 281‚Äì305 (2008) 25. Mak, W.-K., Morton, D.P., Wood, R.K.: Monte Carlo bounding techniques for determining solution quality in stochastic programs. Oper. Res. Lett. 24(1‚Äì2), 47‚Äì56 (1999) 26. Mallozzi, L., Morgan, J.: Hierarchical Systems with Weighted Reaction Set, pp. 271‚Äì282. Springer, US, Boston, MA, (1996) https://doi.org/10.1007/978-1-48990289-4 19 27. MuÀú noz, G., Salas, D., Svensson, A.: Exploiting the polyhedral geometry of stochastic linear bilevel programming (2023). (preprint - arXiv:2211.02268. Former title: Linear bilevel programming with uncertain lower-level costs) 28. Rambau, J., Ziegler, G.M.: Projections of polytopes and the generalized baues conjecture. Discrete Comput. Geom. 16(3), 215‚Äì237 (1996) 29. Salas, D., Svensson, A.: Existence of solutions for deterministic bilevel games under a general bayesian approach (2020) (preprint - arXiv:2010.05368) 30. Shapiro, A., Dentcheva, D., Ruszczy¬¥ nski, A.: Lectures on stochastic programming‚Äì modeling and theory, volume 28 of MOS-SIAM Series on Optimization. 3rd eds Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA; Mathematical Optimization Society, Philadelphia, PA, (2021) 31. Stackelberg, V.H.: Marktform und Gleichgewitch. Springer (1934) https://doi.org/ 10.1007/978-3-642-12586-7 32. Zhou, S., Zemkoho, A.B., Tin, A.: BOLIB: Bilevel Optimization LIBrary of Test Problems. In: Dempe, S., Zemkoho, A. (eds.) Bilevel Optimization. SOIA, vol. 161, pp. 563‚Äì580. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-521196 19  
   
  Towards an Optimal Contention Resolution Scheme for Matchings Pranav Nuti(B) and Jan Vondr¬¥ ak Stanford University, Stanford, CA 94305, USA {pranavn,jvondrak}@stanford.edu  
   
  Abstract. In this paper, we study contention resolution schemes for matchings. Given a fractional matching x and a random set R(x) where each edge e appears independently with probability xe , we want to select a matching M ‚äÜ R(x) such that Pr[e ‚àà M | e ‚àà R(x)] ‚â• c, for c as large as possible. We call such a selection method a c-balanced contention resolution scheme. Our main results are (i) an asymptotically (in the limit as x‚àû goes to 0) optimal  0.544-balanced contention resolution scheme for general matchings, and (ii) a 0.509-balanced contention resolution scheme for bipartite matchings. To the best of our knowledge, this result establishes for the Ô¨Årst time, in any natural relaxation of a combinatorial optimization problem, a separation between (i) oÔ¨Ñine and random order online contention resolution schemes, and (ii) monotone and non-monotone contention resolution schemes.  
   
  Keywords: Contention resolution  
   
  1  
   
  ¬∑ Matching ¬∑ Random graphs  
   
  Introduction  
   
  Suppose that there are n employees looking for jobs. Each employee likes a random set of jobs which, on average, has cardinality one. n jobs are available in total, and no job is especially popular amongst the employees, though some employees might have a strong preference for some particular jobs. We would like to match the employees to jobs. We are immediately faced with many natural questions: On average, what fraction of employees can we match to a job they like? Can we match employees to jobs in a fair way, without partially favoring any particular employee? What if no employee has a strong preference for any particular job? Is it easier to match employees if we learn about their preferences all at once, rather than if we learn The full version of this paper can be found at https://arxiv.org/abs/2211.03599. It contains omitted proofs, discussion of the relationship of this paper with van der Waerden‚Äôs conjecture, and an application of our contention resolution scheme to a combinatorial allocation problem. J. Vondr¬¥ ak‚ÄîSupported by NSF Award 2127781. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 378‚Äì392, 2023. https://doi.org/10.1007/978-3-031-32726-1_27  
   
  Towards an Optimal Contention Resolution Scheme for Matchings  
   
  379  
   
  about them in an online fashion? Our paper provides answers to these questions, through the lens of contention resolution schemes. Contention resolution schemes aim to solve the following problem: Given a family of feasible sets F ‚äÇ 2E and a random set R sampled from a distribution on 2E , how can we choose a feasible subset I ‚äÜ R, I ‚àà F, so that each element from R is picked with some guaranteed conditional probability: Pr[e ‚àà I | e ‚àà R] ‚â• c for some Ô¨Åxed c > 0 and all e ‚àà E? We call such a scheme c-balanced. This condition is a kind of fairness constraint, ensuring every element e has a reasonable chance of making it into I. In this paper, we think about E as the set of edges in a graph, and F as the set of matchings of the graph. The constant c is the conditional probability with which we can ensure an edge ends up in the matching I we pick, given it appears in R. A natural assumption on the random set R is that it comes from a product distribution with marginal probabilities xe such that x is in a polytope corresponding to the family F (either the exact convex hull, or a suitable relaxation, depending on the application), i.e., roughly speaking, R on average, is in F. For matchings on graphs, this corresponds to an assumption that each edge e appears in R independently with probability xe , and the vector (xe )e‚ààE belongs to the matching polytope, i.e, is a fractional matching. The formal notion of contention resolution was Ô¨Årst investigated as a tool for randomized rounding. In this setting, we have an optimization problem subject to a constraint, and x represents a fractional solution to a relaxation of the problem. Contention resolution is one of the phases of a randomized rounding approach to converting this fractional solution into an integral solution: First we generate a random set R, by sampling each element e independently with probability xe , and then we select a subset of R which satisÔ¨Åes the desired constraint. The Ô¨Çexibility of the approach enables its wide applicability in combinatorial optimization. This approach was introduced by Feige [2], who developed a contention resolution scheme (CRS) for matchings on the restricted class of star graphs, in the context of an application to combinatorial auctions. CRSs were then investigated more systematically in [4] in the context of submodular optimization. In particular, an optimal (1 ‚àí 1/e)-balanced CRS was identiÔ¨Åed in [4] for the case where F forms a matroid. The 1 ‚àí 1/e factor is optimal even for F = {I : |I| ‚â§ 1}. For applications in submodular optimization, it turns out that an additional property of monotonicity is often useful: A CRS is called monotone, if for every element e, the probability that e is selected from a set R is non-increasing as a function on the sets R containing e. This property is generally needed for the analysis of randomized rounding with a submodular objective function [4]. However, for some applications it is not necessary that a CRS is monotone; in particular it was not needed in Feige‚Äôs original application in [2], and it is also unnecessary for a related application that we present in the full version of this paper.  
   
  380  
   
  P. Nuti and J. Vondr¬¥ ak  
   
  Contention resolution has also been studied in online settings (where it has seen applications to prophet inequalities and sequential pricing problems, for example) with either adversarial or random ordering of elements [6,8‚Äì10,13]. For example, for matroids there is a 1/2-balanced adversarial order online CRS [9]. We do not investigate online contention resolution here, but we should mention that in prior results, random order online contention resolution schemes (RCRS) are able to match the best known oÔ¨Ñine results: For matroids, there is a (1 ‚àí 1/e)-balanced RCRS, due to an elegant LP duality connection with prophet inequalities [9]. The situation is much more complicated when F encodes constraints such as matchings and the optimal factors are generally unknown. The cases of bipartite and general matchings have attracted attention due to their fundamental nature and their frequent appearance in applications. We can think of matching constraints as an intersection of two matroid constraints, and for an intersection of 1 -balanced RCRS [8]; in particular, this gives k matroid constraints, there is a k+1 a 1/3-balanced RCRS (and hence also an oÔ¨Ñine CRS with the same factor) for bipartite matchings. Recent work in both oÔ¨Ñine and online settings has signiÔ¨Åcantly improved the factor of 1/3. In the oÔ¨Ñine setting, [7] gives a (1 ‚àí e‚àí2 )/2  0.432-balanced scheme for general matchings, which can be improved slightly further [11]. Very interestingly, [11] identiÔ¨Åes the optimal monotone scheme for bipartite matchings, which achieves a balancedness of  0.476. Nevertheless, the optimal CRSs for bipartite and general matchings are still unknown. The primary reason that it seems to be harder to obtain the optimal non-monotone scheme is that decisions on whether an edge should be included in the matching need not be local (i.e., a function of the edge‚Äôs immediate neighborhood), and it is harder to analyze the behavior of an algorithm that makes non-local decisions. In terms of impossibility results, an upper bound of  0.544 follows from a classical paper of Karp and Sipser [1], as discussed in [7]. In the online setting, the best known CRSs are due to recent results in [12]: In the random order case, they provide a 0.474-balanced scheme for general matchings and a 0.476-balanced scheme for bipartite matchings, and in the bipartite case, they also establish an upper bound of 0.5. Notably, the 0.474-balanced scheme is in fact the best known CRS for general matchings, whether oÔ¨Ñine or online. In the adversarial order case, they provide a 0.344-balanced scheme for general matchings and a 0.349-balanced scheme for bipartite matchings. 1.1  
   
  Our Results  
   
  To explain our results, we start by formally setting up some notation. Given a graph G = (V, E), a fractional matching is a point x ‚àà [0, 1]E in the matching polytope, i.e., a point in the convex hull of vectors 1M for all matchings M in G. For a fractional matching x, let xuv be the component of x corresponding to the edge (u, v).  
   
  Towards an Optimal Contention Resolution Scheme for Matchings  
   
  381  
   
  The problem we are interested in studying is: Contention Resolution for Matchings. We are given a fractional matching x, and a random set R(x) where edges appear independently with probabilities xuv . Our goal is to choose a matching M ‚äÜ R(x) such that for every edge (u, v), Pr[(u, v) ‚àà M | (u, v) ‚àà R(x)] ‚â• c. Such a scheme is called c-balanced, and we want to Ô¨Ånd a scheme with c as large as possible. The main questions we ask are: (i) Is there a contention resolution scheme for matchings achieving the upper bound  0.544 of Karp and Sipser? (ii) Is there a separation between the optimal c for online and oÔ¨Ñine contention resolution schemes? (iii) Is there a separation between the optimal c for monotone and non-monotone contention resolution schemes? In this paper, we prove the following results. The Ô¨Årst result, which applies to both bipartite and non-bipartite matchings, is an attempt to answer (i). Theorem 1. Assuming that x is a fractional matching such that x‚àû ‚â§ , there is (Œ≥ ‚àí f ())-balanced contention resolution scheme, where Œ≥  0.544 is the impossibility bound of Karp and Sipser and lim‚Üí0 f () = 0. For fractional matchings without any assumption on their ‚àû norm1 , we present an improved CRS in the bipartite case. Theorem 2. There is 0.509-balanced contention resolution scheme for bipartite matchings. This theorem answers questions (ii) and (iii), since the optimal RCRS for bipartite matchings is at most 0.5-balanced, and the optimal monotone CRS for bipartite matchings is  0.476-balanced. Our theorem thus establishes separations that, to our knowledge, have not been demonstrated in any other natural relaxations of combinatorial optimization problems before. (Note that for matroids, the known optimal (1 ‚àí 1/e)-balanced schemes are monotone.) Returning to the context we started this paper with, our results establish that we can match more than half of all the employees to jobs they like without partially favoring any particular employee, and in case no employee has a strong preference for any particular job, we can do better, and match 54% of employees to jobs. This is a signiÔ¨Åcant improvement over what we can do if we learn the employees preferences in an online fashion. We should also mention here the important concept of a correlation gap. Informally, the correlation gap measures how much we might lose while optimizing a function, in the worst case, by assuming that the distributions that deÔ¨Åne 1  
   
  It might appear from the work of Bruggmann and Zenklusen (see lemma 7 in [11]) that the assumption of x‚àû ‚â§  should be easy to drop from Theorem 1. This would be the case if our theorem applied to graphs with parallel edges, which unfortunately, it does not.  
   
  382  
   
  P. Nuti and J. Vondr¬¥ ak  
   
  the function are independent rather than correlated. In the context of bipartite matchings,  the correlation gap is deÔ¨Åned as the minimumpossible ratio between E[max{ e‚ààM we : M ‚äÜ R(x), M is a matching}] and e‚ààE we xe , where x is a fractional bipartite matching and w is any vector of weights. By LP duality (see [4]), Theorem 2 also provides (the best known) lower bound of 0.509 on the correlation gap for bipartite matchings. In light of Theorem 1, we believe that the correlation gap for bipartite (and perhaps even non-bipartite) matchings is indeed the Karp-Sipser bound of Œ≥  0.544, and the optimal CRS is Œ≥-balanced. 1.2  
   
  Our Techniques  
   
  Our Theorem 1 follows from an improved and simpliÔ¨Åed analysis of Karp and Sipser‚Äôs algorithm [1] for constructing matchings by adding random edges adjacent to leaves. While we utilize many of the ideas from Karp and Sipser‚Äôs paper, our analysis of the algorithm is an improvement in several ways: ‚Äì We obtain a contention resolution scheme, while Karp and Sipser only compute the expected size of the maximum matching. This yields the somewhat surprising conclusion that Karp and Sipser‚Äôs algorithm works just as well for weighted matchings as it does for unweighted matchings. ‚Äì We avoid Karp and Sipser‚Äôs (technically complicated) use of the so-called diÔ¨Äerential equation method. We also avoid the use of generating functions, another method used recently to calculate the expected size of the maximum matching in random graphs [5]. ‚Äì We obtain results for any random graph R(x) constructed from a fractional matching satisfying x‚àû ‚â§ , unlike Karp and Sipser who only consider the Erdos-Renyi random graph Gn,c/n . Many previous results require that there be some kind of symmetry in the random graph to obtain bounds on the size of the matching. We stress that we do not need to make any such assumption on R(x). We do need to assume that x‚àû ‚â§ . This assumption is useful because it ensures that the neighbourhood of any particular edge looks like a random tree. A closely related assumption (‚Äúlocal weak convergence‚Äù) has been considered previously in the literature. This assumption, together with recursive distributional equations, is used to formalize various statistical mechanical heuristics regarding matchings in random graphs. Most related to our work is the work of Bordenave, Lelarge, and Salez [3]. Once again, the advantage of our method is that we obtain a CRS (as opposed to computing the expected size of the maximum matching) and we avoid the use of technically complicated tools. These improvements come at a cost‚Äìwe assume that the average degree of each vertex is less than or equal to 1. The theoretical and practical signiÔ¨Åcance of this case, and the importance of contention resolution schemes, make this trade-oÔ¨Ä a good choice. Our Theorem 2 requires several new techniques, although the basic idea can be traced back to Karp and Sipser as well: When deciding which edge incident  
   
  Towards an Optimal Contention Resolution Scheme for Matchings  
   
  383  
   
  to a vertex we should add to a matching, it is beneÔ¨Åcial to pick an edge which is adjacent to a leaf, since it doesn‚Äôt block us from adding other edges into the matching. It turns out that in general, it is actually better not to follow this rule absolutely (at least in our analysis) but we still pick degree-1 edges with signiÔ¨Åcant priority over other edges. We present two diÔ¨Äerent schemes using these ideas; the Ô¨Årst one is simpler and achieves a factor  0.480 (already establishing the separation between monotone and non-monotone schemes). An interesting feature of this scheme is that it can be implemented as a parallel algorithm with each vertex independently making decisions about whether to include an edge adjacent to it in the matching by looking only at its immediate neighborhood. The best schemes known previously did not have this useful property. Our more complicated scheme achieves a factor  0.509 (thus demonstrating a separation between oÔ¨Ñine CRSs and RCRSs). Both schemes rely on an extended version of contention resolution for choosing 1 element from a possibly correlated distribution, which we present in Sect. 3, and the 0.509-balanced scheme uses the FKG inequality to handle correlations between edges in the Ô¨Ånal stage. Throughout this paper, even though we state our theorems for fractional matchings x,we will actually only need to assume that x satisÔ¨Åes the vertex constraints v xuv ‚â§ 1. Furthermore, we can always assume that x satisÔ¨Åes  x = 1 for every u. We can achieve this by adding vertices and edges with v uv probabilities such that the edge probabilities at each vertex add up to 1; this only makes the task of designing a CRS more diÔ¨Écult.  
   
  2 2.1  
   
  An Optimal CRS When x‚àû ‚Üí 0 The Karp-Sipser Algorithm  
   
  The Karp-Sipser algorithm is a method to select a matching in a graph. Given a graph G, the algorithm deletes all the degree 0 vertices, selects a random degree 1 vertex (if one exists), and adds the edge adjacent to it to the matching. Then, it deletes all the edges adjacent to the edge just added to the matching, and recurses on the newly obtained graph G . Note that unlike in the paper of Karp and Sipser, we do not use a two stage process to generate the matching. An attractive feature of the Karp-Sipser algorithm is that it doesn‚Äôt ‚Äúmake any mistakes‚Äù. This is because for any vertex v of degree 1 in a graph G, G has a maximum matching in which v is matched. If an edge is deleted by the algorithm at some stage, we will say that it disappears. We also say that a vertex is added to the matching if an edge adjacent to it is added to the matching. Before we discuss the analysis of the algorithm, we take a brief detour. 2.2  
   
  Random Trees  
   
  Consider the following method to generate a random tree in steps. Fix two special vertices, u and v, and draw an edge between them. In step i, for each vertex at  
   
  384  
   
  P. Nuti and J. Vondr¬¥ ak  
   
  the depth i ‚àí 1, independently sample a Poisson random variable with mean 1, and add as many children to the vertex as the obtained sample. Stop at step j if there are no vertices at depth j ‚àí 1. Let us call the random tree generated by this process T . Since the two subtrees of u and v are independent copies of a Galton-Watson process with 1 expected child at each node, it is straightforward to prove that this process terminates with probability 1. So it is almost always true that this process produces a Ô¨Ånite tree. The following lemma explains why we care about the process T : Up to small errors, it describes the distribution of the connected component containing a given edge (u, v) in R(x). We omit the proof which involves a coupling argument and an application of Le Cam‚Äôs theorem.  Lemma 1. Let x be a fractional matching with w xvw = 1 for every vertex v, and x‚àû ‚â§ . Let R(x) be the corresponding random graph. Let us condition on (u, v) ‚àà R(x) and define N ((u, v)) to be the connected component in R(x) containing (u, v). Let T be a random tree produced by the process described above and T0 be any finite realization of the process. Then | Pr[N ((u, v)) = T0 | (u, v) ‚àà R(x)] ‚àí Pr[T = T0 ]| = O(|T0 |2 ). We stress that the lemma is only true for graphs without parallel edges. For graphs with parallel edges, the lemma fails, even for the simple case of a graph with only two vertices. The use of this lemma makes it impossible to apply the work of Bruggmann and Zenklusen (lemma 7 in [11]). 2.3  
   
  The Karp-Sipser Algorithm on Trees  
   
  It is easy to prove by induction (using the fact that trees always have degree 1 vertices) that in an execution of the Karp-Sipser algorithm on a forest, an edge must eventually either disappear, or else, is added to the matching. Together with the fact that the Karp-Sipser algorithm does not make mistakes, this shows that the Karp-Sipser algorithm Ô¨Ånds a maximum matching in a tree. Given a tree, we would like to be able to analyze which vertices and edges end up in the matching the algorithm selects, independent of the random choices the algorithm makes. To that end, consider the following algorithm to label the vertices of a tree (this is similar to, but not exactly same as the scheme in [1]): Root the tree at an arbitrary vertex. Starting at the maximum possible depth, look at all the vertices at a Ô¨Åxed depth. If a vertex has no L children (this can perhaps be true vacuously), label it L. Else, label it W. Iteratively label vertices higher in the tree, until the root of the tree receives a label. The following claims are true (regardless of the chosen root, and regardless of the random choices the algorithm makes): 1. If an edge between a W parent and an L child disappears, it must be because the W vertex was added to the matching. 2. Every W vertex is added to the matching.  
   
  Towards an Optimal Contention Resolution Scheme for Matchings  
   
  385  
   
  3. Every edge between two W vertices disappears. Proof of claim 1. Suppose by way of contradiction that an edge between a W parent and L child disappears because the L vertex was added to the matching. Certainly, this does not happen in the Ô¨Årst step of the execution of the algorithm. Consider the very Ô¨Årst time it happens. The L vertex must have been added to the matching through a W labelled child it has. This W vertex must have degree 1, and so the edge connecting it to an L child must have disappeared. This contradicts our assumption of the original edge being the Ô¨Årst edge between a W parent and an L child that has disappeared because the L vertex was added to the matching.  
   
  Proof of claim 2. Every W vertex has an edge connecting it to an L child; either that edge disappears, and the claim follows by claim 1, or that edge is added to the matching and the claim still follows.  
   
  Proof of claim 3. Suppose by way of contradiction that an edge between two W vertices is added to the matching. Consider the state of the graph just before this edge is added. One of the vertices must have degree 1, so an edge connecting to its L child must have disappeared. But the only way such an edge can disappear is by the W vertex being added to the matching, contradiction!  
   
  2.4  
   
  Putting It Together  
   
  We can now calculate the probability with which the Karp-Sipser algorithm, when executed on the random tree T , adds the special edge between u and v to the matching. To this end, Ô¨Årst label the trees rooted at u and v using the procedure described in the previous section (imagining the special edge connecting u and v does not exist, and we are just labelling two diÔ¨Äerent rooted trees). Let us Ô¨Årst calculate the probability Œª that u is labelled L: Œª = Pr[u is labelled L] = Pr[u has no children labelled L] ‚àû  Pr[u has k children and none of them are labelled L] = =  
   
  k=0 ‚àû  k=0  
   
  e‚àí1 (1 ‚àí Œª)k = e‚àí1 ¬∑ e1‚àíŒª = e‚àíŒª k!  
   
  Œª is thus the unique real number which solves the equation x = e‚àíx . Second, let us calculate the probability that the edge between u and v is added to the matching, and v is labelled L. Imagine now rooting the random tree T at u. This does not change the label of any of the vertices except possibly u which is now labelled W. This means that u must end up in the matching. None of the edges connecting u with any of its W children end up in the matching. All the edges connecting u  
   
  386  
   
  P. Nuti and J. Vondr¬¥ ak  
   
  with any of its L children, and the special edge between u and v are completely symmetric from the standpoint of the execution of the Karp-Sipser algorithm. Therefore, Pr[(u, v) is added to the matching, v is labelled L] ‚àû  Pr[(u, v) is added to the matching, v is labelled L, u has k L children] = =  
   
  k=0 ‚àû  k=0  
   
  ‚àû   ‚àû  Œª  r k e‚àí1 Œªk+1 e‚àíŒª = = (eŒª ‚àí 1)e‚àíŒª = 1 ‚àí Œª Œª (1 ‚àí Œª)r‚àík k k+1 r! (k + 1)! r=k  
   
  k=0  
   
  Third, note that if we initially labelled both u and v L, then (u, v) must end up in the matching. This is because if we imagine rooting the tree at u, u is labelled W, so ends up in the matching, but the only way this can happen is if (u, v) ends up in the matching since all of its other children are labelled W. Fourth, note that if we initially labelled both u and v W, then (u, v) must disappear. This is because if we imagine rooting the tree at u, the labelling remains the same, and every edge between W vertices disappears. Finally, we can compute the probability that the special edge (u, v) ends up in the matching selected by Karp-Sipser as the sum of the probabilities of the edge ending up in the matching when u and v are labelled (respectively) L and L, L and W, W and L, and W and W. This is equal to 2(1 ‚àí Œª) ‚àí Œª2 . Theorem 1 now follows from a careful application of Lemma 1. Lemma 1 involves establishing a close correspondence between the neighborhood of an edge in the Galton-Watson process and the neighborhood of an edge in R(x). This correspondence can be exploited to obtain a correspondence in the behavior of the Karp-Sipser algorithm in the two settings. We omit the details, but we show here how Lemma 1 can be applied to prove the weaker statement that for fractional matchings x such that x‚àû ‚Üí 0, the neighborhood of any edge (u, v) does not contain a cycle with high probability. This weaker statement contains the main idea of the proof of Theorem 1. Lemma 1 implies that for any set of Ô¨Ånite trees F = {T1 , T2 , . . . , Tm } that are realizations of the random process T ,    | Pr[N ((u, v)) ‚àà F | (u, v) ‚àà R(x)] ‚àí Pr[T ‚àà F ]| = O  |Ti |2 and hence it follows that  
   
     | Pr[N ((u, v)) ‚àà / F | (u, v) ‚àà R(x)] ‚àí (1 ‚àí Pr[T ‚àà F ])| = O  |Ti |2  
   
  and so, for any F we have lim Pr[N ((u, v)) contains a cycle | (u, v) ‚àà R(x)]  
   
  ‚Üí0  
   
  ‚â§ lim Pr[N ((u, v)) ‚àà / F | (u, v) ‚àà R(x)] ‚Üí0  
   
  = 1 ‚àí Pr[T ‚àà F ]  
   
  Towards an Optimal Contention Resolution Scheme for Matchings  
   
  387  
   
  Since we know that T produces a Ô¨Ånite tree with probability 1, we can take F larger and larger to prove that lim Pr[N ((u, v)) contains a cycle | (u, v) ‚àà R(x)] = 0  
   
  ‚Üí0  
   
  3  
   
  Improved CRSs for Bipartite Matchings  
   
  Now we turn to contention resolution for bipartite matchings, without any assumption on the ‚àû norm of the fractional matching. A basic building block of our CRSs is the following theorem which establishes the existence of a scheme for choosing 1 out of n elements (historically the Ô¨Årst CRS [2]). Theorem 3. Suppose that D is a distribution on 2E such that for every set S ‚äÜ E,  Pr [S ‚à© R = ‚àÖ] ‚â• Œ≤i . R‚àºD  
   
  i‚ààS  
   
  Then there is a monotone contention resolution scheme for choosing one element e(R) from R ‚àº D such that Pr[e(R) = i] ‚â• Œ≤i for every i ‚àà E. This theorem can be proved using a max-Ô¨Çow/min-cut argument, as brieÔ¨Çy discussed in [2]. We omit the proof. 3.1  
   
  A 0.480-Balanced Scheme for Bipartite Matchings  
   
  Before going to the proof of Theorem 2, we will show the existence of a simple 2(1 ‚àí e‚àí1/e ) ‚àí e‚àí2 -balanced contention resolution scheme for bipartite matchings2 . We remark that 2(1 ‚àí e‚àí1/e ) ‚àí e‚àí2 ‚â• 0.480 and hence this already beats the optimal monotone scheme for bipartite matchings. By necessity, our scheme is non-monotone and this result establishes a strict separation between monotone and non-monotone schemes for bipartite matchings. The Simple Scheme: 1. For each edge (u, v) (with probability xuv of appearing), independently ‚àíxuv declare it active with probability 1‚àíexuv given it appears. 2. For each vertex u, call an active edge (u, v) ‚Äúavailable at vertex u‚Äù if v has no other edges adjacent to it which are active. 3. Using a contention resolution scheme for 1 element, select one of the available edges at each vertex. Ensure that an edge (u, v) gets selected at vertex u with  2xuv xuv . probability at least xuv (1 ‚àí e‚àí1/e ) ‚àí 2e12 + e 2e‚àíe 2 4. The set of edges selected at all the vertices form the matching M . 2  
   
  Note the similarity in the expression to 2(1 ‚àí Œª) ‚àí Œª2 = 2(1 ‚àí e‚àíŒª ) ‚àí Œª2 , the constant from our previous analysis. This similarity is not a coincidence, and we can think of the scheme we describe as Ô¨Årst-order approximation to Karp-Sipser.  
   
  388  
   
  P. Nuti and J. Vondr¬¥ ak  
   
  The Ô¨Årst step of the scheme is a kind of pre-processing which ensures that edges with high probabilities of appearing don‚Äôt destroy the chances of neighbouring edges getting picked by the scheme. This strategy has also been used in the literature previously [11]. The second step is a Ô¨Årst attempt at using the idea that it is always useful to add vertices with degree 1 into the matching. This does not seem to have been explicitly exploited by previous CRSs. To analyze the algorithm, note Ô¨Årstly that it is easy to see that the selected edges really do form a matching. Secondly, let us note that the probability that an edge (u, v) is available at a vertex u is  
   
    exuv ‚àí 1 e‚àíxu v = 1 ‚àí e‚àíxuv e‚àí(1‚àíxuv ) = F (xuv ) = 1 ‚àí e‚àíxuv e   u ‚ààŒ¥(v),u =u  
   
  and similarly, the probability that an edge (u, v) is isolated amongst active edges is 2   e2xuv ‚àí exuv 1 ‚àí e‚àíxuv e‚àí(1‚àíxuv ) = e2 Hence, if a CRS of the sort used in step 3 exists, the desired result follows since Pr[(u, v) ‚àà M ] = 2 Pr[(u, v) is selected at u] ‚àí Pr[(u, v) is selected at both u, v] = 2 Pr[(u, v) is selected at u] ‚àí Pr[(u, v) is isolated amongst active edges]   1 e2xuv ‚àí exuv e2xuv ‚àí exuv = 2xuv (1 ‚àí e‚àí1/e ) ‚àí 2 + ‚àí 2e e2 e2   1 = 2xuv (1 ‚àí e‚àí1/e ) ‚àí 2 2e Furthermore whether (u, vi ) is available at u is independent of whether (u, vj ) is available at u (this is where we use the fact that the graph is bipartite; note that we actually only need to assume that the graph is triangle-free; it is unclear how to drop this assumption). Therefore, it follows from Theorem 3 that if the probability of (u, vi ) appearing is xi (short for xuvi ), the existence of the required CRS depends only on whether Pr[at least one of (u, vi ) is available] = 1 ‚àí ‚â•  
   
  n  
   
  (1 ‚àí F (xi ))  
   
  i=1  
   
    1 e2xi ‚àí exi xi (1 ‚àí e‚àí1/e ) ‚àí 2 + 2e 2e2 i=1  
   
  n   
   
  Therefore, the desired result follows from the following lemma, whose proof we omit.  x Lemma 2. If xi ‚â§ 1, xi ‚â• 0, and F (x) = e e‚àí1 , then   n n  
   
   1 e2xi ‚àí exi ‚àí1/e (1 ‚àí F (xi )) ‚â• xi (1 ‚àí e )‚àí 2 + 1‚àí 2e 2e2 i=1 i=1  
   
  Towards an Optimal Contention Resolution Scheme for Matchings  
   
  3.2  
   
  389  
   
  A 0.509-Balanced Scheme for Bipartite Matchings  
   
  In this section we present our best CRS for bipartite matchings which beats the best possible RCRS. We call the set of vertices on the left V1 , and on the right, V2 . There are two main ideas. One idea is to select edges in two stages, with the Ô¨Årst stage devoted to running contention resolution on the edges at each vertex in V1 , and the second stage is devoted to running contention resolution on the edges picked in stage 1 at each vertex in V2 . The other main idea involves noticing that edges (u, v) such that v has degree 1 in R have no competition in the second stage and hence should be preferentially selected in the Ô¨Årst stage, since if selected they will certainly survive in our matching. The Red/Blue/Gray Scheme: 1. Recall that Pr[(u, v) ‚àà R] = xuv . Decide for each edge (u, v) ‚àà R independently at random whether to mark it gray, so that Pr[(u, v) is gray] = xuv ‚àí (1 ‚àí e‚àíxuv ). We call the edges (u, v) ‚àà R which are not gray active. 2. For each (u, v) such that (u, v) is the only active edge incident to v, decide independently at random whether to mark (u, v) red, so that Pr[(u, v) is red] = 1 ‚àí e‚àíxuv /e . Mark all other active edges blue. We have Pr[(u, v) is blue] = e‚àíxuv /e ‚àí e‚àíxuv . 3. For each u ‚àà V1 , if there are any red edges incident to u, perform contention resolution to include one of them in R1 , so that Pr[(u, v) ‚àà R1 ] = (1 ‚àí e‚àí1/e )xuv . 4. For each u ‚àà V1 , if there are no red edges incident to u, and there are some blue edges incident to u, perform contention resolution to include one of them in R2 , so that Pr[(u, v) ‚àà R2 ] = (e‚àí1/e ‚àí e‚àí1 )xuv . 5. For each u ‚àà V1 , if there are no active (red or blue) edges incident to u, and there are some gray edges incident to u, perform contention resolution 1 2 xuv . to include one of them in R3 , so that Pr[(u, v) ‚àà R3 ] ‚â• 2e 6. Finally, for each v ‚àà V2 , perform contention resolution among all edges in R1 ‚à™ R2 ‚à™ R3 incident to v, to include one of them in M , so that Pr[(u, v) ‚àà M ] ‚â• 0.509 xuv . Implicit in step 2 is the claim that the deÔ¨Ånition of red edges is valid. Implicit in each of steps 3, 4, 5, and 6 above is a claim that there exists a certain contention resolution scheme for choosing 1 out of n elements. The existence of such schemes can be proved by applying Theorem 3, if we can calculate the probability that at least one of a subset of edges ‚Äúis available for consideration at that stage‚Äù (i.e., is red, is blue, etc.). For steps 3, 4, and 5, this quantity is fairly simple to calculate, because all the choices (to designate edges red or blue or gray) are made independently. The following lemma, whose proof we omit, summarizes the claimed existences in these cases.  
   
  390  
   
  P. Nuti and J. Vondr¬¥ ak  
   
  Lemma 3. The definition of red edges is valid. There are CRSs among red, blue, and gray edges such that Pr[(u, v) ‚àà R1 ] = (1 ‚àí e‚àí1/e )xuv , Pr[(u, v) ‚àà 1 2 xuv for every edge (u, v). R2 ] = (e‚àí1/e ‚àí e‚àí1 )xuv , and Pr[(u, v) ‚àà R3 ] ‚â• 2e To Ô¨Ånish, we need to analyze Step 6 of the algorithm, which is contention resolution among all the surviving edges on the right-hand side. Here, there can be at most one red edge incident to a vertex v ‚àà V2 , possibly multiple gray edges which appear independently, and possibly multiple blue edges whose survival up to this stage of the scheme is correlated. This correlation causes the main trouble in our analysis of this Ô¨Ånal step, because it makes it harder to calculate the probability that at least one of a subset of edges incident at a vertex v is in R1 ‚à™ R2 ‚à™ R3 . Ideally, we would like to prove that the appearance of blue edges satisÔ¨Åes some form of negative correlation. At the moment, we are able to prove only pairwise negative correlation which is suÔ¨Écient to achieve the factor of 0.509. A stronger correlation result (for example negative cylinder dependence) would lead to an improved factor. We include a full proof of the following lemma due to its conceptual importance in the overall proof. Lemma 4. For any two incident edges (u, v) and (u , v), Pr[(u, v) ‚àà R2 & (u , v) ‚àà R2 | (u, v), (u , v) are blue] ‚â§ Pr[(u, v) ‚àà R2 | (u, v), (u , v) are blue]¬∑Pr[(u , v) ‚àà R2 | (u, v), (u , v) are blue]. Proof. DeÔ¨Åne Œì (u) = {v  : (u, v  ) active} and Œì (u ) = {v  : (u , v  ) active}. Note that conditioning on (u, v), (u , v) being blue edges is the same as conditioning on v ‚àà Œì (u) ‚à© Œì (u ), because edges (u, v), (u , v) being active also means that they must be blue. We claim that conditioned on Œì (u), Œì (u ) such that v ‚àà Œì (u) ‚à© Œì (u ), the probability that (u, v) ‚àà R2 is decreasing in Œì (u) and increasing in Œì (u ), while conversely the probability that (u , v) ‚àà R2 is increasing in Œì (u) and decreasing in Œì (u ). We prove this by considering a Ô¨Åxed choice of the active edges incident to V1 \ {u, u }, and at the end averaging over these choices. Consider Œì (u), Œì (u ) where v ‚àà Œì (u) ‚à© Œì (u ). For (u, v) to be selected in R2 , there cannot be any red edge incident to u. The only candidates for such red edges are (u, vÀú) where vÀú ‚àà Œì (u) \ Œì (u ), because edges incident to Œì (u) ‚à© Œì (u ) are blue by deÔ¨Ånition. For each vÀú ‚àà Œì (u) \ Œì (u ), (u, vÀú) is red if vÀú does not have any other incident active edges (and an additional independent coin Ô¨Çip succeeds, as deÔ¨Åned in Step 2). Clearly, the event of no red edge incident to u is monotonically decreasing in Œì (u) \ Œì (u ). In case there is no red edge incident to u, we perform contention resolution among the blue edges incident to u, which are all the edges (u, v  ), v  ‚àà Œì (u). Since this scheme is monotone, the probability of survival is monotonically decreasing in Œì (u). This monotonicity property also remains preserved when we average over the choices of active edges incident to V1 \ {u, u }. Overall, the probability of (u, v) surviving in R2 is monotonically decreasing in Œì (u) and  
   
  Towards an Optimal Contention Resolution Scheme for Matchings  
   
  391  
   
  increasing in Œì (u ). Symmetrically, the probability of (u , v) surviving in R2 is monotonically decreasing in Œì (u ) and increasing in Œì (u). Given this monotonicity property, we use the FKG inequality to prove our result. The appearances of vertices in Œì (u) and Œì (u ) are independent (since these are determined by the edges incident to u and u respectively). Let us deÔ¨Åne Œ≥ ‚àà {0, 1}2n where Œ≥i = 0 if i ‚àà Œì (u) and Œ≥n+i = 1 if i ‚àà Œì (u ). As we argued, Pr[(u, v) ‚àà R2 | Œ≥] is increasing in Œ≥ and Pr[(u , v) ‚àà R2 | Œ≥] is decreasing in Œ≥, for all Œ≥ consistent with v ‚àà Œì (u)‚à©Œì (u ). Therefore, by the FKG inequality applied to this subspace (conditioned on v ‚àà Œì (u) ‚à© Œì (u )), Pr[(u, v) ‚àà R2 & (u , v) ‚àà R2 | v ‚àà Œì (u) ‚à© Œì (u )] ‚â§ Pr[(u, v) ‚àà R2 | v ‚àà Œì (u) ‚à© Œì (u )] ¬∑ Pr[(u , v) ‚àà R2 | v ‚àà Œì (u) ‚à© Œì (u )].  

  as desired.  
   
  The main takeaway from this lemma is that if we let Œ≤ be a lower bound on the probability that an edge survives in R2 given that it is blue, then, conditioned on having at least two active (and hence blue) edges at a vertex v in V2 in step 6, the probability that one of them survives in R2 is at least 2Œ≤ ‚àí Œ≤ 2 . In the Ô¨Ånal analysis of step 6, if there are more than 2 blue edges at a vertex v, we only use two of them. This allows us to establish our desired conclusion. Theorem 4. There is a CRS in Step 6 which achieves a factor of 0.509. To prove the theorem, we consider a vertex v ‚àà V2 and all the edges incident to v which are in R1 ‚à™ R2 ‚à™ R3 (i.e. survived contention resolution on the lefthand side). To show that the required kind of CRS exists, we consider a subset of edges S incident to v, and compute the probability that at least one of them survives as follows:  
   
  ‚àÖ] Pr[S ‚à© (R1 ‚à™ R2 ‚à™ R3 ) = ‚àÖ] = Pr[S ‚à© R1 = + Pr[S ‚à© R1 = ‚àÖ & S ‚à© R2 = ‚àÖ] + Pr[S ‚à© (R1 ‚à™ R2 ) = ‚àÖ & S ‚à© R3 = ‚àÖ]. We Ô¨Ånish the proof by establishing lower bounds on each of the three terms and applying Theorem 3. Lemma 4 is useful in the analysis of the second term. We omit the proof here. We refer the reader to the full version for details. Acknowledgements. We would like to thank Chandra Chekuri for stimulating discussions.  
   
  References 1. Karp, R.M., and Sipser, M.: Maximum matchings in sparse random graphs. In: 22nd Annual Symposium on Foundations of Computer Science, Nashville, Tennessee, USA, 28‚Äì30 October 1981, pp. 364‚Äì375. IEEE Computer Society (1981)  
   
  392  
   
  P. Nuti and J. Vondr¬¥ ak  
   
  2. Feige, U.: On maximizing welfare when utility functions are subadditive. SIAM J. Comput. 39(1), 122‚Äì142 (2009) 3. Bordenave, C., Lelarge, M., Salez, J.: Matchings on in nite graphs. Probab. Theory Relat. Fields 157(1), 183‚Äì208 (2013) 4. Chekuri, C., Vondr¬¥ ak, J., Zenklusen, R.: Submodular function maximization via the multilinear relaxation and contention resolution schemes. SIAM J. Comput. 43(6), 1831‚Äì1879 (2014) 5. Balister, P., and Gerke, S.: Controllability and matchings in random bipartite graphs. In: Czumaj, A., Georgakopoulos, A., Kr¬¥ al, D., Lozin, V., Pikhurko, O. (eds.) Surveys in Combinatorics 2015. London Mathematical Society Lecture Note Series. Cambridge University Press, pp. 119‚Äì146 (2015) 6. Feldman, M., Svensson, O., Zenklusen, R.: Online contention resolution schemes. In: Krauthgamer, R. (ed.) Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2016, Arlington, VA, USA, 10‚Äì12 January 2016, pp. 1014‚Äì1033. SIAM (2016) 7. Guruganesh, G., Lee, E.: Understanding the correlation gap for matchings. In: Lokam, S.V., and Ramanujam, R. (eds.) 37th IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science, FSTTCS 2017, 11‚Äì15 December 2017, Kanpur, India. LIPIcs, 1‚Äì15. Schloss Dagstuhl - LeibnizZentrum f¬® ur Informatik (2017) 8. Adamczyk, M., Wlodarczyk, M.: Random order contention resolution schemes. In: Thorup, M. (ed.) 59th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2018, Paris, France, 7‚Äì9 October 2018, pp. 790‚Äì801. IEEE Computer Society (2018) 9. Lee, E., Singla, S.: Optimal online contention resolution schemes via ex-ante prophet inequalities. In: Azar, Y., Bast, H., and Herman, G. (eds.) 26th Annual European Symposium on Algorithms, ESA 2018, 20‚Äì22 August 2018, Helsinki, Finland. LIPIcs, 1‚Äì14. Schloss Dagstuhl - Leibniz-Zentrum f¬® ur Informatik (2018) 10. Fu, H., Tang, Z.G., Wu, H., Wu, J., Zhang, Q.: Random order vertex arrival contention resolution schemes for matching, with applications. In: Bansal, N., Merelli, E., Worrell, J. (eds.) 48th International Colloquium on Automata, Languages, and Programming, ICALP 2021, 12‚Äì16 July 2021, Glasgow, Scotland (Virtual Conference). LIPIcs, 1‚Äì20. Schloss Dagstuhl - Leibniz-Zentrum f¬® ur Informatik (2021) 11. Bruggmann, S., Zenklusen, R.: An optimal monotone contention resolution scheme for bipartite matchings via a polyhedral viewpoint. Math. Program. 191(2), 795‚Äì 845 (2022) 12. MacRury, C., Ma, W., Grammel, N.: On (random-order) online contention resolution schemes for the matching polytope of (bipartite) graphs (2022). https://arxiv. org/abs/2209.07520 13. Pollner, T., Roghani, M., Saberi, A., Wajc, D.: Improved online contention resolution for matchings and applications to the gig economy. In: Proceedings of the 23rd ACM Conference on Economics and Computation. EC 2022, pp. 321‚Äì322. Association for Computing Machinery, Boulder, CO, USA (2022)  
   
  Advances on Strictly Œî-Modular IPs Martin N¬® agele1(B) , Christian N¬® obel2 , Richard Santiago2 , and Rico Zenklusen2 1  
   
  Research Institute for Discrete Mathematics and HausdorÔ¨Ä Center for Mathematics, University of Bonn, Bonn, Germany [email protected]  2 Department of Mathematics, ETH Zurich, Zurich, Switzerland {cnoebel,rtorres,ricoz}@ethz.ch  
   
  Abstract. There has been signiÔ¨Åcant work recently on integer programs (IPs) min{c x : Ax ‚â§ b, x ‚àà Zn } with a constraint marix A with bounded subdeterminants. This is motivated by a well-known conjecture claiming that, for any constant Œî ‚àà Z>0 , Œî-modular IPs are eÔ¨Éciently solvable, which are IPs where the constraint matrix A ‚àà Zm√ón has full column rank and all n √ó n minors of A are within {‚àíŒî, . . . , Œî}. Previous progress on this question, in particular for Œî = 2, relies on algorithms that solve an important special case, namely strictly Œî-modular IPs, which further restrict the n√ón minors of A to be within {‚àíŒî, 0, Œî}. Even for Œî = 2, such problems include well-known combinatorial optimization problems like the minimum odd/even cut problem. The conjecture remains open even for strictly Œî-modular IPs. Prior advances were restricted to prime Œî, which allows for employing strong number-theoretic results. In this work, we make Ô¨Årst progress beyond the prime case by presenting techniques not relying on such strong number-theoretic prime results. In particular, our approach implies that there is a randomized algorithm to check feasibility of strictly Œî-modular IPs in strongly polynomial time if Œî ‚â§ 4. Keywords: Bounded subdeterminants  
   
  1  
   
  ¬∑ Congruency constraints  
   
  Introduction  
   
  Integer Programs (IPs) min{c x : Ax ‚â§ b, x ‚àà Zn } are a central NP-hard problem class in Combinatorial Optimization. There is substantial prior work Funded through the Swiss National Science Foundation grants 200021 184622 and P500PT 206742, the European Research Council (ERC) under the European Union‚Äôs Horizon 2020 research and innovation programme (grant agreement No 817750), and the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany‚Äôs Excellence Strategy ‚Äì EXZ-2047/1 ‚Äì 390685813.  
   
  c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 393‚Äì407, 2023. https://doi.org/10.1007/978-3-031-32726-1_28  
   
  394  
   
  M. N¬® agele et al.  
   
  and interest in identifying special classes of polynomial-time solvable IPs while remaining as general as possible. One of the best-known such classes are IPs with a constraint matrix that is totally unimodular (TU), i.e., the determinant of any of its square submatrices is within {‚àí1, 0, 1}. A long-standing open conjecture in the Ô¨Åeld is whether this result can be generalized to Œî-modular constraint matrices for constant Œî. Here, we say that a matrix A ‚àà Zk√ón is Œî-modular if it has full column rank and all n √ó n submatrices have determinants in {‚àíŒî, . . . , Œî}.1 For brevity, we call an IP with Œî-modular constraint matrix a Œî-modular IP. We recap the above-mentioned conjecture below. Unfortunately, we do not know its precise origin; it may be considered folklore in the Ô¨Åeld. Conjecture 1. For constant Œî ‚àà Z‚â•0 , Œî-modular IPs can be solved in polynomial time. First progress on Conjecture 1 was made by Artmann, Weismantel, and Zenklusen [3], who showed that it holds for Œî = 2 (the bimodular case). Fiorini, Joret, Weltge, and Yuditsky [11] show that the conjecture is true for an arbitrary constant Œî under the extra condition that the constraint matrix has at most two non-zero entries per row or column. Through a non-trivial extension of the techniques in [3], it was shown by N¬® agele, Santiago, and Zenklusen [24] that there is a randomized algorithm to check feasibility of an IP with a strictly 3-modular constraint matrix in polynomial time. Here, a matrix A ‚àà Zk√ón is called strictly Œî-modular if it has full column rank and all its n √ó n submatrices have determinants in {‚àíŒî, 0, Œî}. As a key ingredient, all these prior approaches solve certain combinatorial optimization problems with congruency constraints. This is not surprising, as even strictly Œî-modular IPs include the following class of MCCTU problems:2 Multi-Congruency-Constrained TU Problem (MCCTU): Let T ‚àà Zk√ón be TU, b ‚àà Zk , c ‚àà Rk , m ‚àà Zq>0 , Œ≥i ‚àà Zn for i ‚àà [q], r ‚àà Zq . Solve min{c x : T x ‚â§ b, Œ≥i x ‚â° ri  
   
  (mod ‚àó)mi ‚àÄi ‚àà [q], x ‚àà Zn } .  
   
  Unless mentioned otherwise, we assume that in the context of MCCTU problems, q and mi are constant. Even MCCTU with just a single congruency constraint, i.e., q = 1, already contains the classical and well-studied odd and even cut problems, and, more generally, the problem of Ô¨Ånding a minimum cut whose number of vertices is r (mod m). (See [5,14,19,25,26,29] for related work.) It can also 1  
   
  2  
   
  A weaker variant of the conjecture claims eÔ¨Écient solvability of IPs with totally Œîmodular constraint matrices, where all subdeterminants are bounded by Œî in absolute value. The conjecture involving Œî-modular matrices implies the weaker variant. Indeed, an IP min{c x : Ax ‚â§ b, x ‚àà Zn } with a totally Œî-modular constraint matrix can be reformulated as min{c (x+ ‚àí x‚àí ) : A(x+ ‚àí x‚àí ) ‚â§ b, x+ , x‚àí ‚àà Zn ‚â•0 }. It is not hard to see that the constraint matrix of the new LP remains totally Œî-modular; moreover, it has full column rank because of the non-negativity constraints. To capture an MCCTU problem as a strictly Œî-modular IP, replace each congruency constraint Œ≥i x ‚â° ri (mod mi ) by an equality constraint Œ≥i x+mi yi = r with yi ‚àà Z. The corresponding constraint matrix is strictly Œî-modular for Œî = qi=1 mi .  
   
  Advances on Strictly Œî-Modular IPs  
   
  395  
   
  capture the minimum T -join problem, congruency-constrained Ô¨Çow problems, and many other problems linked to TU matrices. Combinatorial optimization problems with congruency constraints are highly non-trivial and many open questions remain. As they are already captured by strictly Œî-modular IPs, this motivates the following weakening of Conjecture 1. Conjecture 2. Strictly Œî-modular IPs can be solved in polynomial time for constant Œî ‚àà Z‚â•0 . Even resolving this weaker conjecture would settle several open problems, including congruency-constrained min cuts (in both directed and undirected graphs), or the problem of eÔ¨Éciently and deterministically Ô¨Ånding a perfect matching in a red/blue edge-colored bipartite graph such that the number of red matching edges is r (mod m). (This is a simpliÔ¨Åed version of the famous red-blue matching problem, where the task is to Ô¨Ånd a perfect matching with a speciÔ¨Åed number of red edges; for both versions, randomized algorithms are known.) Interestingly, for the bimodular case (Œî = 2), a result by Veselov and Chirkov [33] implies that Conjecture 1 and Conjecture 2 are equivalent (see [3]). Our goal is to shed further light on Conjecture 2 and overcome some important hurdles of prior approaches. In a Ô¨Årst step, we note that a positive resolution of Conjecture 2 does not only imply eÔ¨Écient solvability of MCCTU problems, but also vice versa, and this reduction works in strongly polynomial time. Lemma 1. Let Œî > 0. Every strictly Œî-modular IP can, in strongly polynomial q time, be reduced to an MCCTU problem with moduli mi such that Œî = i=1 mi . Without the strongly polynomial time condition, this also follows from very recent work of Gribanov, Shumilov, Malyshev, and Pardalos [15, Lemma 4]. Further, we are interested in making progress regarding the feasibility version of Conjecture 2, i.e., eÔ¨Éciently deciding whether a strictly Œî-modular IP is feasible. Prior approaches settle this question for Œî = 2 [3] and‚Äîusing a randomized algorithm‚Äîfor Œî = 3 [24]. A main hurdle to extend these is that they crucially rely on Œî being prime, for example through the use of the Cauchy-Davenport Theorem. Our main contribution here is to address this. In particular, we can check feasibility for Œî = 4 with a randomized algorithm, which is the Ô¨Årst result in this context for non-prime Œî. More importantly, our techniques will hopefully prove useful for future advances on this challenging question. Theorem 1. There exists a strongly polynomial-time randomized algorithm to find a feasible solution of a strictly 4-modular IP, or detect that it is infeasible. We remark that the randomization appearing in the above theorem comes from the fact that one building block of our result is a reduction to a problem class that includes the aforementioned congruency-constrained red/blue-perfect matching problem, for which only randomized approaches are known.  
   
  396  
   
  M. N¬® agele et al.  
   
  1.1  
   
  Group-Constrained Problems and Proof Strategy for Theorem 1  
   
  To show Theorem 1, we exploit its close connection to MCCTU. Capturing the congruency constraints of an MCCTU problem through an abelian group constraint, we attain the following group-constrained TU feasibility problem. Group-Constrained TU Feasibility (GCTUF): Let T ‚àà Rk√ón be a TU matrix, let b ‚àà Zk , let (G, +) be a Ô¨Ånite abelian group, and let Œ≥ ‚àà Gn and r ‚àà G. The task is to show infeasibility or Ô¨Ånd a solution of the system T x ‚â§ b, Œ≥  x = r, x ‚àà Zn . Here, the scalar product Œ≥  x denotes the linear combination of the group elements Œ≥1 , . . . , Œ≥n with multiplicities x1 , . . . , xn in G. Group constraints generalize congruency constraints, which are obtained in the special case where G is cyclic. More generally, by the fundamental theorem of Ô¨Ånite abelian groups, a Ô¨Ånite abelian group G is, up to isomorphism, a direct product of cyclic groups. Hence, a group constraint can be interpreted as a set of congruency constraints and vice versa. Thus, GCTUF and MCCTU feasibility are two views on the same problem. We stick to GCTUF mostly for convenience of notation. Moreover, the GCTUF setting also allows for an elegant use of group-related results later on. One may assume that the group is given through its multiplication table (the Cayley table). In fact, the precise group representation is not of great importance to us. Concretely, for constant Œî, strictly Œî-modular IP feasibility problems reduce to GCTUF problems with a constant size group. Many of our polynomial-time algorithmic results can even be extended to settings where the group size is not part of the input, and access to group operations is provided through an oracle. By Lemma 1 and the aforementioned equivalent viewpoint of multiple congruency constraints and a group constraint, in order to prove Theorem 1, it is enough for us to show the equivalent statement below. Theorem 2. There exists a strongly polynomial time randomized algorithm for GCTUF problems with a group of cardinality at most 4. On a high level, we follow a well-known strategy for TU-related problems by employing Seymour‚Äôs decomposition [31] to decompose the problem into problems on simpler, more structured TU matrices. (See, e.g., [1,3,9,24].) Roughly speaking, a TU matrix is either very structured‚Äîin which case we call it a base block ‚Äîor can be decomposed into smaller TU matrices through a small set of well-deÔ¨Åned operations. (See the discussion following Theorem 7.) The use of Seymour‚Äôs decomposition typically comes with two main challenges, namely (i) solving the base block cases, and (ii) propagating solutions of the base block cases back through the decomposition eÔ¨Éciently to solve the original problem. First, we show that this propagation can be done eÔ¨Éciently for our problem. Theorem 3. Let G be an abelian group of size at most 4. Given an oracle for solving base block GCTUF problems with group G, we can solve GCTUF problems with group G in strongly polynomial time with strongly polynomially many calls to the oracle.  
   
  Advances on Strictly Œî-Modular IPs  
   
  397  
   
  In fact, our approach underlying Theorems 2 and 3 operates in a hierarchy of GCTUF problems with increasingly relaxed group constraints of the form Œ≥  x ‚àà R for subsets R ‚äÜ G of increasing size, and allows for proving the above results for such relaxed GCTUF problems for arbitrary constant-size groups G as long as |G| ‚àí |R| ‚â§ 3. (See Sect. 3 for more details.) In principle, this is along the lines of the approach to congruency-constrained TU problems in [24], but incorporates the new viewpoint of group constraints, and additionally improves over earlier results in two ways: First, our approach applies to arbitrary Ô¨Ånite abelian groups, while previous setups heavily relied on the group cardinality being a prime. Secondly, in the setting with relaxed group constraints, we extend the admissible range of |G| ‚àí |R| by one, thus proceeding further in the hierarchy of GCTUF problems, and newly covering GCTUF problems with groups of cardinality 4. Besides being a key part of our approach, Theorem 3 underlines that base block GCTUF problems are not merely special cases, but play a key role in progress on general GCTUF problems. There are only two non-trivial types of such base block GCTUF problems, namely when the constraint matrix is a socalled network matrix or a transpose thereof. Both cases cover combinatorial problems that are interesting on their own, and their complexity status remains open to date. If the constraint matrix is a network matrix, GCTUF can be cast as a circulation problem with a group constraint. By reducing to and exploiting results of Camerini, Galbiati, and MaÔ¨Éoli [7] on exact perfect matching problems, a randomized algorithm for the congruency-constrained case has been presented in [24]. We observe that these results extend to the group-constrained setting. The other base block case, where the constraint matrix is the transpose of a network matrix, can be cast as a group-constrained directed minimum cut problem by leveraging a result in [24]. Prior work combined this reduction with results on congruency-constrained submodular minimization [25] to solve the optimization version of the problem for congruency-constraints of prime power modulus. We show that the feasibility question on this base block can be solved eÔ¨Éciently on any Ô¨Ånite abelian group of constant order, thus circumventing the prime power restriction that is intrinsic in prior approaches. Theorem 4. Let G be a finite abelian group. There is a strongly polynomial time algorithm for solving GCTUF problems with group G where the constraint matrix is the transpose of a network matrix. 1.2  
   
  Further Related Work  
   
  The parameter Œî has been studied from various viewpoints. While eÔ¨Écient recognition of (totally) Œî-modular matrices is open for any Œî ‚â• 2, approaches to approximate the largest subdeterminant in absolute value were studied [8,27]. Also, focusing on more restricted subdeterminant patterns proved useful [2,12,33]. Aiming at generalizing a bound of Heller [21] for Œî = 1, bounds on the maximum number of rows of a Œî-modular matrix were obtained [4,13,23]. Also, the inÔ¨Çuence of the parameter Œî on structure and properties of IPs and polyhedra is multi-faceted (see, e.g., [6,10,16‚Äì18,22,28,32] and references therein).  
   
  398  
   
  M. N¬® agele et al.  
   
  1.3  
   
  Structure of the Paper  
   
  In Sect. 2, we prove Theorem 4. Section 3 illustrates our approach and new contributions towards Theorem 3 on a more technical level, and explains the main new ingredients of our proof. Due to space constraints, some proofs are deferred to a long version of this paper, including the proof of Lemma 1.  
   
  2  
   
  GCTUF with Transposed Network Constraint Matrices  
   
  In the setting with a congruency constraint instead of a group constraint, [24] shows that every base block problem with a constraint matrix that is a transposed network matrix can be reduced to a node-weighted minimization problem over a lattice with a congruency constraint,3 i.e., a problem of the form min{w(S) : S ‚àà L, Œ≥(S) ‚â° r(mod m)} ,  
   
  (1)  
   
  where L ‚äÜ 2N is a lattice on some Ô¨Ånite  ground set N , Œ≥ : N ‚Üí Z,r ‚àà Z, m ‚àà Z>0 , w : N ‚Üí R, and we use Œ≥(S) := v‚ààS Œ≥(v) as well as w(S) := v‚ààS w(v).4 Being a special case of congruency-constrained submodular minimization, it is known that such problems, and thus the corresponding congruency-constrained TU problems with a transposed network constraint matrix, can be solved in strongly polynomial time for constant prime power moduli m, while the case of general constant composite moduli remains open [25]. The progress on GCTUF, particularly the reduction to base block feasibility problems through Theorem 3 and its generalization (Theorem 8 in Sect. 3), motivates studying these reductions and results in the feasibility setting and with a group constraint instead of a congruency constraint, giving rise to the following problem. Group-Constrained Lattice Feasibility (GCLF): Let N be a Ô¨Ånite set, L ‚äÜ 2N a lattice, (G, +) a Ô¨Ånite abelian group, Œ≥ : N ‚Üí G, r ‚àà G. The task is to Ô¨Ånd X ‚àà L with Œ≥(X) = r, or decide infeasibility. We observe that the reduction in [24] from congruency-constrained TU problems with transposed network constraint matrices to problems of the form given in (1) extends to the group-constrained case. In particular, we obtain the following result in the feasibility setting. Proposition 1. Let G be a finite abelian group. Any GCTUF problem with group G and a constraint matrix that is a transposed network matrix can in strongly polynomial time be reduced to a GCLF problem with group G. 3 4  
   
  In fact, the proof in [24] claims a reduction to a submodular minimization problem, but shows the stronger one presented here. We recall that a lattice L ‚äÜ 2N is a set family such that for any A, B ‚àà L, we have A ‚à© B, A ‚à™ B ‚àà L. We assume such a lattice to be given by a compact encoding in a directed acyclic graph H on the vertex set N such that X ‚äÜ N is an element of the ‚àí (X) = ‚àÖ (cf. [20, Section 10.3]). Here, as usual, in a digraph lattice if and only if Œ¥H G = (V, A) and for X ‚äÜ V , we denote by Œ¥ + (X) and Œ¥ ‚àí (X) the arcs in A leaving and entering X, respectively. Moreover, we write Œ¥ ¬± (v) := Œ¥ ¬± ({v}) for v ‚àà V .  
   
  Advances on Strictly Œî-Modular IPs  
   
  399  
   
  Thus, it remains to study GCLF problems. Interestingly, for the pure feasibility question, we can circumvent the barriers present in the optimization setting, and obtain the following result through a concise argument. Theorem 5. Let G be a finite abelian group. GCLF problems with group G can be solved in strongly polynomial time. Clearly, Proposition 1 and Theorem 5 together imply Theorem 4. The main observation towards a proof of Theorem 5 is the following elementary lemma. Lemma 2. Let G be a finite abelian group, and let Œ≥1 , . . . , Œ≥ ‚àà G. If  ‚â• |G|, then there is a non-empty subset I ‚äÜ [] such that i‚ààI Œ≥i = 0.  Proof. Either si := j‚â§i Œ≥j = 0 for some i ‚àà [], or there exist i < j with si = sj ; hence I = [i] or I = {i + 1, . . . , j}, respectively, has the desired properties.  
   
  To prove Theorem 5, we work with a representation of the lattice L through an acyclic digraph H (see Footnote 4). We exploit that every X ‚àà L is uniquely deÔ¨Åned by the subset CX := {x ‚àà X : Œ¥ + (x) ‚äÜ Œ¥ + (X)}. Proof of Theorem 5. We claim that if the given GCLF problem is feasible, there is a feasible X with |CX | < |G|. If so, we obtain an eÔ¨Écient procedure for GCLF with group G through enumerating all such CX and checking if Œ≥(X) = r. To prove the claim, assume for contradiction that it is wrong, and let X ‚àà L be minimal with Œ≥(X) = r. Then |CX | ‚â• |G|, and applying Lemma 2 to CX gives a non-empty subset Y ‚äÜ CX with Œ≥(Y ) = 0. Thus, X \ Y is a strictly smaller lattice element with Œ≥(X \ Y ) = Œ≥(X) ‚àí Œ≥(Y ) = Œ≥(X) = r, a contradiction.  

  3  
   
  Overview of Our Techniques Leading to Theorem 3  
   
  In order to tackle GCTUF problems, following ideas from [24], we introduce a hierarchy of slightly relaxed GCTUF problems by weakening the group constraint. R-Group-Constrained TU Feasibility (R-GCTUF): Let T ‚àà {‚àí1, 0, 1}k√ón be TU, b ‚àà Zk , let (G, +) be a Ô¨Ånite abelian group, Œ≥ ‚àà Gn and R ‚äÜ G. The task is to show infeasibility or Ô¨Ånd a solution of T x ‚â§ b, Œ≥  x ‚àà R, x ‚àà Zn . Here, we typically call R the set of target elements. The above setup allows us to measure progress between GCTUF (the case of |R| = 1) and an unconstrained IP with TU constraint matrix (captured by setting R = G). In particular, the diÔ¨Éculty of an R-GCTUF problem increases as the size of R, i.e., the number of target elements, decreases. The main parameter capturing this hardness is the depth d := |G| ‚àí |R| of the problem. We show the following generalization of Theorem 2. Theorem 6. Let G be a finite abelian group. There is a strongly polynomial randomized algorithm solving R-GCTUF problems with group G and |G| ‚àí |R| ‚â§ 3.  
   
  400  
   
  M. N¬® agele et al.  
   
  Our argument uses Seymour‚Äôs decomposition theorem. To this end, for matrices A ‚àà ZkA √ónA and B ‚àà ZkB √ónB as well as vectors e, f , g,and h of appropriate       size, we recall that the 3-sum of hA 0e 1e and g0 g1 fB is ghA efB .5 Theorem 7 (Seymour‚Äôs Decomposition). Let T ‚àà Zk√ón be TU. Then either (i) T is a base block matrix, or (ii) T can, possibly after row and column permutations and pivoting once, be decomposed into a 3-sum of TU matrices with nA , nB ‚â• 2. Additionally, we can in time poly(n) decide which of the cases holds and determine the involved matrices. Item (i) covers three types of matrices: network matrices, transposes thereof, and matrices obtainable through basic operations from one of two speciÔ¨Åc 5 √ó 5 TU matrices. (For more details on Seymour‚Äôs decomposition, see, e.g., [30], and for a version tailored to our setting, see [24, Theorem 2.2].) By combining results for base blocks from [24] with our results from Sect. 2, it follows that GCTUF problems can be solved in strongly polynomial time if the constraint matrix is a base block matrix; hence dealing with Item (i) above. In Item (ii), the potential pivoting step can be handled by extending a result from [24] to the group setting. Hence, it remains to discuss how to deal with constraint matrices that are 3-sums. We devote the rest of this section to discuss the main ingredients needed to cover this case. Altogether, we proof the following generalization of Theorem 3. Theorem 8. Let G be a finite abelian group and  ‚àà Z‚â•1 with  ‚â• |G| ‚àí 3. Given an oracle for solving base block R-GCTUF problems with group G and any R ‚äÜ G with |R| ‚â• , we can solve R-GCTUF problems with group G and R ‚äÜ G with |R| ‚â•  in strongly polynomial time with strongly polynomially many calls to the oracle. 3.1  
   
  Reducing to a Simpler Problem When the Target Elements Form a Union of Cosets  
   
  If R, the set of target elements, is a union ofcosets of the same non-trivial proper k subgroup H of G (i.e., it is of the form R = i=1 (gi +H) for some g1 , . . . , gk ‚àà G, or equivalently, R = R + H), we can directly reduce to a simpler problem. Indeed, assume R = R + H for a non-trivial proper subgroup H of G. Then, we can equivalently rewrite the R-GCTUF problem with a group constraint in the quotient group G/H and new target set R = R/H. The depth of the < new problem in the corresponding hierarchy is d = |G/H| ‚àí |R/H| = |G|‚àí|R| |H| |G|‚àí|R|, so we end up with an easier problem. Since existence of such a subgroup H can be checked eÔ¨Éciently (given that G has constant size), we can always determine upfront whether the R-GCTUF problem at hand is reducible, and if so, reduce it to a simpler R-GCTUF problem. Thus, for the rest of this section we assume R is not a union of cosets. This assumption allows us to apply a special case of the Cauchy-Davenport theorem that holds despite the fact that the group order is not assumed to be prime. We refer to Lemma 3 for details. 5  
   
  For simplicity, we use a notion of a 3-sum that allows one or both of ef  and gh to be zero matrices. Typically, those cases would be called 2- and 1-sums, respectively.  
   
  Advances on Strictly Œî-Modular IPs  
   
  3.2  
   
  401  
   
  Decomposing the Problem  
   
  We now focus on an R-GCTUF problem with a constraint  matrix T that can A ef  be decomposed into a 3-sum of the form T = gh B . The decomposition allows for splitting x, b, and Œ≥ into two parts accordingly, giving the equivalent formulation  

  xA ‚àà ZnA A ef  b xA   xA + Œ≥B xB ‚àà R , . (2) ‚â§ A , Œ≥A ¬∑  xB bB gh B xB ‚àà ZnB In the inequality system, the variables xA and xB interact only through the rankone blocks ef  and gh . Fixing values of Œ± := f  xB and Œ≤ := h xA allows for rephrasing (2) through the following two almost independent problems AxA ‚â§ bA ‚àí Œ±e xA ‚àà ZnA  
   
  h xA = Œ≤  
   
  ,  
   
  and  
   
  BxB ‚â§ bB ‚àí Œ≤g xB ‚àà ZnB  
   
  f  xB = Œ±  
   
  ,  
   
  (3)  
   
  where we seek to Ô¨Ånd solutions xA and xB such that their corresponding group   xA and rB := Œ≥B xB , respectively, satisfy rA + rB ‚àà R. Hence, elements rA := Œ≥A this desired relation between the target elements rA and rB is the only dependence between the two problems once Œ± and Œ≤ are Ô¨Åxed. We assume without loss of generality that A has no fewer columns than B, and refer to the problem on the left as the A-problem, and the problem on the right as the B-problem. We denote by Œ† the set of all (Œ±, Œ≤) ‚àà Z2 such that both the A- and B-problem are feasible. (Note that both problems are described through a TU constraint matrix; hence, feasibility can be checked eÔ¨Éciently.) Also, for (Œ±, Œ≤) ‚àà Œ†, let œÄA (Œ±, Œ≤) ‚äÜ G be all group elements rA ‚àà G for which there is a solution xA to the A-problem with Œ≥  xA = rA , and deÔ¨Åne œÄB analogously. We refer to œÄA and œÄB as patterns. Hence, (2) is feasible if and only if there is a pair (Œ±, Œ≤) ‚àà Œ† such that, for some rA ‚àà œÄA (Œ±, Œ≤) and rB ‚àà œÄB (Œ±, Œ≤), we have rA + rB ‚àà R. Thus, patterns contain all information needed to decide feasibility. Using techniques from [24], we can restrict our search for feasible solutions ‚äÜ Œ†. More precisely, for i ‚àà {0, 1, 2}, we can in to a constant-size subset Œ† strongly polynomial time Ô¨Ånd i , ui ‚àà Z with ui ‚àí i ‚â§ d such that if (2) is feasible, then there is a pair (Œ±, Œ≤) in  
   
  := (Œ±, Œ≤) ‚àà Z2 : 0 ‚â§ Œ± + Œ≤ ‚â§ u0 , 1 ‚â§ Œ± ‚â§ u1 , 2 ‚â§ Œ≤ ‚â§ u2 (4) Œ† for which there is a solution xA to the A-problem and a solution xB to the Bproblem with Œ≥  xA + Œ≥  xB ‚àà R. Therefore, the challenges lie less in the size of Œ†, but rather in how to obtain information on the sets œÄA (Œ±, Œ≤) and œÄB (Œ±, Œ≤) for pairs (Œ±, Œ≤) ‚àà Œ†. Opposed to previous techniques, which almost solely focused on œÄB , we investigate both œÄA and œÄB and their interplay‚Äîsee Sect. 3.3. As B has at most half the columns of the constraint matrix T of the original R-GCTUF problem (2), we can aÔ¨Äord (runtime-wise) to recursively call our algorithm multiple times on the B-problem for diÔ¨Äerent targets RB of the same depth d = |G| ‚àí |R| as the original problem, i.e., with |RB | = |R|. (We refrain  
   
  402  
   
  M. N¬® agele et al.  
   
  from using larger depths, as GCTUF become harder with increasing depth.) This œÄB (Œ±, Œ≤))| = min{d + allows us to compute a set œÄ ¬ØB (Œ±, Œ≤) ‚äÜ œÄB (Œ±, Œ≤) of size |¬Ø ¬ØB (Œ±, Œ≤) = ‚àÖ and, as long as |¬Ø œÄB (Œ±, Œ≤)| < 1, œÄB (Œ±, Œ≤)}. Indeed, we can start with œÄ min{d + 1, œÄB (Œ±, Œ≤)}, we solve an RB -GCTUF B-problem (i.e., we look for a BœÄB (Œ±, Œ≤) being a set of size problem solution xB with Œ≥  xB ‚àà RB ) with RB = G\¬Ø at least |G| ‚àí d. If RB ‚à© œÄB (Œ±, Œ≤) = ‚àÖ, then we Ô¨Ånd an element in RB ‚à© œÄB (Œ±, Œ≤) that can be added to œÄ ¬ØB (Œ±, Œ≤) and we repeat; otherwise, RB ‚à© œÄB (Œ±, Œ≤) = ‚àÖ and we know that we computed œÄ ¬ØB (Œ±, Œ≤) = œÄB (Œ±, Œ≤). To the contrary, note that the A-problem may be almost as big as the original GCTUF problem (possibly with just two fewer columns). Hence, here we cannot aÔ¨Äord (runtime-wise) a similar computation as for the B-problem. However, we can aÔ¨Äord to solve multiple RA -GCTUF A-problems of smaller depth, i.e., |RA | > |R|, because the runtime decreases signiÔ¨Åcantly with decreasing depth. By using the same approach as in the B-problem, but with sets RA of size |RA | ‚â• œÄA (Œ±, Œ≤)| = min{d, œÄA (Œ±, Œ≤)}. |R|+1, we obtain a set œÄ ¬ØA (Œ±, Œ≤) ‚äÜ œÄA (Œ±, Œ≤) of size |¬Ø Let us next take a closer look at patterns. Fix some (Œ±, Œ≤) ‚àà Œ† and let A 1 i , . . . , rA } for some A ‚â• 1 and pairwise diÔ¨Äerent rA ‚àà G, and œÄA (Œ±, Œ≤) = {rA  1  i i A let xA , . . . , xA be corresponding solutions of the A-problem with Œ≥A xA = rA . i i DeÔ¨Åne B , rB , and xB analogously. Observe that if A ‚â§ d and B ‚â§ d + 1, we have œÄ ¬ØX (Œ±, Œ≤) = œÄX (Œ±, Œ≤) for both X ‚àà {A, B}. Hence, we can compute j i + rB ‚àà R for some all feasible group elements and check explicitly whether rA i ‚àà [A ] and j ‚àà [B ], i.e., whether a solution exists. If B ‚â• d + 1, we can (independently of A ) even show that there always exists a feasible solution, and we can also Ô¨Ånd one: Indeed, we can compute d + 1 solutions xi := (x1A , xiB ) 1 i + rB ‚àà G, at least one of which must satisfy with pairwise diÔ¨Äerent sums rA 1 i + rB ‚àà R. If A ‚â• d and B ‚â• 2, we can argue similarly: We show that among rA ¬ØB (Œ±, Œ≤) (which we can any d elements of œÄ ¬ØA (Œ±, Œ≤), and any two elements of œÄ j j i i , rB with rA + rB ‚àà R. Note that while for groups compute), there is a pair rA of prime order this can be shown via the Cauchy-Davenport theorem, the above result does not hold in general. We show, however, that as long as R is not a union of cosets in G, we can recover the implication (cf. Section 3.1 for why this assumption is legit). Lemma 3. Let G be a finite abelian group, and let R ‚äÜ G be such that R = R + H for any non-trivial subgroup H of G. Then, for any subsets X, Y ‚äÜ G with |X| = |G| ‚àí |R| and |Y | ‚â• 2, we have (X + Y ) ‚à© R = ‚àÖ. Proof. Let b1 , b2 ‚àà Y with b1 = b2 , and set h = b1 ‚àíb2 . Assume (X + Y )‚à©R = ‚àÖ. Then |X| = |G| ‚àí |R| implies |X + Y | = |X|. Thus, X + b1 = X + b2 and hence X = X + h. Iterating gives X = X + h, where h denotes the subgroup generated by h. As R = G \ (X + b1 ), we get R = R + h, a contradiction.  
   
  The following observation summarizes the above discussion. If |¬Ø Observation 1 Let (Œ±, Œ≤) ‚àà Œ†. œÄA (Œ±, Œ≤)| ‚â§ d ‚àí 1 or |¬Ø œÄB (Œ±, Œ≤)| ‚â• 2, we can immediately determine whether a feasible solution to the original R-GCTUF problem exists for such (Œ±, Œ≤), and if so, obtain one by combining solutions com¬ØB . puted for the A- and B-subproblem when determining œÄ ¬ØA and œÄ  
   
  Advances on Strictly Œî-Modular IPs  
   
  403  
   
  Thus, the only case in which we cannot immediately check whether a feasible solution exists for some (Œ±, Œ≤), is when B = 1 and A ‚â• d + 1 (which imply œÄB (Œ±, Œ≤)| = 1). This is the only case where we may have |¬Ø œÄA (Œ±, Œ≤)| = d and |¬Ø œÄA (Œ±, Œ≤) + œÄ ¬ØB (Œ±, Œ≤)) ‚à© R = ‚àÖ, in which case (œÄA (Œ±, Œ≤) + œÄB (Œ±, Œ≤)) ‚à© R = ‚àÖ but (¬Ø we say that (Œ±, Œ≤) contains a hidden solution. 3.3  
   
  Handling Patterns  
   
  In the rest of this section, we describe how our new techniques allow for overcoming barriers restricting previous approaches to depth d = 2. Recall that we as deÔ¨Åned in (4). We call sets of this form, for focus on a constant size subset Œ† any choice of i and ui , pattern shapes, and denote by  1  D := ¬±( 10 ), ¬±( 01 ), ¬± ‚àí1 (5) Focusing on Œ† allows for eÔ¨Éciently the possible edge directions of conv(Œ†). to the extent discussed computing œÄ ¬ØX (Œ±, Œ≤) for X ‚àà {A, B} and all (Œ±, Œ≤) ‚àà Œ† earlier. In order to proceed, we use a structural result from [24], called averaging, that allows us to relate solutions‚Äîand thus elements of œÄX ‚Äîacross diÔ¨Äerent (Œ±, Œ≤). Despite being true in more generality, the exposition here requires the following special case only. Proposition 2 ([24, special case of Lemma 5.3]). Consider an R-GCTUF with problem as described in (2). Let X ‚àà {A, B}, v ‚àà D, and (Œ±, Œ≤) ‚àà Œ† (Œ±, Œ≤) + 2v ‚àà Œ†. Given a solution x1 of the X-problem for (Œ±, Œ≤) and, similarly, x2 for (Œ±, Œ≤) + 2v, there are solutions x3 , x4 for the X-problem for (Œ±, Œ≤) + v such that x1 + x2 = x3 + x4 . We remark that the proof of the above result for congruency-constrained problems given in [24] only exploits that congruency-constraints are linear constraints; therefore, the result carries over to group-constraints seamlessly. In previous approaches for depth d = 2, it was enough to only compute a single element from œÄA (e.g., by solving the A-problem after dropping the group constraint). Concretely, consider patterns of the shape as given in Fig. 1. For d = 2, Proposition 2 can be used to show that, if there is a hidden feasible solution for (Œ±, Œ≤) = (0, 0) or (Œ±, Œ≤) = (2, 0), then there must also be a feasible solution for (Œ±, Œ≤) = (1, 0). The example in Fig. 1 shows that this is no longer true if the depth d exceeds 2, as only (Œ±, Œ≤) = (0, 0) admits a feasible solution. This problem can be circumvented by analyzing the A-pattern œÄ ¬ØA . As argued in Sect. 3.2, if a pair (Œ±, Œ≤) has a hidden solution, then |œÄA (Œ±, Œ≤)| ‚â• d + 1 (and hence |¬Ø œÄA (Œ±, Œ≤)| = d), hence we assume that there exists at least one such pair. The following result uses averaging (i.e., Proposition 2) to show that pairs (Œ± , Œ≤  ) adjacent to such a pair (Œ±, Œ≤) containing a hidden solution also have large œÄ ¬ØA (Œ± , Œ≤  ). such that |œÄA (Œ±, Œ≤)| ‚â• d + 1 Lemma 4. Let d ‚àà {1, 2, 3}, v ‚àà D, and (Œ±, Œ≤) ‚àà Œ† and (Œ±, Œ≤) + 2v ‚àà Œ†. Then |¬Ø œÄA ((Œ±, Œ≤) + v)| = d.  
   
  404  
   
  M. N¬® agele et al.  
   
  œÄA  
   
  Œ≤ 0  
   
  œÄB  
   
  Œ≤  
   
  0,1,2  
   
  0, 1  
   
  0  
   
  0  
   
  1  
   
  2  
   
  0  
   
  Œ±  
   
  1  
   
  0, 1  
   
  0  
   
  0  
   
  1  
   
  2  
   
  Œ±  
   
  Fig. 1. Possible patterns œÄA and œÄB for a problem with group G = Z/4Z. Every square  and the numbers in the box indicate elements of corresponds to a pair (Œ±, Œ≤) ‚àà Œ†, œÄA (Œ±, Œ≤) and œÄB (Œ±, Œ≤), respectively. For R = {3}, there is a feasible solution with (Œ±, Œ≤) = (0, 0), but this cannot be detected without studying œÄA .  
   
  Proof. It is enough to show that |œÄA ((Œ±, Œ≤) + v)| ‚â• d. To this end, for each of the at least d + 1 elements r ‚àà œÄA (Œ±, Œ≤), let xr1 be a corresponding solution of the A-problem, and let x2 denote any Ô¨Åxed solution for the A-problem on the pair (Œ±, Œ≤) + 2v. Proposition 2 applied to xr1 and x2 gives solutions xr3 and xr4  r  r  r  r corresponding to elements Œ≥A x3 , Œ≥A x4 ‚àà œÄA ((Œ±, Œ≤)+v) with Œ≥A x3 +Œ≥A x4 taking at least d + 1 diÔ¨Äerent values. Assume for the sake of deriving a contradiction of diÔ¨Äerent sums of pairs that |œÄA ((Œ±, Œ≤) + v)| ‚â§ d ‚àí 1. Then, since the   number + d ‚àí 1 = (d ‚àí 1)d/2 < d + 1 for of elements in œÄA ((Œ±, Œ≤) + v) is bounded by d‚àí1 2 d ‚àà {1, 2, 3}, this contradicts the above construction.  
   
  Remark 1. For depth d = 4, one can Ô¨Ånd GCTUF problems with G = Z/5Z and patterns that fail to satisfy Lemma 4; we present one such example in Fig. 2. Moreover, we remark that Lemma 4 is the only place in our proofs where we use the assumption that d = |G| ‚àí |R| ‚â§ 3.  
   
  œÄA  
   
  Œ≤ 0  
   
  0, 1, 0, 1, 2,3,4 2 0  
   
  1  
   
  œÄB  
   
  Œ≤ 0 2  
   
  0  
   
  Œ±  
   
  1  
   
  0, 1  
   
  0  
   
  0  
   
  1  
   
  2  
   
  Œ±  
   
  Fig. 2. Possible patterns œÄA and œÄB for a problem with group G = Z/5Z. Every square  and the numbers in the box indicate the elements of corresponds to a pair (Œ±, Œ≤) ‚àà Œ†, œÄA (Œ±, Œ≤) and œÄB (Œ±, Œ≤), respectively. For d = 4, Lemma 4 fails to hold for (Œ±, Œ≤) = (0, 0) and v = (1, 0).  
   
  The main application of Lemma 4 is the following: If, on top of the assumption œÄA ((Œ±, Œ≤) + in Lemma 4, |œÄB ((Œ±, Œ≤) + v)| ‚â• 2 holds, then Lemma 3 guarantees (¬Ø v)+¬Ø œÄB ((Œ±, Œ≤)+v))‚à©R = ‚àÖ. From now on, we analyze both the A- and B-patterns in detail, in particular through averaging, to guarantee the aforementioned nonempty intersection and thus Ô¨Ånd a solution, or identify additional properties that lead to progress. To distinguish cases of diÔ¨Äerent pattern structure, we need the following deÔ¨Ånition (see Fig. 3 for an illustration).  
   
  Advances on Strictly Œî-Modular IPs  
   
  405  
   
  an interior pair if (Œ±, Œ≤)+ Definition 1. Let D be as in (5). We call (Œ±, Œ≤) ‚àà Œ† v ‚àà Œ† for all v ‚àà D, a border pair if (Œ±, Œ≤) ¬± v ‚àà Œ† for exactly two v ‚àà D, and a vertex pair if it is not an interior or border pair. Note that for a border pair (Œ±, Œ≤), due to symmeŒ≤ try, the two directions v ‚àà D with (Œ±, Œ≤) ¬± v ‚àà Œ† will always be antiparallel, i.e., v and ‚àív for some x x v ‚àà D. To continue, the four types of patterns we b i x distinguish are the following: (I) |œÄB (Œ±, Œ≤)| = 1 for or this is not the case, and (II) Œ† has x b x all (Œ±, Œ≤) ‚àà Œ†, Œ± an interior pair, or (III) Œ† has no interior but border pairs, or (IV) Œ† has only vertex pairs. We sketch how to proceed for each of the types and present the Fig. 3. A pattern shape detailed discussion in the long version. with an interior, border, and vertex pairs (marked i,  
   
  Patterns of type I . In a type I pattern, techniques b, and w, respectively). of [24] enable reducing the problem to a new GCTUF problem with same G and |R|, and at least one variable less, thus allowing to make progress.  
   
  contains an interior pair and a hidden solution for some Patterns of type II . If Œ† pair (Œ±, Œ≤), we can use techniques from [24] to Ô¨Ånd v ‚àà D such that (Œ±, Œ≤) + 2v ‚àà and |œÄB ((Œ±, Œ≤) + v)| ‚â• 2. Using Lemma 4 gives |œÄA ((Œ±, Œ≤) + v)| ‚â• d. So by Œ† ¬ØB (Œ±, Œ≤) + v) ‚à© R = ‚àÖ, hence we can Ô¨Ånd a solution. Lemma 3, (¬Ø œÄA ((Œ±, Œ≤) + v) + œÄ Patterns of type III . In this case, we show that if we fail to Ô¨Ånd a solution in œÄ ¬ØA + œÄ ¬ØB , then we can reduce to a smaller pattern shape Œ†  , allowing to induct. We Ô¨Årst deal with the case where there is no border pair (Œ±, Œ≤) satisfying |œÄB (Œ±, Œ≤)| ‚â• 2. One can show that this implies that all pairs with |œÄB (Œ±, Œ≤)| = 1 Recall that these are the only pairs that lie on a single tight constraint of Œ†. might contain a hidden solution, so we can use as Œ†  all pairs on this tight constraint. In the other case, there is a border pair (Œ±, Œ≤) with |œÄB (Œ±, Œ≤)| ‚â• 2. Assume additionally that there is a hidden solution for a pair sharing a tight constraint with (Œ±, Œ≤) (note that the latter is unique). Using Lemma 4 and Propoof Œ† sition 2, we get that there must exist a pair (Œ± , Œ≤  ) with |œÄA (Œ± , Œ≤  )| ‚â• d and ¬ØA + œÄ ¬ØB , |œÄB (Œ± , Œ≤  )| ‚â• 2. By Lemma 3, this implies existence of a solution in œÄ contradicting the assumption. Thus, there cannot be a hidden solution anywhere on the tight constraint at (Œ±, Œ≤), so taking Œ† and strengthening that constraint by one unit leads to the desired Œ†  . Patterns of type IV . For type IV pattern structure, we Ô¨Årst observe that, by with |œÄB (Œ±, Œ≤)| ‚â• 2, Observation 1, if there are any solutions for pairs (Œ±, Œ≤) ‚àà Œ† we can also Ô¨Ånd one eÔ¨Éciently by combining solutions computed for the A- and ¬ØB . In the other case, i.e., when no B-subproblem when determining œÄ ¬ØA and œÄ solutions exist for such (Œ±, Œ≤), it turns out that a type IV pattern is structured enough to allow a reduction to a type I pattern, analogous to an argument of [24].  
   
  406  
   
  M. N¬® agele et al.  
   
  References 1. Aprile, M., Fiorini, S.: Regular matroids have polynomial extension complexity. Math. Oper. Res. 47(1), 540‚Äì559 (2021). https://doi.org/10.1287/moor.2021.1137 2. Artmann, S., Eisenbrand, F., Glanzer, C., Oertel, T., Vempala, S., Weismantel, R.: A note on non-degenerate integer programs with small sub-determinants. Oper. Res. Lett. 44(5), 635‚Äì639 (2016). https://doi.org/10.1016/j.orl.2016.07.004 3. Artmann, S., Weismantel, R., and Zenklusen, R.: A Strongly Polynomial Algorithm for Bimodular Integer Linear Programming. In: Proceedings of the 49th Annual ACM Symposium on Theory of Computing (STOC ‚Äô17), pp. 1206‚Äì1219, Montreal (2017). https://doi.org/10.1145/3055399.3055473 4. Averkov, G., Schymura, M.: On the Maximal Number of Columns of a Œî - modular Matrix. In: Proceedings of the 23rd International Conference on Integer Programming and Combinatorial Optimization (IPCO ‚Äô22), pp. 29‚Äì42, Eidhoven (2022). https://doi.org/10.1007/978-3-031-06901-7 3 5. Barahona, F., Conforti, M.: A construction for binary matroids. Discret. Math. 66(3), 213‚Äì218 (1987). https://doi.org/10.1016/0012-365X(87)90097-5 6. Bonifas, N., Di Summa, M., Eisenbrand, F., H¬® ahnle, N., Niemeier, M.: On Subdeterminants and the Diameter of Polyhedra. Discrete Comput. Geometry 52(1), 102‚Äì115 (2014). https://doi.org/10.1007/s00454-014-9601-x 7. Camerini, P.M., Galbiati, G., MaÔ¨Éoli, F.: Random pseudo-polynomial algorithms for exact matroid problems. J. Algorithms 13, 258‚Äì273 (1992). https://doi.org/10. 1016/0196-6774(92)90018-8 8. Di Summa, M., Eisenbrand, F., Faenza, Y., Moldenhauer, C.: On Largest Volume Simplices and Sub-determinants. In: Proceedings of the 26th Annual ACMSIAM Symposium on Discrete Algorithms (SODA ‚Äô15), pp. 315‚Äì323, San Diego (2015). https://doi.org/10.1137/1.9781611973730.23 9. Dinitz, M., Kortsarz, G.: Matroid secretary for regular and decomposable matroids. SIAM J. Comput. 43(5), 1807‚Äì1830 (2014). https://doi.org/10.1137/13094030X 10. Eisenbrand, F., Vempala, S.: Geometric random edge. Math. Program. 1, 325‚Äì339 (2016). https://doi.org/10.1007/s10107-016-1089-0 11. Fiorini, S., Joret, G., Weltge, S., and Yuditsky, Y.: Integer programs with bounded subdeterminants and two nonzeros per row. In: Proceedings of the 62nd Annual Symposium on Foundations of Computer Science (FOCS ‚Äô22), pp. 13‚Äì24 (2022). https://doi.org/10.1109/FOCS52979.2021.00011 12. Glanzer, C., Stallknecht, I., and Weismantel, R.: On the recognition of a, b, cmodular matrices. In: Proceedings of the 22nd International Conference on Integer Programming and Combinatorial Optimization (IPCO ‚Äô21), pp. 238‚Äì251, Atlanta (2021). https://doi.org/10.1007/978-3-030-73879-2 17 13. Glanzer, C., Weismantel, R., Zenklusen, R.: On the number of distinct rows of a matrix with bounded subdeterminants. SIAM J. Discret. Math. 32(3), 1706‚Äì1720 (2018). https://doi.org/10.1137/17M1125728 14. Goemans, M.X., Ramakrishnan, V.S.: Minimizing submodular functions over families of sets. Combinatorica 15(4), 499‚Äì513 (1995). https://doi.org/10.1007/ BF01192523 15. Gribanov, D., Shumilov, I., Malyshev, D., Pardalos, P.: On Œî-modular integer linear problems in the canonical form and equivalent problems. J. Global Optim. (2022). https://doi.org/10.1007/s10898-022-01165-9  
   
  Advances on Strictly Œî-Modular IPs  
   
  407  
   
  16. Gribanov, D.V.: An FPTAS for the Œî-modular multidimensional knapsack problem. In: Proceedings of the International Conference on Mathematical Optimization Theory and Operations Research (MOTOR), pp. 79‚Äì95 (2021). https://doi. org/10.1007/978-3-030-77876-7 6 17. Gribanov, D.V., Zolotykh, N.Y.: On lattice point counting in Œî-modular polyhedra. Optim. Lett. (1), 1‚Äì28 (2021). https://doi.org/10.1007/s11590-021-01744x 18. Gribanov, D.V., Veselov, S.I.: On integer programming with bounded determinants. Optim. Lett. 10(6), 1169‚Äì1177 (2015). https://doi.org/10.1007/s11590-0150943-y 19. Gr¬® otschel, M., Lov¬¥ asz, L., Schrijver, A.: Corrigendum to our paper ‚ÄòThe ellipsoid method and its consequences in combinatorial optimization‚Äô. Combinatorica 4(4), 291‚Äì295 (1984). https://doi.org/10.1007/BF02579139 20. Gr¬® otschel, M., Lov¬¥ asz, L., Schrijver, A.: Geometric Algorithms and Combinatorial Optimization. Springer, Cham (1993). https://doi.org/10.1007/978-3-642-78240-4 21. Heller, I.: On linear systems with integral valued solutions. Pac. J. Math. 7(3), 1351‚Äì1364 (1957). https://doi.org/10.2140/pjm.1957.7.1351 22. Lee, J., Paat, J., Stallknecht, I., Xu, L.: Improving proximity bounds using sparsity. In: Ba¬®ƒ±ou, M., Gendron, B., G¬® unl¬® uk, O., Mahjoub, A.R. (eds.) ISCO 2020. LNCS, vol. 12176, pp. 115‚Äì127. Springer, Cham (2020). https://doi.org/10.1007/978-3030-53262-8 10 23. Lee, J., Paat, J., Stallknecht, I., Xu, L.: Polynomial upper bounds on the number of diÔ¨Äering columns of Œî-modular integer programs. Math. Oper. Res. (2022). https://doi.org/10.1287/moor.2022.1339 24. N¬® agele, M., Santiago, R., Zenklusen, R.: Congruency-constrained TU problems beyond the bimodular case. In: Proceedings of the 33rd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2022), pp. 2743‚Äì2790 (2022). https://doi. org/10.1137/1.9781611977073.108 25. N¬® agele, M., Sudakov, B., Zenklusen, R.: Submodular minimization under congruency constraints. Combinatorica 39(6), 1351‚Äì1386 (2019). https://doi.org/10. 1007/s00493-019-3900-1 26. N¬® agele, M., Zenklusen, R.: A new contraction technique with applications to congruency-constrained cuts. Math. Program. (6), 455‚Äì481 (2020). https://doi. org/10.1007/s10107-020-01498-x 27. Nikolov, A.: Randomized rounding for the largest simplex problem. In: Proceedings of the 47th Annual ACM Symposium on Theory of Computing (STOC 2015), pp. 861‚Äì870, Portland (2015). https://doi.org/10.1145/2746539.2746628 28. Paat, J., Schl¬® oter, M., Weismantel, R.: The integrality number of an integer program. Math. Program. (6), 1‚Äì21 (2021). https://doi.org/10.1007/s10107-02101651-0 29. Padberg, M.W., Rao, M.R.: Odd minimum cut-sets and b-matchings. Math. Oper. Res. 7(1), 67‚Äì80 (1982). https://doi.org/10.1287/moor.7.1.67 30. Schrijver, A.: Theory of Linear and Integer Programming. Wiley, New York (1998) 31. Seymour, P.D.: Decomposition of regular matroids. J. Comb. Theory, Ser. B 28(3), 305‚Äì359 (1980). https://doi.org/10.1016/0095-8956(80)90075-1 ¬¥ A strongly polynomial algorithm to solve combinatorial linear pro32. Tardos, E.: grams. Oper. Res. 34(2), 250‚Äì256 (1986). https://doi.org/10.1287/opre.34.2.25 33. Veselov, S.I., Chirkov, A.J.: Integer program with bimodular matrix. Discret. Optim. 6(2), 220‚Äì222 (2009). https://doi.org/10.1016/j.disopt.2008.12.002  
   
  Cut-SuÔ¨Écient Directed 2-Commodity MultiÔ¨Çow Topologies Joseph Poremba(B) and F. Bruce Shepherd Computer Science, University of British Columbia, Vancouver, BC, Canada {jporemba,fbrucesh}@cs.ubc.ca  
   
  Abstract. In multicommodity network Ô¨Çows, a supply-demand graph pair (G, H) (called a multiÔ¨Çow topology) is cut-suÔ¨Écient if, for all capacity weights u and demand weights d, the cut condition is enough to guarantee the existence of a feasible multiÔ¨Çow. We characterize the cut-suÔ¨Écient topologies for two classes of directed 2-commodity Ô¨Çows: roundtrip demands, where H is a 2-cycle, and 2-path demands, where H is a directed path of length two. To do so, we introduce a theory of relevant minors. Unlike the undirected setting, for directed graphs the cutsuÔ¨Écient topologies are not closed under taking minors. They are however closed under taking relevant minors. Respectively, the cut-suÔ¨Écient topologies for roundtrip and two-path demands are characterized by one and two forbidden relevant minors. As an application of our results, we show that recognizing cut-suÔ¨Éciency for directed multiÔ¨Çow topologies is NP-hard, even for roundtrip demands. This is in contrast to undirected 2-commodity Ô¨Çows, for which topologies are always cut-suÔ¨Écient. Keywords: Network Flows  
   
  1  
   
  ¬∑ MultiÔ¨Çows ¬∑ Cuts ¬∑ Flow-cut Gap  
   
  Introduction  
   
  Network Ô¨Çows are one of the fundamental areas of combinatorial optimization and we study its feasibility question. Given an edge-capacitated supply network (directed or undirected) G = (V, E, u) and an edge-weighted demand network H = (V, F, d), can we route (fractionally) in G the demands from H without violating G‚Äôs edge capacities? Perhaps the most natural requirement for this Ô¨Çow to exist is the cut condition: for each non-empty S  V , the total demand on edges in H in the cut induced by S, is at most the total capacity on G‚Äôs edges in this cut. For undirected graphs this is: d(Œ¥H (S)) ‚â§ u(Œ¥G (S))  
   
  for all S  V, S = ‚àÖ.  
   
  (1)  
   
  For directed graphs, we replace Œ¥ with Œ¥ + . The classical Max-Flow Min-Cut (aka Menger‚Äôs) Theorem asserts that this condition is suÔ¨Écient in the case where |E(H)| = 1. This is not always the case. The authors are grateful for an NSERC Discovery Grant which supported this work. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 408‚Äì422, 2023. https://doi.org/10.1007/978-3-031-32726-1_29  
   
  Cut-SuÔ¨Écient Directed 2-Commodity Multiow Topologies  
   
  409  
   
  For instance, in Fig. 1 the cut condition holds for unit capacities and demands. However, the shortest path between s, t for any demand edge st ‚àà E(H) is 2. Hence any Ô¨Çow that satisÔ¨Åes this demand would require 8 units of capacity, whereas G only has 6. This is a proof that we must scale up some capacity to 43 in order to route H. It is easy to check in fact that we may route H as long as every edge has capacity 43 . This idea of Ô¨Ånding the minimum scaling up of capacities so that H is routable is the basis for the notion of Ô¨Çow-cut gap. A multiÔ¨Çow topology consists of a pair of graphs (G, H) on the same node set V . We say the topology has Ô¨Çow-cut gap Œ± ‚â• 1 if the following holds for every multiÔ¨Çow instance on (G, H), i.e. for every endowment of capacities u and demands d: if (G, H, u, d) satisÔ¨Åes the cut condition, 4 Fig. 1. Flow-cut gap 3 . The edges of G are then a multiÔ¨Çow exists if we scale the capacities up to Œ±u. We emphasize solid and the edges of H are dashed. that the Ô¨Çow-cut gap is a property of a topology (G, H), while the cut condition and feasibility are properties of a weighted instance (G, H, u, d). The foundational works of Leighton-Rao [21], Auman-Rabani [3], and Linial-London-Rabinovich [22] show that for arbitrary undirected topologies the Ô¨Çow-cut gap is O(log n); moreover this is tight due to an expander example in [21]. There is also a conjecture which asserts that the socalled integral Ô¨Çow-cut gap is quantitatively linked to this standard Ô¨Çow-cut gap [7]. There have also been investigations on establishing constant-factor Ô¨Çow-cut gaps for restricted families [5‚Äì7,10,19,20]. Our focus is on topologies that are cut-suÔ¨Écient, that is, multiÔ¨Çow topologies with Ô¨Çow-cut gap 1. Several important classes of undirected topologies have been shown to have this property. The most renowned results are the Max-Flow MinCut Theorem (|E(H)| = 1), Hu‚Äôs 2-commodity Theorem (|E(H)| = 2) [12], and the Okamura-Seymour Theorem (planar G where all demand edges have their endpoints on a single face) [26]. There is no over-arching theorem, however, that captures all cut-suÔ¨Écient undirected topologies. On a related note, the problem of recognizing cut-suÔ¨Éciency has not been studied as far as we know. The lack of a recognition algorithm for cut-suÔ¨Éciency is curious since it is a minor-closed property. Namely, if an undirected pair (G, H) has a Ô¨Çow-cut gap ‚â§ Œ±, then so does any pair which arises by contracting or deleting edges in G. This is veriÔ¨Åed by noting that deleting an edge corresponds to setting its capacity to 0 and contracting corresponds to setting its capacity to +‚àû. Hence it is tempting to claim that there is an algorithm to detect ‚â§ Œ± Ô¨Çowcut gap topologies, since this minor-closed property should conÔ¨Årm a Ô¨Ånite list of ‚Äúforbidden multiÔ¨Çow topologies‚Äù. This is not the case however, due to the subtlety that the demand graph is also part of the topology. SpeciÔ¨Åcally, [4]  
   
  410  
   
  J. Poremba and F. B. Shepherd  
   
  gives a characterization of when the Ô¨Çow-cut gap is 1 for pairs (G, H) where G is series-parallel. They prove there are inÔ¨Ånitely many minimal ‚Äúbad‚Äù topologies called the odd spindles. These generalize the graph of Fig. 1 by replacing the ‚Äúdemand triangle‚Äù by a ‚Äúdemand odd cycle‚Äù. We are not aware of an algorithm for recognizing this property for series-parallel topologies. Of course, when |E(H)| = 2, Hu‚Äôs Theorem gives us a very simple polynomial time recognition algorithm for undirected cut-suÔ¨Éciency. Given such a topology (G, H), it always outputs YES! We consider the directed analogue of this question and prove the following contrasting result. Theorem 1. It is NP-hard to determine whether a directed multiÔ¨Çow topology (G, H) is cut-suÔ¨Écient, even if H is a 2-cycle. In general, there is much less work for directed cut-suÔ¨Éciency. One beautiful result is a theorem of Nagamochi and Ibaraki [24] which shows that any cutsuÔ¨Écient directed topology (G, H) is also ‚Äúintegrally cut-suÔ¨Écient‚Äù. In other words, if all the capacities and demands are integral, then the cut condition is suÔ¨Écient to guarantee an integral routing. Theorem 1 relies on an understanding of the forbidden minors in directed 2-commodity topologies, much along the lines of the undirected series-parallel characterization of Chakrabarti et al. [4]. There are a number of diÔ¨Éculties we face with this approach. Most signiÔ¨Åcantly, directed cut-suÔ¨Éciency does not enjoy the same minor-closed property as the undirected setting. The key issue is that contracting an edge e in the directed setting may not correspond to deÔ¨Åning its capacity to +‚àû. This is because it may create entirely new paths for Ô¨Çows to use. This requires us to develop a theory of ‚Äúrelevant minors‚Äù. Namely, a minor (G , H  ) is relevant if the cut condition in the minor is directly connected to the cut condition in (G, H) with an appropriate setting of edge weights in (G, H). The technical results we need are developed in Sect. 3. We partition the class of 2-commodity topologies into one of three types. H is said to have roundtrip demands if E(H) = {(s, t), (t, s)} for distinct nodes s, t ‚àà V , it has 2-path demands if E(H) = {(s, t), (t, r)} where s, t, r are distinct nodes, and it has 2-matching demands if E(H) = {(s1 , t1 ), (s2 , t2 )} for distinct nodes s1 , s2 , t1 , t2 . There is also the case where H has a single common head or common tail, but these are always cut-suÔ¨Écient by a simple reduction from the Max-Flow Min-Cut Theorem. Using the notion of relevant minors, we Ô¨Ånd that the two topologies from Fig. 2a-2b, are the only minimal forbidden relevant minors for roundtrip and 2-path topologies. More precisely we have the following. Theorem 2. A directed multiÔ¨Çow topology with roundtrip demands is cutsuÔ¨Écient if and only if it does not contain the bad dual triangles (Fig. 2a) as a relevant minor. Theorem 3. A directed multiÔ¨Çow topology with 2-path demands is cut-suÔ¨Écient if and only if it does not contain the bad triangle (Fig. 2b) or the bad dual triangles (Fig. 2a) as a relevant minor.  
   
  Cut-SuÔ¨Écient Directed 2-Commodity Multiow Topologies  
   
  (a) The bad dual triangles.  
   
  411  
   
  (b) The bad triangle.  
   
  Fig. 2. The minimal bad roundtrip and 2-path demand topologies. With unit capacities and demands, each satisÔ¨Åes the cut condition but is not feasible.  
   
  These results imply that, in the case of roundtrip and 2-path demands, the minimal non-cut-suÔ¨Écient directed topologies are certiÔ¨Åed as such by multiÔ¨Çow instances with 0,1 data (i.e., demands and capacities are 0, 1-valued). Ultimately, this implies that any non-cut-suÔ¨Écient topology in these classes is certiÔ¨Åed by 0,1 demand values and 0,1,+‚àû capacity values. This 2 property need not always hold; Fig. 3 is an undirected example where the cut condition is suÔ¨Écient for unit demand weights but not when one of the demand edges has weight 2. We conjecture - see Sect. 6 - that this 0,1 property is true for all directed Fig. 3. An undirected topology where 2-commodity topologies. If true, this the cut condition is suÔ¨Écient for unit would give a complete characterization demands, but not in general. of 2-commodity cut-suÔ¨Éciency using a result in [27], which establishes that the non-cut-suÔ¨Écient topologies certiÔ¨Åed by 0, 1 demands are characterized via the minors in Fig. 2b and 2a. We achieve Theorems 2 and 3 by showing that, for roundtrip and 2-path demands, if G contains two paths for its diÔ¨Äerent commodities that share an edge, then we Ô¨Ånd either the bad dual triangles or the bad triangle as a relevant minor. This argument also yields the following characterization of cut-suÔ¨Éciency, which is in a convenient form to prove Theorem 1. Theorem 4. Suppose (G, H) is a directed multiÔ¨Çow topology with roundtrip or 2-path demands, say (s, t) and (t, r) where r may equal s. The topology is cutsuÔ¨Écient if and only if every st-path is arc-disjoint from every tr-path in G. 1.1  
   
  Other Related Work  
   
  There are several other known cut-suÔ¨Écient undirected classes. Lomonosov and Seymour ( [23,30], cf. Corollary 72.2a [29]) yield a characterization of the demand  
   
  412  
   
  J. Poremba and F. B. Shepherd  
   
  graphs H such that every supply graph G ‚Äúworks‚Äù for H, i.e., (G, H) is cutsuÔ¨Écient for any graph G with V (H) ‚äÜ V (G). They prove that any such H is (a subgraph of) either K4 , C5 or the union of two stars. Another question asks for which (supply) graphs G is it the case that (G, H) is cut-suÔ¨Écient for every H which is a subgraph of G; Seymour [31] shows that this is precisely the class of K5 minor-free graphs. A related result is to characterize which pairs (G, Z) with Z ‚äÜ V (G) are cut-suÔ¨Écient for every demand graph H with V (H) = Z. In [25] it is shown that this occurs if and only if G is (reducible to) planar and Z is a subset of one face, i.e., essentially Okamura-Seymour instances [26]. We refer the reader to [7] for additional discussion. Similar to our relevant minors, other works have deÔ¨Åned restricted types of directed graph minors to achieve interesting characterizations in other applications. Some examples include butterÔ¨Çy minors (introduced in [13], used in [1,14,17]), strongly-connected contractions [16], d-minors [8], and shallow minors [18]. There is no uniÔ¨Åed theory of directed minors that is useful in every context. Our context is diÔ¨Äerent than the aforementioned in that we need to consider both G and H to achieve the desired properties (though in Corollary 1, connections to butterÔ¨Çy and strongly-connected contractions can be seen). A related but diÔ¨Äerent gap result for directed multicommodity Ô¨Çows has been studied that is deÔ¨Åned in terms of the directed non-bipartite sparsest cut problem. Here, a ‚Äúcut‚Äù is any set F ‚äÜ E(G), and the sparsity of F is the ratio between u(F ) and the total demand of all commodities separated by F . The gap is the smallest ratio, over all u and d, of the maximum Ô¨Çow to the smallest sparsity of a cut. This is the ‚Äúnon-bipartite‚Äù Ô¨Çow-cut gap. For undirected graphs, the two gaps coincide, but not so in ‚àö the directed setting. For this non-bipartite Ô¨Çowacke [11], cut gap, a general bound of O( n) was proven by Hajiaghayi and R¬® Àú 11/23 ) by Agarwal et al. [2]. Recently, several results have later improved to O(n given improved bounds for certain classes of supply graphs, such as directed series-parallel and graphs of bounded pathwidth [28], and planar [15].  
   
  2  
   
  Preliminaries  
   
  By paths we always mean simple paths, and for directed graphs we mean these to be simple directed paths. We use tail(e) and head(e) to represent the tail and head of an edge e, respectively. For an edge set F , we deÔ¨Åne tail(F ) = {tail(e) : e ‚àà F } and head(F ) = {head(e) : e ‚àà F }. For multicommodity Ô¨Çows, we always assume integer capacities u and demand weights d. This assumption does not aÔ¨Äect the value of the Ô¨Çow-cut gap, or whether a topology is cut-suÔ¨Écient. We let Gu be the multigraph obtained by splitting each e ‚àà E(G) into u(e) parallel copies (and deÔ¨Åne Hd similarly). 2.1  
   
  Cut-Deceptive Weights and Minors of Multiflow Topologies  
   
  For discussion of contraction, we allow parallel edges and loops. We write deletion and contraction of an edge set F as G‚àíF and G/F respectively. Edges have their  
   
  Cut-SuÔ¨Écient Directed 2-Commodity Multiow Topologies  
   
  413  
   
  own identity beyond their incident nodes. Contractions may change the ends of non-contracted edges, but those edges themselves still exist. For notational convenience we still write edges in terms of incident nodes, such as e = (w, v). A minor of an undirected or directed multiÔ¨Çow topology (G, H) is obtained by a sequence of edge deletions from G and H, and contractions of edges in G. Contracting e ‚àà E(G) identiÔ¨Åes its ends in both G and H, and we denote this topology (G, H)/e. For a multiÔ¨Çow topology (G, H), we say weights (u, d) are cut-deceptive if (G, H, u, d) satisÔ¨Åes the cut condition but is not feasible. Hence, a topology is cut-suÔ¨Écient if and only if it does not have any cut-deceptive weights. For undirected multiÔ¨Çow topologies, the family of cut-suÔ¨Écient topologies is closed under taking minors. In particular, if a minor (G , H  ) has cut-deceptive weights (u, d), then those weights can be extended to cut-deceptive weights (uext , dext ) for (G, H) as follows. Definition 1 (Extension of Weights). Let (G , H  ) be a minor of an undirected or directed multiÔ¨Çow topology (G, H). Let (u, d) be weights for (G , H  ). We deÔ¨Åne the extension1 of (u, d) to be the weights (uext , dext ) of (G, H) where: ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì  
   
  3  
   
  uext (e) = 0 if e ‚àà E(G) was deleted, uext (e) = +‚àû if e ‚àà E(G) was contracted, uext (e) = u(e) for all other edges e ‚àà E(G ), dext (e) = 0 if e ‚àà E(H) was deleted, dext (e) = d(e) for all other edges e ‚àà E(H  ).  
   
  Relevant Minors and Entry-Exit Connected Edge Sets  
   
  In contrast to the undirected setting, the cut-suÔ¨Écient directed multiÔ¨Çow topologies are not closed under taking minors. For example, consider the topology in Fig. 4. It is trivially cut-suÔ¨Écient, since it is impossible to satisfy the cut condition with nonzero demands. However, contracting Fig. 4. A cut-suÔ¨Écient topology, with a the highlighted edge yields the bad non-cut-suÔ¨Écient minor. triangle (Fig. 2b), which is not cutsuÔ¨Écient. Without restricting the kinds of minors allowed, it does not make sense to speak of forbidden minors. 1  
   
  Recall that deletions and contractions commute. The extension is invariant to the speciÔ¨Åc ordering of these operations. However, there may be operation sequences that are diÔ¨Äerent beyond simple re-ordering but still produce the same minor. For example, a single vertex is a minor of a triangle, but any two edges can be contracted and the third deleted to obtain it. The speciÔ¨Åc choice aÔ¨Äects the extension. It is tedious to instrument this nuance throughout the text, but we associate a minor with a set of deletions and contractions that produce it.  
   
  414  
   
  3.1  
   
  J. Poremba and F. B. Shepherd  
   
  Relevant Minors  
   
  We introduce a theory of relevant minors for directed multiÔ¨Çow topologies that gives closure for cut-suÔ¨Éciency. In particular, if (G , H  ) is a relevant minor of (G, H), and (G , H  ) has cut-deceptive weights (u, d), then (uext , dext ) are cutdeceptive weights of (G, H). There are two potential reasons for (uext , dext ) to not be cut-deceptive: either (G, H, uext , dext ) is feasible, or the cut condition is not satisÔ¨Åed. The feasibility aspect is not a problem: it is easy to observe that if (G, H, uext , dext ) is feasible, then (G , H  , u, d) is also feasible using the same Ô¨Çow paths (after contracting). Thus the following deÔ¨Ånition gives us the desired properties. Definition 2 (Relevant Minor). A minor (G , H  ) of directed multiÔ¨Çow topology (G, H) is relevant if, for all weights (u, d) of (G , H  ) that satisfy the cut condition, (G, H, uext , dext ) also satisÔ¨Åes the cut condition. Proposition 1. Let (G , H  ) be a relevant minor of directed multiÔ¨Çow topology (G, H). If (u, d) are cut-deceptive weights for (G , H  ), then (uext , dext ) are cutdeceptive weights for (G, H). Hence, a directed multiÔ¨Çow topology (G, H) is cutsuÔ¨Écient if and only if every relevant minor of (G, H) is cut-suÔ¨Écient. As a consequence, we conclude one direction of Theorems 2 and 3: if a directed multiÔ¨Çow topology contains either the bad triangle or the bad dual triangles as a relevant minor, then it is not cut-suÔ¨Écient. 3.2  
   
  Contractions of Entry-Exit Connected Sets  
   
  The deÔ¨Ånition of a relevant minor is abstract and does not immediately suggest how to show that a minor is relevant. We develop some useful tools for this purpose. First, it is easily shown that deletions always produce relevant minors. Proposition 2. Let (G, H) be a directed multiÔ¨Çow topology. ‚Äì For any e ‚àà E(G), (G ‚àí e, H) is a relevant minor of (G, H). ‚Äì For any e ‚àà E(H), (G, H ‚àí e) is a relevant minor of (G, H). Proof. In either case, let (G , H  ) be the minor in question. Consider weighted instances (G , H  , u, d) and (G, H, uext , dext ). Cuts in the two instances that are induced by the same node subset have the same weights. Hence, the cut condition is satisÔ¨Åed in both or neither. So the only potential issues are contractions. We say an edge or edge set is safe if its contraction produces a relevant minor. What might make an edge unsafe? Consider Fig. 4. Contracting the highlighted edge produces a minor that is not relevant. The issue is that new connectivity is created by the contraction, so it is easier to satisfy the cut condition than in the original topology. This notion of safety seems to be intricate and does not satisfy simple properties such as: safe edge sets being closed under unions. While we are progressing towards  
   
  Cut-SuÔ¨Écient Directed 2-Commodity Multiow Topologies  
   
  415  
   
  a full characterization of safety, in this paper, we rely on a suÔ¨Écient condition for a set of edges to be safe. SpeciÔ¨Åcally, we show that if contracting an edge set does not create new terminal connectivity, then it is safe. We formalize this notion as follows. Definition 3 (Entry/Exit Points, Entry-Exit Connected). Let (G, H) be a directed multiÔ¨Çow topology. Let F ‚äÜ E(G) be weakly connected. ‚Äì We denote by Entry(F ) the set of nodes x ‚àà V (F ) such that there exists an sx-path in G ‚àí F for some s ‚àà tails(H). We call these entry points of F . ‚Äì We denote by Exit(F ) the set of nodes y ‚àà V (F ) such that there exists a yt-path in G ‚àí F for some t ‚àà heads(H). We call these exit points of F . We say F is entry-exit connected if G[F ] contains an xy-path for every x ‚àà Entry(F ) and y ‚àà Exit(F ). To prove such sets are safe, we recast the cut condition from discussing density of cuts to discussing paths connecting terminals. We call this new form the path cut condition. In essence, the cut condition in (G, H, u, d) is equivalent to the existence of, for every F ‚äÜ E(H), d(F ) many arc-disjoint tails(F )‚àíheads(F ) paths (i.e., paths between tails(F ) and heads(F )) in Gu . Definition 4 (Weakly Feasible Routing). Let F ‚äÜ E(H) be a demand subset of directed multiÔ¨Çow instance (G, H, u, d). A weakly feasible routing of F is a set PF of d(F ) arc-disjoint tails(F ) ‚àí heads(F ) paths in Gu . Furthermore, the weakly feasible routing is fair or marginal-satisfying if both:   ‚Äì exactly d Œ¥F+ (s) paths in PF start at s for every s ‚àà tails(F ), and  ‚àí  ‚Äì exactly d Œ¥F (t) paths in PF end at t for every t ‚àà heads(F ). On a technical note, this deÔ¨Ånition is problematic if tails(F ) ‚à© heads(F ) = ‚àÖ. Fortunately, in such a case no cut contains the entirety of F . It is a degenerate case that ultimately can be excluded from the path cut condition2 . Now, we deÔ¨Åne the path cut condition. In fact, we deÔ¨Åne two versions. Each is useful in diÔ¨Äerent circumstances, and both are equivalent to the cut condition. Definition 5 (Path Cut Condition). Let (G, H, u, d) be a directed multiÔ¨Çow instance. The (fair) path cut condition is the property that, for every F ‚äÜ E(H) where tails(F ) ‚à© heads(F ) = ‚àÖ, there exists a (fair) weakly feasible routing of F . Theorem 5. Let (G, H, u, d) be a directed multiÔ¨Çow instance. The cut condition, the path cut condition, and the fair path cut condition are equivalent. The path cut condition gives a convenient way to prove that it is safe to contract entry-exit connected edge sets. 2  
   
  An equally valid alternative would be to allow a weakly feasible routing of F to pack inÔ¨Ånitely many copies of the length-zero path on a node in tails(F ) ‚à© heads(F ).  
   
  416  
   
  J. Poremba and F. B. Shepherd  
   
  Theorem 6. Let (G, H) be a directed multiÔ¨Çow topology. If a weakly connected edge set F ‚äÜ E(G) is entry-exit connected, then F is safe. Proof. Let (G , H  ) = (G, H)/F . Let vF be the identiÔ¨Åed node for F in (G , H  ). Let (u, d) be weights for (G , H  ) that satisfy the path cut condition. We prove that (G, H, uext , dext ) satisÔ¨Åes the path cut condition by showing that for each J ‚äÜ E(H) with tailsH (J) ‚à© headsH (J) = ‚àÖ, there is a weakly feasible routing of J in (G, H, uext , dext ). Let k = d(J) = dext (J). For each x ‚àà Entry(F ) and y ‚àà Exit(F ), let Fx,y be an xy path in G[F ] (which exist, since F is entry-exit connected). Note that in Guext , there are inÔ¨Ånitely many copies of each Fx,y , since uext (e) = +‚àû for all e ‚àà F . For each x and y, select k of them, say Fx,y,1 , . . . , Fx,y,k . Recall that, per our convention on minors, the edges of J still exist in H  . However, the ends of edges may change. We split into two cases. In the Ô¨Årst case, suppose there exists v ‚àà tailsH  (J) ‚à© headsH  (J). Since tailsH (J) ‚à© headsH (J) = ‚àÖ, but the intersection is non-empty after contracting F into vF , it must be that v = vF . Then there exist s ‚àà tailsH (J) and t ‚àà headsH (J) such that s, t ‚àà V (G[F ]). Note that s ‚àà Entry(F ) and t ‚àà Exit(F ). Then taking Fs,t,i for i = 1, . . . , k gives a weakly feasible routing of F . In the second case, suppose that tailsH  (J) ‚à© headsH  (J) = ‚àÖ. Then, the path cut condition of (G , H  , u, d) implies that there is a set P  of k arc-disjoint tailsH  (J) ‚àí headsH  (J) paths in Gu . We map each of the paths P1 , . . . , Pk in P  to a tailsH (J)‚àíheadsH (J) path in Guext . Each Pi ‚àà P  is a path from some s ‚àà tailsH  (J) to some t ‚àà headsH  (J). If Pi avoids vF , then s ‚àà tailsH (J) and t ‚àà headsH (J), and we map Pi to itself. If Pi uses vF , then in Guext its edges form two paths: a path Xi from some sÀÜ ‚àà tailsH (J) to some x ‚àà Entry(F ), and a path Yi from some y ‚àà Exit(F ) to some tÀÜ ‚àà headsH (J) (either path may have length zero). Joining Xi and Yi with Fx,y,i yields a tailsH (J) ‚àí headsH (J) path. We map Pi to this path. In this way, each Pi ‚àà P  maps to a tailsH (J) ‚àí headsH (J) path in Guext that uses only the edges of Pi and edges in some Fx,y,i . Then the image of P  under this mapping is a set of k arc-disjoint tailsH (J) ‚àí headsH (J) paths in Guext , as desired. There are several special cases of entry-exit connected sets that lead to quick ways to justify safety. Definition 6. A subdivision of directed multiÔ¨Çow topology (G, H) is obtained by a sequence of the following operation: select an edge e ‚àà E(G), and replace it with a path of length at least one. Corollary 1. Let (G, H) be a directed multiÔ¨Çow topology. 1. 2. 3. 4.  
   
  If F ‚äÜ E(G) is strongly connected, then F is safe. ‚àí If e = (a, b) ‚àà E(G) with deg+ G (a) = 1 and degH (a) = 0, then e is safe. ‚àí + If e = (a, b) ‚àà E(G) with degG (b) = 1 and degH (b) = 0, then e is safe. If (G, H) is a subdivision of (G , H  ), then (G , H  ) is a relevant minor of (G, H).  
   
  Cut-SuÔ¨Écient Directed 2-Commodity Multiow Topologies  
   
  417  
   
  Items 1 and 4 are used to prove Theorem 2, and Item 3 is additionally used to prove Theorem 3, though the proof for the latter is omitted in this paper since it is very similar to the former. We note that these theorems do not require the full generality of this entry-exit connected machinery, and Corollary 1 can be proven in an ad-hoc fashion without appealing to Theorem 6. However it shows that the cases of Corollary 1 are diÔ¨Äerent expressions of the same general connectivity property. More importantly, the entry-exit machinery is general in the sense of not being restricted to a two demand setting. Moreover, the general form of entry-exit connected sets is used in [27] for studying safe contractions when there are 2-matching demands. We expect it to be useful in investigating characterizations of cut-suÔ¨Éciency for more general H.  
   
  4  
   
  Characterizations of Cut-SuÔ¨Éciency  
   
  We now apply our theory of relevant minors to prove Theorems 2, 3, and 4, which characterize the cut-suÔ¨Écient directed multiÔ¨Çow topologies for roundtrip and 2-path demands. What remains to prove is that if there are cut-deceptive weights for a topology, then one of the desired relevant minors exists. 4.1  
   
  Opposingly Ordered Paths  
   
  The cut condition implies the existence of certain paths in Gu . From there, infeasibility implies particular interactions of these paths, which we use to prove the existence of the desired relevant minor. To that end, we use the following terminology to describe interactions between paths. Definition 7 (Overlap Segments, Bridges). Let P and Q be paths in a directed graph that share at least one node. ‚Äì An overlap segment is a maximal common subpath of P and Q. It is trivial if it is one node. It is terminal if it contains the start or end of either path. ‚Äì A P -bridge of Q is a maximal subpath B of P that has at least one edge and shares no edges or internal nodes of B with Q. In the above, note that P can be written as an alternating sequence of P bridges of Q and the overlap segments of P and Q. When we say that we are listing objects, such as overlap segments, nodes, or edges, in P -order, we mean we list them by their order of occurrence when following the (directed) path P . Paths can intersect numerous times in varied conÔ¨Ågurations, so it may not be obvious how to proceed looking for a particular minor. To manage this complexity, we reduce to the case where the overlap segments follow a special pattern. Definition 8 (Opposingly Ordered). Let P and Q be paths in a directed graph that share at least one node. We say that P and Q are opposingly ordered if the P -order of their overlap segments is the reverse of the Q-order.  
   
  418  
   
  J. Poremba and F. B. Shepherd  
   
  Lemma 1. Let P be an s1 t1 -path and let Q be an s2 t2 -path in a directed graph, where s1 = s2 and t1 = t2 , such that P and Q share an edge. In P ‚à™ Q, there exists an s2 t2 -path Q‚àó such that: ‚Äì P and Q‚àó are opposingly ordered, and ‚Äì P and Q‚àó share an edge. 4.2  
   
  Characterization for Roundtrip and Two-Path Demands  
   
  We now have the tools to prove our characterizations. We begin with roundtrip demands. Lemma 2. Let (G, H) be a directed multiÔ¨Çow topology with roundtrip demands between s and t. Suppose G contains an st-path P and a ts-path Q that are not arc-disjoint. Then (G, H) contains the bad dual triangles (Fig. 2a) as a relevant minor. Proof. Consider a counterexample that minimizes |E(P )|+|E(Q)|. By Lemma 1, we may assume P and Q are opposingly ordered, otherwise we replace Q with Q‚àó . Note that s and t themselves are trivial terminal overlap segments. Since P and Q share an edge, there is a non-trivial overlap segment J ‚àó . Note that J ‚àó is not terminal. We claim J ‚àó is the only non-terminal overlap segment. Suppose there are at least two, for the sake of contradiction. Let JP be the Ô¨Årst non-terminal overlap segment in P -order (last in Q-order), and let JQ be the Ô¨Årst in Q-order (last in P -order). At least one of these two is not J ‚àó . Without loss of generality, suppose JP = J ‚àó . Say JP starts at w and ends at v. Let C be the cycle obtained by starting at s, following the Ô¨Årst (in P -order) P -bridge of Q to w, following JP from w to v, then following the last (in Q-order) Q-bridge of P from v to s. Consider the topology (G , H  ) = (G, H)/C, which has roundtrip demands. Since directed cycles are strongly connected, this contraction is safe by Corollary 1. Additionally, P  = P/(P ‚à© C) and Q = Q/(Q ‚à© C) are paths for the two commodities. Moreover, they have a non-trivial overlap segment, namely J ‚àó . By minimality, we obtain the desired relevant minor in (G , H  ), and hence in (G, H), a contradiction. So, there is exactly one non-terminal overlap segment J ‚àó , and it is nontrivial. Let w‚àó and v ‚àó be the Ô¨Årst and last nodes of J ‚àó , respectively (in either P -order or Q-order, both are the same for nodes of an overlap segment). The only other overlap segments are the terminal ones, s and t themselves. There are thus exactly two P -bridges of Q, connecting s to w‚àó and v ‚àó to t, respectively. Similarly there are exactly two Q-bridges of P , connecting t to w‚àó and v ‚àó to s, respectively. Then, the topology (P ‚à™ Q, H) is exactly a subdivision of the bad dual triangles. By Corollary 1, the bad dual triangles is a relevant minor of (P ‚à™ Q, H) and hence also of (G, H). Lemma 2 implies that if (G, H) has roundtrip demands and does not contain the bad dual triangles as a relevant minor, then every st-path in G is arc-disjoint  
   
  Cut-SuÔ¨Écient Directed 2-Commodity Multiow Topologies  
   
  419  
   
  from every ts-path. Such topologies are easily veriÔ¨Åed to be cut-suÔ¨Écient, since for any weights, any pair of weakly feasible routings for the two commodities together form a feasible routing for the whole instance. Hence Theorem 2 follows, as well as the part of Theorem 4 pertaining to roundtrip demands. For 2-path demands, we proceed along similar lines as for roundtrip demands. Theorem 3, and the remainder of Theorem 4, is straightforwardly implied by the following lemma whose proof we defer to the full version. Lemma 3. Let (G, H) be a directed multiÔ¨Çow topology with 2-path demands (s, t) and (t, r). If G has an st-path and a tr-path that are not arc-disjoint, then (G, H) contains either the bad dual triangles or the bad triangle as a relevant minor.  
   
  5  
   
  NP-Hardness of Recognizing Cut-SuÔ¨Éciency  
   
  The proofs of Theorems 2 and 3 can be adapted to give a polynomial time algorithm. Given (G, H) with roundtrip or 2-path demands, and weights (u, d) that satisfy the cut condition, the algorithm outputs either a feasible integer routing for (G, H, u, d) or one of the two forbidden relevant minors. The algorithm does not however determine whether (G, H) is cut-suÔ¨Écient, since particular (u, d) may be feasible despite (G, H) not being cut-suÔ¨Écient in general. We show that determining if a directed multiÔ¨Çow topology (G, H) is cut-suÔ¨Écient (the CutSufficient decision problem) is NP-hard, even if we restrict to roundtrip demands (the CutSufficientRT decision problem). We reduce from an NP-hard decision problem we call UsefulEdge. Given a directed graph G, distinct nodes s, t ‚àà V (G), and an edge e ‚àà E(G), it asks whether there exists a (simple directed) st-path in G that uses the edge e. The directed 2-node-disjoint path problem, proved by Fortune et al. [9] to be NPhard, can be reduced to UsefulEdge. Theorem 1 is implied by the following. Theorem 7. There is a polynomial time reduction from the UsefulEdge problem to the CutSufficientRT problem. Proof. Given an input (G, s, t, e = (w, v)) for UsefulEdge, we construct a multiÔ¨Çow topology (G , H  ) with roundtrip demands. We obtain G from G as follows: 1. Subdivide e into e1 = (w, w ), e2 = (w , v  ), e3 = (v  , v), where w , v  are new nodes and w, v maintain their other incident edges. 2. Add two new nodes s , t , and edges (s , s), (t, t ), (t , w ), and (v  , s ). Finally, deÔ¨Åne H  = (V (G ), {(s , t ), (t , s )}). We claim that (G , H  ) is not cutsuÔ¨Écient if and only if there is an st-path in G that uses e, which proves the result. DeÔ¨Åne Q to be the path (t , w ), (w , v  ), (v  , s ). For the ‚Äúonly if‚Äù direction, suppose that (G , H  ) is not cut-suÔ¨Écient. Then by Theorem 4, G has some s t -path P  that shares an edge with some t s -path. The only t s -path is Q , and the only possible shared edge is e2 = (w , v  ),  
   
  420  
   
  J. Poremba and F. B. Shepherd  
   
  so P  uses e1 , e2 , e3 . It also uses (s , s) and (t, t ). Swapping e1 , e2 , e3 for e and removing (s , s) and (t, t ) from P  , we Ô¨Ånd an st-path of G that uses e. For the ‚Äúif‚Äù direction, suppose there is an st-path P in G that uses e. By swapping e for e1 , e2 , e3 and adding (s , s) and (t, t ), we obtain an s t -path P  in G that uses e2 . Now, P  shares e2 = (w , v  ) with Q . By Theorem 4, the topology is not cut-suÔ¨Écient.  
   
  6  
   
  Towards a Complete 2-Commodity Characterization  
   
  For two commodities, the only remaining case is 2-matching demands. We conjecture that the bad dual triangles and the bad triangle are the only forbidden relevant minors for this case. Conjecture 1. A directed multiÔ¨Çow topology with 2-matching demands (and hence, two demands in general) is cut-suÔ¨Écient if and only if it does not contain the bad triangle or the bad dual triangles as a relevant minor. In [27], the following is proved. Proposition 3. If directed multiÔ¨Çow topology (G, H) has 2-matching demands and (integer) cut-deceptive weights (u, d) where d(e) = 1 for all e ‚àà E(H), then it contains either the bad triangle or the bad dual triangles as a relevant minor. The argument considers more intricate interactions of paths arising from the fair path cut condition. It also requires general entry-exit connected contractions, rather than the specialized cases of Corollary 1. For roundtrip and 2-path demands, there is a clear reduction to the case of unit weight demands: if there is a cut-deceptive weighting (u, d), then by taking any paths P and Q for the two commodities that share an edge, Lemmas 2 and 3 show there is a cut-deceptive weighting for (P ‚à™ Q, H) where the demands are unit. This observation is encapsulated in Theorem 4. However, we do not have so strong a result for 2-matching demands. It is not enough to just take paths for the two commodities that share an edge, as this may not even satisfy the cut condition.  
   
  References 1. Adler, I.: Directed tree-width examples. J. Comb. Theory Ser. B 97(5), 718‚Äì725 (2007). https://doi.org/10.1016/j.jctb.2006.12.006, https://linkinghub. elsevier.com/retrieve/pii/S0095895606001444 2. Agarwal, A., Alon, N., Charikar, M.S.: Improved approximation for directed cut problems. In: Proceedings of the Thirty-Ninth Annual ACM Symposium on Theory of Computing - STOC ‚Äô07, p. 671. ACM Press, San Diego, California, USA (2007). https://doi.org/10.1145/1250790.1250888, http://portal.acm.org/citation. cfm?doid=1250790.1250888 3. Aumann, Y., Rabani, Y.: An o (log k) approximate min-cut max-Ô¨Çow theorem and approximation algorithm. SIAM J. Comput. 27(1), 291‚Äì301 (1998)  
   
  Cut-SuÔ¨Écient Directed 2-Commodity Multiow Topologies  
   
  421  
   
  4. Chakrabarti, A., Fleischer, L., Weibel, C.: When the cut condition is enough: a complete characterization for multiÔ¨Çow problems in series-parallel networks. In: Proceedings of the Forty-Fourth Annual ACM Symposium on Theory of Computing, pp. 19‚Äì26. ACM (2012) 5. Chakrabarti, A., JaÔ¨Äe, A., Lee, J.R., Vincent, J.: Embeddings of topological graphs: lossy invariants, linearization, and 2-sums. In: 2008 49th Annual IEEE Symposium on Foundations of Computer Science, pp. 761‚Äì770. IEEE, Philadelphia, PA, USA, October 2008. https://doi.org/10.1109/FOCS.2008.79, http://ieeexplore.ieee.org/ document/4691008/ 6. Chekuri, C., Gupta, A., Newman, I., Rabinovich, Y., Sinclair, A.: Embedding k-outerplanar graphs into l1. SIAM J. Discret. Math. 20(1), 119‚Äì136 (2006). https://doi.org/10.1137/S0895480102417379, http://epubs.siam.org/doi/10.1137/ S0895480102417379 7. Chekuri, C., Shepherd, F.B., Weibel, C.: Flow-cut gaps for integer and fractional multiÔ¨Çows. J. Comb. Theory Ser. B 103(2), 248‚Äì273 (2013) 8. Deligkas, A., Meir, R.: Directed graph minors and serial-parallel width. In: 43rd International Symposium on Mathematical Foundations of Computer Science, vol. 21, p. 38 (2018) 9. Fortune, S., Hopcroft, J., Wyllie, J.: The directed subgraph homeomorphism problem. Theor. Comput. Sci. 10(2), 111‚Äì121 (1980). https://doi.org/10.1016/03043975(80)90009-2, http://www.sciencedirect.com/science/article/pii/030439758090 0092 10. Gupta, A., Newman, I., Rabinovich, Y., Sinclair, A.: Cuts, trees and 1 -embeddings of graphs. Combinatorica 24(2), 233‚Äì269 (2004) ‚àö 11. Hajiaghayi, M.T., R¬® acke, H.: An O( n)-approximation algorithm for directed sparsest cut. Inf. Process. Lett. 97(4), 156‚Äì160 (2006). https://doi.org/10.1016/j. ipl.2005.10.005, https://linkinghub.elsevier.com/retrieve/pii/S0020019005002929 12. Hu, T.C.: Multi-commodity network Ô¨Çows. Oper. Res. 11(3), 344‚Äì360 (1963). https://doi.org/10.1287/opre.11.3.344, http://pubsonline.informs.org/doi/ 10.1287/opre.11.3.344 13. Johnson, T., Robertson, N., Seymour, P., Thomas, R.: Directed tree-width. J. Comb. Theory Ser. B 82(1), 138‚Äì154 (2001). https://doi.org/10.1006/jctb.2000. 2031, https://linkinghub.elsevier.com/retrieve/pii/S0095895600920318 14. Kawarabayashi, K.i., Kreutzer, S.: The directed grid theorem. In: Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing, pp. 655‚Äì 664. ACM, Portland Oregon USA, June 2015. https://doi.org/10.1145/2746539. 2746586, https://dl.acm.org/doi/10.1145/2746539.2746586 15. Kawarabayashi, K.I., Sidiropoulos, A.: Embeddings of planar quasimetrics into directed  1 and polylogarithmic approximation for directed sparsest-cut. In: 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pp. 480‚Äì491. IEEE, Denver, CO, USA, February 2022. https://doi.org/10.1109/ FOCS52979.2021.00055, https://ieeexplore.ieee.org/document/9719783/ 16. Kim, I., Seymour, P.: Tournament minors. J. Comb. Theory Ser. B 112, 138‚Äì153 (2015). https://doi.org/10.1016/j.jctb.2014.12.005, https://linkinghub. elsevier.com/retrieve/pii/S0095895614001403 17. Kintali, S., Zhang, Q.: Forbidden directed minors and Kelly-width. Theor. Comput. Sci. 662, 40‚Äì47 (2017). https://doi.org/10.1016/j.tcs.2016.12.008, https:// linkinghub.elsevier.com/retrieve/pii/S0304397516307149  
   
  422  
   
  J. Poremba and F. B. Shepherd  
   
  18. Kreutzer, S., Tazari, S.: Directed nowhere dense classes of graphs. In: Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 1552‚Äì1562. Society for Industrial and Applied Mathematics, January 2012. https://doi.org/10.1137/1.9781611973099.123, https://epubs.siam.org/doi/ 10.1137/1.9781611973099.123 19. Lee, J.R., Raghavendra, P.: Coarse diÔ¨Äerentiation and multi-Ô¨Çows in planar graphs. Discret. Comput. Geom. 43(2), 346‚Äì362 (2010). https://doi.org/10.1007/s00454009-9172-4, http://link.springer.com/10.1007/s00454-009-9172-4 20. Lee, J.R., Sidiropoulos, A.: On the geometry of graphs with a forbidden minor. In: Proceedings of the 41st Annual ACM Symposium on Theory of Computing - STOC ‚Äô09, p. 245. ACM Press, Bethesda, MD, USA (2009). https://doi.org/10.1145/ 1536414.1536450, http://portal.acm.org/citation.cfm?doid=1536414.1536450 21. Leighton, T., Rao, S.: Multicommodity max-Ô¨Çow min-cut theorems and their use in designing approximation algorithms. J. ACM 46(6), 787‚Äì832 (1999) 22. Linial, N., London, E., Rabinovich, Y.: The geometry of graphs and some of its algorithmic applications. Combinatorica 15(2), 215‚Äì245 (1995) 23. Lomonosov, M.V.: Combinatorial approaches to multiÔ¨Çow problems. NorthHolland (1985) 24. Nagamochi, H., Ibaraki, T.: On max-Ô¨Çow min-cut and integral Ô¨Çow properties for multicommodity Ô¨Çows in directed graphs. Inf. Process. Lett. 31, 279‚Äì285 (1989) 25. Naves, G., Shepherd, B.: When do Gomory-Hu subtrees exist? SIAM J. Discret. Math. 36(3), 1567‚Äì1585 (2022) 26. Okamura, H., Seymour, P.D.: Multicommodity Ô¨Çows in planar graphs. J. Comb. Theory Ser. B 31(1), 75‚Äì81 (1981). http://www.sciencedirect.com/science/article/ B6WHT-4KBW025-8/2/9b4489ece0a97e9d8340d69948600501 27. Poremba, J.C.: Directed multicommodity Ô¨Çows: cut-suÔ¨Éciency and forbidden relevant minors. Master‚Äôs thesis, University of British Columbia (2022) 28. Salmasi, A., Sidiropoulos, A., Sridhar, V.: On constant multi-commodity Ô¨Çowcut gaps for families of directed minor-free graphs. In: Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 535‚Äì553. SIAM (2019) 29. Schrijver, A.: Combinatorial Optimization: Polyhedra and EÔ¨Éciency, vol. 24. Springer, Heidelberg (2003) 30. Seymour, P.D.: Four-terminus Ô¨Çows. Networks 10(1), 79‚Äì86 (1980) 31. Seymour, P.D.: Matroids and multicommodity Ô¨Çows. Eur. J. Comb. 2(3), 257‚Äì290 (1981)  
   
  Constant-Competitiveness for Random Assignment Matroid Secretary Without Knowing the Matroid Richard Santiago(B) , Ivan Sergeev, and Rico Zenklusen Department of Mathematics, ETH Zurich, Zurich, Switzerland {rtorres,isergeev,ricoz}@ethz.ch  
   
  Abstract. The Matroid Secretary Conjecture is a notorious open problem in online optimization. It claims the existence of an O(1)-competitive algorithm for the Matroid Secretary Problem (MSP). Here, the elements of a weighted matroid appear one-by-one, revealing their weight at appearance, and the task is to select elements online with the goal to get an independent set of largest possible weight. O(1)-competitive MSP algorithms have so far only been obtained for restricted matroid classes and for MSP variations, including Random-Assignment MSP (RA-MSP), where an adversary Ô¨Åxes a number of weights equal to the ground set size of the matroid, which then get assigned randomly to the elements of the ground set. Unfortunately, these approaches heavily rely on knowing the full matroid upfront. This is an arguably undesirable requirement, and there are good reasons to believe that an approach towards resolving the MSP Conjecture should not rely on it. Thus, both Soto [SIAM Journal on Computing 2013] and Oveis Gharan & Vondrak [Algorithmica 2013] raised as an open question whether RA-MSP admits an O(1)-competitive algorithm even without knowing the matroid upfront. In this work, we answer this question aÔ¨Érmatively. Our result makes RA-MSP the Ô¨Årst well-known MSP variant with an O(1)-competitive algorithm that does not need to know the underlying matroid upfront and without any restriction on the underlying matroid. Our approach is based on Ô¨Årst approximately learning the rank-density curve of the matroid, which we then exploit algorithmically.  
   
  1  
   
  Introduction  
   
  The Matroid Secretary Problem (MSP), introduced by BabaioÔ¨Ä, Immorlica, and Kleinberg [1], is a natural and well-known generalization of the classical Secretary Problem [6], motivated by strong connections and applications in mechanism design. Formally, MSP is an online selection problem where we are given This project received funding from Swiss National Science Foundation grant 200021 184622 and the European Research Council (ERC) under the European Union‚Äôs Horizon 2020 research and innovation programme (grant agreement No 817750).  
   
  c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 423‚Äì437, 2023. https://doi.org/10.1007/978-3-031-32726-1_30  
   
  424  
   
  R. Santiago et al.  
   
  a matroid M = (N, I),1 with elements of unknown weights w : N ‚Üí R‚â•0 that appear one-by-one in uniformly random order. Whenever an element appears, one has to immediately and irrevocably decide whether to select it, and the goal is to select a set of elements I ‚äÜ N that (i) is independent, i.e., I ‚àà I, and (ii) has weight w(I) = e‚ààI w(e) as large as possible. The key challenge in the area is to settle the notorious Matroid Secretary Problem (MSP) Conjecture: Conjecture 1 ( [1]). There is an O(1)-competitive algorithm for MSP. The best-known procedures for MSP are O(log log(rank(M)))-competitive [7,13], where rank(M) is the rank of the matroid M, i.e., the cardinality of a largest independent set. Whereas the MSP Conjecture remains open, extensive work in the Ô¨Åeld has led to constant-competitive algorithms for variants of the problem and restricted settings. This includes constant-competitive algorithms for speciÔ¨Åc classes of matroids [2,4,5,8,9,11,12,14,16]. Moreover, in terms of natural variations of the problem, Soto [16] showed that constant-competitiveness is achievable in the so-called Random-Assignment MSP, RA-MSP for short. Here, an adversary chooses |N | weights, which are then assigned uniformly at random to ground set elements N of the matroid. (Soto‚Äôs result was later extended by Oveis Gharan and Vondr¬¥ ak [15] to the setting where the arrival order of the elements is adversarial instead of uniformly random.) Constant-competitive algorithms also exist for the Free Order Model, where the algorithm can choose the order in which elements appear [9]. Intriguingly, a key aspect of prior advances on constant-competitive algorithms for special cases and variants of MSP is that they heavily rely on knowing the full matroid M upfront. This is also crucially exploited in Soto‚Äôs work on RA-MSP. In fact, if the matroid is not known upfront in full, there is no natural variant of MSP for which a constant-competitive algorithm is known. A high reliance on knowing the matroid M = (N, I) upfront (except for its size |N |) is undesirable when trying to approach the MSP Conjecture, because it is easy to obstruct an MSP instance by adding zero-weight elements. Not surprisingly, all prior advances on the general MSP conjecture, like the abovementioned O(log log(rank(M)))-competitive algorithms [7,13] and also earlier procedures [1,3], only need to know |N | upfront and make calls to an independence oracle on elements revealed so far. Thus, for RA-MSP, it was raised as an open question both in [16] and [15], whether a constant-competitive algorithm exists without knowing the matroid upfront. The key contribution of this work is to aÔ¨Érmatively answer this question, making the random assignment setting the Ô¨Årst MSP variant for which a constant-competitive algorithm is known without knowing the matroid and without any restriction on the underlying matroid.  
   
  1  
   
  A matroid M is a pair M = (N, I) where N is a Ô¨Ånite set and I ‚äÜ 2N is a nonempty family satisfying: 1) if A ‚äÜ B and B ‚àà I then A ‚àà I, and 2) if A, B ‚àà I and |B| > |A| then ‚àÉe ‚àà B \ A such that A ‚à™ {e} ‚àà I.  
   
  O(1)-Competitive Random Assignment MSP Without Knowing the Matroid  
   
  425  
   
  Theorem 1. There is a constant-competitive algorithm for RA-MSP with only the cardinality of the matroid known upfront. Moreover, our result holds in the more general adversarial order with a sample setting, where we are allowed to sample a random constant fraction of the elements and all remaining (non-sampled) elements arrive in adversarial order. As mentioned, when the matroid is fully known upfront, an O(1)-competitive algorithm was known for RA-MSP even when the arrival order of all elements is adversarial [15]. Interestingly, for this setting it is known that, without knowing the matroid upfront, no constant-competitive algorithm exists. More precisely, a lower bound on the competitiveness of Œ©(|N |/log log|N |) was shown in [15]. Organization of the Paper. We start in Sect. 2 with a brief discussion on the role of (matroid) densities in the context of random assignment models, as our algorithm heavily relies on densities. Decomposing the matroid into parts of diÔ¨Äerent densities has been central in prior advances on RA-MSP. However, this crucially relies on knowing the matroid upfront. We work with a rank-density curve, introduced in Sect. 3.1, which is also unknown upfront; however, we show that it can be learned approximately (in a well-deÔ¨Åned sense) by observing a random constant fraction of the elements. Section 3 provides an outline of our approach based on rank-density curves and presents the main ingredients which allow us to derive Theorem 1. Sect. 4 showcases the main technical tool that allows us to approximate the rank-density curve from a sample set. Finally, Sect. 5 discusses our main algorithmic contribution and a sketch of its analysis. We emphasize that we predominantly focus on providing a simple algorithm and analysis, refraining from optimizing the competitive ratio of our procedure at the cost of complicating the presentation. Moreover, due to space constraints, some proofs are deferred to the long version of this paper. We assume that all matroids are loopless, i.e., every element is independent by itself. This is without loss of generality, as loops can simply be ignored in matroid secretary problems.  
   
  2  
   
  Random-Assignment MSP and Densities  
   
  A main challenge in the design and analysis of MSP algorithms is how to protect heavier elements (or elements of an oÔ¨Ñine optimum) from being spanned by lighter ones that are selected earlier during the execution of the algorithm. In the random assignment setting, however, weights are assigned to elements uniformly at random, which allows for shifting the focus from protecting elements based on their weights to protecting elements based on their role in the matroid structure. Intuitively speaking, an element arriving in the future is at a higher risk of being spanned by the algorithm‚Äôs prior selection if it belongs to an area of the matroid with larger cardinality and smaller rank (‚Äúdenser‚Äù area) than an area with smaller cardinality and larger rank (‚Äúsparser‚Äù area).  
   
  426  
   
  R. Santiago et al.  
   
  This is formally captured by the notion of density: the density of a set U ‚äÜ N in a matroid M = (N, I) is |U |/r(U ), where r : 2N ‚Üí Z‚â•0 is the rank function of M.2 Densities play a crucial role in RA-MSP [15,16]. Indeed, prior approaches decomposed M into its principal sequence, which is the chain ‚àÖ  S1  . . .  Sk = N of sets of decreasing densities obtained as follows. S1 ‚äÜ N is the densest set of M (in case of ties it is the unique maximal densest set), S2 is the union of S1 and the densest set in the matroid obtained from M after contracting S1 , and so on until a set Sk is obtained with Sk = N . Figure 1a shows an example of the principal sequence of a graphic matroid.  
   
  Fig. 1. Figure 1a shows a graph representing a graphic matroid together with its principal sequence ‚àÖ  S1  ¬∑ ¬∑ ¬∑  S7 = N , where N are all edges of the graph. Figure 1b shows its rank-density curve. Each step in the rank-density curve (highlighted by a circle) corresponds to one Si and has y-coordinate equal to the density of Mi = (M/Si‚àí1 )|Si \Si‚àí1 and x-coordinate equal to r(Si ).  
   
  Previous approaches then considered, independently for each i ‚àà [k] := {1, . . . , k}, the matroid Mi := (M/Si‚àí1 )|Si \Si‚àí1 , i.e., the matroid obtained from M by contracting Si‚àí1 and then restricting to Si \ Si‚àí1 . (By convention, we set S0 := ‚àÖ.) These matroids are also known as the principal minors of M. Given an independent set in each principal minor, their union is guaranteed to be independent in the original matroid M. Prior approaches (see, in particular, [16] for details) then exploited the following two key properties of the principal minors Mi :  
   
  2  
   
  The rank function r : 2N ‚Üí Z‚â•0 assigns to any set U ‚äÜ N the cardinality of a maximum cardinality independent set in U , i.e., r(U ) := max{|I| : I ‚äÜ U, I ‚àà I}.  
   
  O(1)-Competitive Random Assignment MSP Without Knowing the Matroid  
   
  427  
   
  k  
   
  i=1 E[w(OPT(Mi ))] = Œ©(E[w(OPT(M))]), where OPT(M) (and analogously OPT(Mi )) is an (oÔ¨Ñine) maximum weight independent set in M and the expectation is over all random weight assignments. (ii) Each matroid Mi is uniformly dense, which means that the (unique maximal) densest set in Mi is the whole ground set of Mi .  
   
  (i)  
   
  Property (i) guarantees that, to obtain an O(1)-competitive procedure, it suÔ¨Éces to compare against the (oÔ¨Ñine) optima of the matroids Mi . Combining this with property (ii) implies that it suÔ¨Éces to design a constant-competitive algorithm for uniformly dense matroids. Since uniformly dense matroids behave in many ways very similarly to uniform matroids, which are a special case of uniformly dense matroids, it turns out that the latter admit a simple yet elegant O(1)-competitive algorithm. (See [16] for details.)  
   
  3  
   
  Outline of Our Approach  
   
  As discussed, prior approaches [15,16] for RA-MSP heavily rely on knowing the matroid upfront, as they need to construct its principal sequence upfront. A natural approach would be to observe a sample set S ‚äÜ N containing a constant fraction of all elements and then try to mimic the existing approaches using the principal sequence of M|S , the matroid M restricted to the elements in S. A main hurdle lies in how to analyze such a procedure as the principal sequence of M|S can diÔ¨Äer signiÔ¨Åcantly from the one of M. In particular, one can construct matroids where the density of some parts is likely to be underestimated by a super-constant factor. Moreover, generally M|S may have many diÔ¨Äerent densities not present in M (e.g., when M is uniformly dense). We overcome these issues by not dealing with principal sequences directly, but rather using what we call the rank-density curve of a matroid, which captures certain key parameters of the principal sequence. As we show, rank-density curves have three useful properties: (i) They provide a natural way to derive a quantity that both relates to the oÔ¨Ñine optimum and can be easily compared against to bound the competitiveness of our procedure. (ii) They can be learned approximately by observing an O(1)-fraction of N . (iii) Approximate rank-density curves can be used algorithmically to protect denser areas from sparser ones without having to know the matroid upfront. Section 3.1 introduces rank-density curves and shows how they conveniently allow for deriving a quantity that compares against the oÔ¨Ñine optimum. Section 3.2 then discusses our results on approximately learning rank-density curves and how this can be exploited algorithmically. 3.1  
   
  Rank-Density Curves  
   
  Given a matroid M = (N, I), one natural way to deÔ¨Åne its rank-density curve œÅM : R>0 ‚Üí R‚â•0 , is through its principal minors M1 , . . . , Mk , which are deÔ¨Åned  
   
  428  
   
  R. Santiago et al.  
   
  through the principal sequence ‚àÖ  S1  ¬∑ ¬∑ ¬∑  Sk = N as explained in Sect. 2. For a value t ‚àà (0, rank(M)], let it ‚àà [k] be the smallest index such that r(Sit ) > t. The value œÅM (t) is then given by the density of Mit . (See Fig. 1b for an example.) In addition, we set œÅM (t) = 0 for any t > rank(M). A formally equivalent way to deÔ¨Åne œÅM , which is more convenient for what we do later, is as follows. For any S ‚äÜ N and Œª ‚àà R‚â•0 , we deÔ¨Åne DM (S, Œª) ‚àà argmax {|U | ‚àí Œªr(U )} U ‚äÜS  
   
  (1)  
   
  to be the unique maximal maximizer of maxU ‚äÜS {|U | ‚àí Œªr(U )}. It is well-known that each set in the principal sequence S1 , . . . , Sk is nonempty and of the form DM (N, Œª) for Œª ‚àà R‚â•0 . This leads to the following way to deÔ¨Åne the rank-density curve, which is the one we use in what follows. Definition 1 (rank-density curve). Let M = (N, I) be a matroid. Its rankdensity curve œÅM : R>0 ‚Üí R‚â•0 is deÔ¨Åned by  max {Œª ‚àà R‚â•0 : r(DM (N, Œª)) ‚â• t} ‚àÄt ‚àà (0, rank(M)] œÅM (t) := 0 ‚àÄt > rank(M). When the matroid M is clear from context, we usually simply write œÅ instead of œÅM for its rank-density curve and D(N, Œª) instead of DM (N, Œª). Note that œÅ is piecewise constant, left-continuous, and non-increasing. (See Fig. 1b for an example.) If M is a uniformly dense matroid with density Œª, we have œÅ(t) = Œª for t ‚àà (0, rank(M)]. We now expand on how œÅM is related to the expected oÔ¨Ñine optimum value E[OPT(M)] of an RA-MSP instance. To this end, we use the function Œ∑ : [0, |N |] ‚Üí R‚â•0 deÔ¨Åned by   Œ∑(a) := ER‚àºUnif(N,a) max w(e) , (2) e‚ààR  
   
  where Unif(N, a ) is a uniformly random set of a many elements out of N ; and we set Œ∑(a) = 0 for a ‚àà [0, 1) (i.e., when the set R above is empty) by convention. In words, Œ∑(a) is the expected maximum weight out of a weights chosen uniformly at random from all the weights {we }e‚ààN . Based on this notion, we assign the following value F (œÅ) to a rank-density curve œÅ:  ‚àû F (œÅ) := Œ∑(œÅ(t))dt. (3) 0  
   
  Note that as the graph of œÅ is a staircase, the above integral is just a Ô¨Ånite sum. One can then show that the values of the oÔ¨Ñine optimum and F (œÅ) diÔ¨Äer by at most a constant factor ‚Äî proof deferred. Lemma 1. Let (M, w) be a random-assignment MSP instance. Let M be any matroid minor of M and let F be as deÔ¨Åned above. Then E[w(OPT(M ))] ‚â§ 3e  e‚àí1 ¬∑ F (œÅM ). Thus, to be constant-competitive, it suÔ¨Éces to provide an algorithm returning an independent set of expected weight Œ©(F (œÅ)).  
   
  O(1)-Competitive Random Assignment MSP Without Knowing the Matroid  
   
  429  
   
  RA-MSP Subinstances. We will often work with minors of the matroid that is originally given in our RA-MSP instance, and apply certain results to such minors instead of the original matroid. To avoid confusion, we Ô¨Åx throughout the paper one RA-MSP instance with matroid Morig = (Norig , Iorig ) and unknown but (adversarially) Ô¨Åxed weights w : Norig ‚Üí R‚â•0 , and our goal is to design an O(1)-competitive algorithm for this instance. The weights w of the original instance are the only weights we consider, even when working with RA-MSP subinstances on minors of Morig , as their elements also obtain their weights uniformly at random from w. In particular, the function F as deÔ¨Åned in (3) is always deÔ¨Åned with respect to the original weights w. Many of our statements hold not just for minors of M but any matroid with weights uniformly drawn from w. For simplicity, we typically also state these results for minors of M. For a matroid M = (N, I) with |N | ‚â§ |Norig |, we denote by (M, w) the RA-MSP instance on the matroid M obtained by assigning a uniformly random subset of |N | weights among the weights w uniformly at random to the elements in N . Our subinstances will be of this type (with M being a minor of Morig ). Even though there may be more weights than elements, such instances (M, w) can indeed be interpreted as RA-MSP instances, as they correspond to the adversary Ô¨Årst choosing uniformly at random a subset of |N | weights among the weights in w, which then get assigned uniformly at random to the elements. 3.2  
   
  Proof Plan for Theorem 1 via Rank-Density Curves  
   
  We now expand on how one can learn an approximation œÅÀú of the rank-density curve œÅMorig and how this can be exploited algorithmically to return an independent set of expected weight Œ©(F (œÅMorig )), which by Lemma 1 implies O(1)competitiveness of the procedure. To this end, we start by formalizing the notion of an approximate rank-density curve, which relies on the notion of downshift. Definition 2. Let œÅ : R>0 ‚Üí R‚â•0 be a non-increasing function and let Œ±, Œ≤ ‚àà R‚â•1 . The (Œ±, Œ≤)-downshift œÅ : R>0 ‚Üí R‚â•0 of œÅ is deÔ¨Åned via an auxiliary function œÜ : R>0 ‚Üí R‚â•0 as follows:  œÅ(Œ±)  ‚àÄt ‚àà (0, 1], 1 if œÜ(t) ‚àà (0, 1), Œ≤ œÜ(t) := œÅ(Œ±¬∑t) œÅ (t) := œÜ(t) otherwise . ‚àÄt > 1; Œ≤ Moreover, a function œÅÀú : R>0 ‚Üí R‚â•0 is called an (Œ±, Œ≤)-approximation of œÅ if it is non-increasing and œÅ ‚â§ œÅÀú ‚â§ œÅ, where œÅ is the (Œ±, Œ≤)-downshift of œÅ. The reason we round up values in (0, 1) in the above deÔ¨Ånition of downshift, is that while we deÔ¨Åne the latter for a more general type of curves, throughout the paper we mainly use it with rank-density curves, and density values are always at least one. One issue when working with an (O(1), O(1))-approximation œÅÀú of œÅ is that F (Àú œÅ) may be more than a constant factor smaller than F (œÅ) and we thus cannot compare against F (Àú œÅ) to obtain an O(1)-competitive procedure. However, as the  
   
  430  
   
  R. Santiago et al.  
   
  following lemma shows, also in this case we can obtain a simple lower bound for the value F (Àú œÅ) in terms of F (œÅ) and the largest weight wmax in w. Lemma 2. Let M be a matroid minor of Morig , let Œ±, Œ≤ ‚àà R‚â•1 , and let œÅÀú be œÅ) + Œ±wmax . an (Œ±, Œ≤)-approximation of œÅM . Then F (œÅM ) ‚â§ 2Œ±Œ≤F (Àú Proof. By the deÔ¨Ånition of an (Œ±, Œ≤)-approximate curve we have    ‚àû  ‚àû  Œ± 1 1 Œ∑(Àú œÅ(t))dt ‚â• Œ∑(œÅ(t))dt = Œ∑(œÅ(t))dt F (Àú œÅ) = F (œÅ) ‚àí 2Œ±Œ≤ Œ± 2Œ±Œ≤ 0 0  1  ‚â• F (œÅ) ‚àí Œ±wmax , 2Œ±Œ≤ where the Ô¨Årst inequality follows since œÅÀú is an (Œ±, Œ≤)-approximation of œÅM and from properties of Œ∑ (proof deferred), and the last inequality holds by deÔ¨Ånition of Œ∑. A key implication of Lemma 2 is that it suÔ¨Éces to obtain an algorithm that returns an independent set of expected weight Œ©(F (Àú œÅ)) for some (O(1), O(1))œÅ) = Œ©(F (œÅMorig ))‚àí approximation œÅÀú of œÅMorig . Indeed, Lemma 2 then implies F (Àú O(wmax ). By running this algorithm with some probability (say 0.5) and otherwise Dynkin‚Äôs [6] classical secretary algorithm, which picks the heaviest element with constant probability, an overall algorithm is obtained that returns an independent set of expected weight Œ©(F (œÅMorig )). Hence, Lemma 2 helps to provide bounds on the competitiveness of algorithms that are competitive with the F -value of an approximate rank-density curve. This technique is also used in the following key statement, which shows that an algorithm with strong guarantees can be obtained if we are given an (O(1), O(1))-approximation of the rank-density curve of the matroid on which we work ‚Äî see Sect. 5. Theorem 2. Let M be a matroid minor of Morig , and let œÅM denote the rankdensity curve of M. Assume we are given an (Œ±, Œ≤)-approximation œÅÀú of œÅM for integers Œ± ‚â• 24 and Œ≤ ‚â• 3. Then there is an eÔ¨Écient procedure ALG(Àú œÅ, Œ±, Œ≤) that, when run on the RA-MSP subinstance given by M, returns an independent  
   
  1 F (œÅM ) ‚àí Œ±2 wmax . set I of M of expected weight at least 1440eŒ± 2 Œ≤2 The last main ingredient of our approach is to show that such an accurate proxy œÅÀú can be computed with constant probability. More precisely, we show that, after observing a sample set S containing every element of Norig independently with probability 1/2, the rank-density curve of (the observed) Morig |S ‚Äì is close to the rank-density curve of Morig |Norig \S , allowing us to use œÅ Morig |S as desired proxy for the RA-MSP subinstance given by Morig |Norig \S , and ‚Äì is close to the rank-density curve of Morig , which allows for relating the oÔ¨Ñine optimum of the RA-MSP subinstance given by Morig |Norig \S to the one of Morig . We highlight that the next result is purely structural and hence independent of weights or the MSP setting. See Sect. 4 for details.  
   
  O(1)-Competitive Random Assignment MSP Without Knowing the Matroid  
   
  431  
   
  Theorem 3. Let M = (N, I) be a matroid and S ‚äÜ N be a random set containing every element of N independently with probability 1/2. Then, with probability at least 1/100, œÅ M|S and œÅ M|N \S are both (288, 9)-approximations of œÅM . Combining the above results, we get the desired O(1)-competitive algorithm. Proof of Theorem 1. For brevity, let M := Morig and N := Norig throughout this proof. Recall that by Lemma 1, it suÔ¨Éces to provide an algorithm returning an independent set of expected weight Œ©(F (œÅM )). Consider the following procedure: First observe (without picking any element) a set S ‚äÜ N containing every element of N independently with probability 1/2 and let œÅÀú denote the (288, 9)downshift of œÅ M|S . Then run the algorithm described in Theorem 2 on M|N \S with œÅÀú as the approximate rank-density curve. Let I denote the output of the above procedure and let A be the event deÔ¨Åned in Theorem 3, that is, A = {S ‚äÜ N : œÅ M|S and œÅ M|N \S are (288, 9)-approximations of œÅM }. Then, observe that for any Ô¨Åxed S ‚àà A, we have  

  1 F œÅ M|N \S ‚àí 2884 wmax E[w(I) | S] ‚â• 1440e¬∑288 4 ¬∑94 wmax 1 ‚â• 2880e¬∑288 F (œÅM ) ‚àí 720e¬∑9 5 ¬∑95 3, where the Ô¨Årst inequality follows from Theorem 2 and that, for every S ‚àà A, the curve œÅÀú is a (2882 , 92 )-approximation of œÅ M|N \S ,3 while the second inequality follows from Lemma 2 and the fact that for every S ‚àà A the curve œÅ M|N \S is a (288, 9)-approximation of œÅM . Moreover, the Ô¨Årst inequality uses that conditioning on any Ô¨Åxed S ‚àà A does not have any impact on the uniform assignment of the weights w to the elements. This holds because the event A only depends on the sampled elements S but not the weights of its elements. Hence, the RA-MSP subinstance given by M|N \S on which we use the algorithm described in Theorem 2, indeed assigns weights of w uniformly at random to elements, as required. It then follows that the output of the above procedure satisÔ¨Åes  
   
  wmax 1 1 E[w(I) | S] Pr[S] ‚â• 100 E[w(I)] ‚â• 2880e¬∑2885 ¬∑95 F (œÅM ) ‚àí 720e¬∑93 , S‚ààA  
   
  where the last inequality uses that Pr[A] ‚â• 1/100 by Theorem 3. Since running the classical secretary algorithm on Morig returns an independent set of expected weight at least wmax/e, the desired result now follows by running the procedure described above with probability 1/2, and running the classical secretary algorithm otherwise. 3  
   
  œÅÀú is a (2882 , 92 )-approximation of œÅ M|N \S because œÅÀú is the (288, 9)-downshift of œÅ M|S , and, both œÅ M|N \S and œÅ M|S are (288, 9)-approximations of œÅM . First, this implies that œÅ M|N \S lies above œÅÀú. Second, the approximation parameter (2882 , 92 ) can be derived by observing that the (Œ±2 , Œ≤2 )-downshift of the (Œ±1 , Œ≤1 )-downshift of some rank-density function is an (Œ±1 Œ±2 , Œ≤1 Œ≤2 )-approximation of that rank-density function ‚Äî proof deferred.  
   
  432  
   
  4  
   
  R. Santiago et al.  
   
  Learning Rank-Density Curves from a Sample  
   
  One of the main challenges when designing and analyzing algorithms for MSP is understanding what kind of (and how much) information can be learned about the underlying instance after observing a random sample of it. In this section, we discuss the main ingredient to show that, with constant probability, after observing a sample set S one can learn a good approximation of the rank-density curve of both M and M|N \S ‚Äî see Theorem 3. However, even if one knew the exact (instead of an approximate) rank-density curve of M|N \S , given that the matroid is not known upfront (and hence neither which elements are associated to each of the diÔ¨Äerent density areas of the curve), it is a priori not clear how to proceed. A second main contribution of this section is to show that the set of elements in N \ S that are spanned by a subset of S of a given density is well-structured. In particular, this will allow us to build k a (chain) decomposition M of M| i i=1 N \S where all the Mi ‚Äôs satisfy some desired properties with constant probability ‚Äî see Sect. 5.1 for details. The main technical contribution in this section is the following result. Theorem 4. Let M = (N, I) be a matroid containing 3h disjoint bases for some h ‚àà Z‚â•1 . Let S ‚àº B(N, 1/2). Then   
   
   | |N | ‚â§ exp ‚àí 144 , (4) Pr |span(D(S, h)) \ S| ‚â§ |N 12  Pr r(D(S, h)) ‚â§  
   
  r(N ) 8  
   
    

  ) ‚â§ exp ‚àí r(N . 48  
   
  (5)  
   
  Proof. We prove (4) and defer the proof of (5) to the full version. Let Mh = (N, Ih ) denote the h-fold union of M and let rh denote its rank function. Consider the procedure described in Algorithm 1, which is loosely inspired by [10]. Algorithm 1: Algorithm for lower bounding ES [|span(D(S, h)) \ S|] Set W ‚Üê ‚àÖ, G ‚Üê ‚àÖ, and C ‚Üê ‚àÖ for every e ‚àà N considered in an arbitrary order do if W ‚à™ {e} ‚àà Ih then if e ‚àà S then Update W ‚Üê W ‚à™ {e} else if e ‚àà S then Update C ‚Üê C ‚à™ {e} else Update G ‚Üê G ‚à™ {e}  
   
  Note that the following three properties hold at all times: W , G, and C are pairwise disjoint; W ‚äÜ S and C ‚äÜ S, while G ‚à© S = ‚àÖ; and W ‚àà Ih . In addition, by construction, at the end of the procedure we have: (i) S = C W . Moreover, the random sets G and C have identical distributions, because each element belongs to S with probability 1/2 independently.  
   
  O(1)-Competitive Random Assignment MSP Without Knowing the Matroid  
   
  433  
   
  (ii) G ‚äÜ span(D(S, h)) \ S. Because G ‚à© S = ‚àÖ, it is enough to show G ‚äÜ span(D(S, h)). Given an arbitrary e ‚àà G, by construction we have W ‚à™{e} ‚àà / Ih , i.e., rh (W ‚à™ {e}) = rh (W ). As W ‚äÜ S, this yields rh (S ‚à™ {e}) = rh (S), which then implies (proof deferred) e ‚àà span(D(S, h)). As G ‚äÜ span(D(S, h)) \ S, and G and C have the same distribution, we get       | | | ‚â§ Pr |G| ‚â§ |N = Pr |C| ‚â§ |N (6) Pr |span(D(S, h) \ S)| ‚â§ |N 12 12 12 . Moreover, |C| = |S| ‚àí |W | ‚â• |S| ‚àí hr(N ) ‚â• |S| ‚àí |N |/3,  
   
  (7)  
   
  where the equality follows from S = C  W , the Ô¨Årst inequality from W ‚àà Ih (which implies |W | = rh (W ) ‚â§ hr(N )), and the last one from the fact that M contains 3h many disjoint bases (and hence |N | ‚â• 3hr(N )). Combining (6) and (7) we obtain       | | ‚â§ Pr |S| ‚àí |N3 | ‚â§ |N ‚â§ Pr |S| ‚â§ 56 E[|S|] , Pr |span(D(S, h) \ S)| ‚â§ |N 12 12 (8) where the second inequality follows from E[|S|] = |N |/2. Relation (4) now follows by applying a ChernoÔ¨Ä bound Pr[X < (1 ‚àí Œ¥)E[X]] < exp[ ‚àí Œ¥2 E[X]/2] for X = |S| to the right-hand side expression in (8) and using E[|S|] = |N |/2. The proof of Theorem 3 is based on the concentration result (5). In summary, rather than directly showing that œÅ M|S approximates œÅM well everywhere, we consider a discrete set of points on œÅM associated to minors of M of geometrically increasing ranks. We then apply (5) to these minors and employ a union bound to show that we get a good approximation for these grid points. The union bound works out because the ranks are geometrically increasing and appear in the exponent of the right-hand side of (5). (Complete proof is deferred.)  
   
  5  
   
  The Main Algorithm and Its Analysis  
   
  In this section we describe the procedure from Theorem 2 and discuss the two main ingredients of its analysis. The Ô¨Årst one is to show that if the approximate curve œÅÀú is well-structured (in some well-deÔ¨Åned sense), then there is an algorithm retrieving a constant factor of F (Àú œÅ) on expectation ‚Äî see Theorem 5. The second one is then to show that given any initial approximate curve œÅÀú, one can Ô¨Ånd wellstructured curves whose F function value is close to F (Àú œÅ) ‚Äî see Theorem 6. The next result, whose proof is sketched in Sect. 5.1, formalizes the Ô¨Årst step above. Theorem 5. Let M = (N, I) be a matroid minor of Morig , and let r and œÅM denote the rank function and rank-density curve of M, respectively. Let œÅ ‚â§ œÅM be a rank-density curve with densities {Œªi }i‚àà[m] such that the Œªi are powers of some integer Œ≤ ‚â• 3 and Œª1 > ¬∑ ¬∑ ¬∑ > Œªm ‚â• 1. Assume r(D(N, Œªi+1 )) ‚â• œÅ, Œ≤) 24r(D(N, Œªi/Œ≤ )) for i ‚àà [m ‚àí 1]. Then there is an eÔ¨Écient procedure ALG(Àú that, when run on the RA-MSP subinstance given by M, returns an independent set I of M of expected weight at least (1/180e)F (œÅ).  
   
  434  
   
  R. Santiago et al.  
   
  The second main ingredient in the proof of Theorem 2 is the following result. Due to space constraints, we defer its proof to the long version. Theorem 6. Let M = (N, I) be a matroid minor of Morig , and let r and œÅM denote the rank function and rank-density curve of M, respectively. Given an (Œ±, Œ≤)-approximate curve œÅÀú of œÅM with Œ± ‚àà R‚â•24 and Œ≤ ‚àà Z‚â•3 , there is a procedure ALG(Àú œÅ, Œ±, Œ≤) returning rank-density curves œÅ, œÅ1 , œÅ2 , œÅ3 , œÅ4 such that: 2 2 (i) œÅ is an (Œ± , Œ≤ )-approximation of œÅM . (ii) i‚àà[4] F (œÅi ) ‚â• F (œÅ). (iii) For each i ‚àà [4], œÅi satisÔ¨Åes the following properties: Let {Œºj }j‚àà[] be the densities of œÅi , then all the Œºj are powers of Œ≤ ‚â• 3, and r(D(N, Œºj+1 )) ‚â• Œ±r(D(N, Œºj/Œ≤ )) ‚â• 24r(D(N, Œºj/Œ≤ )) for j ‚àà [ ‚àí 1]. Moreover, œÅi ‚â§ œÅM .  
   
  We now show how Theorem 5 and Theorem 6 combined imply Theorem 2. Proof of Theorem 2. Given an (Œ±, Œ≤)-approximation œÅÀú of œÅM , Ô¨Årst run the procedure from Theorem 6 to get curves œÅ, œÅ1 , œÅ2 , œÅ3 , œÅ4 . Then choose an index i ‚àà [4] uniformly at random and run the procedure from Theorem 5 on œÅi to get an independent set with expected weight at least  4    1 1 1 1 F (œÅ) ‚â• F (œÅM ) ‚àí Œ±2 wmax , F (œÅi ) ‚â• 2 2 180e 4 i=1 720e 1440eŒ± Œ≤ where the last inequality uses Lemma 2 and the fact that œÅ is an (Œ±2 , Œ≤ 2 ) approximation of œÅM . Thus, to show Theorem 2, it remains to prove Theorem 5. 5.1  
   
  Proof (Sketch) of Theorem 5  
   
  Throughout this section we use the notation and assumptions from Theorem 5. We prove the theorem in two steps.First, we argue that after observing k a sample set S, we can build a chain i=1 Mi of M|N \S satisfying certain properties with at least constant probability. Then we argue that, given such a chain, there is a procedure returning an independent set I of M with E[w(I)] = Œ©(F (œÅ)), leading to the desired result. We start by discussing the former claim. Given a sample set S ‚äÜ N , we build a chain of matroids as follows. For i ‚àà [m] let Ni := span(D(S, Œªi/Œ≤ )) \ (S ‚à™ span(D(S, Œªi‚àí1/Œ≤ ))), and Mi := (M/span(D(S, Œªi‚àí1/Œ≤ )))|Ni ,  
   
  (9)  
   
  where D(S, Œª0/Œ≤ ) = ‚àÖ by convention. In addition, for every i ‚àà [m] let N i := D(N, Œªi ), and deÔ¨Åne Œõ := {i ‚àà [m] : r(N i ) ‚â• 24, Œªi ‚â• Œ≤}. Note that Œõ and the N i ‚Äôs do not depend on the  
   
  O(1)-Competitive Random Assignment MSP Without Knowing the Matroid  
   
  435  
   
  sample set S. Moreover, from the assumptions of Theorem 5 it follows that Œõ ‚äá [m] \ {1, m}. The next result shows that with constant probability, the sample set S is such that for each i ‚àà Œõ, the set Ni contains a subset Ui of large rank and density; more precisely, r(Ui ) ‚â• Œ©(r(N i )) and |Ui |/r(Ui ) ‚â• Œ©(Œªi ). Lemma 3. Let S ‚àº B(N, 1/2), and let Ni , N i , and Œõ be as deÔ¨Åned above. Then, , every Ni with i ‚àà Œõ contains Œªi disjoint independent with probability at least 1/3 sets I1 , . . . , IŒªi such that j‚àà[Œªi ] |Ij | ‚â• (1/24)Œªi r(N i ). The second main ingredient in the proof is to show that the above result can be exploited algorithmically. More precisely, we prove the following. Lemma 4. Let M = (N, I) be a matroid minor of Morig containing h disjoint h independent sets I1 , . . . , Ih such that s := (1/h) j=1 |Ij | ‚â• 1. Then there is a procedure that, when run on the RA-MSP subinstance given by M, and with only h given upfront, returns an independent set of M with expected weight at least (s/2e)Œ∑(h). This is still the case even if the elements of M are revealed in adversarial (rather than uniformly random) order. We can now combine Lemmas 3 and 4 to prove Theorem 5 as follows. Proof of Theorem 5. Let OSP(M, h) denote the online selection procedure described in Lemma 4. Additionally, m for i ‚àà [m], let ri denote the coeÔ¨Écient of Œ∑(Œªi ) in F (œÅ). Hence, F (œÅ) = i=1 ri Œ∑(Œªi ). Consider the following algorithm: choose and execute one of the three branches presented below with probability 12/15, 2/15, and 1/15, respectively. k (i) Observe S ‚àº B(N, 1/2), construct the chain i=1 Mi as deÔ¨Åned in (9), and run OSP(Mi , Œªi ) for every i ‚àà [m] (independently in parallel), returning all the picked elements. (ii) Run the classical secretary algorithm on M without observing anything and return the picked element (if any). (iii) Run OSP(M, 1) without observing anything and return all picked elements. Suppose we execute branch (i). By Lemma 3, with probability at least 1/3, every Mi with i ‚àà Œõ satisÔ¨Åes the conditions of Lemma 4 with parameters h = Œªi and s = (1/24)r(N i ). Note that s ‚â• 1 holds given that r(N i ) ‚â• 24 for all i ‚àà Œõ. As additionally all matroids in the chain form a direct sum, executing the Ô¨Årst branch of the algorithm returns an independent set with expected weight at least 1 1 r(N i ) 1 1 ¬∑ Œ∑(Œªi ) = r(N i )Œ∑(Œªi ) ‚â• ri Œ∑(Œªi ), 3 2e 24 144e 144e i‚ààŒõ  
   
  i‚ààŒõ  
   
  i‚ààŒõ  
   
  where the inequality follows from œÅ ‚â§ œÅM and N i = D(N, Œªi ) for every i ‚àà [m]. Therefore, if i ‚àà Œõ, then the corresponding term ri Œ∑(Œªi ) in F (œÅ) is accounted for by branch (i). Thus it only remains to consider i ‚àà [m] \ Œõ ‚äÜ {1, m}. Assume Ô¨Årst that 1 ‚àà / Œõ. In this case, we must have r(N 1 ) < 24. Since the expected weight yielded by running the classical secretary algorithm is at least  
   
  436  
   
  R. Santiago et al.  
   
  and Œ∑(|N |) ‚â• Œ∑(Œª1 ), then by running branch (ii) the expected weight of the output set is at least  
   
  Œ∑(|N |)/e,  
   
  1 r(N 1 )Œ∑(Œª1 ) 1 Œ∑(|N |) ‚â• ¬∑ r1 Œ∑(Œª1 ), ‚â• e e 23e r(N 1 ) where the last inequality follows from r1 ‚â§ r(N 1 ) ‚â§ 23. Finally, assume that m ‚àà / Œõ. Then Œªm = 1, in which case running branch (iii) yields E[w(OSP(M, 1))] ‚â•  
   
  1 1 r(N )Œ∑(1) ‚â• r(N m )Œ∑(Œªm ), 2e 2e  
   
  where the Ô¨Årst inequality holds by Lemma 4 with h = 1 and s = r(N ) ‚â• 1, as any basis of M is an independent set of rank r(N ), and the second inequality holds because r(N ) ‚â• r(N m ) and Œªm = 1. The desired lower bound on the expected weight of the set returned by the algorithm now follows by combining the above results with the respective probabilities that each branch is executed. To sum up, we discuss that our main result (i.e., Theorem 1) still holds in the more general adversarial order with a sample setting, where we are allowed to sample a set S ‚äÜ N containing every element of N independently with probability 1/2, and the remaining (non-sampled) elements arrive in adversarial order. In order to see this, Ô¨Årst note that the only place in the proof of Theorem 5 where we use that the non-sampled elements (i.e., N \ S) arrive in random order, is to argue that when running the classical secretary algorithm in branch (ii) we obtain an expected weight of at least wmax/e. Indeed, branches (i) and (iii) rely on running the procedure from Lemma 4, whose guarantees hold in the case where the elements arrive in adversarial order. However, note that running the classical secretary procedure in the above adversarial order with a sample setting outputs an element with expected weight of at least wmax/4. Indeed, the probability of selecting wmax in the latter setting is at least the probability of the event that wmax is not sampled and the second largest weight is; which occurs with probability 1/4. Thus, Theorem 5 holds (up to possibly a slightly worse constant) in the adversarial order with a sample setting. Next, observe that this implies that Theorem 2 also holds in the above setting (again, up to possibly a slightly worse constant). This follows because its proof relies on combining the procedures from Theorems 5 and 6, and the latter is completely oblivious to the arrival order of the elements. Finally, note that the proof of Theorem 1 uses the procedure from Theorem 5 and the classical secretary algorithm. Because (as discussed above) both of these algorithms have very similar guarantees in the adversarial order with a sample setting to the ones shown in this paper for random order, the claim follows.  
   
  O(1)-Competitive Random Assignment MSP Without Knowing the Matroid  
   
  437  
   
  References 1. BabaioÔ¨Ä, M., Immorlica, N., Kleinberg, R.: Matroids, secretary problems, and online mechanisms. In: Symposium on Discrete Algorithms (SODA 2007), pp. 434‚Äì 443 (2007) 2. BabaioÔ¨Ä, M., et al.: Matroid secretary problems. J. ACM 65(6), 1‚Äì26 (2018) 3. Chakraborty, S., Lachish, O.: Improved competitive ratio for the matroid secretary problem. In: Proceedings of the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 1702‚Äì1712 (2012) 4. Dimitrov, N.B., Plaxton, C.G.: Competitive weighted matching in transversal matroids. Algorithmica 62(1), 333‚Äì348 (2012) 5. Dinitz, M., Kortsarz, G.: Matroid secretary for regular and decomposable matroids. In: Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 108‚Äì117 (2013) 6. Evgenii Borisovich Dynkin: The optimum choice of the instant for stopping a Markov process. Soviet Math. 4, 627‚Äì629 (1963) 7. Feldman, M., Svensson, O., Zenklusen, R.: A simple O(log log(rank))-competitive algorithm for the matroid secretary problem. Math. Oper. Res. 43(2), 638‚Äì650 (2018) 8. Im, S., Wang, Y.: Secretary problems: laminar matroid and interval scheduling. In: Proceedings of the 22nd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 1265‚Äì1274 (2011) 9. Jaillet, P., Soto, J.A., Zenklusen, R.: Advances on matroid secretary problems: free order model and laminar case. In: Goemans, M., Correa, J. (eds.) IPCO 2013. LNCS, vol. 7801, pp. 254‚Äì265. Springer, Heidelberg (2013). https://doi.org/10. 1007/978-3-642-36694-9 22 10. Karger, D.: Random sampling and greedy sparsiÔ¨Åcation for matroid optimization problems. Math. Program. 82 (1998). https://doi.org/10.1007/BF01585865 11. Kesselheim, T., et al.: An optimal online algorithm forweighted bipartite matching and extensions to combinatorial auctions. In: Proceedings of the 21st Annual European Symposium on Algorithms (ESA), pp. 589‚Äì600 (2013) 12. Korula, N., P¬¥ al, M.: Algorithms for secretary problems on graphs and hypergraphs. In: Proceedings of the 36th International Colloquium on Automata, Languages and Programming (ICALP), pp. 508‚Äì520 (2009) 13. Lachish, O.: O(log log(rank)) competitive ratio for the matroid secretary problem. In: IEEE 55th Annual Symposium on Foundations of Computer Science. IEEE 2014, pp. 326‚Äì335 (2014) 14. Ma, T., Tang, B., Wang, Y.: The simulated greedy algorithm for several submodular matroid secretary problems. In: Proceedings of the 30th International Symposium on Theoretical Aspects of Computer Science (STACS), pp. 478‚Äì489 (2013) 15. Gharan, S.O., Vondr¬¥ ak, J.: On variants of the matroid secretary problem. Algorithmica 67(4), 472‚Äì497 (2013) 16. Soto, J.A.: Matroid secretary problem in the random-assignment model. SIAM J. Comput. 42(1), 178‚Äì211 (2013)  
   
  A Fast Combinatorial Algorithm for the Bilevel Knapsack Problem with Interdiction Constraints Noah Weninger(B)  
   
  and Ricardo Fukasawa  
   
  University of Waterloo, Waterloo, ON, Canada {nweninger,rfukasawa}@uwaterloo.ca Abstract. We consider the bilevel knapsack problem with interdiction constraints, a fundamental bilevel integer programming problem which generalizes the 0-1 knapsack problem. In this problem, there are two knapsacks and n items. The objective is to select some items to pack into the first knapsack such that the maximum profit attainable from packing some of the remaining items into the second knapsack is minimized. We present a combinatorial branch-and-bound algorithm which outperforms the current state-of-the-art solution method in computational experiments by 4.5 times on average for all instances reported in the literature. On many of the harder instances, our algorithm is hundreds of times faster, and we solved 53 of the 72 previously unsolved instances. Our result relies fundamentally on a new dynamic programming algorithm which computes very strong lower bounds. This dynamic program solves a relaxation of the problem from bilevel to 2n-level where the items are processed in an online fashion. The relaxation is easier to solve but approximates the original problem surprisingly well in practice. We believe that this same technique may be useful for other interdiction problems. Keywords: Bilevel programming ¬∑ Interdiction ¬∑ Knapsack problem ¬∑ Combinatorial algorithm ¬∑ Dynamic programming ¬∑ Branch and bound  
   
  1  
   
  Introduction  
   
  Bilevel integer programming (BIP), a generalization of integer programming (IP) to two-round two-player games, has been increasingly studied due to its wide real-world applicability [5,12,17]. In the BIP model, there are two IPs, called the upper level (or leader ) and lower level (or follower ), which share some variables between them. The objective is to optimize the upper level IP but with the constraint that the shared variables must be optimal for the lower level IP. The term interdiction is used to describe bilevel problems in which the upper level IP has the capability to block access to some resources used by the lower level IP. The upper level is typically interested in blocking resources in a way that produces the worst possible outcome for the lower level IP. For instance, the resources may be nodes or edges in a graph, or items to be packed into a knapsack. These problems often arise in military defense settings (e.g., see [17]). c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 438‚Äì452, 2023. https://doi.org/10.1007/978-3-031-32726-1_31  
   
  A Fast Combinatorial Algorithm for BKP  
   
  439  
   
  In this paper we study the bilevel knapsack problem with interdiction constraints (BKP), which was introduced by DeNegre in 2011 [6]. This problem is a natural extension of the 0-1 knapsack problem (KP) to the bilevel setting. Formally, we are given n items. Each item i ‚àà {1, . . . , n} has an associated proÔ¨Åt pi ‚àà Z>0 , upper-level weight wiU ‚àà Z>0 and lower-level weight wiL ‚àà Z‚â•0 . The upper-level knapsack has capacity C U ‚àà Z‚â•0 and the lower-level knapsack has capacity C L‚àà Z‚â•0 . We use the standard notation: for a vector x and set S we let x(S) := i‚ààS xi . The problem BKP can then be stated as follows: min  
   
  max p(Y )  
   
  X‚ààU Y ‚ààL(X)  
   
    where U = X ‚äÜ {1, . . . , n} : wU (X) ‚â§ C U ,   and L(X) = Y ‚äÜ {1, . . . , n} \ X : wL (Y ) ‚â§ C L .  
   
  (objective) (upper level) (lower level)  
   
  We call a solution (X, Y ) feasible if X ‚àà U and Y ‚àà argmax{p(YÀÜ ) : YÀÜ ‚àà L(X)}. A solution (X, Y ) is optimal if it minimizes p(Y ) over all feasible solutions. Note that determining whether (X, Y ) is feasible is weakly NP-Hard. Given that ‚Äúthe knapsack problem is believed to be one of the ‚Äòeasier‚Äô NPhard problems,‚Äù [16] one may propose that BKP may also be one of the ‚Äòeasier‚Äô Œ£p2 -hard problems. While this may indeed be the case, unlike KP, which admits a pseudopolynomial time algorithm, BKP remains NP-complete when the input is described in unary and thus has no pseudopolynomial time algorithm unless P = NP [1]. In addition, BKP is a Œ£p2 -complete problem, which means it cannot even be modelled as an IP with polynomially many variables and constraints, unless the polynomial hierarchy collapses. A recent positive theoretical result for BKP is a polynomial-time approximation scheme [3]. This theoretical hardness seemed to have been conÔ¨Årmed by the struggle of computational approaches to solve small instances. Until recently, proposed algorithms ‚Äì either generic BIP algorithms [6,8,19] or more speciÔ¨Åc algorithms for BKP (or slight generalizations of it) [2,9,13] ‚Äì were only able to solve instances with at most 55 items. A breakthrough result came in a paper by Della Croce and Scatamacchia [4], that proposed a BKP-speciÔ¨Åc algorithm (henceforth referred to as DCS ) which was able to solve instances containing up to 500 items. It is worth noting that all papers prior to DCS only consider instances which were generated in an uncorrelated fashion, meaning that weights and proÔ¨Åts were chosen uniformly at random with no correlation between the values. The DCS algorithm is able to solve uncorrelated instances with 500 items in less than a minute, but its performance drops signiÔ¨Åcantly even for weakly correlated instances, and most strongly correlated instances remain unsolved after an hour of computing time. These results seem to mimic what is known for KP: uncorrelated KP instances are some of the easiest types of instances to solve [16] and early KP algorithms such as expknap [15] could quickly solve uncorrelated instances but struggled with strongly correlated ones. A common aspect among all methods in the literature is that they rely fundamentally on MIP solvers. In this paper, we present a simple combinatorial branch-and-bound algorithm for solving BKP. Our algorithm improves on the  
   
  440  
   
  N. Weninger and R. Fukasawa  
   
  performance of the DCS algorithm for 94% of instances, even achieving a speedup of orders of magnitude in many cases. Furthermore, our algorithm appears to be largely impervious to correlation: it solves strongly correlated instances with ease, only signiÔ¨Åcantly slowing down when the lower-level weights equal the profits (i.e., the subset sum case). In Sect. 2, we describe our algorithm. Our algorithm relies fundamentally on a new strong lower bound computed by dynamic programming which we present in Sect. 3. Section 4 details our computational experiments. We conclude in Sect. 5 with some directions for future research. We note that some proofs were omitted for brevity.  
   
  2  
   
  A Combinatorial Algorithm for BKP  
   
  In this section we describe our exact solution method for BKP. At a high level, the algorithm is essentially just standard depth-Ô¨Årst branch-and-bound. Our strong lower bound, deÔ¨Åned later in Sect. 3, is essential for reducing the search space. To begin formalizing this, we Ô¨Årst deÔ¨Åne the notion of a subproblem. Definition 1. A subproblem (X, i) consists of some i ‚àà {1, . . . , n + 1} and set of items X ‚äÜ {1, . . . i ‚àí 1} such that X ‚àà U. Note that this deÔ¨Ånition depends on the ordering of the items, which throughout the paper we assume to be such that wp1L ‚â• wp2L ‚â• ¬∑ ¬∑ ¬∑ ‚â• wpnL with ties broken by n 1 2 placing items with larger pi Ô¨Årst. These subproblems will form the nodes of the branch-and-bound tree; (‚àÖ, 1) is the root node, and for every X ‚àà U, (X, n+1) is a leaf. Every non-leaf subproblem (X, i) has the child (X, i+1), which represents omitting item i from the upper-level solution. Non-leaf subproblems (X, i) with X ‚à™ {i} ‚àà U have an additional child (X ‚à™ {i}, i + 1) which represents including item i in the upper-level solution. The algorithm simply starts at the root and traverses the subproblems in a depth-Ô¨Årst manner, preferring the child (X ‚à™ {i}, i + 1) if it exists because it is more likely to lead to a good solution. Every time the search reaches a leaf (X, n + 1), we solve the knapsack problem max{p(Y ) : Y ‚àà L(X)} to get a feasible solution, and updating the incumbent if appropriate. 2.1  
   
  The Bound Test  
   
  At each node (X, i) of the branch-and-bound, we Ô¨Ånd a lower bound on the optimal value of that subproblem by the use of a bound test algorithm, which tests lower bounds against a known incumbent solution value z ‚àó . The lower bound used to prune a subproblem is computed in three steps: (1) we solve a knapsack problem on items {1, . . . , i ‚àí 1} \ X, (2) we compute a lower bound for BKP restricted to items {i, . . . , n}, and (3) we combine (1) and (2) into a lower bound for the descendants of (X, i). ¬Ø c), which, for a given X ¬Ø ‚äÜ {1, . . . , n} For step (1), we deÔ¨Åne a function K(X, and c ‚â• 0, returns the optimal value of the knapsack problem with weights wL , ¬Ø cannot be used: proÔ¨Åts p, and capacity c, under the restriction that items in X  
   
  A Fast Combinatorial Algorithm for BKP  
   
  441  
   
    ¬Ø c) = max p(Y ) : Y ‚äÜ {1, . . . , n} \ X ¬Ø and wL (Y ) ‚â§ c . K(X, For step (2), we need a function œâ(i, cU , cL ) which is a lower bound on BKP but with upper-level capacity cU , lower-level capacity cL , and restricted to items {i, . . . , n}. So, formally, œâ must satisfy œâ(i, cU , cL ) ‚â§ min{K(X  ‚à™ {1, . . . , i ‚àí 1}, cL ) : X  ‚äÜ {i, . . . , n}, wU (X  ) ‚â§ cU }. We will deÔ¨Åne precisely what œâ is in Sect. 3; for now, we only need to know that it has this property. We now prove the following lemma, which describes how to achieve step (3). Lemma 1. Let (X, i) be a subproblem. For all c ‚àà {0, . . . , C L },   K (X ‚à™ {i, . . . , n}, c) + œâ i, C U ‚àí wU (X), C L ‚àí c   ¬Ø Y¬Ø ) is feasible for BKP and X ¬Ø ‚à© {1, . . . , i ‚àí 1} = X . ‚â§ min p(Y¬Ø ) : (X, Proof. First, note that for any X  ‚äÜ {i, . . . , n}, K(X ‚à™ {i, . . . , n}, c) + K(X  ‚à™ {1, . . . , i ‚àí 1}, C L ‚àí c) ‚â§ K(X ‚à™ X  , C L ). Thus, if we let œá := {X  ‚äÜ {i, . . . , n} : wU (X  ) ‚â§ C U ‚àí wU (X)} and take the minimum with respect to œá we get K(X ‚à™{i, . . . , n}, c)+œâ(i, C U ‚àíwU (X), C L ‚àíc) ‚â§ min{K(X ‚à™X  , C L ) : X  ‚àà œá }. and now just note that this last term is equal to   ¬Ø Y¬Ø ) is feasible for BKP and X ¬Ø ‚à© {1, . . . , i ‚àí 1} = X . min p(Y¬Ø ) : (X,  
   
  From this, it follows that for any c ‚àà {0, . . . , C L }, if we have   K (X ‚à™ {i, . . . , n}, c) + œâ i, C U ‚àí wU (X), C L ‚àí c ‚â• z ‚àó then we can prune subproblem (X, i). We also note that the lower bound still remains valid if we replace K (X ‚à™ {i, . . . , n}, c) by a feasible solution to that problem. This is what is done in the function BoundTest in Algorithm 1 whose correctness is established by the following lemma. Lemma 2. If BoundTest(X, i) returns true, then subproblem (X, i) can be pruned. We end with an important consideration regarding the eÔ¨Écient implementation of Algorithm 1. The greedy part of the bound test (Lines 2 to 4) appears to require time O(n). However, considering how we choose to branch, the values wg and pg can be computed in time O(1) given their values for the parent subproblem. To determine K(X ‚à™ {i, . . . , n}, c), we use the standard dynamic program (DP) for knapsack. However, for each bound test, it is only necessary to compute a single row of a DP table (i.e., Ô¨Åll in all C L capacity values for the row associated with item i) from the row computed in the parent subproblem. By doing this, the entire BoundTest function will run in time O(C L ). Furthermore, when the branch-and-bound reaches a leaf, the knapsack solution needed to update the upper bound will already have been found by the bound test.  
   
  442  
   
  N. Weninger and R. Fukasawa  
   
  Algorithm 1: Returns true if the subproblem (X, i) can be pruned. 1 2 3 4 5 6 7  
   
  8  
   
  2.2  
   
  Precondition: z ‚àó is the value of the best incumbent solution function BoundTest(X, i) wg , pg ‚Üê 0; for j = 1, . . . , i ‚àí 1 do if j ‚àà / X and wg + wjL ‚â§ C L then wg ‚Üê wg + wjL , pg ‚Üê pg + pj ; if pg + œâ(i, C U ‚àí wU (X), C L ‚àí wg ) ‚â• z ‚àó then return true; for c = 0, . . . , C L do if K(X ‚à™ {i, . . . , n}, c) + œâ(i, C U ‚àí wU (X), C L ‚àí c) ‚â• z ‚àó then return true; return false;  
   
  Computing Initial Bounds  
   
  In our algorithm, a strong initial upper bound z ‚àó can help decrease the size of the search tree. For this we use a simple heuristic we call GreedyHeuristic. ¬Ø is picked by GreedyHeuristic works in two steps. First, an upper level set X ¬Ø solving  max {p(X) : X ‚àà U}. Then the lower level solution Y is picked by solving ¬Ø . We say GreedyHeuristic returns (X, ¬Ø Y¬Ø , z¬Ø), where z¬Ø max p(Y ) : Y ‚àà L(X) ¬Ø ¬Ø is the objective value of the solution (X, Y ). We now establish a case in which GreedyHeuristic actually returns an optimal solution. Lemma 3. GreedyHeuristic() returns an optimal solution if there exists an optimal solution (X, Y ) for BKP where Y = {1, . . . , n} \ X. The proof is skipped for brevity, but we remark that previous work has noted that BKP is very easily solved for such instances [2,4]. Following [4], we use an LP to detect some cases where the GreedyHeuristic is optimal. The below formulation LB(i) is a simpliÔ¨Åed version of the LP in [4]. LB(i) = min such that  
   
  i‚àí1  
   
  j=1  
   
  pj (1 ‚àí xj )  
   
  i‚àí1  
   
  wjU xj ‚â§ C U i‚àí1 C ‚àí wiL + 1 ‚â§ j=1 wjL (1 ‚àí xj ) ‚â§ C L j=1 L  
   
  0 ‚â§ x ‚â§ 1, x ‚àà Ri‚àí1 We deÔ¨Åne LB(i) = ‚àû if the LP is infeasible. This LP is used by the following lemma. We skip the proof for brevity. ¬Ø Y¬Ø , z¬Ø). If z¬Ø ‚â§ min{LB(c) : Lemma 4. Suppose GreedyHeuristic() returns (X, ¬Ø ¬Ø 1 ‚â§ c ‚â§ n} then (X, Y ) is optimal for BKP. Before starting our branch-and-bound algorithm, we run GreedyHeuristic and check if it is optimal using Lemma 4. This enables us to quickly solve trivial instances without needing to run our main algorithm.  
   
  A Fast Combinatorial Algorithm for BKP  
   
  3  
   
  443  
   
  Lower Bound  
   
  In this section we deÔ¨Åne the lower bound œâ(i, cU , cL ) that we use in our algorithm. Recall that œâ(i, cU , cL ) must lower bound the restriction of BKP where we can only use items {i, . . . , n}, have upper-level capacity cU , and lower-level capacity cL . Our lower bound is based on dynamic programming (DP), which computes œâ(i, cU , cL ) for all parameter values with time and space complexity O(nC U C L ). The main idea for the lower bound is to obtain good feasible solutions for the lower-level problem. Perhaps the most obvious way is to assume that the lower-level problem Ô¨Ånds a greedy solution. It is not hard to see why this is a lower bound: a greedy lower-level solution will always achieve proÔ¨Åt at most that of an optimal lower-level solution. We can compute this lower bound with the following recursively-deÔ¨Åned DP algorithm: œâg (i, cU , cL ) = ‚éß ‚àû ‚é™ ‚é™ ‚é™ ‚é™ ‚é™ ‚é™ ‚é™‚àí‚àû ‚é™ ‚é™ ‚é™ ‚é®0  
   
  if cU < 0, if cL < 0, if cU ‚â• 0, cL ‚â• 0 and i > n,  
   
  ‚é™ if cU ‚â• 0, wiL > cL and i ‚â§ n. œâg (i + 1, cU , cL ) ‚é™ ‚é™ ‚é™ ‚é™ ‚é™ ‚é™ œâg (i + 1, cU ‚àí wiU , cL ), ‚é™ ‚é™ if cU ‚â• 0, wiL ‚â§ cL and i ‚â§ n. ‚é™ ‚é©min œâ (i + 1, cU , cL ‚àí wL ) + p g i i The Ô¨Årst three expressions are to take care of trivial cases. The fourth case skips any item which cannot Ô¨Åt in the lower-level knapsack, as it would be pointless for the upper level to take such an item. The Ô¨Åfth case picks the worse (i.e., better for the upper level) out of the two possible greedy solutions from the two children nodes of subproblem (X, i). This lower bound already has very good performance in practice. However, we can do better by making a deceptively simple modiÔ¨Åcation: giving the lower level the option to ignore an item. This modiÔ¨Åcation produces our strong DP lower bound, œâ, which is equal to œâg in the Ô¨Årst three cases, but instead of the last two cases we get: ‚éß ‚é´ œâ(i + 1, cU ‚àí wiU , cL ), ‚é™ ‚é™ ‚é® ‚é¨ if cU ‚â• 0, cL ‚â• 0 œâ(i, cU , cL ) = min œâ(i + 1, cU , cL ‚àí wiL ) + pi , ‚é™ ‚é™ ‚é© max ‚é≠ and i ‚â§ n. œâ(i + 1, cU , cL ) It is not a hard exercise to show that œâg (i, cU , cL ) ‚â§ œâ(i, cU , cL ) for all 1 ‚â§ i ‚â§ n, 0 ‚â§ cU ‚â§ C U and 0 ‚â§ cL ‚â§ C L . Extrapolating our intuition about œâg , formulation œâ appears to actually Ô¨Ånd optimal lower-level solutions, so one might guess that œâ(1, C U , C L ) is actually optimal for BKP, if it weren‚Äôt that this is impossible unless P = NP [1]. The subtlety is that by giving the lower level a choice of whether to take an item, we have also given the upper level the  
   
  444  
   
  N. Weninger and R. Fukasawa  
   
  power to react to that choice. SpeciÔ¨Åcally, the upper level choice of whether to take item i can depend on how much capacity the lower level has used on items {1, . . . , i ‚àí 1}. Evidently, this is not permitted by the deÔ¨Ånition of BKP, which dictates that the upper level solution is completely decided prior to choosing the lower level solution. However, our experiments show that this actually gives the upper level an extremely small amount of additional power in practice. The lower bound œâ may also be interpreted as a relaxation from a 2-round game to a 2n-round game. This may seem to be making the problem more diÔ¨Écult, but each round is greatly simpliÔ¨Åed, so the problem becomes easier to solve. This 2n-round game is as follows. In round 2i ‚àí 1, the leader (the upper level player) decides whether to include the item i. In round 2i, the follower (the lower level player) responds to the leader‚Äôs decision: if item i is still available, then the follower decides whether to include item i. The score of the game is simply the total proÔ¨Åt of all items chosen by the follower. It is straightforward to see that the minimax value of this game (i.e., the score given that both players follow an optimal strategy) is equal to œâ(1, C U , C L ). We now show formally that œâ(1, C U , C L ) lower bounds the optimal objective value of BKP. To this end we deÔ¨Åne œâX , a modiÔ¨Åed version of œâ where instead of the minimization in the case where cU ‚â• 0, cL ‚â• 0 and i ‚â§ n, the choice is made depending on whether i ‚àà X for some given set X. œâX (i, cU , cL ) is equal to œâg in the Ô¨Årst three cases, but replaces the last two cases with: œâX (i, cU , cL ) = ‚éß if cU ‚â• 0, cL ‚â• 0, i ‚â§ n and i ‚àà X, œâ (i + 1, cU ‚àí wiU , cL ) ‚é™ ‚é® X œâX (i + 1, cU , cL ‚àí wiL ) + pi , ‚é™ / X. if cU ‚â• 0, cL ‚â• 0, i ‚â§ n and i ‚àà ‚é©max œâX (i + 1, cU , cL ) With this simple modiÔ¨Åcation, we claim that œâX (1, C U , C L ) = max{p(Y ) : Y ‚àà L(X)} (and similarly for other i, cU , and cL ). To formalize this, we show that œâX and K (as deÔ¨Åned in Sect. 2.1) are equivalent in the following sense. Lemma 5. For all 1 ‚â§ i ‚â§ n, X ‚äÜ {i, . . . , n}, cU ‚â• wU (X) and cL ‚â• 0, œâX (i, cU , cL ) = K(X ‚à™ {1, . . . , i ‚àí 1}, cL ). Proof. Given that cU ‚â• wU (X), the case cU < 0 can not occur in the expansion of œâX (i, cU , cL ), so œâX (i, cU , cL ) = œâX (i, ‚àû, cL ). Consider the 0-1 knapsack problem with proÔ¨Åts p and weights w formed by taking p = p and w = wL except with pj = wj = 0 for items j ‚àà X. We can then simplify the deÔ¨Ånition of œâX (i, ‚àû, cL ) by using p and w to eÔ¨Äectively skip items in X: ‚éß ‚àí‚àû if cL < 0, ‚é™ ‚é™ ‚é™ ‚é™ ‚é®0 if cL ‚â• 0 and i > n, œâX (i, ‚àû, cL ) = L   ‚é™ ‚é™ ‚é™max œâX (i + 1, ‚àû, c ‚àí wi ) + pi , ‚é™ if cL ‚â• 0 and i ‚â§ n. ‚é© œâX (i + 1, ‚àû, cL )  
   
  A Fast Combinatorial Algorithm for BKP  
   
  445  
   
  The recursive deÔ¨Ånition of œâX (i, ‚àû, cL ) above describes the standard DP algorithm for 0-1 knapsack with capacity cL , proÔ¨Åts p and weights w but restricted to items {i, . . . , n}; this is the same problem which is solved by  
   
  K(X ‚à™ {1, . . . , i ‚àí 1}, cL ). We now establish that œâ(i, cU , cL ) is a lower bound as desired. Theorem 1. For all 1 ‚â§ i ‚â§ n, cU ‚â• 0 and cL ‚â• 0, œâ(i, cU , cL ) ‚â§  
   
  min  
   
  X‚äÜ{i,...,n} : wU (X)‚â§cU  
   
  K(X ‚à™ {1, . . . , i ‚àí 1}, cL ).  
   
  Proof. By deÔ¨Ånition, œâX (i, cU , cL ) = ‚àû if wU (X) > cU , so min  
   
  X‚äÜ{i,...,n}  
   
  œâX (i, cU , cL ) = =  
   
  min  
   
  œâX (i, cU , cL )  
   
  min  
   
  K(X ‚à™ {1, . . . , i ‚àí 1}, cL ).  
   
  X‚äÜ{i,...,n} : wU (X)‚â§cU X‚äÜ{i,...,n} : wU (X)‚â§cU  
   
  where the last equality follows from Lemma 5. Therefore, it suÔ¨Éces to show that œâ(i, cU , cL ) ‚â§ minX‚äÜ{i,...,n} œâX (i, cU , cL ). The proof is by induction on i from n + 1 to 1. Let cU ‚â• 0 and cL ‚â• 0 be arbitrary. Our inductive hypothesis is that œâ(i, cU , cL ) ‚â§ minX‚äÜ{i,...,n} œâX (i, cU , cL ). For the base case, where i = n + 1, by deÔ¨Ånition we have œâ(i, cU , cL ) = œâX (i, cU , cL ) = 0 for any X ‚äÜ {i, . . . , n} = ‚àÖ. Now we prove the inductive case. Let 1 ‚â§ i ‚â§ n be arbitrary and assume that the inductive hypothesis holds for i + 1, with every cU ‚â• 0 and cL ‚â• 0. We present only the case where wiU ‚â§ cU and wiL ‚â§ cL . The remaining cases (where wiU > cU or wiL > cL ) are just simpler versions of this. œâ(i + 1, cU ‚àí wiU , cL ), U L œâ(i, c , c ) = min   max œâ(i + 1, cU , cL ‚àí wiL ) + pi , œâ(i + 1, cU , cL ) ‚é´ ‚éß min œâX (i + 1, cU ‚àí wiU , cL ), ‚é™ ‚é™ ‚é™ ‚é™ X‚äÜ{i+1,...,n} ‚é™ ‚é™ ‚é™ ‚éß ‚é´‚é™ ‚é¨ ‚é® U L L ‚é™ min œâX (i + 1, c , c ‚àí wi ) + pi ,‚é™ ‚é® X‚äÜ{i+1,...,n} ‚é¨ ‚â§ min ‚é™ ‚é™ ‚é™ ‚é™ ‚é™ ‚é™ max ‚é™ ‚é™ U L ‚é™ ‚é™ ‚é© min œâX (i + 1, c , c ) ‚é© ‚é≠‚é≠ ‚éß ‚é™ ‚é™ ‚é®  
   
  X‚äÜ{i+1,...,n}  
   
  ‚é´ œâX (i + 1, cU ‚àí wiU , cL ), ‚é™ ‚é™ ‚é¨ U L L ‚â§ min œâX (i + 1, c , c ‚àí wi ) + pi , ‚é™ ‚é™ ‚é™ ‚é™ min max ‚é© ‚é≠ X‚äÜ{i+1,...,n} œâX (i + 1, cU , cL ) ‚éß ‚é´ œâ (i + 1, cU ‚àí wiU , cL ), ‚é™ ‚é™ ‚é® X ‚é¨ U L L = min min œâX (i + 1, c , c ‚àí wi ) + pi , ‚é™ ‚é™ X‚äÜ{i+1,...,n} ‚é© max ‚é≠ œâX (i + 1, cU , cL ) =  
   
  min  
   
  min  
   
  X‚äÜ{i+1,...,n}  
   
  X‚äÜ{i,...,n}  
   
  œâX (i, cU , cL ).  

  446  
   
  N. Weninger and R. Fukasawa  
   
  Note that in particular, this implies that œâ(1, C U , C L ) ‚â§ minX‚ààU K(X, C L ) = minX‚ààU maxY ‚ààL(X) p(Y ), i.e., œâ(1, C U , C L ) is a lower bound for BKP. We end this section with a simple observation. The approach we derived for our problem was based on obtaining good feasible solutions to the lower problem. Now, if the lower problem is already NP-hard, one may ask how useful can an approximate solution to the lower level be. For this, we consider a very generic problem: (1) z ‚àó := min max c(x, y) x‚ààU y‚ààL(x)  
   
  For each x ‚àà U, assume there exists y ‚àà L(x) that maximizes the inner problem. Let y ‚àó (x) be such a maximizer of c(x, y) for y ‚àà L(x). The following lemma then shows that if we can solve the problem with an approximate lower level, instead of an exact one, we get an approximate solution to (1). Lemma 6. Suppose we have a function f (x) such that for all x ‚àà U: ‚Äì f (x) ‚àà L(x), and ‚Äì c(x, f (x)) ‚â§ c(x, y ‚àó (x)) ‚â§ Œ±c(x, f (x)), for some Œ± ‚â• 1. x, y ‚àó (Àú x)) ‚â§ Œ±z ‚àó . Let x Àú ‚àà arg min c(x, f (x)). Then c(Àú x‚ààU  
   
  Proof. Let (x‚àó , y ‚àó (x‚àó )) be the optimal solution to (1). Then 1  
   
  c(Àú x, y ‚àó (Àú x)) ‚â§ c(Àú x, f (Àú x)) ‚â§ c(x‚àó , f (x‚àó )) ‚â§ c(x‚àó , y ‚àó (x‚àó )) = z ‚àó . Œ± While this does not immediately give an approximation algorithm for the problem, we believe it may be useful to simplify some Œ£p2 -hard bilevel interdiction problems and, for that reason, we include this lemma in this work. Note that an analogous result can be also derived for a max ‚àí min problem.  
   
  4  
   
  Computational Results  
   
  In this section, we perform computational experiments to compare our algorithm (Comb) with the method from [4] (DCS ). Given that the superiority of the DCS algorithm over other approaches has been well demonstrated we do not compare our algorithm directly to the prior works [2,6,8,9,18,19]. 4.1  
   
  Implementation  
   
  We were unable to obtain either source code or a binary from the authors of [4], so we reimplemented their algorithm. We use Gurobi 9.5 instead of CPLEX 12.9, and obviously run it on a diÔ¨Äerent machine, so an exact replication of their results is nearly impossible. Nonetheless, we found our reimplementation produces results very similar to what is reported in [4], and even solves three additional instances which were not solved in [4]. Therefore, we believe that any comparison with our version of the DCS algorithm is reasonably fair.  
   
  A Fast Combinatorial Algorithm for BKP  
   
  447  
   
  Both Comb and DCS were run using 16 threads. However, not all parts of the algorithms were parallelized. SpeciÔ¨Åcally, in the DCS implementation, the only part which is parallelized is the MIP solver. In the implementation of Comb, we only parallelized two parts: the computation of the lower bound œâ and the computation of the initial lower bound min{LB(c) : 1 ‚â§ c ‚â§ n}. Our code is implemented in C++ and relies on OpenMP 4.5 for parallelism, Gurobi 9.5 for solving MIPs, and the implementation of the combo knapsack algorithm [14] from [11]. The code was executed on a Linux machine with four 16-core Intel Xeon Gold 6142 CPUs @ 2.60 GHz and 256 GB of RAM. All code and instances are available at https://github.com/nwoeanhinnogaehr/bkpsolver. 4.2  
   
  Instances  
   
  Our test set contains all instances described in the literature [2,4,6,10,19] and 1660 new instances which we generated. The Ô¨Årst 1500 were generated as follows. For each n ‚àà {10, 25, 50, 102 , 103 , 104 }, INS ‚àà {0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5} and R ‚àà {10, 25, 50, 100, 1000}, we generated Ô¨Åve instances according to Ô¨Åve diÔ¨Äerent methods, which we call classes 1-5. All weights and proÔ¨Åts were selected uniformly at random in the range [1, R], but for some of the Ô¨Åve classes, we equated wL , wU or p with each other: wL , wU and p are independent (uncorrelated) wL = p but wU is independent (lower subset-sum) wU = p but wL is independent (upper subset-sum) wL = wU = p (both subset-sum) wL = wU but p is independent (equal weights)    The capacities are chosen as follows. Let C L = INS/11 ¬∑ i wiL and choose C U uniformly at random in the range [C L ‚àí 10, C L + 10]. If there is any item with wiL < C L or wiU < C U , then we increase the appropriate capacity so that this is not the case. This is the same way that the capacities are selected in [2,4,10], except that we exclude instances that would almost certainly be solved by the initial bound test and we include half integral values of INS. Note that the easiest and hardest instances reported in the literature were uncorrelated and lower subset-sum, respectively [4]. Hence, we expect these new instances to capture both best-case and worst-case behavior from the solvers. The remaining 160 instances were intended to test the case where the capacity is very large but the number of items is small. These instances were generated following the same scheme as above except that we only generated uncorrelated instances, and we chose n ‚àà {5, 10, 20, 30} and R ‚àà {103 , 104 , 105 , 106 }. To the best of our knowledge, instances with such large capacity have not been evaluated previously. 1. 2. 3. 4. 5.  
   
  4.3  
   
  Results  
   
  Our results on instances from the literature are summarized in Table 1. To best match the test environment used for the original DCS implementation, we ran  
   
  448  
   
  N. Weninger and R. Fukasawa Table 1. Results for instances from the literature, grouped by instance type.  
   
  Group  
   
  Num DCS Opt Best Avg  
   
  uncorrelated 940 50 weak correlated 50 strong correlated 50 inverse strong corr. 50 almost strong corr. 50 subset-sum 50 even-odd subset-sum even-odd strong corr. 50 similar weight uncorr. 50  
   
  940 66 50 0 41 0 38 0 40 0 35 0 36 0 41 0 50 0  
   
  2.32 13.49 689.58 919.91 815.4 1,087.18 1,033.98 747.12 22.89  
   
  Max  
   
  Comb Opt Best Avg  
   
  15.73 940 874 72.64 50 50 3,600 50 50 3,600 50 50 3,600 50 50 3,600 42 42 3,600 42 42 3,600 50 50 79.85 50 50  
   
  0.31 0.26 0.34 1.07 0.24 586.29 581.42 0.61 0.05  
   
  Max 6.48 3.54 3.98 34.69 3.16 3,600 3,600 17.21 0.08  
   
  the tests with a time limit of 1 h, and used the same parameters for the DCS algorithm as reported by the authors [4]. For each instance group and each solver, we reported the number of instances solved to optimality (column Opt), the number of instances on which the solver took strictly less time than the other solver (column Best), the average wall-clock running time in seconds (column Avg) and the maximum wall-clock running time in seconds (column Max). Note that measuring wall-clock time as opposed to CPU time only disadvantages our algorithm, if anything, because the DCS implementation utilizes all 16 threads for a large proportion of the time due to parallelization within Gurobi, whereas the same is not true for our algorithm. Overall, our solver had better performance on 1258 of the 1340 instances (about 94%), achieving about 4.5 times better performance on average, and solving 53 of the 69 instances which our DCS implementation did not (the original DCS implementation did not solve 72 instances). These results demonstrate the remarkable advantage that Comb has on hard instances. DCS struggles with all instances involving strong or subset-sum correlation, but Comb only signiÔ¨Åcantly slows down for subset-sum instances. In Fig. 1, we plot a performance proÔ¨Åle for instances from the literature comparing the DCS algorithm to some variants of our algorithm. This type of graph plots, for each instance, the ratio of each algorithm‚Äôs performance to the performance of the best algorithm for that instance. The instances are sorted by diÔ¨Éculty. Note that instances which timed out are counted as 3600 s seconds. For a comprehensive introduction to performance proÔ¨Åles, see [7]. The two variants of Comb included are Comb-weak, which uses the lower bound œâg instead of œâ, and Comb-greedy, which uses a greedy lower bound test, i.e., where Lines 6 to 7 are omitted from Algorithm 1. The graph indicates that while Comb does better with more threads and the main variant is best, the single-threaded version and the variants still outperform 16-thread DCS. Although it is not depicted in the performance proÔ¨Åle, we also tested variants with diÔ¨Äerent item orderings, and found the one described in Sect. 2 to be the best. This is somewhat expected as this is the same ordering that gives rise to the greedy algorithm for 0-1 knapsack.  
   
  A Fast Combinatorial Algorithm for BKP  
   
  449  
   
  100  
   
  Percent of instances  
   
  80  
   
  60 Comb (1 thread) Comb (4 threads) Comb (16 threads)  
   
  40  
   
  Comb-weak (16 threads) 20  
   
  0  
   
  Comb-greedy (16-threads) DCS (16 threads) 21  
   
  23  
   
  25 27 29 211 Number of times worse than best solver  
   
  213  
   
  215  
   
  Fig. 1. Performance profile for all instances from the literature.  
   
  We now turn our attention to the new instances. Due to the large number of new instances and high diÔ¨Éculty, we used a reduced time limit of 15 min (900 s) in order to complete the testing in a timely fashion. For these tests we use the same DCS parameters used by the DCS authors for testing their own instances [4]. The results for the new instances are summarized in Tables 2 and 3. Note that there are 300 instances of each group, but Table 2 only instances for which our test machine had enough memory to store the DP table used in Comb. The performance of DCS is reported on the remaining instances in Table 3. We can see that Comb oÔ¨Äers a signiÔ¨Åcant speed improvement for all classes, and although both solvers are roughly equally capable of solving instances in the uncorrelated, upper subset-sum, equal weights, and large capacity classes, Comb solved 122 more of the instances in the lower subset-sum and both subset-sum classes than DCS. Extrapolating from the results in Table 2, we suspect that given suÔ¨Écient memory, our solver would be able to solve many more of these instances with better performance than DCS. In Tables 5 and 6, we report some statistics collected during the tests of our algorithm. In these tables, RootTime is the average number of (wall clock) seconds required to perform the initial bound test and compute the DP table, OptTime is the average number of seconds that branch-and-bound takes to Ô¨Ånd an optimal solution (excluding RootTime), ProofTime is the average number of seconds needed to prove optimality after an optimal solution is found, Nodes is the average number of nodes searched by branch-and-bound, and RootGap% z where z¬Ø is the value of the solution returned by is 100 ¬∑ (¬Ø z ‚àí œâ(1, C U , C L ))/¬Ø GreedyHeuristic. These tables only consider instances which Ô¨Åt in memory and did not time out, as some of the columns are undeÔ¨Åned otherwise. Considering all  
   
  450  
   
  N. Weninger and R. Fukasawa Table 2. Summary of results for new instances, grouped by class.  
   
  Group  
   
  DCS Num Opt Best Avg  
   
  Max  
   
  Comb Opt Best Avg  
   
  Max  
   
  uncorrelated lower subset-sum upper subset-sum both subset-sum equal weights large capacity  
   
  243 256 235 235 236 92  
   
  900 900 89.25 900 120.55 900  
   
  243 236 235 189 236 92  
   
  24.02 900 21.08 900 18.38 22.72  
   
  241 173 235 130 236 90  
   
  3 0 5 0 9 1  
   
  12.42 320.36 2.7 423.83 2.2 38.78  
   
  Table 3. Summary of results for DCS on new instances which our solver could not fit in memory, grouped by class. Group  
   
  Num Opt Avg  
   
  uncorrelated  
   
  57  
   
  240 236 230 189 227 91  
   
  0.96 73.62 0.79 184.9 0.7 2.31  
   
  Table 4. An instance with n items, C U = n‚àí1 and C L = n that has optimal objective value n ‚àí 1 but œâ(1, C U , C L ) = 1.  
   
  Max  
   
  41  
   
  487.85 900  
   
  item no. p  
   
  wU wL  
   
  lower subset-sum 44  
   
  1  
   
  886.75 900  
   
  upper subset-sum 65  
   
  59  
   
  283.34 900  
   
  1 . ..  
   
  1 . ..  
   
  1 . ..  
   
  1 . ..  
   
  n-1  
   
  1  
   
  1  
   
  1  
   
  n  
   
  n-1 n-1 n  
   
  both subset-sum  
   
  65  
   
  0  
   
  equal weights  
   
  64  
   
  62  
   
  175.92 900  
   
  large capacity  
   
  68  
   
  5  
   
  864.33 900  
   
  900  
   
  900  
   
  instances which Ô¨Åt in memory, the average root gap is 3.22%, and the maximum is 57.89%. Evidently, the root gap is typically very small and in fact œâ(1, C U , C L ) ‚â• 0.5 OPT in all tested instances. However, the worst case approximation factor of œâ is actually unbounded. Table 4 describes a family of instances with n items for which OPT = (n‚àí1)œâ(1, C U , C L ). Despite this, branch-and-bound is able to solve these instances using only O(n) nodes. On the other hand, it is interesting to note that the subset-sum instances typically have very small root gaps, but solving them to optimality is evidently very hard. Evidently, the main disadvantage of our algorithm is its high memory usage. In our solver, we use a few simple tricks to reduce memory usage slightly: when possible, we store the DP table entries as 16-bit integers, and we avoid computing table entries for capacity values which cannot be seen in any feasible solution. Other optimizations to reduce memory usage are certainly possible as well, such as a DP-with-lists type approach, but we have not implemented this.  
   
  A Fast Combinatorial Algorithm for BKP  
   
  451  
   
  Table 5. Statistics from our solver, for instances from the literature. Group  
   
  RootTime OptTime ProofTime NumNodes  
   
  RootGap%  
   
  uncorrelated  
   
  0.3  
   
  0.03  
   
  0.02  
   
  12,053.23  
   
  5.4  
   
  weak correlated  
   
  0.25  
   
  0.04  
   
  0.03  
   
  7,130.98  
   
  2.04  
   
  strong correlated  
   
  0.26  
   
  0.03  
   
  0.11  
   
  571,473.6  
   
  2.98  
   
  inverse strong corr.  
   
  0.37  
   
  0.04  
   
  0.73  
   
  4,751,888.08  
   
  0.39  
   
  almost strong corr.  
   
  0.24  
   
  0.03  
   
  0.03  
   
  735.76  
   
  2.99  
   
  subset-sum  
   
  0.08  
   
  0.05  
   
  12.22  
   
  even-odd subset-sum  
   
  0.07  
   
  0.04  
   
  6.42  
   
  61,246,957.64  
   
  0.02  
   
  even-odd strong corr.  
   
  0.26  
   
  0.03  
   
  0.37  
   
  2,551,213.42  
   
  2.95  
   
  similar weight uncorr. 0.05  
   
  0.05  
   
  0.05  
   
  0  
   
  0  
   
  104,067,718.79 0.02  
   
  Table 6. Statistics from our solver, for new instances.  
   
  5  
   
  Group  
   
  RootTime OptTime ProofTime Nodes  
   
  RootGap%  
   
  uncorrelated lower subset-sum upper subset-sum both subset-sum equal weights large capacity  
   
  0.86 0.95 0.79 0.16 0.7 2.31  
   
  9.86 1.27 5.75 3.44 1.64 10.15  
   
  0.27 0.8 0 3.02 0.02 0  
   
  0.17 3.43 0 7.68 0.02 0  
   
  73,616.37 11,293,085.12 182.99 117,664,085.82 14.65 29.82  
   
  Conclusion  
   
  We presented a new combinatorial algorithm for solving BKP that is on average 4.5 times better, and achieves up to 3 orders of magnitude improvement in runtime over the performance of the previous state-of-the-art algorithm, DCS. The only disadvantage of our algorithm that we identiÔ¨Åed in computational testing is the high memory usage. Because of this, if memory is limited and time is not a concern, it may be a better idea to use DCS. However, if there is any correlation between the lower-level weights and proÔ¨Åts, DCS is unlikely to solve the instance in any reasonable amount of time, so it is preferable to use our algorithm on a machine with a large amount of memory, and/or to use additional implementation tricks to reduce the memory usage. For future work, it would be of interest to investigate whether our lower bound can be strengthened further (say, to an O(1)-approximation). We expect that it would be straightforward to generalize this work to the multidimensional variant of BKP (i.e., where there are multiple knapsack constrains at each level), although the issues with high memory usage would likely become worse in this setting. It may also be straightforward to apply this technique to covering interdiction problems. Beyond this, we suspect that a similar lower bound and search algorithm can be used to eÔ¨Éciently solve a variety of interdiction problems.  
   
  452  
   
  N. Weninger and R. Fukasawa  
   
  References 1. Caprara, A., Carvalho, M., Lodi, A., Woeginger, G.J.: A study on the computational complexity of the bilevel knapsack problem. SIAM J. Optim. 24(2), 823‚Äì838 (2014) 2. Caprara, A., Carvalho, M., Lodi, A., Woeginger, G.J.: Bilevel knapsack with interdiction constraints. INFORMS J. Comput. 28(2), 319‚Äì333 (2016) 3. Chen, L., Wu, X., Zhang, G.: Approximation algorithms for interdiction problem with packing constraints. arXiv preprint arXiv:2204.11106 (2022) 4. Della Croce, F., Scatamacchia, R.: An exact approach for the bilevel knapsack problem with interdiction constraints and extensions. Math. Program. 183(1), 249‚Äì 281 (2020) 5. Dempe, S.: Bilevel optimization: theory, algorithms, applications and a bibliography. In: Dempe, S., Zemkoho, A. (eds.) Bilevel Optimization. SOIA, vol. 161, pp. 581‚Äì672. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-52119-6 20 6. DeNegre, S.: Interdiction and discrete bilevel linear programming, Ph. D. thesis, Lehigh University (2011) 7. Dolan, E.D., Mor¬¥e, J.J.: Benchmarking optimization software with performance profiles. Math. Program. 91(2), 201‚Äì213 (2002) 8. Fischetti, M., Ljubi¬¥c, I., Monaci, M., Sinnl, M.: A new general-purpose algorithm for mixed-integer bilevel linear programs. Oper. Res. 65(6), 1615‚Äì1637 (2017) 9. Fischetti, M., Ljubic, I., Monaci, M., Sinnl, M.: Interdiction games and monotonicity, with application to knapsack problems. INFORMS J. Comput. 31, 390‚Äì410 (2019) 10. Fischetti, M., Monaci, M., Sinnl, M.: A dynamic reformulation heuristic for generalized interdiction problems. Eur. J. Oper. Res. 267, 40‚Äì51 (2018) 11. Fontan, F.: Knapsack solver (Github repository). https://github.com/fontanf/ knapsacksolver (2017) 12. Kleinert, T., Labb¬¥e, M., Ljubi¬¥c, I., Schmidt, M.: A survey on mixed-integer programming techniques in bilevel optimization. EURO J. Comput. Optimiz. 9, 100007 (2021) 13. Lozano, L., Bergman, D., Cire, A.A.: Constrained shortest-path reformulations for discrete bilevel and robust optimization. arXiv preprint arXiv:2206.12962 (2022) 14. Martello, S., Pisinger, D., Toth, P.: Dynamic programming and strong bounds for the 0‚Äì1 knapsack problem. Manage. Sci. 45(3), 414‚Äì424 (1999) 15. Pisinger, D.: An expanding-core algorithm for the exact 0‚Äì1 knapsack problem. Eur. J. Oper. Res. 87(1), 175‚Äì187 (1995) 16. Pisinger, D.: Where are the hard knapsack problems? Comput. Oper. Res. 32, 2271‚Äì2284 (2005) 17. Smith, J.C., Song, Y.: A survey of network interdiction models and algorithms. Eur. J. Oper. Res. 283(3), 797‚Äì811 (2020) 18. Tahernejad, S., Ralphs, T.K., DeNegre, S.T.: A branch-and-cut algorithm for mixed integer bilevel linear optimization problems and its implementation. Math. Program. Comput. 12(4), 529‚Äì568 (2020) 19. Tang, Y., Richard, J.P.P., Smith, J.C.: A class of algorithms for mixed-integer bilevel min-max optimization. J. Global Optim. 66, 225‚Äì262 (2016)  
   
  Multiplicative Auction Algorithm for Approximate Maximum Weight Bipartite Matching Da Wei Zheng1(B) and Monika Henzinger2 1  
   
  University of Illinois Urbana-Champaign, Urbana, IL, USA [email protected]  2 IST Austria, Klosterneuburg, Austria [email protected]   
   
  Abstract. We present an auction algorithm using multiplicative instead of constant weight updates to compute a (1 ‚àí Œµ)-approximate maximum weight matching (MWM) in a bipartite graph with n vertices and m edges in time O(mŒµ‚àí1 log(Œµ‚àí1 )), matching the running time of the linear-time approximation algorithm of Duan and Pettie [JACM ‚Äô14]. Our algorithm is very simple and it can be extended to give a dynamic data structure that maintains a (1 ‚àí Œµ)-approximate maximum weight matching under (1) one-sided vertex deletions (with incident edges) and (2) one-sided vertex insertions (with incident edges sorted by weight) to the other side. The total time time used is O(mŒµ‚àí1 log(Œµ‚àí1 )), where m is the sum of the number of initially existing and inserted edges.  
   
  1  
   
  Introduction  
   
  Let G = (U ‚à™ V, E) be an edge-weighted bipartite graph with n = |U ‚à™ V | vertices and m = |E| edges where each edge uv ‚àà E with u ‚àà U and v ‚àà V has a non-negative weight w(uv). The maximum weight matching (MWM) problem  asks for a matching M ‚äÜ E that attains the largest possible weight w(M ) = uv‚ààM w(uv). This paper will focus on approximate solutions to the MWM problem. More speciÔ¨Åcally, if we let M ‚àó denote a maximum weight matching of G, our goal is to Ô¨Ånd a matching M such that w(M ) ‚â• (1 ‚àí Œµ)w(M ‚àó ) for any small constant Œµ > 0. Matchings are a very well studied problem in combinatorial optimization. Kuhn [13] in 1955 published a paper that started algorithmic work in matchings, and presented what he called the ‚ÄúHungarian algorithm‚Äù which he attributed the work to KÀù onig and Egerv¬¥ ary. Munkres [15] showed that this algorithm runs in O(n4 ) time. The running time for computing the exact MWM has been improved many times since then. Recently this year, Chen et al. [6] showed that it was possible to solve the more general problem of max Ô¨Çow in O(m1+o(1) ) time.  
   
  c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023  A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 453‚Äì465, 2023. https://doi.org/10.1007/978-3-031-32726-1_32  
   
  454  
   
  D. W. Zheng and M. Henzinger  
   
  For (1 ‚àí Œµ)-approximation algorithms ‚àö for MWM in bipartite graphs, Gabow and Tarjan in 1989 showed an O(m n log(n/Œµ)) algorithm. Since then there were a number of results for diÔ¨Äerent running times and diÔ¨Äerent approximation ratios. The current best approximate algorithm is by Duan and Pettie [8] which computes a (1‚àíŒµ)-approximate maximum weight matching in O(mŒµ‚àí1 log(Œµ‚àí1 )) time with a scaling algorithm. We defer to their work for a more thorough survey of the history on the MWM problem. We show in our work that the auction algorithm for matchings using multiplicative weights can give a (1 ‚àí Œµ)-approximate maximum weight matching with a running time of O(mŒµ‚àí1 log(Œµ‚àí1 )) for bipartite graphs. This matches the best known running time of Duan and Pettie [8]. However, in comparison to their rather involved algorithm, our algorithm is simple and only uses elementary data structures. Furthermore, we are able to use properties of the algorithm to support two dynamic operations, namely one where vertices on one side are deleted and vertices on the other side are added with all incident edges given in sorted order of weight. 1.1  
   
  Dynamic Matching Algorithms  
   
  Dynamic Weighted Matching. There has been a large body of work on dynamic matching and many variants of the problem have been studied, e.g., the maximum, maximal, as well as Œ±-approximate setting for a variety of values of Œ±, both in the weighted as well as in the unweighted setting. See [10] for a survey of the current state of the art for the fully dynamic setting. We just mention here a few of the most relevant prior works. For any constant Œ¥ > 0 there is a conditional lower bound based on the OMv conjecture that shows that any dynamic algorithm that returns the exact value of a maximum cardinality matching in a bipartite graph with polynomial preprocessing time cannot take time O(m1‚àíŒ¥ ) per query and O(m1/2‚àíŒ¥ ) per edge update operation [11]. For general weighted graphs Gupta and Peng [9] gave the Ô¨Årst algorithm in the fully dynamic setting with edge‚àöinsertions and deletions to maintain a (1 ‚àí Œµ)-approximate matching in O(Œµ‚àí1 m log wmax ) time, where the edges fall into the range [1, wmax ]. Vertex Updates. By vertex update we refer to updates that are vertex insertion (resp. deletion) that also inserts (resp. deletes) all edges incident to the vertex. There is no prior work on maintaining matchings in weighted graphs under vertex updates. However, vertex updates in the unweighted bipartite setting has been studied. Bosek et al. [4] gave an algorithm that maintains the (1 ‚àí Œµ)-approximate matching when vertices of one side are deleted in O(Œµ‚àí1 ) amortized time per changed edge. The algorithm can be adjusted to the setting where vertices of one side are inserted in the same running time, but it cannot handle both vertex insertions and deletions. Le et al. [14] gave an algorithm for maintaining a maximal matching under vertex updates in constant amortized time per changed edge. They also presented an e/(e ‚àí 1) ‚âà 1.58 approximate algorithm for maximum matchings in an unweighted graph when vertex updates are only allowed on one side of a bipartite graph.  
   
  Algorithm for Approximate Maximum Weight Bipartite Matching  
   
  455  
   
  We give the Ô¨Årst algorithm to maintain a (1 ‚àí Œµ)-approximate maximum weight matching where vertices can undergo vertex insertions on one side and vertex deletions on the other side in O(Œµ‚àí1 log(Œµ‚àí1 )) amortized time per edge inserted. 1.2  
   
  Linear Program for MWM  
   
  The MWM problem can be expressed as the following linear program (LP) where the variable xuv denotes whether the edge uv is in the matching. It is well known [17] that the below LP is integral, that is the optimal solution has all variables xuv ‚àà {0, 1}.  max w(uv)xuv uv‚ààE  
   
  s.t.  
   
    
   
  xuv ‚â§ 1  
   
  ‚àÄu ‚àà U  
   
  xuv ‚â§ 1  
   
  ‚àÄv ‚àà V  
   
  v‚ààN (u)  
   
    
   
  u‚ààN (u)  
   
  xuv ‚â• 0  
   
  ‚àÄuv ‚àà E  
   
  We can also consider the dual problem that aims to Ô¨Ånd dual weights yu and yv for every vertex u ‚àà U and v ‚àà V respectively.   min yu + yv u‚ààU  
   
  s.t.  
   
  yu + yv ‚â• w(uv) yu ‚â• 0 yv ‚â• 0  
   
  1.3  
   
  v‚ààV  
   
  ‚àÄuv ‚àà E ‚àÄu ‚àà U ‚àÄv ‚àà V  
   
  Multiplicative Weight Updates for Packing LPs  
   
  Packing LPs are LPs of the form max{cT x | Ax ‚â§ b} for c ‚àà Rn‚â•0 , b ‚àà Rm ‚â•0 and A ‚àà Rn√óm ‚â•0 . The LP for MWM is a classical example of a packing LP. The multiplicative weight update method (MWU) has been investigated extensively to provide faster algorithms for Ô¨Ånding approximate solutions1 to packing LPs [1,5,12,16,18,19]. Typically the running times for solving these LPs have a dependence on Œµ of Œµ‚àí2 , e.g. the algorithm of Koufogiannakis and Young [12] would obtain a running time of O(mŒµ‚àí2 log n) when applied to the matching LP. The fastest multiplicative weight update algorithm for solving packing LPs by Allen-Zhu and Orecchia [1] would obtain an O(mŒµ‚àí1 log n) running time for 1  
   
  By approximate solution we mean a possibly fractional assignments of variables that obtains an approximately good LP objective. If we Ô¨Ånd such an approximate solution to MWM, fractional solutions need to be rounded to obtain an actual matching.  
   
  456  
   
  D. W. Zheng and M. Henzinger  
   
  MWM. Very recently, work by Battacharya, Kiss, and Saranurak [3] extended the MWU for packing LPs to the partially dynamic setting. When restricted to the MWM problem means the weight of edges either only increase or only decrease. However as packing LPs are more general than MWM, these algorithms are signiÔ¨Åcantly more complicated and are slower by log n factors (and worse dependence on Œµ for [3]) when compared to our static and dynamic algorithms. We remark that our algorithm, while it uses multiplicative weight updates, is unlike typical MWU algorithms as it has an additional monotonicity property. We only increase dual variables on one side of the matching, and only (implicitly) decrease dual variables on the other side. 1.4  
   
  Auction Algorithms  
   
  Auction algorithms are a class of primal dual algorithms for solving the MWM problem that view U as a set of goods to be sold, V as a set of buyers. The goal of the auction algorithm is to Ô¨Ånd a welfare-maximizing allocation of goods to buyers. The algorithm is commonly attributed Bertsekas [2], as well as to Demange, Gale, and Sotomayor [7]. An auction algorithm initializes the prices of all the goods u ‚àà U with a price yu = 0 (our choice of yu is intentional, as prices correspond directly to dual variables), and has buyers initially unallocated. For each buyer v ‚àà V , the utility of that buyer upon being allocated u ‚àà U is util(uv) = w(uv) ‚àí yu . The auction algorithm proceeds by asking an unallocated buyer v ‚àà V for the good they desire that maximizes their utility, i.e. for uv = arg maxu‚ààN (v) util(uv). If util(uv v) < 0, the buyer remains unallocated. Otherwise the algorithm allocates uv to v, then increases the price yu to yu + Œµ. The algorithm terminates when all buyers are either allocated or for every unallocated buyer v, it holds that util(uv v) < 0. If the maximum weight among all the edges is wmax , then the auction algorithm terminates after O(nŒµ‚àí1 wmax ) rounds and outputs a matching that diÔ¨Äers from the optimal by an additive factor of at most nŒµ. 1.5  
   
  Our Contribution  
   
  We present the following modiÔ¨Åcation of the auction algorithm: When v is allocated u, increase yu to yu + Œµ ¬∑ util(uv) instead of yu + Œµ. Note that this decreases util(v) by a factor of (1 ‚àí Œµ) and, thus, we will call algorithms with this modiÔ¨Åcation multiplicative auction algorithms. Surprisingly, we were not able to Ô¨Ånd any literature on this simple modiÔ¨Åcation. Changing the constant additive weight update to a multiplicative weight update has the eÔ¨Äect of taking much larger steps when the weights are large, and so we are able to show that the algorithm can have no dependence on the size of the weights. In fact, we are able to improve the running time to O(mŒµ‚àí1 log(Œµ‚àí1 )), the same as the fastest known matching algorithm of Duan and Pettie [8]. While the algorithm of [8] has the advantage that it works for general graphs and ours  
   
  Algorithm for Approximate Maximum Weight Bipartite Matching  
   
  457  
   
  is limited to bipartite graphs, our algorithm is simpler as it avoids the scaling algorithm framework and is easier to implement. Theorem 1. Let G = (U ‚à™ V, E) be a weighted biparitite graph. There is a multiplicative auction algorithm running in time O(mŒµ‚àí1 log(Œµ‚àí1 )) that finds a (1 ‚àí Œµ)-approximate maximum weight matching of G. Furthermore, it is straightforward to extend our algorithm to a setting where edges are deleted and vertices on one side are added with all incident edges given in sorted order of weight. When the inserted edges are not sorted by weight, the running time per inserted edge increases by an additive term of O(log n) to sort all incident inserted edges. Theorem 2. Let G = (U ‚à™ V, E) be a weighted biparitite graph. There exists a dynamic data structure that maintains a (1 ‚àí Œµ)-approximate maximum weight matching of G and supports the following operations: (1) Deleting a vertex in U (2) Adding a new vertex into V along with all its incident edges sorted by weight in total time O(mŒµ‚àí1 log(Œµ‚àí1 )), where m is sum of the number of initially existing, and inserted edges.  
   
  2  
   
  The Static Algorithm  
   
  We assume that the algorithm is given as input some Ô¨Åxed 0 < Œµ < 1. Notation For sake of notation let N (u) = {v ‚àà V | uv ‚àà E} be the set of neighbors of u ‚àà U in G, and similarly for N (v) for v ‚àà V . Preprocessing of the Weights. Let wmax > 0 be the maximum weight edge of E. For our static auction algorithm we may ignore any edge uv ‚àà E of weight less than Œµ ¬∑ wmax /n as w(M ‚àó ) ‚â• wmax as taking n of these small weight edges would not even contribute Œµ ¬∑ w(M ‚àó ) to the matching. Thus, we only consider edges of weight at least Œµ ¬∑ wmax /n, which allows us to rescale all edge weights by dividing them by Œµ ¬∑ wmax /n. As a result we can assume (by slight abuse of notation) in the following that the minimum edge weight is 1 and the largest edge weight wmax equals n/Œµ . Furthermore, since we only care about approximations, we will also round down all edge weights to the nearest power of (1 + Œµ) for some Œµ < Œµ /2 and, again by slight abuse of notation, we will use w to denote these edge weights. Formally to round, we deÔ¨Åne iLog(x) = log1+Œµ (x) and Round(x) = (1 + Œµ)iLog(x) . Let kmax = iLog(wmax ) = iLog(n/Œµ) = O(Œµ‚àí1 log(n/Œµ)). Let kmin be the smallest integer such that (1 + Œµ)‚àíkmin ‚â§ Œµ. Observe that as log(1 + Œµ) ‚â§ Œµ for 0 ‚â§ Œµ ‚â§ 1 it holds that kmin ‚â•  
   
  log(Œµ‚àí1 ) ‚â• Œµ‚àí1 log(Œµ‚àí1 ). log(1 + Œµ)  
   
  Thus we see that kmin = Œò(Œµ‚àí1 log(Œµ‚àí1 )).  
   
  458  
   
  D. W. Zheng and M. Henzinger  
   
  Algorithm. The algorithm Ô¨Årst builds for every v ‚àà V a list Qv of pairs (i, uv) for each edge uv and each value i with ‚àíkmin ‚â§ i ‚â§ juv = iLog(wuv ) and then sorts Qv by decreasing value of i. After, it calls the function MatchR(v) on every v ‚àà V . The function MatchR(v) matches v to the item that maximizes its utility and updates price according to our multiplicative update rule. While matching v, another vertex v  originally matched to v may become unmatched. If this happens, MatchR(v  ) is called immediately after MatchR(v). Algorithm 2.1: MultiplicativeAuction(G = (U ‚à™ V, E)) M ‚Üê ‚àÖ. yu ‚Üê 0 for all u ‚àà U . jv ‚Üê kmax for all v ‚àà V Qv ‚Üê ‚àÖ for all v ‚àà V . For v ‚àà V :  
   
  # This is only used in the analysis  
   
  1. For u ‚àà N (v): (a) juv ‚Üê iLog(w(uv)) (b) jv ‚Üê max{jv , juv } (c) For i from juv to ‚àíkmin : i. Insert the pair (i, uv) into Qv . 2. Sort all (i, uv) ‚àà Qv so elements are in non-increasing order of i. For v ‚àà V : 1. MatchR(v). Return M . MatchR(v) While Qv is not empty: 1. 2. 3. 4.  
   
  (j, uv) ‚Üê the Ô¨Årst element of Qv , and remove it from Qv . jv ‚Üê j # This is only used in the analysis util(uv) ‚Üê w(uv) ‚àí yu If util(uv) ‚â• (1 + Œµ)j : # util(uv) ‚Üê (1 ‚àí Œµ) ¬∑ util(uv) (a) yu ‚Üê yu + Œµ ¬∑ (util(uv)) (b) If u was matched to v  in M : ‚Äì Remove (u, v  ) from M ‚Äì Add (u, v) to M ‚Äì MatchR(v  ) (c) Else: ‚Äì Add (u, v) to M ‚Äì Return  
   
  Algorithm for Approximate Maximum Weight Bipartite Matching  
   
  459  
   
  Data Structure. We store for each vertex v ‚àà V the list Qv as well as its currently matched edge if it exists. In the pseudocode below we keep for each vertex v a value jv corresponding to the highest weight threshold (1 + Œµ)jv that we will consider. This value is only needed in the analysis. Running Time. The creation and sorting of the lists Qv takes time O(|N (v)|(kmax + kmin )) if we use bucket sort as there are only kmax + kmin distinct weights. The running time of all calls to MatchR(v) is dominated by the size of Qv , as each iteration in MatchR(v)  removes an element of Qv and takes O(1) time. Thus, the total time is O v‚ààV |N (v)|(kmax + kmin ) = O(m(kmax + kmin )) = O(mŒµ‚àí1 log(n/Œµ)). Invariants Maintained by the Algorithm. Consider the following invariants maintained throughout by the algorithm: Invariant 1. For all v ‚àà V , and all u ‚àà N (v), util(uv) < (1 + Œµ)jv +1 . Proof. This clearly is true at the beginning, since jv is initialized to maxu‚ààN (v) juv , and util(uv) = w(uv) < (1 + Œµ)juv +1 . As the algorithm proceeds, util(uv) which equals w(uv) ‚àí yu only decreases as yu only increases. Thus, we only have to make sure that the condition holds whenever jv decreases. The value jv only decreases from some value, say j + 1, to a new value j, in MatchR(v) and when this happens Qv does not contain any pairs (j  , uv) with j  > j anymore. Thus, there does not exist a neighbor u of v with util(uv) ‚â• (1 + Œµ)j+1 . It follows that when jv decreases to j for all u ‚àà N (v) it holds that util(uv) < (1 + Œµ)jv +1 . Invariant 2. If uv ‚àà M , then for all other u ‚àà N (v), util(uv) ‚â• (1 ‚àí 2Œµ) ¬∑ util(u v). Proof. When v was matched to u, right before we updated yu , we had that (1 + Œµ)jv ‚â§ util(uv) and, by Invariant 1, util(u v) ‚â§ (1 + Œµ)jv +1 . Thus, (1 + Œµ)util(uv) ‚â• util(u v). The update of yu decreases yu by Œµ ¬∑ util(uv), which decreases util(uv) by a factor of (1 ‚àí Œµ), but does not aÔ¨Äect util(u v). Thus we have now that: util(uv) ‚â• (1 ‚àí Œµ)(1 + Œµ)‚àí1 ¬∑ util(u v) ‚â• (1 ‚àí 2Œµ) ¬∑ util(u v). Invariant 3. If u ‚àà U is not matched, then yu = 0. If uv ‚àà M , then yu > 0. Proof. If u is never matched, we never increment yu , so it stays 0. The algorithm increments yu by Œµ ¬∑ util(uv) > 0 when we add uv into the matching M . Invariant 4. For all v ‚àà V for which MatchR(v) was called at least once, either v is matched, or Qv is empty. Proof. MatchR(v) terminates (i) after it matches v and recurses or (ii) if Qv is empty. It is possible that for some other v  ‚àà V with v  = v, that v becomes temporarily unmatched during MatchR(v  ), but we would immediately call MatchR(v) to rematch v.  
   
  460  
   
  D. W. Zheng and M. Henzinger  
   
  Approximation Factor. We will show the approximation factor of the matching M found by the algorithm by primal dual analysis. We remark that it is possible to show this result purely combinatorially as well which we include in Appendix A, as it may be of independent interest. We will show that this M and a vector y satisfy the complementary slackness condition up to a 1 ¬± Œµ factor, which implies the approximation guarantee. This was proved by Duan and Pettie [8] (the original lemma was for general matchings, we have specialized it here to bipartite matchings). Lemma 1 (Lemma 2.3 of [8]). Let M be a matching and let y be an assignment of the dual variables. Suppose y is a valid solution to the LP in the following approximate sense: For all uv ‚àà E, yu + yv ‚â• (1 ‚àí Œµ0 ) ¬∑ w(uv) and for all e ‚àà M , yu + yv ‚â§  (1 + Œµ1 ) ¬∑ w(uv). If the y-values of all unmatched vertices are zero, then M is a (1 + Œµ1 )‚àí1 (1 ‚àí Œµ0 ) -approximate maximum weight matching. This lemma is enough for us to prove the approximation factor of our algorithm. Lemma 2. MultiplicativeAuction(G = (U ‚à™ V, E)) outputs a (1 ‚àí Œµ )approximate maximum weight matching of the bipartite graph G. Proof. Let Œµ > 0 be a parameter depending on Œµ that we will choose later. We begin by choosing an assignment of the dual variables yu for u ‚àà U and yv for v ‚àà V . Let all yu ‚Äôs be those obtained by the algorithm for u ‚àà U . For v ‚àà V , let yv = 0 if v is not matched in M and yv = util(uv) = w(uv) ‚àí yu if v is matched to u in M . By Invariant 3 all unmatched vertices u ‚àà U have yu = 0. Observe that for uv ‚àà M we have yu + yv = util(uv). It remains to show that for uv ‚àà M we have that yu + yv ‚â• (1 ‚àí Œµ0 )w(uv) for some Œµ0 > 0. First we consider if v is unmatched, so yv = 0. Since v is unmatched, by Invariant 4 then for all u ‚àà N (v), we must have util(uv) < (1 + Œµ)‚àíkmin ‚â§ Œµ. Since we rescaled weights so that w(uv) ‚â• 1, we know that util(uv) < Œµ ‚â§ Œµ ¬∑ w(uv). Furthermore, observe that as yu = w(uv) ‚àí util(uv) by deÔ¨Ånition of utility, it follows that: yu + yv = yu = w(uv) ‚àí util(uv) > (1 ‚àí Œµ)w(uv).  
   
  (1)  
   
  Now we need to consider if v was matched to some vertex u = u. To do so we use Invariant 2: yu + yv = yu + util(u v)  
   
  By deÔ¨Ånition of y  
   
  ‚â• yu + (1 ‚àí 2Œµ) ¬∑ util(uv) = yu + (1 ‚àí 2Œµ) ¬∑ (w(uv) ‚àí yu )  
   
  By Invariant 2 By deÔ¨Ånition of util  
   
  ‚â• (1 ‚àí 2Œµ)w(uv) + 2Œµ ¬∑ yu ‚â• (1 ‚àí 2Œµ)w(uv)  
   
  Since yu ‚â• 0  
   
  Thus we have satisÔ¨Åed Lemma 1 with Œµ0 = 2Œµ and Œµ1 = 0. Setting Œµ = Œµ /2 gives us a (1 ‚àí Œµ )-approximate maximum weight matching.  
   
  Algorithm for Approximate Maximum Weight Bipartite Matching  
   
  461  
   
  Thus we have shown the following result that is weaker than what we have set out to prove by a factor of log(nŒµ‚àí1 ) that we will show how to get rid of in the next section. Theorem 3. Let G = (U ‚à™ V, E) be a weighted biparitite graph. There exists a multiplicative auction algorithm running in time O(mŒµ‚àí1 log(nŒµ‚àí1 )) that finds a (1 ‚àí Œµ)-approximate maximum weight matching of G. 2.1  
   
  Improving the Running Time  
   
  To improve the running time to O(mŒµ‚àí1 log(Œµ‚àí1 )), we observe that all we actually need for Lemma 2 in Equation (1) is that util(uv) ‚â§ Œµ ¬∑ w(uv). Recall that juv = iLog(w(uv)). Thus it suÔ¨Éces if we change line (b) in MultiplicativeAuction to range from juv to juv ‚àí kmin , since: (1 + Œµ)juv ‚àíkmin = (1 + Œµ)‚àíkmin ¬∑ (1 + Œµ)juv ‚â§ Œµ ¬∑ w(uv). This change implies that we insert O(kmin |N (v)|) items into Qv for every v ‚àà V . However, sorting Qv for every vertex individually, even with bucket sort, would be too slow. We will instead perform one bucket sort on all the edges, then go through the weight classes in decreasing order to insert the pairs into the corresponding Qv . We explicitly give the pseudocode below as MultiplicativeAuction+. Algorithm 2.2: MultiplicativeAuction+(G = (U ‚à™ V, E)) M ‚Üê ‚àÖ. yu ‚Üê 0 for all u ‚àà U . Qv ‚Üê ‚àÖ for all v ‚àà V . Li ‚Üê ‚àÖ for all i from ‚àíkmin to kmax . For uv ‚àà E: 1. juv ‚Üê iLog(w(uv)) 2. For i from juv to juv ‚àí kmin : (a) Insert the pair (i, uv) into Li . For i from kmax to ‚àíkmin : 1. For all (i, uv) ‚àà Li : (a) Insert the pair (i, uv) to the back of Qv . For v ‚àà V : 1. MatchR(v). Return M .  
   
  462  
   
  D. W. Zheng and M. Henzinger  
   
  New Runtime. Bucket sorting all mkmin pairs and initializing the sorted Qv for all v ‚àà V takes total time O(mkmin + (kmax + kmin )) = O(mŒµ‚àí1 log(Œµ‚àí1 )). The total amount of work done in MatchR(v) for a vertex v ‚àà V is O(|N (v)|kmin ) which also sums to O(mŒµ‚àí1 log(Œµ‚àí1 )). Thus we get our desired running time and have proven our main theorem that we restate here. Theorem 1. Let G = (U ‚à™ V, E) be a weighted biparitite graph. There is a multiplicative auction algorithm running in time O(mŒµ‚àí1 log(Œµ‚àí1 )) that finds a (1‚àíŒµ)approximate maximum weight matching of G.  
   
  3  
   
  Dynamic Algorithm  
   
  There are many monotonic properties of our static algorithm. For instance, for all u ‚àà U , the yu values strictly increase. As another example, for all v ‚àà V , the value of jv strictly decreases. These monotonic properties allow us to extend MultiplicativeAuction+ to a dynamic setting with the following operations. Theorem 2. Let G = (U ‚à™ V, E) be a weighted biparitite graph. There exists a dynamic data structure that maintains a (1 ‚àí Œµ)-approximate maximum weight matching of G and supports the following operations: (1) Deleting a vertex in U (2) Adding a new vertex into V along with all its incident edges sorted by weight in total time O(mŒµ‚àí1 log(Œµ‚àí1 )), where m is sum of the number of initially existing, and inserted edges. Type (1) operations: Deleting a vertex in U. To delete a vertex u ‚àà U , we can mark u as deleted and skip all edges uv in Qv for any v ‚àà V in all further computation. If u were matched to some vertex v ‚àà V , that is if there exists an edge uv ‚àà M , we need to unmatch v and remove uv from M . All our invariants hold except Invariant 4 for the unmatched v. To restore this invariant we simply call MatchR(v). Type (2) Operations: Adding a New Vertex to V Along with All Incident Edges. To add a new vertex v to V with  incident edges to u1 v, ..., u v with w(u1 v) > ¬∑ ¬∑ ¬∑ > w(u v), we can create the queue Qv by inserting the O(Œµ‚àí1 log(Œµ‚àí1 )) pairs such that it is non-increasing in the Ô¨Årst element of the pair. Afterwards we call MatchR(v). All invariants hold after doing so. If the edges are not given in sorted order, we can sort the  edges in O( log ) time, or in O( + Œµ‚àí1 log(w(u1 v)/w(u v))) time by bucket sort. Acknowledgements. The Ô¨Årst author thanks to Chandra Chekuri for useful discussions about this paper. This project has received funding from the European Research Council (ERC) under the European Union‚Äôs Horizon 2020 research and innovation programme (Grant agreement No. 101019564 ‚ÄúThe Design of Modern Fully Dynamic Data Structures (MoDynStruct)‚Äù and from the Austrian Science Fund (FWF) project ‚ÄúFast Algorithms for a Reactive Network Layer (ReactNet)‚Äù, P 33775-N, with additional funding from the netidee SCIENCE Stiftung, 2020‚Äì2024.  
   
  Algorithm for Approximate Maximum Weight Bipartite Matching  
   
  A  
   
  463  
   
  Combinatorial proof of Lemma 2  
   
  We start with a simple lemma. Lemma 3. Let G = (U ‚à™ V, E) be a weighted bipartite graph. Let M be the matching found by MultiplicativeAuction+(G) for Œµ > 0, and M  be any other matching. Then for any alternating path, i.e. a set of edges of the form u1 v1 , u2 v1 , u2 v2 , ..., uk vk , uk+1 vk with all edges of ui vi ‚àà M  and ui+1 vi ‚àà M , we have that: (1 ‚àí 2Œµ) ¬∑  
   
  k   
   
  w(ui vi ) ‚â§  
   
  i=1  
   
  k   
   
  w(ui+1 vi ) + (1 ‚àí 2Œµ) ¬∑ yu1 ‚àí yuk+1  
   
  i=1  
   
  Proof. By Invariant 2, for all i from 1 to k, since M matched vi+1 to ui we have that: (1 ‚àí 2Œµ)util(ui vi ) ‚â§ util(ui vi+1 ) Adding all such equations together we get (1 ‚àí 2Œµ) ¬∑  
   
  k   
   
  util(ui vi ) ‚â§  
   
  i=1 k   
   
  (1 ‚àí 2Œµ) ¬∑  (1 ‚àí 2Œµ) ¬∑  
   
  (w(ui vi ) ‚àí yui ) ‚â§  
   
  w(ui vi ) ‚àí  
   
  i=1  
   
  k   
   
  yui  
   
  k   
   
  ‚â§  
   
  w(ui+1 vi ) ‚àí yui+1  
   
  k  i=1  
   
  w(ui vi ) ‚â§  
   
  i=1  
   
  k    
   
    
   
  i=1  
   
    
   
  i=1  
   
  (1 ‚àí 2Œµ) ¬∑  
   
  util(ui vi+1 )  
   
  i=1  
   
  i=1 k   
   
  k   
   
  k   
   
  w(ui+1 vi ) ‚àí  
   
  k   
   
  yui+1  
   
  i=1  
   
  w(ui+1 vi ) + (1 ‚àí 2Œµ) ¬∑ yu1 ‚àí yuk+1  
   
  i=1  
   
  Theorem 3. Let G = (U ‚à™ V, E) be a weighted bipartite graph and Œµ > 0 be an input parameter. Let M be the matching found by MultiplicativeAuction+(G) with Œµ = Œµ /2. M is a (1 ‚àí Œµ )-approximate maximum weight matching of the bipartite graph G. Proof. Let M ‚àó be a maximum weight matching of G. Consider the symmetric diÔ¨Äerence of M with M ‚àó . It consists of paths and and even cycles. It suÔ¨Éces to show that the weight obtained by M on the path or even cycle is at least (1 ‚àí Œµ) the weight of M ‚àó . We consider the following cases: 1. Consider any even cycle u1 v1 , u2 v1 , u2 v2 , ..., uk vk , u1 vk with ui vi ‚àà M ‚àó for all i = 1, ..., k and the other edges in M . Applying Lemma 3 with uk+1 = u1 , and by Invariant 3 yu1 > 0 as u1 is matched gives: (1 ‚àí 2Œµ) ¬∑  
   
  k  i=1  
   
  w(ui vi ) ‚â§  
   
  k  i=1  
   
  w(ui+1 vi ) + (1 ‚àí 2Œµ)yu1 ‚àí yu1  
  0. Applying Lemma 3 gives: (1 ‚àí 2Œµ) ¬∑  
   
  k   
   
  w(ui vi ) ‚â§  
   
  i=1  
   
  k   
   
  w(ui+1 vi ) + (1 ‚àí 2Œµ) ¬∑ 0 ‚àí yuk+1  
  (1 ‚àí 2Œµ)w(uk+1 vk+1 ). Adding this equation to Lemma 3, and by Invariant 3 we have yu1 = 0, so: (1 ‚àí 2Œµ) ¬∑  
   
  k+1   
   
  w(ui vi )  
  2. Theorem 3. Let G = (V, A) be a Pst -covered, acyclic digraph with source vertex s and sink vertex t and let d ‚àà N be a constant. A basis of the subspace Ld of the linearizable instances of the SPPd can be computed in polynomial time. Proof (Sketch). The proof idea is to specify a k ‚àà N and a matrix M of polyno(d) mially bounded dimensions, such that for f : RA ‚Üí Rk with f (x) = M x, we have: f (x) = 0 iÔ¨Ä x is a linearizable instance of the SPPd . Thus, the linearizable instances x of the SPPd form ker(M ) which can be eÔ¨Éciently computed.  
   
  478  
   
  E. C ¬∏ ela et al.  
   
  The construction of M is done iteratively and exploits the relationship between SP Pd and AP ECPd‚àí1 similarly as in the algorithm A from Sect. 4.2. In particular we use the following two facts: (i) For each strongly basic arc a = (u, v), the function which maps x ‚àà RA (a) (d‚àí1) qd‚àí1 : Au ‚Üí R is linear (see Eq. (6) and recall DeÔ¨Ånition 5 for Au ). (ii) The function c ‚Üí reduced(c) (deÔ¨Åned after Lemma 1) is linear.  
   
  (d)  
   
  to  
   
  Using (i) and (ii) iteratively as in algorithm A, we show by induction that for each strongly basic arc a = (u, v) and each d ‚â• 2 there exist ka ‚àà N and a linear (d‚àí1) ‚Üí Rka s.t. ga (x) = 0 iÔ¨Ä if x corresponds to a YES-instance function ga : RAu of APECPd‚àí1 . Then we construct the linear function ga on the same domain as ga , by setting ga (x) = Œ≤ whenever ga (x) = 0, where Œ≤ is the common path cost of the corresponding instance xproject/63605ec6eb4e243dcb2eeb7d of APECPd‚àí1 . Next we show that for each vertex u there exists a ku ‚àà N and a linear function (d‚àí1) ‚Üí Rku such that fu (x) = 0 iÔ¨Ä x is a linearizable instance of SPPd‚àí1 fu : RAu corresponding to APECPd‚àí1 (see Lemma 4). Then we construct the linear function fu on the same domain as fu by setting fu (x) equal to the linearizing cost function of the instance x of the SPPd‚àí1 whenever x is linearizable (i.e. when fu (x) = 0). The construction of M is done by repeating these steps iteratively until d = 1. One can ensure that the size of the matrix representations of all involved functions stays polynomial.  
   
  Acknowledgement. This research has been supported by the Austrian Science Fund (FWF): W1230.  
   
  References 1. Bookhold, I.: A contribution to quadratic assignment problems. Optimization 21(6), 933‚Äì943 (1990) 2. C ¬∏ ela, E., Klinz, B., Lendl, S., Orlin, J.B., Woeginger, G.J., Wulf, L.: Linearizable special cases of the quadratic shortest path problem. In: Kowalik, L  ., Pilipczuk, M., Rz¬∏azÀô ewski, P. (eds.) WG 2021. LNCS, vol. 12911, pp. 245‚Äì256. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-86838-3 19 3. Cela, E., Deineko, V.G., Woeginger, G.J.: Linearizable special cases of the QAP. J. Comb. Optim. 31(3), 1269‚Äì1279 (2016) ¬¥ 4. Custi¬¥ c, A., Punnen, A.P.: A characterization of linearizable instances of the quadratic minimum spanning tree problem. J. Comb. Optim. 35(2), 436‚Äì453 (2018) 5. De Meijer, F., Sotirov, R.: The quadratic cycle cover problem: special cases and eÔ¨Écient bounds. J. Comb. Optim. 39(4), 1096‚Äì1128 (2020) 6. ErdoÀò gan, G.: Quadratic assignment problem: linearizations and polynomial time solvable cases, Ph. D. thesis, Bilkent University (2006) 7. ErdoÀò gan, G., Tansel, B.: A branch-and-cut algorithm for quadratic assignment problems based on linearizations. Comput. Oper. Res. 34(4), 1085‚Äì1106 (2007) 8. ErdoÀò gan, G., Tansel, B.C.: Two classes of quadratic assignment problems that are solvable as linear assignment problems. Discret. Optim. 8(3), 446‚Äì451 (2011) 9. Gamvros, I.: Satellite network design, optimization and management. University of Maryland, College Park (2006)  
   
  A linear time algorithm for the linearization of the QSPP and SPPd  
   
  479  
   
  10. Hu, H., Sotirov, R.: Special cases of the quadratic shortest path problem. J. Comb. Optim. 35(3), 754‚Äì777 (2018) 11. Hu, H., Sotirov, R.: On solving the quadratic shortest path problem. INFORMS J. Comput. 32(2), 219‚Äì233 (2020) 12. Hu, H., Sotirov, R.: The linearization problem of a binary quadratic problem and its applications. Annal. Oper. Res. 307, 229‚Äì249 (2021) 13. Kabadi, S.N., Punnen, A.P.: An O(n4 ) algorithm for the QAP linearization problem. Math. Oper. Res. 36(4), 754‚Äì761 (2011) 14. Murakami, K., Kim, H.S.: Comparative study on restoration schemes of survivable ATM networks. In: Proceedings of INFOCOM1997, vol. 1, pp. 345‚Äì352. IEEE (1997) 15. Nie, Y.M., Wu, X.: Reliable a priori shortest path problem with limited spatial and temporal dependencies. In: Lam, W., Wong, S., Lo, H. (eds.) Transportation and TraÔ¨Éc Theory 2009: Golden Jubilee, pp. 169‚Äì195. Springer, Boston (2009). https://doi.org/10.1007/978-1-4419-0820-9 9 16. Punnen, A.P., Kabadi, S.N.: A linear time algorithm for the Koopmans-Beckmann QAP linearization and related problems. Discret. Optim. 10(3), 200‚Äì209 (2013) 17. Punnen, A.P., Walter, M., Woods, B.D.: A characterization of linearizable instances of the quadratic traveling salesman problem. arXiv preprint arXiv:1708.07217 (2017) 18. Rostami, B., et al.: The quadratic shortest path problem: complexity, approximability, and solution methods. Eur. J. Oper. Res. 268(2), 473‚Äì485 (2018) 19. Rostami, B., Malucelli, F., Frey, D., Buchheim, C.: On the quadratic shortest path problem. In: Bampis, E. (ed.) SEA 2015. LNCS, vol. 9125, pp. 379‚Äì390. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-20086-6 29 20. Sen, S., Pillai, R., Joshi, S., Rathi, A.K.: A mean-variance model for route guidance in advanced traveler information systems. Transp. Sci. 35(1), 37‚Äì49 (2001) 21. Sivakumar, R.A., Batta, R.: The variance-constrained shortest path problem. Transp. Sci. 28(4), 309‚Äì316 (1994) 22. Sotirov, R., Verch¬¥ere, M.: The quadratic minimum spanning tree problem: lower bounds via extended formulations. arXiv preprint arXiv:2102.10647 (2021) 23. Waddell, L., Adams, W.: Characterizing linearizable QAPs by the level-1 reformulation-linearization technique. (2021). https://optimization-online.org/? p=17020, preprint  
   
  Author Index  
   
  A Achterberg, Tobias  
   
  14  
   
  B Balas, Egon 275 Basu, Amitabh 1 Bestuzheva, Ksenia 14 Borst, Sander 29 Bruckmeier, Sabrina 44 B√ºsing, Christina 58 C Cardinal, Jean 72 √áela, Eranda 466 Chmiela, Antonia 87 D Dadush, Daniel  
   
  29, 100, 115  
   
  E Eberle, Franziska 127 Eisenbrand, Friedrich 100 F Fujishige, Satoru 142 Fukasawa, Ricardo 438 G Gersing, Timo 58 Gerstbrein, Matthew 157 Gleixner, Ambros 14 Gupta, Anupam 127 H Hanguir, Oussama 172 Henzinger, Monika 453 Hertrich, Christoph 187 Huiberts, Sophie 29 Hunkenschr√∂der, Christoph 44 Husi¬¥c, Edin 203  
   
  J J√§ger, Sven 246 Jiang, Hongyi 1 Jin, Billy 217 Joswig, Michael 231 K Karlin, Anna R. 261 Kashaev, Danish 29 Kazachkov, Aleksandr M. 275 Kerger, Phillip 1 Kitahara, Tomonari 142 Klein, Nathan 217, 261 Klimm, Max 231 Klinz, Bettina 466 Kobayashi, Yusuke 291 Koh, Zhuan Khye 203 Koster, Arie M. C. A. 58 L Lendl, Stefan 466 L√©onard, Arthur 115 Loho, Georg 203 M Ma, Will 172 Matuschke, Jannik 306 Megow, Nicole 127, 319 Molinaro, Marco 1 Moseley, Benjamin 127 Mu√±oz, Gonzalo 87, 334, 348, 363 N N√§gele, Martin 393 N√∂bel, Christian 393 Nuti, Pranav 378 O Oveis Gharan, Shayan  
   
  261  
   
  ¬© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 A. Del Pia and V. Kaibel (Eds.): IPCO 2023, LNCS 13904, pp. 481‚Äì482, 2023. https://doi.org/10.1007/978-3-031-32726-1  
   
  482  
   
  Author Index  
   
  P Paat, Joseph 334, 348 Poremba, Joseph 408  
   
  R Rohwedder, Lars 115 Rothvoss, Thomas 100 Ryan, Christopher Thomas  
   
  Steiner, Raphael 72 Svensson, Anton 363 V V√©gh, L√°szl√≥ A. 142, 203 Verberk, Lucy 157 Verschae, Jos√© 115 Vondr√°k, Jan 378 172  
   
  S Sagnol, Guillaume 246 Salas, David 363 Sanit√†, Laura 157 Santiago, Richard 393, 423 Schl√∂ter, Jens 319 Schmidt genannt Waldschmidt, Daniel Sergeev, Ivan 423 Sering, Leon 187 Serrano, Felipe 87, 334 Shepherd, F. Bruce 408 Spitz, Sylvain 231  
   
  W Warode, Philipp 246 Weismantel, Robert 44 Weninger, Noah 438 Williamson, David P. 217 Woeginger, Gerhard J. 466 Wulf, Lasse 466  
   
  246  
   
  X Xavier, √Ålinson S.  
   
  348  
   
  Z Zenklusen, Rico 393, 423 Zheng, Da Wei 453 Zhou, Rudy 127  

 Report "Integer Programming and Combinatorial Optimization. 24th International Conference, IPCO 2023 Madison, WI, USA, June 21‚Äì23, 2023 Proceedings 9783031327254, 9783031327261"  
 √ó    

 --- Select Reason ---  Pornographic  Defamatory  Illegal/Unlawful  Spam  Other Terms Of Service Violation  File a copyright complaint     

 Close  Submit    

    Contact information  
 Michael Browner   
   [email protected]    
   
   Address:   
 1918 St.Regis, Dorval, Quebec, H9P 1H6, Canada.   
   
 Support & Legal  
  O nas 
  Skontaktuj siƒô z nami 
  Prawo autorskie 
  Polityka prywatno≈õci 
  Warunki 
  FAQs 
  Cookie Policy 
    
 Subscribe to our newsletter  
  Be the first to receive exclusive offers and the latest news on our products and services directly in your inbox.  
   Subscribe     

 Copyright ¬© 2024 DOKUMEN.PUB. All rights reserved.        

 Unsere Partner sammeln Daten und verwenden Cookies zur Personalisierung und Messung von Anzeigen. Erfahren Sie, wie wir und unser Anzeigenpartner Google Daten sammeln und verwenden  .   Cookies zulassen