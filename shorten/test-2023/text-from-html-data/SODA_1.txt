 Proceedings  
 Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)  
 Editor(s): Nikhil Bansal | and 
  Viswanath Nagarajan 
  Control and its Applications 
  Data Mining 
  Discrete Algorithms (SODA) 
  Mathematics for Industry 
  Parallel Processing for Scientific Computing (PP) 
  Email 
 Home  Proceedings  Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)   
 Description | This symposium focuses on research topics related to efficient algorithms and data structures for discrete problems. In addition to the design of such methods and structures, the scope also includes their use, performance analysis, and the mathematical problems related to their development or limitations. Performance analyses may be analytical or experimental and may address worst-case or expected-case performance. Studies can be theoretical or based on data sets that have arisen in practice and may address methodological issues involved in performance analysis. 
  PDF 
  Abstract   In the dynamic linear program  (LP) problem, we are given an LP undergoing updates and we need to maintain an approximately optimal solution. Recently, significant attention (e.g. [Gupta et al. STOC'17; Arar et al. ICALP'18, Wajc STOC'20]) has been devoted to the study of special cases of dynamic packing and covering LPs, such as the dynamic fractional matching and set cover problems. But until now, there is no non-trivial dynamic algorithm for general packing and covering LPs.  
 In this paper, we settle the complexity of dynamic packing and covering LPs, up to a polylogarithmic factor in update time. More precisely, in the partially dynamic  setting (where updates can either only relax or only restrict the feasible region), we give near-optimal deterministic Îµ-approximation algorithms with polylogarithmic amortized update time. Then, we show that both partially dynamic updates and amortized update time are necessary; without any of these conditions, the trivial algorithm that recomputes the solution from scratch after every update is essentially the best possible, assuming SETH.  
 To obtain our results, we initiate a systematic study of the multiplicative weights update  (MWU) method in the dynamic setting. As by-products of our techniques, we also obtain the first online (1 + Îµ)-competitive algorithms for both covering and packing LPs with polylogarithmic recourse, and the first streaming algorithms for covering and packing LPs with linear space and polylogarithmic passes.  
 There is a large literature on the question of the tradeoff between the optimal size of a shortcut set / hopset and the value of Î². In particular we highlight the most natural point on this tradeoff: what is the minimum value of Î², such that for any graph G  , there exists a Î²-shortcut set H  with O(n)  edges? Similarly, what is the minimum value of Î² such that there exists a ( Î², Îµ  )-hopset with O  ( n  ) edges? Not only is this a very natural structural question in its own right, but shortcuts sets / hopsets form the core of a large number of distributed, parallel, and dynamic algorithms for reachability / shortest paths.  
 Our result in this paper is to close the gap between shortcut sets and hopsets introduced by the result of Kogan and Parter. That is, we show that for any graph G  and any fixed Îµ  there is a ( Ã•  ( n  1/3  ), Îµ  ) hopset with O  ( n  ) edges. Our hopset improves upon the ( Ã•  (n 2/5  ), Îµ  ) hopset of Kogan and Parter. More generally, we achieve a smooth tradeoff between hopset size and Î²  which exactly matches the tradeoff of Kogan and Parter for the simpler problem of shortcut sets (up to polylog factors).  
 Additionally, using a very recent black-box reduction of Kogan and Parter, our new hopset immediately implies improved bounds for approximate distance preservers.  
 Abstract 
  PDF 
  Abstract   For an n  -vertex m  -edge digraph G  , a D-shortcut  is a small set H  of directed edges taken from the transitive closure of G  , satisfying that the diameter of G  âˆª H  is at most D  . In a sequence of works [Kogan and Parter, SODA 2022 & ICALP 2022] provided shortcut algorithms with improved diameter vs. size tradeoffs. In this paper, we present faster and unified shortcut algorithms for general digraphs. These algorithms also yield improved tradeoffs for the family of bounded-width DAGs. We show:  
 â€¢ A unified and faster shortcutting algorithm which implements the [KP,SODA'22] framework in almost optimal  time, conditioned on the combinatorial Boolean Matrix Multiplication (BMM) conjecture. For example, Ã•  ( n  1/3  )-shortcuts with Ã•  ( n  ) edges can be computed in time Ã•  ( n  1/3  Â· m  ); and Ã•  ( n  1/2  )- shortcuts with Ã•  ( n  3/4  ) edges can be computed in time Ã•  ( n  1/4  Â· m  ). This improves the time bounds of Ã•  ( n  1/3  Â· m  + n  3/2  ) and Ã•  ( n  1/4  Â· m  + n  7/4  ) respectively, by [KP, ICALP'22].  
 â€¢ An improved algorithm for computing a Minimum Chain Cover (MCC) of DAGs. For an n  -vertex m  -edge DAG G  of width k  , the algorithm runs in Ã•  (âˆšk Â· n  + m  1+ o  (1)  time. For sparse digraphs, we show faster Ã•  ( k  1/3  Â· n  + n  1+ o  (1)  )-time algorithms. This improves the time bounds of Ã•  ( n  3/2  + m  ) [KP, ICALP'22] and Ã•  ( k  2  Â· n  + m  ) [Caceres et al., SODA 2022].  
 â€¢ An MCC-based shortcut algorithm for DAGs with improved size and time bounds, as a function of the width k  . For example, providing a linear-size -âˆš k  -shortcut in time Ã•  (min{âˆš k  Â· m  + m  1+o(1)  , n  2  }), improving the general graph's size and time bounds for k  = o  ( n  2/3  ).  
 Abstract 
  PDF 
  Abstract   We introduce the notion of fair cuts  as an approach to leverage approximate ( s, t  )-mincut (equivalently ( s, t  )-maxflow) algorithms in undirected graphs to obtain near-linear time approximation algorithms for several cut problems. Informally, for any Î± â‰¥ 1, an Î±-fair ( s, t  )-cut is an ( s, t  )-cut such that there exists an ( s, t  )-flow that uses 1/Î± fraction of the capacity of every  edge in the cut. (So, any Î±-fair cut is also an Î±-approximate mincut, but not vice-versa.) We give an algorithm for (1 + Îµ)-fair ( s, t  )-cut in Ã•  ( m  )-time, thereby matching the best runtime for (1 + Îµ)-approximate ( s, t  )-mincut [Peng, SODA '16]. We then demonstrate the power of this approach by showing that this result almost immediately leads to several applications:  
 â€¢ the first almost-linear-work subpolynomial-depth parallel algorithms for computing (1+Îµ)-approximations for all-pairs maxflow values (again via an approximate Gomory-Hu tree) in unweighted graphs;  
 â€¢ the first near-linear time expander decomposition algorithm that works even when the expansion parameter is polynomially small; this subsumes previous incomparable algorithms [Nanongkai and Saranurak, FOCS '17; Wulff-Nilsen, FOCS '17; Saranurak and Wang, SODA '19].  
 Abstract 
  PDF 
  Abstract   This paper establishes the Price of Stability (PoS) for First Price Auctions, for all equilibrium concepts that have been studied in the literature: Bayesian Nash Equilibrium âŠŠ Bayesian Correlated Equilibrium âŠŠ Bayesian Coarse Correlated Equilibrium.  
 This result indicates that, in the worst cases, efficiency degradation depends not on different selections among Bayesian Nash Equilibria.  
 Abstract 
  PDF 
  Abstract   A patient seller aims to sell a good to an impatient buyer (i.e., one who discounts utility over time). The buyer will remain in the market for a period of time T  , and her private value is drawn from a publicly known distribution. What is the revenue-optimal pricing-curve (sequence of (price, time) pairs) for the seller? Is randomization of help here? Is the revenue-optimal pricing-curve computable in polynomial time? We answer these questions in this paper. We give an efficient algorithm for computing the revenue-optimal pricing curve. We show that pricing curves, that post a price at each point of time and let the buyer pick her utility maximizing time to buy, are revenue-optimal among a much broader class of sequential lottery mechanisms: namely, mechanisms that allow the seller to post a menu of lotteries at each point of time cannot get any higher revenue than pricing curves. We also show that the even broader class of mechanisms that allow the menu of lotteries to be adaptively set, can earn strictly higher revenue than that of pricing curves, and the revenue gap can be as big as the support size of the buyer's value distribution.  
 Solutions to the MMPHF problem are in widespread use in both theory and practice.  
 The best upper bound known for the problem encodes D in O(n  log log log u  ) bits and performs queries in O  (log u  ) time. It has been an open problem to either improve the space upper bound or to show that this somewhat odd looking bound is tight.  
 In this paper, we show the latter: any data structure (deterministic or randomized) for monotone minimal perfect hashing of any collection of n  elements from a universe of size u  requires Î©  ( n  Â· log log log u  ) expected bits to answer every query correctly.  
 We achieve our lower bound by defining a graph G where the nodes are the possible  
 Abstract 
  PDF 
  Abstract   This paper introduces a new data-structural object that we call the tiny pointer. In many applications, traditional log n  -bit pointers can be replaced with o  (log n  )-bit tiny pointers at the cost of only a constant-factor time overhead and a small probability of failure. We develop a comprehensive theory of tiny pointers, and give optimal constructions for both fixed-size tiny pointers (i.e., settings in which all of the tiny pointers must be the same size) and variable-size tiny pointers (i.e., settings in which the average tiny-pointer size must be small, but some tiny pointers can be larger). If a tiny pointer references an element in an array filled to load factor 1 â€” Î´, then the optimal tiny-pointer size is Î˜(log log log n  + log Î´ -1  ) bits in the fixed-size case, and Î˜(log Î´ -1  ) expected bits in the variable-size case.  
 Our tiny-pointer constructions also require us to revisit several classic problems having to do with balls and bins; these results may be of independent interest.  
 Using tiny pointers, we revisit five classic data-structure problems. We show that:  
  PDF 
 In this paper, we prove new bounds on the cost of Greedy in the â€œpattern avoidanceâ€ regime. Our new results include:  
 Abstract 
  PDF 
  Abstract   The pairing heap  , introduced by Fredman et al. [3], is a self-adjusting heap data structure that is both simple and efficient. A variant introduced in the same paper is the multipass pairing heap  . Standard pairing heaps do just two linking passes during delete-min, a pairing pass  and an assembly pass  . In contrast, multipass pairing heaps do repeated pairing passes, in which nodes are linked in adjacent pairs, until only a minimum-key node remains.  
 We obtain the following amortized time bounds for operations on n  -item multipass pairing heaps: O(log n  ) for delete-min and delete; O(log log n  log log log n  ) for decrease-key; and O(1) for all other heap operations, including insert and meld. This is the first analysis giving an O(log n  ) bound for delete-min. Our analysis is tight for all operations except possibly decrease-key, for which Fredman [2] and separately Iacono and Ozkan [6] proved an Î©(log log n  ) lower bound.  
  Abstract   We prove that a uniformly random automaton with n  states on a 2-letter alphabet has a synchronizing word of length  
 with high probability (w.h.p.). That is to say, w.h.p. there exists a word Ï‰  of such length, and a state v 0   , such that Ï‰  sends all states to v  0  . This confirms a conjecture of Kisielewicz, Kowalski, SzykuÅ‚a [KKS13] based on numerical simulations, up to a log factor - the previous best partial result towards the conjecture was the quasilinear bound O(n  log 3  n  ) due to Nicaud [Nic19]. Moreover, the synchronizing word Ï‰  we obtain has small entropy, in the sense that it can be encoded with only O  (log( n  )) bits w.h.p..  
 Our proof introduces the concept of Ï‰  -trees, for a word Ï‰  , that is, automata in which the Ï‰  -transitions induce a (loop-rooted) tree. We prove a strong structure result that says that, w.h.p., a random automaton on n  states is a Ï‰  -tree for some word Ï‰  of length at most (1 + Îµ) log 2  ( n  ), for any Îµ > 0. The existence of the (random) word Ï‰  is proved by the probabilistic method. This structure result is key to proving that a short synchronizing word exists.  
 Abstract 
  PDF 
  Abstract   The well-known trace reconstruction problem  is the problem of inferring an unknown source string x  âˆˆ {0,1} n   from independent â€œtracesâ€, i.e. copies of x  that have been corrupted by a Î´-deletion channel which independently deletes each bit of x  with probability Î´ and concatenates the surviving bits. The current paper considers the extreme data-limited regime in which only a single trace is provided to the reconstruction algorithm. In this setting exact reconstruction is of course impossible, and the question is to what accuracy the source string x  can be approximately reconstructed.  
 We give a detailed study of this question, providing algorithms and lower bounds for the high, intermediate, and low deletion rate regimes in both the worst-case ( x  is arbitrary) and average-case ( x  is drawn uniformly from {0,1} n   ) models. In several cases the lower bounds we establish are matched by computationally efficient algorithms that we provide.  
 We highlight our results for the high deletion rate regime: roughly speaking, they show that  
  Abstract   The independence polynomial of a graph is the generating polynomial of all its independent sets. Formally, given a graph G  , its independence polynomial Z G   (Î») is given by Î£ I   Î» | I  |  , where the sum is over all independent sets I  of G  . The independence polynomial has been an important object of study in both combinatorics and computer science. In particular, the algorithmic problem of estimating Z G  (Î»)  for a fixed positive Î» on an input graph G  is a natural generalization of the problem of counting independent sets, and its study has led to some of the most striking connections between computational complexity and the theory of phase transitions. More surprisingly, the independence polynomial for negative and complex values of Î» also turns out to be related to problems in statistical physics and combinatorics. In particular, the locations of the complex roots of the independence polynomial of bounded degree graphs turn out to be very closely related to the LovÃ¡sz local lemma, and also to the questions in the computational complexity of counting. Consequently, the locations of such zeros have been studied in many works. In this direction, it is known from the work of Shearer [29] and of Scott and Sokal [27] - inspired by the study of the LovÃ¡sz local lemma - that the independence polynomial Z G   (Î») of a graph G  of maximum degree at most d  + 1 does not vanish provided that  
 . Significant extensions of this result have recently been given in the case when Î» is in the right  half-plane (i.e., when â„œÎ» â‰¥ 0) by Peters and Regts [26] and Bencs and CsikvÃ¡ri [9]. In this paper, our motivation is to further extend these results to find new zero free regions not only in the right half plane, but also in the left  half-plane, that is, when â„œÎ» â‰¤ 0.  
 We give new geometric criterions for establishing zero-free regions as well as for carrying out semi-rigorous numerical explorations. We then provide two examples of the (rigorous) use of these criterions, by establishing two new zero-free regions in the left-half plane. We also extend the results of Bencs and CsikvÃ¡ri [9] for the right half-plane using our framework. By a direct application of the interpolation method of Barvinok [5], combined with extensions due to Patel and Regts [25], our results also imply deterministic polynomial time approximation algorithms for the independence polynomial of bounded degree graphs in the new zero-free regions.  
  Abstract   Given disjoint sets S  , S' âŠ† ð”½ q   of size n  and a function f  : S  â†’ ð”½ q   , where ð”½ q   is a finite field, the low-degree extension  (LDE) of f  to S'  is the function f  '  : S  '  â†’ ð”½ q   obtained by restricting the interpolating polynomial of f  to S'  .  
 LDE computation is a fundamental primitive of modern algebraic coding theory and cryptography. The best asymptotic running time for LDE with parameter n  is O(n  log n  ) arithmetic operations over ð”½ q   - when q  and the sets S, S'  are special. This running time is achieved via the Fast Fourier Transform (FFT), and requires ð”½ q   to contain a multiplicative subgroup of smooth order â‰¥ n  (smoothness means being the product of small primes). Another variant uses an additive subgroup of smooth order â‰¥ n  . Most finite fields do not contain such a subgroup, which raises the question of computing the LDE in time O(n  Â· log n  ) over general finite fields, for some disjoint pair of sets S  , S '   of size n  .  
 The main result of this paper is a positive answer to this question, presenting O(n  log n  )-time LDE for special S  , S  '  shown to exist over all fields, as long as q  = Î©( n  2  ). This result is achieved by introducing a new FFT-like transform, the Elliptic Curve Fast Fourier Transform (ECFFT), which gives an approach to fast algorithms (using preprocessing) for polynomial operations over all large finite fields. The key idea is to replace the group of roots of unity with a set of points L  âŠ‚ ð”½ q   suitably related to a well-chosen elliptic curve group over ð”½ q   (the set L  itself is not  a group). The key advantage of this approach is that elliptic curve groups can be of any  size in the Hasse-Weil interval  
  PDF 
  Abstract   We study the problem of testing whether a function f  : â„ n   â†’ â„ is a polynomial of degree at most d  in the distribution-free  testing model. Here, the distance between functions is measured with respect to an unknown distribution D  over â„ n   from which we can draw samples. In contrast to previous work, we do not assume that D  has finite support.  
 We design a tester that given query access to f  , and sample access to D  , makes poly( d  /Îµ) many queries to f  , accepts with probability 1 if f  is a polynomial of degree d  , and rejects with probability at least 2/3 if every degree- d  polynomial P  disagrees with f  on a set of mass at least Îµ with respect to D  . Our result also holds under mild assumptions when we receive only a polynomial number of bits of precision for each query to f  , or when f  can only be queried on rational points representable using a logarithmic number of bits. Along the way, we prove a new stability theorem for multivariate polynomials that may be of independent interest.  
  PDF 
 The end-goal of our research is to prove a similar lower bound for multi-pass  streaming algorithms that guarantee a better-than-2 approximation for Max-Cut, a highly challenging open problem. In this paper, we take a significant step in this direction, showing that even o  (log n  )-pass streaming algorithms need n  Î©(1)  space to solve the cycle-finding problem. Our proof is quite involved, dividing the cycles in the graph into â€œshortâ€ and â€œlongâ€ cycles, and using tailor-made lower bound techniques to handle each case.  
  Full Access    
  PDF 
 Motivated by the rising interest in learning hierarchical structures in large networks, in this paper we introduce ( k  , Î³)- hierarchically clusterable  graphs, a natural hierarchical analog of classical ( k  , Îµ)-clusterable graphs; intuitively, these are graphs that exhibit pronounced hierarchical structure. We give a hierarchical clustering oracle  for this model, i.e. a small space data structure that provides query access to a good hierarchical clustering at cost â‰ˆ poly( k  ) Â· n  1/2+ O  (Î³)  per query; notably, the dependence on k  is polynomial, in contrast to best known flat clustering oracles. The result relies on several structural properties of hierarchically clusterable graphs that we hope will be of independent interest in sublinear time spectral graph algorithms.  
  Full Access    
  PDF 
 In this paper, we show that the MFN relaxation has an integrality gap at most  
 for the CFL problem. This narrows down the range of the integrality gap to one digit. Our ingredient is an iterative rounding algorithm for this sophisticated LP relaxation.  
 with lim k  â†’ âˆž  âˆŠ k   =0 [10]. She further showed her result to be asymptotically best possible in that no algorithm considering local improvements of logarithmically bounded size with respect to some fixed power of the weight function can yield an approximation guarantee better than     
 [10].  
 In this paper, we finally show how to beat the threshold of  
 for the weighted k  -Set Packing problem by Î©( k  ). We achieve this by combining local search with the application of a black box algorithm for the unweighted k  -Set Packing problem to carefully chosen sub-instances. In doing so, we do not only manage to link the approximation ratio for general weights to the one achievable in the unweighted case: In contrast to previous works, which yield an improvement over Berman's long-standing result of     
 due to Neuwohner.  
 Our algorithm is based on the local search procedure of Berman that attempts to improve the sum of squared weights rather than the problem's objective. When using exchanges of size at most k  , this algorithm attains an approximation factor of  
 Abstract 
  PDF 
  Abstract   In this paper, we study problems of connecting classes of points via noncrossing structures. Given a set of colored terminal points, we want to find a graph for each color that connects all terminals of its color with the restriction that no two graphs cross each other. We consider these problems both on the Euclidean plane and in planar graphs.  
 On the negative side, we show that the problem of connecting terminal pairs with noncrossing paths is NP-hard on the Euclidean plane, and that the problem of finding two noncrossing spanning trees is NP-hard in plane graphs.  
  PDF 
  Abstract   Map matching is a common preprocessing step for analysing vehicle trajectories. In the theory community, the most popular approach for map matching is to compute a path on the road network that is the most spatially similar to the trajectory, where spatial similarity is measured using the FrÃ©chet distance. A shortcoming of existing map matching algorithms under the FrÃ©chet distance is that every time a trajectory is matched, the entire road network needs to be reprocessed from scratch. An open problem is whether one can preprocess the road network into a data structure, so that map matching queries can be answered in sublinear time.  
 In this paper, we investigate map matching queries under the FrÃ©chet distance. We provide a negative result for geometric planar graphs. We show that, unless SETH fails, there is no data structure that can be constructed in polynomial time that answers map matching queries in O((pq)  1-Î´  ) query time for any Î´ > 0, where p  and q  are the complexities of the geometric planar graph and the query trajectory, respectively. We provide a positive result for realistic input graphs, which we regard as the main result of this paper. We show that for c  -packed graphs, one can construct a data structure of Ã•(cp)  size that can answer (1 + Îµ)-approximate map matching queries in Ã•  ( c 4  q  log 4  p  ) time, where Ã•  (Â·) hides lower-order factors and dependence of Îµ.  
 Abstract 
  PDF 
  Abstract   We provide the first sub-linear space and sub-linear regret algorithm for online learning with expert advice (against an oblivious adversary), addressing an open question raised recently by Srinivas, Woodruff, Xu and Zhou (STOC 2022). We also demonstrate a separation between oblivious and (strong) adaptive adversaries by proving a linear memory lower bound of any sub-linear regret algorithm against an adaptive adversary. Our algorithm is based on a novel pool selection procedure that bypasses the traditional wisdom of leader selection for online learning, and a generic reduction that transforms any weakly sub-linear regret o  ( T  ) algorithm to T  1-Î±  regret algorithm, which may be of independent interest. Our lower bound utilizes the connection of no-regret learning and equilibrium computation in zero-sum games, leading to a proof of a strong lower bound against an adaptive adversary.  
 Abstract 
  PDF 
  Abstract   Given a graph with edges colored red or blue and an integer k  , the exact perfect matching  problem asks if there exists a perfect matching with exactly k  red edges. There exists a randomized polylogarithmic-time parallel algorithm to solve this problem, dating back to the eighties, but no deterministic polynomial-time algorithm is known, even for bipartite graphs. In this paper we show that there is no sub-exponential sized linear program that can describe the convex hull of exact matchings in bipartite graphs. In fact, we prove something stronger, that there is no sub-exponential sized linear program to describe the convex hull of perfect matchings with an odd number of red edges.  
  Full Access    
  PDF 
  Abstract   A recent breakthrough in Edmonds' problem showed that the noncommutative rank can be computed in deterministic polynomial time, and various algorithms for it were devised. However, only quite complicated algorithms are known for finding a so-called shrunk subspace, which acts as a dual certificate for the value of the noncommutative rank. In particular, the operator Sinkhorn algorithm, perhaps the simplest algorithm to compute the noncommutative rank with operator scaling, does not find a shrunk subspace. Finding a shrunk subspace plays a key role in applications, such as separation in the Brascamp-Lieb polytope, one-parameter subgroups in the null-cone membership problem, and primal-dual algorithms for matroid intersection and fractional matroid matching.  
 In this paper, we provide a simple Sinkhorn-style algorithm to find the smallest shrunk subspace over the complex field in deterministic polynomial time. To this end, we introduce a generalization of the operator scaling problem, where the spectra of the marginals must be majorized by specified vectors. Then we design an efficient Sinkhorn-style algorithm for the generalized operator scaling problem. Applying this to the shrunk subspace problem, we show that a sufficiently long run of the algorithm also finds an approximate shrunk subspace close to the minimum exact shrunk subspace. Finally, we show that the approximate shrunk subspace can be rounded if it is sufficiently close. Along the way, we also provide a simple randomized algorithm to find the smallest shrunk subspace.  
 As applications, we design a faster algorithm for fractional linear matroid matching and efficient weak membership and optimization algorithms for the rank-2 Brascamp-Lieb polytope.  
 Abstract 
  PDF 
  Abstract   The diameter of the graph of a d  -dimensional lattice polytope P  âŠ† [0, k  ] n   is known to be at most dk  due to work by Kleinschmidt and Onn. However, it is an open question whether the monotone diameter, the shortest guaranteed length of a monotone path, of a d  -dimensional lattice polytope P  = { x  : A  x  â‰¤ b  } âŠ† [0, k  ] n   is bounded by a polynomial in d  and k  . This question is of particular interest in linear optimization, since paths traced by the Simplex method must be monotone. We introduce partial results in this direction including a monotone diameter bound of 3 d  for k  = 2, a monotone diameter bound of ( d  â€” 1) m  + 1 for d  -dimensional (â„“ + 1)-level polytopes, a pivot rule such that the Simplex method is guaranteed to take at most dnk  || A  || âˆž  non-degenerate steps to solve a LP on P  , and a bound of dk  for lengths of paths from certain fixed starting points. Finally, we present a constructive approach to a diameter bound of (3/2) dk  and describe how to translate this final bound into an algorithm that solves a linear program by tracing such a path.  
  Full Access    
 Abstract 
  PDF 
  Abstract   In this paper, we study the minimum k  -partition problem of submodular functions, i.e., given a finite set V  and a submodular function f  : 2 V   â†’ â„, computing a k  -partition { V  1  ,â€¦, V k   } of V  with minimum  
 . The problem is a natural generalization of the minimum k  -cut problem in graphs and hypergraphs. It is known that the problem is NP-hard for general k  , and solvable in polynomial time for k  â‰¤ 3. In this paper, we construct the first polynomial-time algorithm for the minimum 4-partition problem.  
 *  Authors are ordered alphabetically.  
 Abstract 
  PDF 
  Abstract   In this work, we prove new bounds on the additive gap between the value of a random integer program max c  T  x, Ax  â‰¤ b  , x  âˆˆ {0,1} n   with m  constraints and that of its linear programming relaxation for a wide range of distributions on ( A,b,c  ). Our investigation is motivated by the work of Dey, Dubey, and Molinaro (SODA'21), who gave a framework for relating the size of Branch-and-Bound (B&B) trees to additive integrality gaps.  
 Our main technical contribution and the key to achieving the above results is a new linear discrepancy theorem for random matrices. Our theorem gives general conditions under which a target vector is equal to or very close to a {0,1} combination of the columns of a random matrix A  . Compared to prior results, our theorem handles a much wider range of distributions on A  , both continuous and discrete, and achieves success probability exponentially close to 1, as opposed to the constant probability shown in earlier results. Our proof uses a Fourier analytic approach, building on the work of Hoberg and Rothvoss (SODA '19) and Franks and Saks (RSA '20) who studied the discrepancy of random set systems and matrices respectively.  
  Full Access    
 , assuming m  â‰¤ n  . In this paper, we improve this result with an O  (Î±)-approximate algorithm that runs in O((n  + mn  /Î±) log 3  n  ) time for any Î± âˆˆ [1, n  ], assuming m  â‰¤ n  and constant dimension d  .  
  PDF 
  Abstract   Coverings of convex bodies have emerged as a central component in the design of efficient solutions to approximation problems involving convex bodies. Intuitively, given a convex body K  and Îµ > 0, a covering  is a collection of convex bodies whose union covers K  such that a constant factor expansion of each body lies within an Îµ expansion of K  . Coverings have been employed in many applications, such as approximations for diameter, width, and Îµ-kernels of point sets, approximate nearest neighbor searching, polytope approximations with low combinatorial complexity, and approximations to the Closest Vector Problem (CVP).  
 It is known how to construct coverings of size n O(n)  /Îµ (n-1)/2   for general convex bodies in â„ n   . In special cases, such as when the convex body is the â„“ p   unit ball, this bound has been improved to 2 O(n   ) /Îµ ( n   -1)/2  . This raises the question of whether such a bound generally holds. In this paper we answer the question in the affirmative.  
 We demonstrate the power and versatility of our coverings by applying them to the problem of approximating a convex body by a polytope, where the error is measured through the Banach-Mazur metric. Given a well-centered convex body K  and an approximation parameter Îµ > 0, we show that there exists a polytope P  consisting of 2 O(n)   /Îµ ( n  -1)/2  vertices (facets) such that K  âŠ‚ P  âŠ‚ K  (1 + Îµ). This bound is optimal in the worst case up to factors of 2 O  ( n  )  . (This bound has been established recently using different techniques, but our approach is arguably simpler and more elegant.) As an additional consequence, we obtain the fastest (1 + Îµ)-approximate CVP algorithm that works in any norm, with a running time of 2 O(n)   /Îµ ( n  -1)/2  up to polynomial factors in the input size, and we obtain the fastest (1 + Îµ)-approximation algorithm for integer programming. We also present a framework for constructing coverings of optimal size for any convex body (up to factors of 2 O(n)   ).  
 Abstract 
  PDF 
  Abstract   In the orthogonal range reporting problem we must pre-process a set P  of multi-dimensional points, so that for any axis-parallel query rectangle q  all points from q  âˆ© P  can be reported efficiently. In this paper we study the query complexity of multi-dimensional orthogonal range reporting in the pointer machine model. We present a data structure that answers four-dimensional orthogonal range reporting queries in almost-optimal time O  (log n  log log n  + k  ) and uses O(n  log 4  n  ) space, where n  is the number of points in P  and k  is the number of points in q  âˆ© P  . This is the first data structure with nearly-linear space usage that achieves almost-optimal query time in 4d. This result can be immediately generalized to d  â‰¥ 4 dimensions: we show that there is a data structure supporting d  -dimensional range reporting queries in time O  (log d   -3  n  log log n  + k  ) for any constant d  â‰¥ 4.  
 Abstract 
  PDF 
  Abstract   Thorup [FOCS'01, JACM'04] and Klein [SODA'01] independently showed that there exists a (1 + Îµ)-approximate distance oracle for planar graphs with O  ( n  (log n  )Îµ -1  ) space and O  (Îµ -1  ) query time. While the dependency on n  is nearly linear, the space-query product of their oracles depend quadratically  on 1/Îµ. Many follow-up results either improved the space or the query time of the oracles while having the same, sometimes worst, dependency on 1/Îµ. Kawarabayashi, Sommer, and Thorup [SODA'13] were the first to improve the dependency on 1/Îµ from quadratic to nearly linear  (at the cost of log *  ( n  ) factors). It is plausible to conjecture that the linear dependency on 1/Îµ is optimal: for many known distance-related problems in planar graphs, it was proved that the dependency on 1/Îµ is at least linear.  
 In this work, we disprove this conjecture by reducing the dependency of the space-query product on 1/Îµ from linear all the way down to subpolynomial  (1/Îµ) o  (1)  . More precisely, we construct an oracle with O(n log( n  )(Îµ - o  (1)  + log *  n  )) space and log 2+ o  (1)  (1/Îµ) query time. Our construction is the culmination of several different ideas developed over the past two decades.  
 Abstract 
  PDF 
  Abstract   In this paper, we investigate two variants of the secretary problem. In these variants, we are presented with a sequence of numbers X i   that come from distributions D i   , and that arrive in either random or adversarial order. We do not know what the distributions are, but we have access to a single sample Y i   from each distribution D i   . After observing each number, we have to make an irrevocable decision about whether we would like to accept it or not with the goal of maximizing the probability of selecting the largest number.  
 Abstract 
  PDF 
  Abstract   For numerous online bipartite matching problems, such as edge-weighted matching and matching under two-sided vertex arrivals, the state-of-the-art fractional algorithms outperform their randomized integral counterparts. This gap is surprising, given that the bipartite fractional matching polytope is integral, and so lossless rounding is possible. This gap was explained by Devanur et al. (SODA'13), who showed that online  lossless rounding is impossible.  
 Despite the above, we initiate the study of lossless online rounding for online bipartite matching problems. Our key observation is that while lossless online rounding is impossible in general  , randomized algorithms induce fractional algorithms of the same competitive ratio which by definition are losslessly roundable online. This motivates the addition of constraints that decrease the â€œonline integrality gapâ€, thus allowing for lossless online rounding. We characterize a set of non-convex constraints which allow for such lossless online rounding, and better competitive ratios than yielded by deterministic algorithms.  
 As applications of our lossless online rounding approach, we obtain two results of independent interest: (i) a doubly-exponential improvement, and a sharp threshold for the amount of randomness (or advice) needed to outperform deterministic online (vertex-weighted) bipartite matching algorithms, and (ii) an optimal semi-OCS, matching a recent result of Gao et al. (FOCS'21) answering a question of Fahrbach et al. (FOCS'20).  
  Abstract   Expander graphs play a central role in graph theory and algorithms. With a number of powerful algorithmic tools developed around them, such as the Cut-Matching game, expander pruning, expander decomposition, and algorithms for decremental All-Pairs Shortest Paths (APSP) in expanders, to name just a few, the use of expanders in the design of graph algorithms has become ubiquitous. Specific applications of interest to us are fast deterministic algorithms for cut problems in static graphs, and algorithms for dynamic distance-based graph problems, such as APSP.  
 Unfortunately, the use of expanders in these settings incurs a number of drawbacks. For example, the best currently known algorithm for decremental APSP in constant-degree expanders can only achieve a (log n  ) O  (1/Îµ2)  -approximation with n  1+ O  (Îµ)  total update time for any Îµ. All currently known algorithms for the Cut Player in the Cut-Matching game are either randomized, or provide rather weak guarantees: expansion 1/(log n  ) 1/Îµ  with running time n  1+ O  (Îµ)  . This, in turn, leads to somewhat weak algorithmic guarantees for several central cut problems: the best current almost linear time deterministic algorithms for Sparsest Cut, Lowest Conductance Cut, and Balanced Cut can only achieve approximation factor (log n  ) Ï‰(1)  . Lastly, when relying on expanders in distance-based problems, such as dynamic APSP, via current methods, it seems inevitable that one has to settle for approximation factors that are at least Î©(log n  ). In contrast, current negative results do not rule out algorithms that achieve any superconstant approximation factor with almost linear total update time.  
 In this paper we propose the use of well-connected graphs, and introduce a new algorithmic toolkit for such graphs that, in a sense, mirrors the above mentioned algorithmic tools for expanders. One of these new tools is the Distanced Matching game, an analogue of the Cut-Matching game for well-connected graphs. We demonstrate the power of these new tools by obtaining better results for several of the problems mentioned above. First, we design an algorithm for decremental APSP in expanders with significantly better guarantees: in a constant-degree expander, the algorithm achieves (log n  ) 1+ o  (1)  -approximation, with total update time n  1+ o  (1)  . We also obtain a deterministic algorithm for the Cut Player in the Cut-Matching game that achieves expansion  
 in time n  1+ o  (1)  , deterministic almost linear-time algorithms for Sparsest Cut, Lowest-Conductance Cut, and Minimum Balanced Cut with approximation factors O  (poly log n  ), as well as improved deterministic algorithm for Expander Decomposition. We believe that the use of well-connected graphs instead of expanders in various dynamic distance-based problems (such as APSP in general graphs) has the potential of providing much stronger guarantees, since we are no longer necessarily restricted to superlogarithmic approximation factors.  
 Abstract 
  PDF 
  Abstract   Two simple undirected graphs are cospectral  if their respective adjacency matrices have the same multiset of eigenvalues. Cospectrality yields an equivalence relation on the family of graphs which is provably weaker than isomorphism. In this paper, we study cospectrality in relation to another well-studied relaxation of isomorphism, namely k  -dimensional Weisfeiler-Leman ( k  -WL) indistinguishability.  
 Abstract 
  PDF 
  Abstract   In this paper we study the fundamental problem of finding small dense subgraphs in a given graph. For a real number s  > 2, we prove that every graph on n  vertices with average degree at least d  contains a subgraph of average degree at least s  on at most  
 vertices. This is optimal up to the polylogarithmic factor, and resolves a conjecture of Feige and Wagner.  
 â€   The first version of this paper proved a weaker statement of Theorem 1.2 with 4 commodities. The current statement has only 3 commodities, and now fully refutes Seymour's conjectures. In addition, the current version describes implications to the 0-extension problem, see Section 1.4.  
  Full Access    
  PDF 
  Abstract   The basic goal of survivable network design is to construct low-cost networks which preserve a sufficient level of connectivity despite the failure or removal of a few nodes or edges. One of the most basic problems in this area is the 2-Edge-Connected Spanning Subgraph problem (2-ECSS): given an undirected graph G  , find a 2-edge-connected spanning subgraph H  of G  with the minimum number of edges (in particular, H  remains connected after the removal of one arbitrary edge).  
 2-ECSS is NP-hard and the best-known (polynomial-time) approximation factor for this problem is 4/3. Interestingly, this factor was achieved with drastically different techniques by [HunkenschrÃ¶der, Vempala and Vetta '00,'19] and [SebÃ¶ and Vygen, '14]. In this paper we present an improved  
 approximation for 2-ECSS.  
 Abstract 
  PDF 
  Abstract   This paper presents significantly improved deterministic algorithms for some of the key problems in the area of distributed graph algorithms, including network decomposition, hitting sets, and spanners. As the main ingredient in these results, we develop novel randomized distributed algorithms that we can analyze using only pairwise independence, and we can thus derandomize efficiently. As our most prominent end-result, we obtain a deterministic construction for O  (log n  )-color O  (log n  Â· log log log n  )- strong diameter network decomposition in Ã•  (log 3  n  ) rounds. This is the first construction that achieves almost log n  in both parameters, and it improves on a recent line of exciting progress on deterministic distributed network decompositions [RozhoÅˆ, Ghaffari STOC'20; Ghaffari, Grunau, RozhoÅˆ SODA'21; Chang, Ghaffari PODC'21; Elkin, Haeupler, RozhoÅˆ, Grunau FOCS'22].  
  PDF 
  Abstract   The Cube versus Cube test is a variant of the well-known Plane versus Plane test of Raz and Safra [10], in which to each 3-dimensional affine subspace C  of ð”½ n   q   , a polynomial of degree at most d  , T  ( C  ), is assigned in a somewhat locally consistent manner: taking two cubes C  1  , C  2  that intersect in a plane uniformly at random, the probability that T  ( C  1  ) and T  ( C  2  ) agree on C  1  âˆ© C  2  is at least some Îµ. An element of interest is the soundness threshold of this test, i.e. the smallest value of Îµ, such that this amount of local consistency implies a global structure; namely, that there is a global degree d  function g  such that g| C  = T (C)  for at least Î©(Îµ) fraction of the cubes.  
 We show that the cube versus cube low degree test has soundness poly( d  )/ q  . This result achieves the optimal dependence on q  for soundness in low degree testing and improves upon previous soundness results of poly( d  )/ q  1/2  due to Bhangale, Dinur and Navon [4].  
  Full Access    
 Abstract 
  PDF 
  Abstract   The starting point of this paper is the problem of scheduling n  jobs with processing times and due dates on a single machine so as to minimize the total processing time of tardy jobs, i.e., 1 || Î£ p j  U j   . This problem was identified by Bringmann et al. (Algorithmica 2022) as a natural subquadratic-time special case of the classic 1 || Î£ w j  U j   problem, which likely requires time quadratic in the total processing time P  , because of a fine-grained lower bound. Bringmann et al. obtain their Ã•  ( P  7/4  ) time scheduling algorithm through a new variant of convolution, dubbed Max-Min Skewed Convolution, which they solve in Ã•  ( n  7/4  ) time. Our main technical contribution is a faster and simpler convolution algorithm running in Ã•  ( n  5/3  ) time. It implies an Ã•  ( P  5/3  ) time algorithm for 11 | Î£ p j  U j   , but may also be of independent interest.  
 Inspired by recent developments for the Subset Sum and Knapsack problems, we study 1 || Î£ p j  U j   parameterized by the maximum job processing time p  max  . With proximity techniques borrowed from integer linear programming (ILP), we show structural properties of the problem that, coupled with a new dynamic programming formulation, lead to an Ã•  ( n  + p  3  max  ) time algorithm. Moreover, in the setting with multiple machines, we use similar techniques to get an n  Â· p  O(m)   max  time algorithm for Pm  || Î£ p j  U j   .  
 Finally, we point out that the considered problems exhibit a particular triangular block structure in the constraint matrices of their ILP formulations. In light of recent ILP research, a question that arises is whether one can devise a generic algorithm for such a class of ILPs. We give a negative answer to this question: we show that already a slight generalization of the structure of the scheduling ILP leads to a strongly NP-hard problem.  
  Abstract   Knapsack and Partition  are two important additive problems whose fine-grained complexities in the (1 â€” Îµ)-approximation setting are not yet settled. In this work, we make progress on both problems by giving improved algorithms.  
 â€¢ Knapsack  can be (1 â€” Îµ)-approximated in Ã•  ( n  + (1/Îµ) 2.2  ) time, improving the previous Ã•  ( n  + (1/Îµ) 2.25  ) by Jin (ICALP'19). There is a known conditional lower bound of ( n  + 1/Îµ) 2- Î¿  (1)  based on (min, +)- convolution hypothesis.  
 â€¢ Partition  can be (1 â€” Îµ)-approximated in Ã•  ( n  + (1/Îµ) 1.25  ) time, improving the previous Ã•  ( n  + (1/Îµ) 1.5  ) by Bringmann and Nakos (SODA'21). There is a known conditional lower bound of (1/Îµ) 1- Î¿  (1)  based on Strong Exponential Time Hypothesis.  
 Abstract 
  PDF 
  Abstract   Unbounded SubsetSum is a classical textbook problem: given integers w  1  , w  2  , â€¦, w n   âˆˆ[1, u  ], c,u  , we need to find if there exists m 1  ,m 2   , â€¦, m n   âˆˆ â„• satisfying c =Î£ n  i=1   w i  m i   . In its all-target version, t  âˆˆ â„¤ +  is given and the answers for all integers c  âˆˆ [0, t  ] are required. In this paper, we study three generalizations of this simple problem: All-Target Unbounded Knapsack, All-Target CoinChange and Residue Table. With new combinatorial insights into the structures of solutions, we present a novel two-phase approach. As a result, we show that:  
  Abstract   A vertex set S  in a graph G  is a minimal separator  if there exist vertices u  and v  that are in distinct connected components of G  â€” S  , but in the same connected component of G  â€” S  ' for every S  ' âŠ‚ S  . A class F  of graphs is called tame  if there exists a constant c  so that every graph in F  on n  vertices contains at most O(n c   ) minimal separators. If there exists a constant c  so that every graph in F  on n  vertices contains at most O(n c   log n   ) minimal separators the class is strongly-quasi-tame  . If there exists a constant c  > 1 so that F  contains n  -vertex graphs with at least c n   minimal separators for arbitrarily large n  then F  is called feral. The classification of graph classes into tame or feral has numerous algorithmic consequences, and has recently received considerable attention.  
 A key graph-theoretic object in the quest for such a classification is the notion of a k-creature  . A k  -creature consists of 4 disjoint vertex sets A,B,X  = { x 1  ,â€¦, x k   }, Y  = { y  1  ,â€¦ y k   } such that: (a) A  and B  are connected, (b) there are no edges from A  to Y  âˆª B  and no edges from B  to X  âˆª A  , ( c  ) A  dominates X  (every vertex in X  has a neighbor in A  ) and B  dominates Y  and ( d  ) X i  y i   is an edge if and only if i  = j  . It is easy to verify that a k  -creature contains at least 2 k   minimal separators. On the other hand, in a recent article Abrishami et al. [1] conjecture that every hereditary class F  that excludes k  -creatures for some fixed constant k  is tame.  
 In this paper we first give a counterexample to the conjecture of Abrishami et al. Our main result is a proof of a weaker form of their conjecture. More concretely, we prove that a hereditary class F  is strongly quasi-tame if it excludes k  -creatures for some fixed constant k  and additionally every minimal separator can be dominated by another fixed constant k  ' number of vertices. The tools developed on the way lead to a number of additional results of independent interest.  
 (i) We obtain a complete classification of all hereditary graph classes defined by a finite set of forbidden induced subgraphs into strongly quasi-tame or feral. This substantially generalizes a recent result of Milanic and Pivac [18], who classified all hereditary graph classes defined by a finite set of forbidden induced subgraphs on at most 4 vertices into tame or feral. (ii) We show that every hereditary class that excludes k  -creatures and additionally excludes all cycles of length at least c  , for some constant c  , is tame. This generalizes the result of Chudnovsky et al. [6] who obtained the same statement for c  =5. (iii) We show that every hereditary class that excludes k  -creatures and additionally excludes a complete graph on c  vertices for some fixed constant c  is tame.  
  PDF 
  Abstract   A class F  of graphs is called tame if every graph in F  on n  vertices contains at most n O(1)   minimal separators, quasi-tame  if every graph in F  on n  vertices contains at most 2 log O  (1)  ( n  )  minimal separators, and feral if there exists a constant c  > 1 so that F  contains n  -vertex graphs with at least c n   minimal separators for arbitrarily large n  . The classification of graph classes into (quasi-) tame or feral has numerous algorithmic consequences, and has recently received considerable attention.  
 In this paper we precisely characterize the structure of graphs which have few minimal separators. Specifically we show that every graph which excludes certain graphs called k-creatures  and k-critters  as induced subgraphs has at most quasi-polynomially many minimal separators. We then demonstrate that this sufficient condition for having few minimal separators is the â€œrightâ€ one. In particular we show that every hereditary graph class F  definable in CMSO logic that contains k  -creatures or k  -critters for every k  is feral.  
  Full Access    
  PDF 
  Abstract   The Strong Exponential Time Hypothesis (SETH) asserts that for every Îµ > 0 there exists k  such that k  -SAT requires time (2 â€” Îµ) n   . The field of fine-grained complexity has leveraged SETH to prove quite tight conditional lower bounds for dozens of problems in various domains and complexity classes, including Edit Distance, Graph Diameter, Hitting Set, Independent Set, and Orthogonal Vectors. Yet, it has been repeatedly asked in the literature whether SETH-hardness results can be proven for other fundamental problems such as Hamiltonian Path, Independent Set, Chromatic Number, MAX- k  -SAT, and Set Cover.  
 In this paper, we show that fine-grained reductions implying even Î» n   -hardness of these problems from SETH for any Î» > 1, would imply new circuit lower bounds: super-linear lower bounds for Boolean series-parallel circuits or polynomial lower bounds for arithmetic circuits (each of which is a four-decade open question).  
 We also extend this barrier result to the class of parameterized problems. Namely, for every Î» > 1, we conditionally rule out fine-grained reductions implying SETH-based lower bounds of Î» k  :  for a number of problems parameterized by the solution size k  .  
 Our main technical tool is a new concept called polynomial formulations. In particular, we show that many problems can be represented by relatively succinct low-degree polynomials, and that any problem with such a representation cannot be proven SETH-hard (without proving new circuit lower bounds).  
 Abstract 
  PDF 
  Abstract   In this paper, we prove that it is W[2]-hard to approximate k-SETCOVER within any constant ratio. Our proof is built upon the recently developed threshold graph composition technique. We propose a strong notion of threshold graphs and use a new composition method to prove this result. Our technique could also be applied to rule out polynomial time  
 ratio approximation algorithms for the non-parameterized k  -SETCOVER problem with k  as small as     
 Abstract 
  PDF 
  Abstract   In a seminal paper (Moser and Tardos, JACM'10), Moser and Tardos developed a simple and powerful algorithm to find solutions to constraint satisfaction problems. Kolipaka and Szegedy (Kolipaka and Szegedy, STOC'11) proved that the Moser-Tardos algorithm is efficient up to the tight condition of the abstract  LovÃ¡sz Local Lemma, known as Shearer's bound. A fundamental problem around the LLL is whether the efficient region of the Moser-Tardos algorithm can be further extended.  
 In this paper, we give a positive answer to this problem. We show that the efficient region of the Moser-Tardos algorithm indeed goes beyond the Shearer's bound of the underlying dependency graph, if the graph is not chordal. This â€œchordal conditionâ€ is sufficient and necessary, since it has been shown that Shearer's bound exactly characterizes the efficient region for chordal dependency graph (Kolipaka and Szegedy, STOC'11; He, Li, Liu, Wang and Xia, FOCS'17). Moreover, we demonstrate that the efficient region can exceed Shearer's bound by a constant amount by explicitly calculating the gaps on several infinite lattices.  
 The core of our proof is a new criterion on the efficiency of the Moser-Tardos algorithm which takes the intersection between dependent events into consideration. Our criterion is strictly larger than Shearer's bound whenever there exist two dependent events with non-empty intersection. Meanwhile, if any two dependent events are mutually exclusive, our criterion becomes the Shearer's bound, which is known to be tight in this situation for the Moser-Tardos algorithm (Kolipaka and Szegedy, STOC'11; Guo, Jerrum and Liu, JACM'19).  
 Our deterministic counting algorithm is a derandomization of the very recent fast sampling algorithm in [17]. It departs substantially from all previous deterministic counting Lovasz local lemma algorithms which relied on linear programming, and gives a deterministic approximate counting algorithm that straightforwardly derandomizes a fast sampling algorithm, hence unifying the fast sampling and deterministic approximate counting in the same algorithmic framework.  
 To obtain the improved regime, in our analysis we develop a refinement of the {2, 3}-trees that were used in the previous analyses of counting/sampling LLL. Similar techniques can be applied to the previous LP-based algorithms to obtain the same improved regime and may be of independent interests.  
  Full Access    
 Abstract 
  PDF 
  Abstract   Estimating the empirical distribution of a scalar-valued data set is a basic and fundamental task. In this paper, we tackle the problem of estimating an empirical distribution in a setting with two challenging features. First, the algorithm does not directly observe the data; instead, it only asks a limited number of threshold queries about each sample. Second, the data are not assumed to be independent and identically distributed; instead, we allow for an arbitrary process generating the samples, including an adaptive adversary. These considerations are relevant, for example, when modeling a seller experimenting with posted prices to estimate the distribution of consumers' willingness to pay for a product: offering a price and observing a consumer's purchase decision is equivalent to asking a single threshold query about their value, and the distribution of consumers' values may be non-stationary over time, as early adopters may differ markedly from late adopters.  
 Our main result quantifies, to within a constant factor, the sample complexity of estimating the empirical CDF of a sequence of elements of [ n  ], up to Îµ additive error, using one threshold query per sample. The complexity depends only logarithmically on n  , and our result can be interpreted as extending the existing logarithmic-complexity results for noisy binary search to the more challenging setting where noise is non-stochastic. Along the way to designing our algorithm, we consider a more general model in which the algorithm is allowed to make a limited number of simultaneous threshold queries on each sample. We solve this problem using Blackwell's Approachability Theorem and the exponential weights method. As a side result of independent interest, we characterize the minimum number of simultaneous threshold queries required by deterministic CDF estimation algorithms.  
 Abstract 
  PDF 
  Abstract   We introduce a new measure for the performance of online algorithms in Bayesian settings, where the input is drawn from a known prior, but the realizations are revealed one-by-one in an online fashion. Our new measure is called order-competitive ratio  . It is defined as the worst case (over all distribution sequences) ratio between the performance of the best order-unaware  and order-aware  algorithms, and quantifies the loss that is incurred due to lack of knowledge of the arrival order. Despite the growing interest in the role of the arrival order on the performance of online algorithms, this loss has been overlooked thus far.  
 We study the order-competitive ratio in the paradigmatic prophet inequality  problem, for the two common objective functions of (i) maximizing the expected value, and (ii) maximizing the probability of obtaining the largest value; and with respect to two families of algorithms, namely (i) adaptive algorithms, and (ii) single-threshold algorithms. We provide tight bounds for all four combinations, with respect to deterministic algorithms. Our analysis requires new ideas and departs from standard techniques. In particular, our adaptive algorithms inevitably go beyond single-threshold algorithms. The results with respect to the order-competitive ratio measure capture the intuition that adaptive algorithms are stronger than single-threshold ones, and may lead to a better algorithmic advice than the classical competitive ratio measure.  
  PDF 
  Abstract   In blockchains such as Bitcoin and Ethereum, users compete in a transaction fee auction to get their transactions confirmed in the next block. A line of recent works set forth the desiderata for a â€œdreamâ€ transaction fee mechanism (TFM), and explored whether such a mechanism existed. A dream TFM should satisfy 1) user incentive compatibility  (UIC), i.e., truthful bidding should be a user's dominant strategy; 2) miner incentive compatibility  (MIC), i.e., the miner's dominant strategy is to faithfully implement the prescribed mechanism; and 3) miner-user side contract proofness  (SCP), i.e., no coalition of the miner and one or more user(s) can increase their joint utility by deviating from the honest behavior. The weakest form of SCP is called 1-SCP, where we only aim to provide resilience against the collusion of the miner and a single  user. Sadly, despite the various attempts, to the best of knowledge, no existing mechanism can satisfy all three properties in all situations.  
 Since the TFM departs from classical mechanism design in modeling and assumptions, to date, our understanding of the design space is relatively little. In this paper, we further unravel the mathematical structure of transaction fee mechanism design by proving the following results:  
 â€¢ Can we have a dream TFM?  We prove a new impossibility result: assuming finite block size  , no singleÂ­parameter, non-trivial, possibly randomized TFM can simultaneously satisfy UIC and 1-SCP. Consequently, no non-trivial TFM can satisfy all three desired properties simultaneously. This answers an important open question raised by Roughgarden in his recent work.  
 â€¢ Rethinking the incentive compatibility notions  . We observe that the prevalently adopted incentive compatibility notions may be too draconian and somewhat flawed. We rectify the existing modeling techniques, and suggest a relaxed incentive compatibility notion that captures additional hidden costs of strategic deviation. We construct a new mechanism called the â€œburning second-price auctionâ€, and show that it indeed satisfies the new incentive compatibility notions. We additionally prove that the use of randomness is necessary under the new incentive compatibility notions for â€œusefulâ€ mechanisms that resist the coalitions of the miner and at least 2 users.  
 To do so, we introduce a constant-factor subspace embedding with the optimal m = O (d)  number of rows, and which can be applied in time  
 for any trade-off parameter Î± > 0, tightening a recent result by Chepurko et. al. [SODA 2022] that achieves an exp(poly(log log n  )) distortion with m  = d  Â· poly(log log d  ) rows in     
 time. Our subspace embedding uses a recently shown property of stacked  Subsampled Randomized Hadamard Transforms (SRHT), which actually increase the input dimension, to â€œspreadâ€ the mass of an input vector among a large number of coordinates, followed by random sampling. To control the effects of random sampling, we use fast semidefinite programming to reweight the rows. We then use our constant-factor subspace embedding to give the first optimal runtime algorithms for finding a maximal linearly independent subset of columns, regression, and leverage score sampling. To do so, we also introduce a novel subroutine that iteratively grows a set of independent rows, which may be of independent interest.  
 Abstract 
  PDF 
  Abstract   Matrix representations are a powerful tool for designing efficient algorithms for combinatorial optimization problems such as matching, and linear matroid intersection and parity. In this paper, we initiate the study of matrix representations using the concept of non-commutative rank (nc-rank), which has recently attracted attention in the research of Edmonds' problem. We reveal that the nc-rank of the matrix representation of linear matroid parity corresponds to the optimal value of fractional linear matroid parity: a half-integral relaxation of linear matroid parity. Based on our representation, we present an algebraic algorithm for the fractional linear matroid parity problem by building a new technique to incorporate the search-to-decision reduction into the half-integral problem represented via the nc-rank. We further present a faster divide-and- conquer algorithm for finding a maximum fractional matroid matching and an algebraic algorithm for finding a dual optimal solution. They together lead to an algebraic algorithm for the weighted fractional linear matroid parity problem. Our algorithms are significantly simpler and faster than the existing algorithms.  
 â€¢ Polynomial equivalence for orbits of ROFs: Given black-box access to f  and g  in the orbits of two unknown ROFs, check if f ~ g  . If yes, output an A âˆˆ GL  ( n  , ð”½) such that f = g(Ax)  .  
 These problems are significant generalizations of two well-studied problems in algebraic complexity, namely reconstruction of ROFs and quadratic form equivalence. In this work, we give the first randomized polynomial-time algorithms (with oracle access to quadratic form equivalence) to solve the two problems. The equivalence test works for general  ROFs; it also implies an efficient learning algorithm for random  arithmetic formulas of unbounded depth and fan-in (in the high number of variables setting). The algorithm for the second problem, which invokes the equivalence test, works for mildly restricted ROFs, namely additive-constant-free ROFs.  
 The equivalence test is based on a novel interplay between the factors and the essential variables of the Hessian determinant of an ROF, the essential variables of the ROF, and certain special structures in the ROF that we call â€œskewed pathsâ€. To our knowledge, the Hessian of a general ROF (or even a depth-4 ROF) has not been analyzed before. Analyzing the Hessian and combining the knowledge gained from it with the skewed paths to recursively discover formulas in the orbits of sub-ROFs of lower depth (without incurring an exponential blow-up due to unbounded depth) constitute the main technical contributions of this work.  
  Full Access    
  PDF 
  Abstract   The minimum-cost k  -edge-connected spanning subgraph ( k  -ECSS) problem is a generalization and strengthening of the well-studied minimum-cost spanning tree (MST) problem. While the round complexity of distributedly computing the latter has been well-understood, the former remains mostly open, especially as soon as k  â‰¥ 3.  
 In this paper, we present the first distributed algorithm that computes an approximation of k  -ECSS in sublinear time for general k  . Concretely, we describe a randomized distributed algorithm that, in  
 rounds, computes a k  -edge-connected spanning subgraph whose cost is within an O  (log n  log k  ) factor of optimal. Here, n  and D  denote the number of vertices and diameter of the graph, respectively. This time complexity is nearly optimal for any k  = poly(log n  ), almost matching an     
  PDF 
 In this paper we prove that Byzantine Agreement in the asynchronous, full information model can be solved with probability 1 against an adaptive adversary that can corrupt f  < n/3 parties, while incurring only polynomial latency with high probability  . Our protocol follows earlier polynomial latency protocols of King and Saia [KS16, KS18] and Huang, Pettie, and Zhu [HPZ22], which had suboptimal resilience  , namely f  â‰ˆ n  /10 9  [KS16, KS18] and f  < n  /4 [HPZ22], respectively.  
 Resilience f  = ( n  - 1)/3 is uniquely difficult as this is the point at which the influence of the Byzantine and honest players are of roughly equal strength. The core technical problem we solve is to design a collective coin-flipping protocol that eventually  lets us flip a coin with an unambiguous outcome. In the beginning the influence of the Byzantine players is too powerful to overcome and they can essentially fix the coin's behavior at will. We guarantee that after just a polynomial number of executions of the coin-flipping protocol, either (a) the Byzantine players fail to fix the behavior of the coin (thereby ending the game) or (b) we can â€œblacklistâ€ players such that the blacklisting rate for Byzantine players is at least as large as the blacklisting rate for good players. The blacklisting criterion is based on a simple statistical test of fraud detection  .  
 Abstract 
  PDF 
  Abstract   This paper presents a randomized parallel single-source shortest paths (SSSP) algorithm for directed graphs with non-negative integer edge weights that solves the problem exactly in O(m)  work and n  1/2+ o  (1)  span, with high probability. All previous exact SSSP algorithms with nearly linear work have linear span, even for undirected unweighted graphs. Our main technical contribution is to show a reduction from the exact SSSP to directed hopsets [6] using the iterative gradual rounding technique [9]. An ( h  , Îµ)-hopset is a set of weighted edges (sometimes called shortcuts) that when added to the graph admit h  -hop paths with weights no more than (1 + Îµ) times the true shortest path distances.  
 Furthermore, we show how to combine this algorithm with Forster and Nanongkai's framework [15] to improve the distributed exact SSSP algorithm. Specifically, we obtain an  
  Abstract   We consider the allocation of m  balls (jobs) into n  bins (servers). In the standard TWO-CHOICE process, at each step t  = 1, 2,â€¦, m  we first sample two  bins uniformly at random and place a ball in the least loaded bin. It is well-known that for any m  î¡ n  , this results in a gap (difference between the maximum and average load) of log 2  log n  + Î¸(1) (with high probability). In this work, we consider the MEMORY process [27] where instead of two choices, we only sample one bin per step but we have access to a cache which can store the location of one bin. Mitzenmacher, Prabhakar and Shah [23] showed that in the lightly loaded case ( m = n  ), the MEMORY process achieves a gap of ð’ª (log log n  ).  
 Extending the setting of Mitzenmacher et al. in two ways, we first allow the number of balls m  to be arbitrary, which includes the challenging heavily loaded case where m î¡ n  . Secondly, we follow the heterogeneous bins model of Wieder [30], where the sampling distribution of bins can be biased up to some arbitrary  multiplicative constant. Somewhat surprisingly, we prove that even in this setting, the MEMORY process still achieves an ð’ª(loglog n  ) gap bound. This is in stark contrast with the TWO-CHOICE (or any d  -CHOICE with d  = ð’ª(1)) process, where it is known that the gap diverges as m  â†’ âˆž [30]. Further, we show that for any sampling distribution independent of m  (but possibly dependent on n  ) the MEMORY process has a gap that can be bounded independently of m  . Finally, we prove a tight gap bound of ð’ª(log n  ) for MEMORY in another relaxed setting with heterogeneous (weighted) balls and a cache which can only be maintained for two steps.  
 *  The full version of the paper can be accessed at [20]  
  Full Access    
  PDF 
  Abstract   We give a near-linear time sampler for the Gibbs distribution of the ferromagnetic Ising models with edge activities Î² > 1 and external fields Î» < 1 (or symmetrically, Î» > 1) on general graphs with bounded or unbounded maximum degree.  
 Our algorithm is based on the field dynamics given in [13]. We prove the correctness and efficiency of our algorithm by establishing spectral independence of distribution of the random cluster model and the rapid mixing of Glauber dynamics on the random cluster model in a low-temperature regime, which may be of independent interest.  
 . The computational problem of finding a planted clique of size     
 In this paper we revisit the original Metropolis algorithm suggested by Jerrum. Interestingly, we find that the Metropolis algorithm actually fails  to recover a planted clique of size k  = Î˜( n  Î±  ) for any  constant Î± âˆˆ (0,1), unlike many other efficient algorithms that succeed when Î± > 1/2. Moreover, we strengthen Jerrum's results in a number of other ways including:  
 â€¢ We show that the simulated tempering version of the Metropolis algorithm, a more sophisticated temperature-exchange variant of it, also fails at the same regime of parameters.  
  PDF 
  Abstract   An important paradigm in the understanding of mixing times of Glauber dynamics for spin systems is the correspondence between spatial mixing properties of the models and bounds on the mixing time of the dynamics. This includes, in particular, the classical notions of weak and strong spatial mixing, which have been used to show the best known mixing time bounds in the high-temperature regime for the Glauber dynamics for the Ising and Potts models.  
 Glauber dynamics for the random-cluster model does not naturally fit into this spin systems framework because its transition rules are not local. In this paper, we present various implications between weak spatial mixing, strong spatial mixing, and the newer notion of spatial mixing within a phase, and mixing time bounds for the random-cluster dynamics in finite subsets of â„¤ d  for general d  î¡ 2. These imply a host of new results, including optimal O(N  log N  ) mixing for the random cluster dynamics on torii and boxes on N  vertices in â„¤ d  at all high temperatures and at sufficiently low temperatures, and for large values of q  quasi-polynomial (or quasi-linear when d  = 2) mixing time bounds from random phase initializations on torii at the critical point (where by contrast the mixing time from worst-case initializations is exponentially large). In the same parameter regimes, these results translate to fast sampling algorithms for the Potts model on â„¤ d  for general d  .  
 Abstract 
  PDF 
  Abstract   The first large-scale deployment of private federated learning uses differentially private counting in the continual release model  as a subroutine (Google AI blog titled â€œFederated Learning with Formal Differential Privacy Guaranteesâ€ on February 28, 2022). For this and several other applications, it is crucial to use a continual counting mechanism with small mean squared error  . In this case, a concrete (or non-asymptotic) bound on the error is very relevant to reduce the privacy parameter Îµ as much as possible, and hence, it is important to improve upon the constant factor in the error term. The standard mechanism for continual counting, and the one used in the above deployment, is the binary mechanism  . We present a novel mechanism and show that its mean squared error is both asymptotically optimal and a factor 10 smaller than the error of the binary mechanism. We also show that the constants in our analysis are almost tight by giving non-asymptotic lower and upper bounds that differ only in the constants of lower-order terms. Our mechanism also has the advantage of taking only constant time per release, while the binary mechanism takes O  (log n  ) time, where n  is the total number of released data values. Our algorithm is a matrix mechanism for the counting matrix  . We also use our explicit factorization of the counting matrix to give an upper bound on the excess risk of the matrix mechanism-based private learning algorithm of Denisov, McMahan, Rush, Smith, and Thakurta (NeurIPS 2022).  
 Our lower bound for any continual counting mechanism is the first tight lower bound on continual counting under (Îµ, Î´) -differential privacy and it holds against a non-adaptive adversary. It is achieved using a new lower bound on a certain factorization norm, denoted by Î³ f  (Â·), in terms of the singular values of the matrix. In particular, we show that for any complex matrix, A  âˆŠâ„‚ m Ã— n   ,  
 We show that the complexity of LCS with threshold d  smoothly interpolates between the two extreme cases up to n o   (1)  factors:  
 â€¢ LCS with threshold d  has a quantum algorithm in n  2/3 +o(1)  / d  1/6  query complexity and time complexity, and requires at least Î©( n  2/3  / d  1/6  ) quantum query complexity.  
 Our result improves upon previous upper bounds Ã•  (min{ n  / d  1/2  , n  2/3  }) (Le Gall and Seddighin ITCS 2022, Akmal and Jin SODA 2022), and answers an open question of Akmal and Jin.  
  PDF 
 In this paper, we make the first in 20 years improvement in n  for this problem by proposing a new compressed suffix array and a new compressed suffix tree which admit o  ( n  )-time construction algorithms while matching the space bounds and the query times of the original CSA/CST and the FM-index. More precisely, our structures take ð’ª (n  log Ïƒ) bits, support SA queries and full suffix tree functionality in ð’ª(log Îµ  n  ) time per operation, and can be constructed in  
 time using ð’ª (n  log Ïƒ) bits of working space. (For example, if Ïƒ = 2, the construction time is     
  PDF 
 In this paper, we give a significantly simplified analysis of the pseudorandom hash family by Chen et al  . Our analysis clearly identifies the key pseudorandom property required to fool the BCM algorithm, allowing us to explore the full potential of this construction. Based on our new analysis, we show the following.  
 â€¢ As a direct application of our technique, we show a more general pseudorandom property of the hash family, which we call the â€œc-connectingâ€ property. It might be of independent interest.  
